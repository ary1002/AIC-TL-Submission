{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport pickle\nfrom skimage import io\nimport matplotlib.pyplot as plt\nimport numpy \nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nimport argparse\nimport glob\nimport cv2\nimport torch.optim as optim\nimport matplotlib\nmatplotlib.use('Agg')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-31T08:17:55.004956Z","iopub.execute_input":"2024-03-31T08:17:55.005431Z","iopub.status.idle":"2024-03-31T08:17:55.890286Z","shell.execute_reply.started":"2024-03-31T08:17:55.005334Z","shell.execute_reply":"2024-03-31T08:17:55.889239Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Attention Modules (CAM,SAM,CBAM)","metadata":{}},{"cell_type":"code","source":"class ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=3):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass CBAM(nn.Module):\n    def __init__(self, in_planes):\n        super(CBAM, self).__init__()\n\n        self.ca = ChannelAttention(in_planes)\n        self.sa = SpatialAttention()\n        \n    def forward(self, x):\n        \n        out = x * (self.ca(x))\n        out = out * (self.sa(out))\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:17:55.891617Z","iopub.execute_input":"2024-03-31T08:17:55.892088Z","iopub.status.idle":"2024-03-31T08:17:55.907407Z","shell.execute_reply.started":"2024-03-31T08:17:55.892058Z","shell.execute_reply":"2024-03-31T08:17:55.906399Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Models (ResNet,MobileNet)","metadata":{}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n        \n        self.cbam = CBAM(planes)\n\n    def forward(self, x):\n        residual = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.cbam(out)\n        out += self.shortcut(residual)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n        \n        self.cbam = CBAM(self.expansion*planes)\n        \n    def forward(self, x):\n        residual = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out = self.cbam(out)\n        out += self.shortcut(residual)\n        out = F.relu(out)\n        return out\n\n\nclass ResNetCBAM(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10): \n        super(ResNetCBAM, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  \n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\ndef ResNetCBAM18():\n    return ResNetCBAM(BasicBlock, [2,2,2,2])\nclass MobileNetCBAM(nn.Module):\n    def __init__(self, classes = 100):\n        super(MobileNetCBAM, self).__init__()\n\n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n\n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                CBAM(oup),\n                nn.ReLU(inplace=True),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(  3,  32, 1), \n            conv_dw( 32,  64, 1),\n            conv_dw( 64, 128, 1),\n            conv_dw(128, 128, 1),\n            conv_dw(128, 256, 2),\n            conv_dw(256, 256, 1),\n            conv_dw(256, 512, 2),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 1024, 2),\n            conv_dw(1024, 1024, 1),\n            nn.AvgPool2d(4),\n        )\n        self.fc = nn.Linear(1024, classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:17:55.909044Z","iopub.execute_input":"2024-03-31T08:17:55.909446Z","iopub.status.idle":"2024-03-31T08:17:55.947856Z","shell.execute_reply.started":"2024-03-31T08:17:55.909407Z","shell.execute_reply":"2024-03-31T08:17:55.947035Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def get_network():\n    net=ResNetCBAM18().cuda()\n    #net=MobileNetCBAM().cuda()\n    \"\"\" return given network\n    \"\"\"\n    return net\n\n\ndef get_training_dataloader(batch_size=16, num_workers=2, shuffle=True):\n    \"\"\" return training dataloader\n    Args:\n        mean: mean of FashionMNIST training dataset\n        std: std of FashionMNIST training dataset\n        path: path to FashionMNIST training python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle \n    Returns: train_data_loader:torch dataloader object\n    \"\"\"\n\n    transform_train = transforms.Compose([\n        #transforms.ToPILImage(),\n        transforms.RandomCrop(28, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ToTensor()\n    ])\n    #FashionMNIST_training = FashionMNISTTrain(path, transform=transform_train)\n    FashionMNIST_training = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_train)\n    FashionMNIST_training_loader = DataLoader(\n        FashionMNIST_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return FashionMNIST_training_loader\n\ndef get_test_dataloader(batch_size=16, num_workers=2, shuffle=True):\n    \"\"\" return training dataloader\n    Args:\n        mean: mean of FashionMNIST test dataset\n        std: std of FashionMNIST test dataset\n        path: path to FashionMNIST test python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle \n    Returns: FashionMNIST_test_loader:torch dataloader object\n    \"\"\"\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor()\n    ])\n    #FashionMNIST_test = FashionMNISTTest(path, transform=transform_test)\n    FashionMNIST_test = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n    FashionMNIST_test_loader = DataLoader(\n        FashionMNIST_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return FashionMNIST_test_loader\n\nclass WarmUpLR(_LRScheduler):\n    \"\"\"warmup_training learning rate scheduler\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        total_iters: totoal_iters of warmup phase\n    \"\"\"\n    def __init__(self, optimizer, total_iters, last_epoch=-1):\n        \n        self.total_iters = total_iters\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"we will use the first m batches, and set the learning\n        rate to base_lr * m / total_iters\n        \"\"\"\n        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:17:58.385923Z","iopub.execute_input":"2024-03-31T08:17:58.386850Z","iopub.status.idle":"2024-03-31T08:17:58.401505Z","shell.execute_reply.started":"2024-03-31T08:17:58.386811Z","shell.execute_reply":"2024-03-31T08:17:58.400446Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nCIFAR10_TRAIN_MEAN = (0.49139968, 0.48215827 ,0.44653124)\nCIFAR10_TRAIN_STD = (0.24703233, 0.24348505, 0.26158768)\n\nCIFAR100_TRAIN_MEAN = (0.5088964127604166, 0.48739301317401956, 0.44194221124387256)\nCIFAR100_TRAIN_STD = (0.2682515741720801, 0.2573637364478126, 0.2770957707973042)\n\n#directory to save weights file\nCHECKPOINT_PATH = 'checkpoint'\n\n#total training epoches\nEPOCH = 5\nMILESTONES = [6, 12, 16]\n\n#initial learning rate\n#INIT_LR = 0.1\n\n#time of we run the script\nTIME_NOW = datetime.now().isoformat()\n\n#tensorboard log dir\nLOG_DIR = 'runs'\n\n#save weights file per SAVE_EPOCH epoch\nSAVE_EPOCH = 10","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:17:58.403261Z","iopub.execute_input":"2024-03-31T08:17:58.403572Z","iopub.status.idle":"2024-03-31T08:17:58.417334Z","shell.execute_reply.started":"2024-03-31T08:17:58.403534Z","shell.execute_reply":"2024-03-31T08:17:58.416493Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"def train(epoch):\n\n    net.train()\n    for batch_index, (images, labels) in enumerate(FashionMNIST_training_loader):\n        if epoch <= 1:\n            warmup_scheduler.step()\n\n        images = Variable(images)\n        labels = Variable(labels)\n\n        labels = labels.cuda()\n        images = images.cuda()\n\n        optimizer.zero_grad()\n        outputs = net(images)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n            loss.item(),\n            optimizer.param_groups[0]['lr'],\n            epoch=epoch,\n            trained_samples=batch_index * 128 + len(images),\n            total_samples=len(FashionMNIST_training_loader.dataset)\n        ))\n\n\n    for name, param in net.named_parameters():\n        layer, attr = os.path.splitext(name)\n        attr = attr[1:]\n\ndef eval_training(epoch):\n    net.eval()\n\n    test_loss = 0.0 # cost function error\n    correct = 0.0\n\n    for (images, labels) in FashionMNIST_test_loader:\n        images = Variable(images)\n        labels = Variable(labels)\n\n        images = images.cuda()\n        labels = labels.cuda()\n\n        outputs = net(images)\n        loss = loss_function(outputs, labels)\n        test_loss += loss.item()\n        _, preds = outputs.max(1)\n        correct += preds.eq(labels).sum()\n\n    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n        test_loss / len(FashionMNIST_test_loader.dataset),\n        correct.float() / len(FashionMNIST_test_loader.dataset)\n    ))\n    print()\n\n\n    return correct.float() / len(FashionMNIST_test_loader.dataset)\n\nif __name__ == '__main__':\n    \n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('-net', type=str, required=True, help='net type')\n#     parser.add_argument('-gpu', type=bool, default=True, help='use gpu or not')\n#     parser.add_argument('-w', type=int, default=2, help='number of workers for dataloader')\n#     parser.add_argument('-b', type=int, default=128, help='batch size for dataloader')\n#     parser.add_argument('-s', type=bool, default=True, help='whether shuffle the dataset')\n#     parser.add_argument('-warm', type=int, default=1, help='warm up training phase')\n#     parser.add_argument('-lr', type=float, default=0.1, help='initial learning rate')\n#     args = parser.parse_args()\n\n    net= get_network()\n    \n        \n    #data preprocessing:\n    FashionMNIST_training_loader = get_training_dataloader(\n        num_workers=2,\n        batch_size=128,\n        shuffle=True\n    )\n    \n    FashionMNIST_test_loader = get_test_dataloader(\n        num_workers=2,\n        batch_size=128,\n        shuffle=True\n    )\n    \n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=0.15) #learning rate decay\n    iter_per_epoch = len(FashionMNIST_training_loader)\n    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * 1)\n    checkpoint_path = os.path.join(CHECKPOINT_PATH, \"resnetcbam18\")\n\n    #create checkpoint folder to save model\n    if not os.path.exists(checkpoint_path):\n        os.makedirs(checkpoint_path)\n    checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n\n    best_acc = 0.0\n    for epoch in range(1, EPOCH):\n        if epoch > 1:\n            train_scheduler.step(epoch)\n\n        train(epoch)\n        acc = eval_training(epoch)\n\n        #start to save best performance model after learning rate decay to 0.01 \n        if epoch > MILESTONES[1] and best_acc < acc:\n            torch.save(net.state_dict(), checkpoint_path.format(net=\"resnetcbam18\", epoch=epoch, type='best'))\n            best_acc = acc\n            continue\n\n        if not epoch % SAVE_EPOCH:\n            torch.save(net.state_dict(), checkpoint_path.format(net=\"resnetcbam18\", epoch=epoch, type='regular'))\n    print()\n    print(\"best_acc: \", best_acc)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:17:58.418658Z","iopub.execute_input":"2024-03-31T08:17:58.419032Z","iopub.status.idle":"2024-03-31T08:22:13.988160Z","shell.execute_reply.started":"2024-03-31T08:17:58.418980Z","shell.execute_reply":"2024-03-31T08:22:13.986855Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Training Epoch: 1 [128/60000]\tLoss: 2.3260\tLR: 0.000213\nTraining Epoch: 1 [256/60000]\tLoss: 2.3881\tLR: 0.000426\nTraining Epoch: 1 [384/60000]\tLoss: 2.3795\tLR: 0.000640\nTraining Epoch: 1 [512/60000]\tLoss: 2.3448\tLR: 0.000853\nTraining Epoch: 1 [640/60000]\tLoss: 2.3361\tLR: 0.001066\nTraining Epoch: 1 [768/60000]\tLoss: 2.3294\tLR: 0.001279\nTraining Epoch: 1 [896/60000]\tLoss: 2.3114\tLR: 0.001493\nTraining Epoch: 1 [1024/60000]\tLoss: 2.3055\tLR: 0.001706\nTraining Epoch: 1 [1152/60000]\tLoss: 2.2801\tLR: 0.001919\nTraining Epoch: 1 [1280/60000]\tLoss: 2.2808\tLR: 0.002132\nTraining Epoch: 1 [1408/60000]\tLoss: 2.2555\tLR: 0.002345\nTraining Epoch: 1 [1536/60000]\tLoss: 2.2788\tLR: 0.002559\nTraining Epoch: 1 [1664/60000]\tLoss: 2.2523\tLR: 0.002772\nTraining Epoch: 1 [1792/60000]\tLoss: 2.2425\tLR: 0.002985\nTraining Epoch: 1 [1920/60000]\tLoss: 2.2072\tLR: 0.003198\nTraining Epoch: 1 [2048/60000]\tLoss: 2.2225\tLR: 0.003412\nTraining Epoch: 1 [2176/60000]\tLoss: 2.1928\tLR: 0.003625\nTraining Epoch: 1 [2304/60000]\tLoss: 2.1978\tLR: 0.003838\nTraining Epoch: 1 [2432/60000]\tLoss: 2.1470\tLR: 0.004051\nTraining Epoch: 1 [2560/60000]\tLoss: 2.1656\tLR: 0.004264\nTraining Epoch: 1 [2688/60000]\tLoss: 2.1086\tLR: 0.004478\nTraining Epoch: 1 [2816/60000]\tLoss: 2.0887\tLR: 0.004691\nTraining Epoch: 1 [2944/60000]\tLoss: 2.1065\tLR: 0.004904\nTraining Epoch: 1 [3072/60000]\tLoss: 2.1087\tLR: 0.005117\nTraining Epoch: 1 [3200/60000]\tLoss: 2.0261\tLR: 0.005330\nTraining Epoch: 1 [3328/60000]\tLoss: 1.9785\tLR: 0.005544\nTraining Epoch: 1 [3456/60000]\tLoss: 1.9641\tLR: 0.005757\nTraining Epoch: 1 [3584/60000]\tLoss: 1.9684\tLR: 0.005970\nTraining Epoch: 1 [3712/60000]\tLoss: 1.9266\tLR: 0.006183\nTraining Epoch: 1 [3840/60000]\tLoss: 1.8904\tLR: 0.006397\nTraining Epoch: 1 [3968/60000]\tLoss: 1.8600\tLR: 0.006610\nTraining Epoch: 1 [4096/60000]\tLoss: 1.8433\tLR: 0.006823\nTraining Epoch: 1 [4224/60000]\tLoss: 1.7833\tLR: 0.007036\nTraining Epoch: 1 [4352/60000]\tLoss: 1.7644\tLR: 0.007249\nTraining Epoch: 1 [4480/60000]\tLoss: 1.7324\tLR: 0.007463\nTraining Epoch: 1 [4608/60000]\tLoss: 1.5964\tLR: 0.007676\nTraining Epoch: 1 [4736/60000]\tLoss: 1.6997\tLR: 0.007889\nTraining Epoch: 1 [4864/60000]\tLoss: 1.6341\tLR: 0.008102\nTraining Epoch: 1 [4992/60000]\tLoss: 1.6995\tLR: 0.008316\nTraining Epoch: 1 [5120/60000]\tLoss: 1.5180\tLR: 0.008529\nTraining Epoch: 1 [5248/60000]\tLoss: 1.5740\tLR: 0.008742\nTraining Epoch: 1 [5376/60000]\tLoss: 1.5145\tLR: 0.008955\nTraining Epoch: 1 [5504/60000]\tLoss: 1.4118\tLR: 0.009168\nTraining Epoch: 1 [5632/60000]\tLoss: 1.4700\tLR: 0.009382\nTraining Epoch: 1 [5760/60000]\tLoss: 1.2687\tLR: 0.009595\nTraining Epoch: 1 [5888/60000]\tLoss: 1.3377\tLR: 0.009808\nTraining Epoch: 1 [6016/60000]\tLoss: 1.2829\tLR: 0.010021\nTraining Epoch: 1 [6144/60000]\tLoss: 1.2860\tLR: 0.010235\nTraining Epoch: 1 [6272/60000]\tLoss: 1.2220\tLR: 0.010448\nTraining Epoch: 1 [6400/60000]\tLoss: 1.2399\tLR: 0.010661\nTraining Epoch: 1 [6528/60000]\tLoss: 1.1730\tLR: 0.010874\nTraining Epoch: 1 [6656/60000]\tLoss: 1.1427\tLR: 0.011087\nTraining Epoch: 1 [6784/60000]\tLoss: 1.1092\tLR: 0.011301\nTraining Epoch: 1 [6912/60000]\tLoss: 1.1461\tLR: 0.011514\nTraining Epoch: 1 [7040/60000]\tLoss: 1.2300\tLR: 0.011727\nTraining Epoch: 1 [7168/60000]\tLoss: 1.0344\tLR: 0.011940\nTraining Epoch: 1 [7296/60000]\tLoss: 1.1764\tLR: 0.012154\nTraining Epoch: 1 [7424/60000]\tLoss: 1.2278\tLR: 0.012367\nTraining Epoch: 1 [7552/60000]\tLoss: 1.0838\tLR: 0.012580\nTraining Epoch: 1 [7680/60000]\tLoss: 1.2662\tLR: 0.012793\nTraining Epoch: 1 [7808/60000]\tLoss: 1.2115\tLR: 0.013006\nTraining Epoch: 1 [7936/60000]\tLoss: 1.0964\tLR: 0.013220\nTraining Epoch: 1 [8064/60000]\tLoss: 1.0247\tLR: 0.013433\nTraining Epoch: 1 [8192/60000]\tLoss: 1.0831\tLR: 0.013646\nTraining Epoch: 1 [8320/60000]\tLoss: 1.0082\tLR: 0.013859\nTraining Epoch: 1 [8448/60000]\tLoss: 0.8833\tLR: 0.014072\nTraining Epoch: 1 [8576/60000]\tLoss: 1.0751\tLR: 0.014286\nTraining Epoch: 1 [8704/60000]\tLoss: 0.9495\tLR: 0.014499\nTraining Epoch: 1 [8832/60000]\tLoss: 1.0172\tLR: 0.014712\nTraining Epoch: 1 [8960/60000]\tLoss: 1.0803\tLR: 0.014925\nTraining Epoch: 1 [9088/60000]\tLoss: 1.0507\tLR: 0.015139\nTraining Epoch: 1 [9216/60000]\tLoss: 0.9231\tLR: 0.015352\nTraining Epoch: 1 [9344/60000]\tLoss: 1.0594\tLR: 0.015565\nTraining Epoch: 1 [9472/60000]\tLoss: 0.8715\tLR: 0.015778\nTraining Epoch: 1 [9600/60000]\tLoss: 1.0405\tLR: 0.015991\nTraining Epoch: 1 [9728/60000]\tLoss: 0.9043\tLR: 0.016205\nTraining Epoch: 1 [9856/60000]\tLoss: 0.7633\tLR: 0.016418\nTraining Epoch: 1 [9984/60000]\tLoss: 0.9263\tLR: 0.016631\nTraining Epoch: 1 [10112/60000]\tLoss: 0.8268\tLR: 0.016844\nTraining Epoch: 1 [10240/60000]\tLoss: 0.8519\tLR: 0.017058\nTraining Epoch: 1 [10368/60000]\tLoss: 0.8746\tLR: 0.017271\nTraining Epoch: 1 [10496/60000]\tLoss: 0.9003\tLR: 0.017484\nTraining Epoch: 1 [10624/60000]\tLoss: 0.9523\tLR: 0.017697\nTraining Epoch: 1 [10752/60000]\tLoss: 0.8072\tLR: 0.017910\nTraining Epoch: 1 [10880/60000]\tLoss: 0.8176\tLR: 0.018124\nTraining Epoch: 1 [11008/60000]\tLoss: 0.9029\tLR: 0.018337\nTraining Epoch: 1 [11136/60000]\tLoss: 0.9905\tLR: 0.018550\nTraining Epoch: 1 [11264/60000]\tLoss: 0.7617\tLR: 0.018763\nTraining Epoch: 1 [11392/60000]\tLoss: 0.8443\tLR: 0.018977\nTraining Epoch: 1 [11520/60000]\tLoss: 0.7213\tLR: 0.019190\nTraining Epoch: 1 [11648/60000]\tLoss: 0.8129\tLR: 0.019403\nTraining Epoch: 1 [11776/60000]\tLoss: 0.7591\tLR: 0.019616\nTraining Epoch: 1 [11904/60000]\tLoss: 0.8683\tLR: 0.019829\nTraining Epoch: 1 [12032/60000]\tLoss: 0.9328\tLR: 0.020043\nTraining Epoch: 1 [12160/60000]\tLoss: 0.8343\tLR: 0.020256\nTraining Epoch: 1 [12288/60000]\tLoss: 0.8670\tLR: 0.020469\nTraining Epoch: 1 [12416/60000]\tLoss: 0.9062\tLR: 0.020682\nTraining Epoch: 1 [12544/60000]\tLoss: 0.8632\tLR: 0.020896\nTraining Epoch: 1 [12672/60000]\tLoss: 0.8404\tLR: 0.021109\nTraining Epoch: 1 [12800/60000]\tLoss: 0.8448\tLR: 0.021322\nTraining Epoch: 1 [12928/60000]\tLoss: 0.9053\tLR: 0.021535\nTraining Epoch: 1 [13056/60000]\tLoss: 0.7813\tLR: 0.021748\nTraining Epoch: 1 [13184/60000]\tLoss: 0.8617\tLR: 0.021962\nTraining Epoch: 1 [13312/60000]\tLoss: 0.7568\tLR: 0.022175\nTraining Epoch: 1 [13440/60000]\tLoss: 0.9215\tLR: 0.022388\nTraining Epoch: 1 [13568/60000]\tLoss: 0.7022\tLR: 0.022601\nTraining Epoch: 1 [13696/60000]\tLoss: 0.8000\tLR: 0.022814\nTraining Epoch: 1 [13824/60000]\tLoss: 0.8284\tLR: 0.023028\nTraining Epoch: 1 [13952/60000]\tLoss: 0.7095\tLR: 0.023241\nTraining Epoch: 1 [14080/60000]\tLoss: 0.7238\tLR: 0.023454\nTraining Epoch: 1 [14208/60000]\tLoss: 0.8387\tLR: 0.023667\nTraining Epoch: 1 [14336/60000]\tLoss: 0.8206\tLR: 0.023881\nTraining Epoch: 1 [14464/60000]\tLoss: 0.7028\tLR: 0.024094\nTraining Epoch: 1 [14592/60000]\tLoss: 0.8460\tLR: 0.024307\nTraining Epoch: 1 [14720/60000]\tLoss: 0.6438\tLR: 0.024520\nTraining Epoch: 1 [14848/60000]\tLoss: 0.7795\tLR: 0.024733\nTraining Epoch: 1 [14976/60000]\tLoss: 0.7422\tLR: 0.024947\nTraining Epoch: 1 [15104/60000]\tLoss: 0.6848\tLR: 0.025160\nTraining Epoch: 1 [15232/60000]\tLoss: 0.7721\tLR: 0.025373\nTraining Epoch: 1 [15360/60000]\tLoss: 0.6170\tLR: 0.025586\nTraining Epoch: 1 [15488/60000]\tLoss: 0.6932\tLR: 0.025800\nTraining Epoch: 1 [15616/60000]\tLoss: 0.6710\tLR: 0.026013\nTraining Epoch: 1 [15744/60000]\tLoss: 0.7047\tLR: 0.026226\nTraining Epoch: 1 [15872/60000]\tLoss: 0.7153\tLR: 0.026439\nTraining Epoch: 1 [16000/60000]\tLoss: 0.6208\tLR: 0.026652\nTraining Epoch: 1 [16128/60000]\tLoss: 0.8253\tLR: 0.026866\nTraining Epoch: 1 [16256/60000]\tLoss: 0.8639\tLR: 0.027079\nTraining Epoch: 1 [16384/60000]\tLoss: 0.6795\tLR: 0.027292\nTraining Epoch: 1 [16512/60000]\tLoss: 0.6523\tLR: 0.027505\nTraining Epoch: 1 [16640/60000]\tLoss: 0.7114\tLR: 0.027719\nTraining Epoch: 1 [16768/60000]\tLoss: 0.6577\tLR: 0.027932\nTraining Epoch: 1 [16896/60000]\tLoss: 0.5686\tLR: 0.028145\nTraining Epoch: 1 [17024/60000]\tLoss: 0.7178\tLR: 0.028358\nTraining Epoch: 1 [17152/60000]\tLoss: 0.7481\tLR: 0.028571\nTraining Epoch: 1 [17280/60000]\tLoss: 0.9099\tLR: 0.028785\nTraining Epoch: 1 [17408/60000]\tLoss: 0.6113\tLR: 0.028998\nTraining Epoch: 1 [17536/60000]\tLoss: 0.7653\tLR: 0.029211\nTraining Epoch: 1 [17664/60000]\tLoss: 0.6351\tLR: 0.029424\nTraining Epoch: 1 [17792/60000]\tLoss: 0.8235\tLR: 0.029638\nTraining Epoch: 1 [17920/60000]\tLoss: 0.8995\tLR: 0.029851\nTraining Epoch: 1 [18048/60000]\tLoss: 0.6774\tLR: 0.030064\nTraining Epoch: 1 [18176/60000]\tLoss: 0.6714\tLR: 0.030277\nTraining Epoch: 1 [18304/60000]\tLoss: 0.6772\tLR: 0.030490\nTraining Epoch: 1 [18432/60000]\tLoss: 0.8966\tLR: 0.030704\nTraining Epoch: 1 [18560/60000]\tLoss: 0.9568\tLR: 0.030917\nTraining Epoch: 1 [18688/60000]\tLoss: 0.8974\tLR: 0.031130\nTraining Epoch: 1 [18816/60000]\tLoss: 0.7923\tLR: 0.031343\nTraining Epoch: 1 [18944/60000]\tLoss: 0.6699\tLR: 0.031557\nTraining Epoch: 1 [19072/60000]\tLoss: 0.7520\tLR: 0.031770\nTraining Epoch: 1 [19200/60000]\tLoss: 0.8053\tLR: 0.031983\nTraining Epoch: 1 [19328/60000]\tLoss: 0.7444\tLR: 0.032196\nTraining Epoch: 1 [19456/60000]\tLoss: 0.7605\tLR: 0.032409\nTraining Epoch: 1 [19584/60000]\tLoss: 0.7081\tLR: 0.032623\nTraining Epoch: 1 [19712/60000]\tLoss: 0.7776\tLR: 0.032836\nTraining Epoch: 1 [19840/60000]\tLoss: 0.7853\tLR: 0.033049\nTraining Epoch: 1 [19968/60000]\tLoss: 0.7451\tLR: 0.033262\nTraining Epoch: 1 [20096/60000]\tLoss: 0.8016\tLR: 0.033475\nTraining Epoch: 1 [20224/60000]\tLoss: 0.8030\tLR: 0.033689\nTraining Epoch: 1 [20352/60000]\tLoss: 0.7221\tLR: 0.033902\nTraining Epoch: 1 [20480/60000]\tLoss: 0.7516\tLR: 0.034115\nTraining Epoch: 1 [20608/60000]\tLoss: 0.6587\tLR: 0.034328\nTraining Epoch: 1 [20736/60000]\tLoss: 0.7263\tLR: 0.034542\nTraining Epoch: 1 [20864/60000]\tLoss: 0.7789\tLR: 0.034755\nTraining Epoch: 1 [20992/60000]\tLoss: 0.6788\tLR: 0.034968\nTraining Epoch: 1 [21120/60000]\tLoss: 0.7096\tLR: 0.035181\nTraining Epoch: 1 [21248/60000]\tLoss: 0.8062\tLR: 0.035394\nTraining Epoch: 1 [21376/60000]\tLoss: 0.6750\tLR: 0.035608\nTraining Epoch: 1 [21504/60000]\tLoss: 0.9349\tLR: 0.035821\nTraining Epoch: 1 [21632/60000]\tLoss: 0.7083\tLR: 0.036034\nTraining Epoch: 1 [21760/60000]\tLoss: 0.7727\tLR: 0.036247\nTraining Epoch: 1 [21888/60000]\tLoss: 0.6375\tLR: 0.036461\nTraining Epoch: 1 [22016/60000]\tLoss: 0.7424\tLR: 0.036674\nTraining Epoch: 1 [22144/60000]\tLoss: 0.7158\tLR: 0.036887\nTraining Epoch: 1 [22272/60000]\tLoss: 0.7915\tLR: 0.037100\nTraining Epoch: 1 [22400/60000]\tLoss: 0.7768\tLR: 0.037313\nTraining Epoch: 1 [22528/60000]\tLoss: 0.7930\tLR: 0.037527\nTraining Epoch: 1 [22656/60000]\tLoss: 0.8112\tLR: 0.037740\nTraining Epoch: 1 [22784/60000]\tLoss: 0.8312\tLR: 0.037953\nTraining Epoch: 1 [22912/60000]\tLoss: 0.7815\tLR: 0.038166\nTraining Epoch: 1 [23040/60000]\tLoss: 0.6689\tLR: 0.038380\nTraining Epoch: 1 [23168/60000]\tLoss: 0.6124\tLR: 0.038593\nTraining Epoch: 1 [23296/60000]\tLoss: 0.7286\tLR: 0.038806\nTraining Epoch: 1 [23424/60000]\tLoss: 0.7835\tLR: 0.039019\nTraining Epoch: 1 [23552/60000]\tLoss: 0.6686\tLR: 0.039232\nTraining Epoch: 1 [23680/60000]\tLoss: 0.7819\tLR: 0.039446\nTraining Epoch: 1 [23808/60000]\tLoss: 0.7453\tLR: 0.039659\nTraining Epoch: 1 [23936/60000]\tLoss: 0.8112\tLR: 0.039872\nTraining Epoch: 1 [24064/60000]\tLoss: 0.7942\tLR: 0.040085\nTraining Epoch: 1 [24192/60000]\tLoss: 0.7338\tLR: 0.040299\nTraining Epoch: 1 [24320/60000]\tLoss: 0.6824\tLR: 0.040512\nTraining Epoch: 1 [24448/60000]\tLoss: 0.6685\tLR: 0.040725\nTraining Epoch: 1 [24576/60000]\tLoss: 0.6356\tLR: 0.040938\nTraining Epoch: 1 [24704/60000]\tLoss: 0.7398\tLR: 0.041151\nTraining Epoch: 1 [24832/60000]\tLoss: 0.5843\tLR: 0.041365\nTraining Epoch: 1 [24960/60000]\tLoss: 0.6005\tLR: 0.041578\nTraining Epoch: 1 [25088/60000]\tLoss: 0.7004\tLR: 0.041791\nTraining Epoch: 1 [25216/60000]\tLoss: 0.6431\tLR: 0.042004\nTraining Epoch: 1 [25344/60000]\tLoss: 0.6579\tLR: 0.042217\nTraining Epoch: 1 [25472/60000]\tLoss: 0.6035\tLR: 0.042431\nTraining Epoch: 1 [25600/60000]\tLoss: 0.8090\tLR: 0.042644\nTraining Epoch: 1 [25728/60000]\tLoss: 0.6639\tLR: 0.042857\nTraining Epoch: 1 [25856/60000]\tLoss: 0.6513\tLR: 0.043070\nTraining Epoch: 1 [25984/60000]\tLoss: 0.7496\tLR: 0.043284\nTraining Epoch: 1 [26112/60000]\tLoss: 0.6637\tLR: 0.043497\nTraining Epoch: 1 [26240/60000]\tLoss: 0.8968\tLR: 0.043710\nTraining Epoch: 1 [26368/60000]\tLoss: 0.5351\tLR: 0.043923\nTraining Epoch: 1 [26496/60000]\tLoss: 0.5367\tLR: 0.044136\nTraining Epoch: 1 [26624/60000]\tLoss: 0.8504\tLR: 0.044350\nTraining Epoch: 1 [26752/60000]\tLoss: 0.6651\tLR: 0.044563\nTraining Epoch: 1 [26880/60000]\tLoss: 0.6768\tLR: 0.044776\nTraining Epoch: 1 [27008/60000]\tLoss: 0.8772\tLR: 0.044989\nTraining Epoch: 1 [27136/60000]\tLoss: 0.6237\tLR: 0.045203\nTraining Epoch: 1 [27264/60000]\tLoss: 0.5468\tLR: 0.045416\nTraining Epoch: 1 [27392/60000]\tLoss: 0.6496\tLR: 0.045629\nTraining Epoch: 1 [27520/60000]\tLoss: 0.4992\tLR: 0.045842\nTraining Epoch: 1 [27648/60000]\tLoss: 0.6497\tLR: 0.046055\nTraining Epoch: 1 [27776/60000]\tLoss: 0.7192\tLR: 0.046269\nTraining Epoch: 1 [27904/60000]\tLoss: 0.4484\tLR: 0.046482\nTraining Epoch: 1 [28032/60000]\tLoss: 0.6253\tLR: 0.046695\nTraining Epoch: 1 [28160/60000]\tLoss: 0.5390\tLR: 0.046908\nTraining Epoch: 1 [28288/60000]\tLoss: 0.6372\tLR: 0.047122\nTraining Epoch: 1 [28416/60000]\tLoss: 0.6601\tLR: 0.047335\nTraining Epoch: 1 [28544/60000]\tLoss: 0.5458\tLR: 0.047548\nTraining Epoch: 1 [28672/60000]\tLoss: 0.6950\tLR: 0.047761\nTraining Epoch: 1 [28800/60000]\tLoss: 0.6608\tLR: 0.047974\nTraining Epoch: 1 [28928/60000]\tLoss: 0.7894\tLR: 0.048188\nTraining Epoch: 1 [29056/60000]\tLoss: 0.7520\tLR: 0.048401\nTraining Epoch: 1 [29184/60000]\tLoss: 0.5983\tLR: 0.048614\nTraining Epoch: 1 [29312/60000]\tLoss: 0.7394\tLR: 0.048827\nTraining Epoch: 1 [29440/60000]\tLoss: 0.6315\tLR: 0.049041\nTraining Epoch: 1 [29568/60000]\tLoss: 0.6176\tLR: 0.049254\nTraining Epoch: 1 [29696/60000]\tLoss: 0.6632\tLR: 0.049467\nTraining Epoch: 1 [29824/60000]\tLoss: 0.6781\tLR: 0.049680\nTraining Epoch: 1 [29952/60000]\tLoss: 0.7232\tLR: 0.049893\nTraining Epoch: 1 [30080/60000]\tLoss: 0.6237\tLR: 0.050107\nTraining Epoch: 1 [30208/60000]\tLoss: 0.5773\tLR: 0.050320\nTraining Epoch: 1 [30336/60000]\tLoss: 0.6214\tLR: 0.050533\nTraining Epoch: 1 [30464/60000]\tLoss: 0.5998\tLR: 0.050746\nTraining Epoch: 1 [30592/60000]\tLoss: 0.5863\tLR: 0.050959\nTraining Epoch: 1 [30720/60000]\tLoss: 0.6801\tLR: 0.051173\nTraining Epoch: 1 [30848/60000]\tLoss: 0.8446\tLR: 0.051386\nTraining Epoch: 1 [30976/60000]\tLoss: 0.5667\tLR: 0.051599\nTraining Epoch: 1 [31104/60000]\tLoss: 0.5222\tLR: 0.051812\nTraining Epoch: 1 [31232/60000]\tLoss: 0.5863\tLR: 0.052026\nTraining Epoch: 1 [31360/60000]\tLoss: 0.6485\tLR: 0.052239\nTraining Epoch: 1 [31488/60000]\tLoss: 0.7384\tLR: 0.052452\nTraining Epoch: 1 [31616/60000]\tLoss: 0.5209\tLR: 0.052665\nTraining Epoch: 1 [31744/60000]\tLoss: 0.5454\tLR: 0.052878\nTraining Epoch: 1 [31872/60000]\tLoss: 0.6072\tLR: 0.053092\nTraining Epoch: 1 [32000/60000]\tLoss: 0.5341\tLR: 0.053305\nTraining Epoch: 1 [32128/60000]\tLoss: 0.5959\tLR: 0.053518\nTraining Epoch: 1 [32256/60000]\tLoss: 0.5586\tLR: 0.053731\nTraining Epoch: 1 [32384/60000]\tLoss: 0.6304\tLR: 0.053945\nTraining Epoch: 1 [32512/60000]\tLoss: 0.7002\tLR: 0.054158\nTraining Epoch: 1 [32640/60000]\tLoss: 0.4012\tLR: 0.054371\nTraining Epoch: 1 [32768/60000]\tLoss: 0.5871\tLR: 0.054584\nTraining Epoch: 1 [32896/60000]\tLoss: 0.6549\tLR: 0.054797\nTraining Epoch: 1 [33024/60000]\tLoss: 0.6452\tLR: 0.055011\nTraining Epoch: 1 [33152/60000]\tLoss: 0.6061\tLR: 0.055224\nTraining Epoch: 1 [33280/60000]\tLoss: 0.7219\tLR: 0.055437\nTraining Epoch: 1 [33408/60000]\tLoss: 0.9385\tLR: 0.055650\nTraining Epoch: 1 [33536/60000]\tLoss: 0.6579\tLR: 0.055864\nTraining Epoch: 1 [33664/60000]\tLoss: 0.6581\tLR: 0.056077\nTraining Epoch: 1 [33792/60000]\tLoss: 0.5332\tLR: 0.056290\nTraining Epoch: 1 [33920/60000]\tLoss: 0.6581\tLR: 0.056503\nTraining Epoch: 1 [34048/60000]\tLoss: 0.4551\tLR: 0.056716\nTraining Epoch: 1 [34176/60000]\tLoss: 0.4847\tLR: 0.056930\nTraining Epoch: 1 [34304/60000]\tLoss: 0.5816\tLR: 0.057143\nTraining Epoch: 1 [34432/60000]\tLoss: 0.6293\tLR: 0.057356\nTraining Epoch: 1 [34560/60000]\tLoss: 0.5181\tLR: 0.057569\nTraining Epoch: 1 [34688/60000]\tLoss: 0.7729\tLR: 0.057783\nTraining Epoch: 1 [34816/60000]\tLoss: 0.6245\tLR: 0.057996\nTraining Epoch: 1 [34944/60000]\tLoss: 0.7078\tLR: 0.058209\nTraining Epoch: 1 [35072/60000]\tLoss: 0.6279\tLR: 0.058422\nTraining Epoch: 1 [35200/60000]\tLoss: 0.6594\tLR: 0.058635\nTraining Epoch: 1 [35328/60000]\tLoss: 0.5384\tLR: 0.058849\nTraining Epoch: 1 [35456/60000]\tLoss: 0.6174\tLR: 0.059062\nTraining Epoch: 1 [35584/60000]\tLoss: 0.5799\tLR: 0.059275\nTraining Epoch: 1 [35712/60000]\tLoss: 0.6758\tLR: 0.059488\nTraining Epoch: 1 [35840/60000]\tLoss: 0.5390\tLR: 0.059701\nTraining Epoch: 1 [35968/60000]\tLoss: 0.7141\tLR: 0.059915\nTraining Epoch: 1 [36096/60000]\tLoss: 0.6368\tLR: 0.060128\nTraining Epoch: 1 [36224/60000]\tLoss: 0.5485\tLR: 0.060341\nTraining Epoch: 1 [36352/60000]\tLoss: 0.5109\tLR: 0.060554\nTraining Epoch: 1 [36480/60000]\tLoss: 0.5991\tLR: 0.060768\nTraining Epoch: 1 [36608/60000]\tLoss: 0.4616\tLR: 0.060981\nTraining Epoch: 1 [36736/60000]\tLoss: 0.6803\tLR: 0.061194\nTraining Epoch: 1 [36864/60000]\tLoss: 0.6622\tLR: 0.061407\nTraining Epoch: 1 [36992/60000]\tLoss: 0.7806\tLR: 0.061620\nTraining Epoch: 1 [37120/60000]\tLoss: 0.6649\tLR: 0.061834\nTraining Epoch: 1 [37248/60000]\tLoss: 0.6985\tLR: 0.062047\nTraining Epoch: 1 [37376/60000]\tLoss: 0.5476\tLR: 0.062260\nTraining Epoch: 1 [37504/60000]\tLoss: 0.6361\tLR: 0.062473\nTraining Epoch: 1 [37632/60000]\tLoss: 0.4749\tLR: 0.062687\nTraining Epoch: 1 [37760/60000]\tLoss: 0.8608\tLR: 0.062900\nTraining Epoch: 1 [37888/60000]\tLoss: 0.4978\tLR: 0.063113\nTraining Epoch: 1 [38016/60000]\tLoss: 0.5573\tLR: 0.063326\nTraining Epoch: 1 [38144/60000]\tLoss: 0.5753\tLR: 0.063539\nTraining Epoch: 1 [38272/60000]\tLoss: 0.7040\tLR: 0.063753\nTraining Epoch: 1 [38400/60000]\tLoss: 0.5795\tLR: 0.063966\nTraining Epoch: 1 [38528/60000]\tLoss: 0.7429\tLR: 0.064179\nTraining Epoch: 1 [38656/60000]\tLoss: 0.7849\tLR: 0.064392\nTraining Epoch: 1 [38784/60000]\tLoss: 0.7009\tLR: 0.064606\nTraining Epoch: 1 [38912/60000]\tLoss: 0.5666\tLR: 0.064819\nTraining Epoch: 1 [39040/60000]\tLoss: 0.7367\tLR: 0.065032\nTraining Epoch: 1 [39168/60000]\tLoss: 0.5232\tLR: 0.065245\nTraining Epoch: 1 [39296/60000]\tLoss: 0.5796\tLR: 0.065458\nTraining Epoch: 1 [39424/60000]\tLoss: 0.5378\tLR: 0.065672\nTraining Epoch: 1 [39552/60000]\tLoss: 0.8582\tLR: 0.065885\nTraining Epoch: 1 [39680/60000]\tLoss: 0.6093\tLR: 0.066098\nTraining Epoch: 1 [39808/60000]\tLoss: 0.5385\tLR: 0.066311\nTraining Epoch: 1 [39936/60000]\tLoss: 0.5913\tLR: 0.066525\nTraining Epoch: 1 [40064/60000]\tLoss: 0.6346\tLR: 0.066738\nTraining Epoch: 1 [40192/60000]\tLoss: 0.5775\tLR: 0.066951\nTraining Epoch: 1 [40320/60000]\tLoss: 0.5432\tLR: 0.067164\nTraining Epoch: 1 [40448/60000]\tLoss: 0.6833\tLR: 0.067377\nTraining Epoch: 1 [40576/60000]\tLoss: 0.5051\tLR: 0.067591\nTraining Epoch: 1 [40704/60000]\tLoss: 0.6037\tLR: 0.067804\nTraining Epoch: 1 [40832/60000]\tLoss: 0.5859\tLR: 0.068017\nTraining Epoch: 1 [40960/60000]\tLoss: 0.7318\tLR: 0.068230\nTraining Epoch: 1 [41088/60000]\tLoss: 0.5851\tLR: 0.068443\nTraining Epoch: 1 [41216/60000]\tLoss: 0.7119\tLR: 0.068657\nTraining Epoch: 1 [41344/60000]\tLoss: 0.7024\tLR: 0.068870\nTraining Epoch: 1 [41472/60000]\tLoss: 0.5663\tLR: 0.069083\nTraining Epoch: 1 [41600/60000]\tLoss: 0.6096\tLR: 0.069296\nTraining Epoch: 1 [41728/60000]\tLoss: 0.8123\tLR: 0.069510\nTraining Epoch: 1 [41856/60000]\tLoss: 0.6853\tLR: 0.069723\nTraining Epoch: 1 [41984/60000]\tLoss: 0.7114\tLR: 0.069936\nTraining Epoch: 1 [42112/60000]\tLoss: 0.8031\tLR: 0.070149\nTraining Epoch: 1 [42240/60000]\tLoss: 0.6087\tLR: 0.070362\nTraining Epoch: 1 [42368/60000]\tLoss: 0.6658\tLR: 0.070576\nTraining Epoch: 1 [42496/60000]\tLoss: 0.7582\tLR: 0.070789\nTraining Epoch: 1 [42624/60000]\tLoss: 0.6520\tLR: 0.071002\nTraining Epoch: 1 [42752/60000]\tLoss: 0.6901\tLR: 0.071215\nTraining Epoch: 1 [42880/60000]\tLoss: 0.6779\tLR: 0.071429\nTraining Epoch: 1 [43008/60000]\tLoss: 0.6944\tLR: 0.071642\nTraining Epoch: 1 [43136/60000]\tLoss: 0.5431\tLR: 0.071855\nTraining Epoch: 1 [43264/60000]\tLoss: 0.6060\tLR: 0.072068\nTraining Epoch: 1 [43392/60000]\tLoss: 0.5690\tLR: 0.072281\nTraining Epoch: 1 [43520/60000]\tLoss: 0.4441\tLR: 0.072495\nTraining Epoch: 1 [43648/60000]\tLoss: 0.7629\tLR: 0.072708\nTraining Epoch: 1 [43776/60000]\tLoss: 0.7039\tLR: 0.072921\nTraining Epoch: 1 [43904/60000]\tLoss: 0.8864\tLR: 0.073134\nTraining Epoch: 1 [44032/60000]\tLoss: 0.4237\tLR: 0.073348\nTraining Epoch: 1 [44160/60000]\tLoss: 0.7173\tLR: 0.073561\nTraining Epoch: 1 [44288/60000]\tLoss: 1.0985\tLR: 0.073774\nTraining Epoch: 1 [44416/60000]\tLoss: 0.5283\tLR: 0.073987\nTraining Epoch: 1 [44544/60000]\tLoss: 0.4920\tLR: 0.074200\nTraining Epoch: 1 [44672/60000]\tLoss: 0.5334\tLR: 0.074414\nTraining Epoch: 1 [44800/60000]\tLoss: 0.6454\tLR: 0.074627\nTraining Epoch: 1 [44928/60000]\tLoss: 0.6633\tLR: 0.074840\nTraining Epoch: 1 [45056/60000]\tLoss: 0.7138\tLR: 0.075053\nTraining Epoch: 1 [45184/60000]\tLoss: 0.6136\tLR: 0.075267\nTraining Epoch: 1 [45312/60000]\tLoss: 0.5362\tLR: 0.075480\nTraining Epoch: 1 [45440/60000]\tLoss: 0.5312\tLR: 0.075693\nTraining Epoch: 1 [45568/60000]\tLoss: 0.5336\tLR: 0.075906\nTraining Epoch: 1 [45696/60000]\tLoss: 0.7570\tLR: 0.076119\nTraining Epoch: 1 [45824/60000]\tLoss: 0.5946\tLR: 0.076333\nTraining Epoch: 1 [45952/60000]\tLoss: 0.7096\tLR: 0.076546\nTraining Epoch: 1 [46080/60000]\tLoss: 0.6047\tLR: 0.076759\nTraining Epoch: 1 [46208/60000]\tLoss: 0.5124\tLR: 0.076972\nTraining Epoch: 1 [46336/60000]\tLoss: 0.6757\tLR: 0.077186\nTraining Epoch: 1 [46464/60000]\tLoss: 0.5714\tLR: 0.077399\nTraining Epoch: 1 [46592/60000]\tLoss: 0.4448\tLR: 0.077612\nTraining Epoch: 1 [46720/60000]\tLoss: 0.6968\tLR: 0.077825\nTraining Epoch: 1 [46848/60000]\tLoss: 0.6193\tLR: 0.078038\nTraining Epoch: 1 [46976/60000]\tLoss: 0.6731\tLR: 0.078252\nTraining Epoch: 1 [47104/60000]\tLoss: 0.5540\tLR: 0.078465\nTraining Epoch: 1 [47232/60000]\tLoss: 0.5470\tLR: 0.078678\nTraining Epoch: 1 [47360/60000]\tLoss: 0.5536\tLR: 0.078891\nTraining Epoch: 1 [47488/60000]\tLoss: 0.5185\tLR: 0.079104\nTraining Epoch: 1 [47616/60000]\tLoss: 0.6006\tLR: 0.079318\nTraining Epoch: 1 [47744/60000]\tLoss: 0.4184\tLR: 0.079531\nTraining Epoch: 1 [47872/60000]\tLoss: 0.7087\tLR: 0.079744\nTraining Epoch: 1 [48000/60000]\tLoss: 0.5019\tLR: 0.079957\nTraining Epoch: 1 [48128/60000]\tLoss: 0.6265\tLR: 0.080171\nTraining Epoch: 1 [48256/60000]\tLoss: 0.7418\tLR: 0.080384\nTraining Epoch: 1 [48384/60000]\tLoss: 0.8072\tLR: 0.080597\nTraining Epoch: 1 [48512/60000]\tLoss: 0.5389\tLR: 0.080810\nTraining Epoch: 1 [48640/60000]\tLoss: 0.5981\tLR: 0.081023\nTraining Epoch: 1 [48768/60000]\tLoss: 0.5480\tLR: 0.081237\nTraining Epoch: 1 [48896/60000]\tLoss: 0.5956\tLR: 0.081450\nTraining Epoch: 1 [49024/60000]\tLoss: 0.6825\tLR: 0.081663\nTraining Epoch: 1 [49152/60000]\tLoss: 0.6607\tLR: 0.081876\nTraining Epoch: 1 [49280/60000]\tLoss: 0.5495\tLR: 0.082090\nTraining Epoch: 1 [49408/60000]\tLoss: 0.6171\tLR: 0.082303\nTraining Epoch: 1 [49536/60000]\tLoss: 0.6177\tLR: 0.082516\nTraining Epoch: 1 [49664/60000]\tLoss: 0.5644\tLR: 0.082729\nTraining Epoch: 1 [49792/60000]\tLoss: 0.5885\tLR: 0.082942\nTraining Epoch: 1 [49920/60000]\tLoss: 0.6693\tLR: 0.083156\nTraining Epoch: 1 [50048/60000]\tLoss: 0.5658\tLR: 0.083369\nTraining Epoch: 1 [50176/60000]\tLoss: 0.5230\tLR: 0.083582\nTraining Epoch: 1 [50304/60000]\tLoss: 0.5186\tLR: 0.083795\nTraining Epoch: 1 [50432/60000]\tLoss: 0.7185\tLR: 0.084009\nTraining Epoch: 1 [50560/60000]\tLoss: 0.5620\tLR: 0.084222\nTraining Epoch: 1 [50688/60000]\tLoss: 0.5513\tLR: 0.084435\nTraining Epoch: 1 [50816/60000]\tLoss: 0.4860\tLR: 0.084648\nTraining Epoch: 1 [50944/60000]\tLoss: 0.7715\tLR: 0.084861\nTraining Epoch: 1 [51072/60000]\tLoss: 0.4612\tLR: 0.085075\nTraining Epoch: 1 [51200/60000]\tLoss: 0.5371\tLR: 0.085288\nTraining Epoch: 1 [51328/60000]\tLoss: 0.5488\tLR: 0.085501\nTraining Epoch: 1 [51456/60000]\tLoss: 0.5696\tLR: 0.085714\nTraining Epoch: 1 [51584/60000]\tLoss: 0.5288\tLR: 0.085928\nTraining Epoch: 1 [51712/60000]\tLoss: 0.5328\tLR: 0.086141\nTraining Epoch: 1 [51840/60000]\tLoss: 0.6300\tLR: 0.086354\nTraining Epoch: 1 [51968/60000]\tLoss: 0.5501\tLR: 0.086567\nTraining Epoch: 1 [52096/60000]\tLoss: 0.6172\tLR: 0.086780\nTraining Epoch: 1 [52224/60000]\tLoss: 0.5201\tLR: 0.086994\nTraining Epoch: 1 [52352/60000]\tLoss: 0.5714\tLR: 0.087207\nTraining Epoch: 1 [52480/60000]\tLoss: 0.5612\tLR: 0.087420\nTraining Epoch: 1 [52608/60000]\tLoss: 0.7280\tLR: 0.087633\nTraining Epoch: 1 [52736/60000]\tLoss: 0.6699\tLR: 0.087846\nTraining Epoch: 1 [52864/60000]\tLoss: 0.5387\tLR: 0.088060\nTraining Epoch: 1 [52992/60000]\tLoss: 0.5718\tLR: 0.088273\nTraining Epoch: 1 [53120/60000]\tLoss: 0.5303\tLR: 0.088486\nTraining Epoch: 1 [53248/60000]\tLoss: 0.4435\tLR: 0.088699\nTraining Epoch: 1 [53376/60000]\tLoss: 0.5174\tLR: 0.088913\nTraining Epoch: 1 [53504/60000]\tLoss: 0.6331\tLR: 0.089126\nTraining Epoch: 1 [53632/60000]\tLoss: 0.6686\tLR: 0.089339\nTraining Epoch: 1 [53760/60000]\tLoss: 0.6341\tLR: 0.089552\nTraining Epoch: 1 [53888/60000]\tLoss: 0.5347\tLR: 0.089765\nTraining Epoch: 1 [54016/60000]\tLoss: 0.4938\tLR: 0.089979\nTraining Epoch: 1 [54144/60000]\tLoss: 0.4819\tLR: 0.090192\nTraining Epoch: 1 [54272/60000]\tLoss: 0.6041\tLR: 0.090405\nTraining Epoch: 1 [54400/60000]\tLoss: 0.5505\tLR: 0.090618\nTraining Epoch: 1 [54528/60000]\tLoss: 0.5347\tLR: 0.090832\nTraining Epoch: 1 [54656/60000]\tLoss: 0.5599\tLR: 0.091045\nTraining Epoch: 1 [54784/60000]\tLoss: 0.6921\tLR: 0.091258\nTraining Epoch: 1 [54912/60000]\tLoss: 0.6266\tLR: 0.091471\nTraining Epoch: 1 [55040/60000]\tLoss: 0.6267\tLR: 0.091684\nTraining Epoch: 1 [55168/60000]\tLoss: 0.7631\tLR: 0.091898\nTraining Epoch: 1 [55296/60000]\tLoss: 0.5293\tLR: 0.092111\nTraining Epoch: 1 [55424/60000]\tLoss: 0.5953\tLR: 0.092324\nTraining Epoch: 1 [55552/60000]\tLoss: 0.5568\tLR: 0.092537\nTraining Epoch: 1 [55680/60000]\tLoss: 0.5605\tLR: 0.092751\nTraining Epoch: 1 [55808/60000]\tLoss: 0.4510\tLR: 0.092964\nTraining Epoch: 1 [55936/60000]\tLoss: 0.5959\tLR: 0.093177\nTraining Epoch: 1 [56064/60000]\tLoss: 0.4838\tLR: 0.093390\nTraining Epoch: 1 [56192/60000]\tLoss: 0.5124\tLR: 0.093603\nTraining Epoch: 1 [56320/60000]\tLoss: 0.4734\tLR: 0.093817\nTraining Epoch: 1 [56448/60000]\tLoss: 0.5038\tLR: 0.094030\nTraining Epoch: 1 [56576/60000]\tLoss: 0.6343\tLR: 0.094243\nTraining Epoch: 1 [56704/60000]\tLoss: 0.5340\tLR: 0.094456\nTraining Epoch: 1 [56832/60000]\tLoss: 0.5564\tLR: 0.094670\nTraining Epoch: 1 [56960/60000]\tLoss: 0.5718\tLR: 0.094883\nTraining Epoch: 1 [57088/60000]\tLoss: 0.5732\tLR: 0.095096\nTraining Epoch: 1 [57216/60000]\tLoss: 0.4627\tLR: 0.095309\nTraining Epoch: 1 [57344/60000]\tLoss: 0.5545\tLR: 0.095522\nTraining Epoch: 1 [57472/60000]\tLoss: 0.4449\tLR: 0.095736\nTraining Epoch: 1 [57600/60000]\tLoss: 0.4377\tLR: 0.095949\nTraining Epoch: 1 [57728/60000]\tLoss: 0.6191\tLR: 0.096162\nTraining Epoch: 1 [57856/60000]\tLoss: 0.5372\tLR: 0.096375\nTraining Epoch: 1 [57984/60000]\tLoss: 0.5162\tLR: 0.096588\nTraining Epoch: 1 [58112/60000]\tLoss: 0.5980\tLR: 0.096802\nTraining Epoch: 1 [58240/60000]\tLoss: 0.4675\tLR: 0.097015\nTraining Epoch: 1 [58368/60000]\tLoss: 0.6583\tLR: 0.097228\nTraining Epoch: 1 [58496/60000]\tLoss: 0.4839\tLR: 0.097441\nTraining Epoch: 1 [58624/60000]\tLoss: 0.4767\tLR: 0.097655\nTraining Epoch: 1 [58752/60000]\tLoss: 0.5794\tLR: 0.097868\nTraining Epoch: 1 [58880/60000]\tLoss: 0.7124\tLR: 0.098081\nTraining Epoch: 1 [59008/60000]\tLoss: 0.4796\tLR: 0.098294\nTraining Epoch: 1 [59136/60000]\tLoss: 0.5152\tLR: 0.098507\nTraining Epoch: 1 [59264/60000]\tLoss: 0.4639\tLR: 0.098721\nTraining Epoch: 1 [59392/60000]\tLoss: 0.4529\tLR: 0.098934\nTraining Epoch: 1 [59520/60000]\tLoss: 0.5361\tLR: 0.099147\nTraining Epoch: 1 [59648/60000]\tLoss: 0.5261\tLR: 0.099360\nTraining Epoch: 1 [59776/60000]\tLoss: 0.5198\tLR: 0.099574\nTraining Epoch: 1 [59904/60000]\tLoss: 0.4565\tLR: 0.099787\nTraining Epoch: 1 [60000/60000]\tLoss: 0.5789\tLR: 0.100000\nTest set: Average loss: 0.0082, Accuracy: 0.6296\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Training Epoch: 2 [128/60000]\tLoss: 0.5217\tLR: 0.100000\nTraining Epoch: 2 [256/60000]\tLoss: 0.5462\tLR: 0.100000\nTraining Epoch: 2 [384/60000]\tLoss: 0.7489\tLR: 0.100000\nTraining Epoch: 2 [512/60000]\tLoss: 0.7856\tLR: 0.100000\nTraining Epoch: 2 [640/60000]\tLoss: 0.5637\tLR: 0.100000\nTraining Epoch: 2 [768/60000]\tLoss: 0.5329\tLR: 0.100000\nTraining Epoch: 2 [896/60000]\tLoss: 0.6622\tLR: 0.100000\nTraining Epoch: 2 [1024/60000]\tLoss: 0.5152\tLR: 0.100000\nTraining Epoch: 2 [1152/60000]\tLoss: 0.5312\tLR: 0.100000\nTraining Epoch: 2 [1280/60000]\tLoss: 0.5410\tLR: 0.100000\nTraining Epoch: 2 [1408/60000]\tLoss: 0.4367\tLR: 0.100000\nTraining Epoch: 2 [1536/60000]\tLoss: 0.6461\tLR: 0.100000\nTraining Epoch: 2 [1664/60000]\tLoss: 0.5955\tLR: 0.100000\nTraining Epoch: 2 [1792/60000]\tLoss: 0.4699\tLR: 0.100000\nTraining Epoch: 2 [1920/60000]\tLoss: 0.5131\tLR: 0.100000\nTraining Epoch: 2 [2048/60000]\tLoss: 0.7644\tLR: 0.100000\nTraining Epoch: 2 [2176/60000]\tLoss: 0.5337\tLR: 0.100000\nTraining Epoch: 2 [2304/60000]\tLoss: 0.5825\tLR: 0.100000\nTraining Epoch: 2 [2432/60000]\tLoss: 0.5687\tLR: 0.100000\nTraining Epoch: 2 [2560/60000]\tLoss: 0.5435\tLR: 0.100000\nTraining Epoch: 2 [2688/60000]\tLoss: 0.7314\tLR: 0.100000\nTraining Epoch: 2 [2816/60000]\tLoss: 0.6030\tLR: 0.100000\nTraining Epoch: 2 [2944/60000]\tLoss: 0.4376\tLR: 0.100000\nTraining Epoch: 2 [3072/60000]\tLoss: 0.5066\tLR: 0.100000\nTraining Epoch: 2 [3200/60000]\tLoss: 0.5497\tLR: 0.100000\nTraining Epoch: 2 [3328/60000]\tLoss: 0.4765\tLR: 0.100000\nTraining Epoch: 2 [3456/60000]\tLoss: 0.5276\tLR: 0.100000\nTraining Epoch: 2 [3584/60000]\tLoss: 0.4597\tLR: 0.100000\nTraining Epoch: 2 [3712/60000]\tLoss: 0.5236\tLR: 0.100000\nTraining Epoch: 2 [3840/60000]\tLoss: 0.5051\tLR: 0.100000\nTraining Epoch: 2 [3968/60000]\tLoss: 0.5975\tLR: 0.100000\nTraining Epoch: 2 [4096/60000]\tLoss: 0.6767\tLR: 0.100000\nTraining Epoch: 2 [4224/60000]\tLoss: 0.4783\tLR: 0.100000\nTraining Epoch: 2 [4352/60000]\tLoss: 0.5367\tLR: 0.100000\nTraining Epoch: 2 [4480/60000]\tLoss: 0.6075\tLR: 0.100000\nTraining Epoch: 2 [4608/60000]\tLoss: 0.4327\tLR: 0.100000\nTraining Epoch: 2 [4736/60000]\tLoss: 0.5179\tLR: 0.100000\nTraining Epoch: 2 [4864/60000]\tLoss: 0.4691\tLR: 0.100000\nTraining Epoch: 2 [4992/60000]\tLoss: 0.6651\tLR: 0.100000\nTraining Epoch: 2 [5120/60000]\tLoss: 0.6126\tLR: 0.100000\nTraining Epoch: 2 [5248/60000]\tLoss: 0.5358\tLR: 0.100000\nTraining Epoch: 2 [5376/60000]\tLoss: 0.5311\tLR: 0.100000\nTraining Epoch: 2 [5504/60000]\tLoss: 0.4341\tLR: 0.100000\nTraining Epoch: 2 [5632/60000]\tLoss: 0.5217\tLR: 0.100000\nTraining Epoch: 2 [5760/60000]\tLoss: 0.5077\tLR: 0.100000\nTraining Epoch: 2 [5888/60000]\tLoss: 0.5265\tLR: 0.100000\nTraining Epoch: 2 [6016/60000]\tLoss: 0.4381\tLR: 0.100000\nTraining Epoch: 2 [6144/60000]\tLoss: 0.5319\tLR: 0.100000\nTraining Epoch: 2 [6272/60000]\tLoss: 0.6441\tLR: 0.100000\nTraining Epoch: 2 [6400/60000]\tLoss: 0.4093\tLR: 0.100000\nTraining Epoch: 2 [6528/60000]\tLoss: 0.4645\tLR: 0.100000\nTraining Epoch: 2 [6656/60000]\tLoss: 0.4359\tLR: 0.100000\nTraining Epoch: 2 [6784/60000]\tLoss: 0.3921\tLR: 0.100000\nTraining Epoch: 2 [6912/60000]\tLoss: 0.4949\tLR: 0.100000\nTraining Epoch: 2 [7040/60000]\tLoss: 0.4430\tLR: 0.100000\nTraining Epoch: 2 [7168/60000]\tLoss: 0.4223\tLR: 0.100000\nTraining Epoch: 2 [7296/60000]\tLoss: 0.3935\tLR: 0.100000\nTraining Epoch: 2 [7424/60000]\tLoss: 0.4097\tLR: 0.100000\nTraining Epoch: 2 [7552/60000]\tLoss: 0.3997\tLR: 0.100000\nTraining Epoch: 2 [7680/60000]\tLoss: 0.3264\tLR: 0.100000\nTraining Epoch: 2 [7808/60000]\tLoss: 0.4720\tLR: 0.100000\nTraining Epoch: 2 [7936/60000]\tLoss: 0.4432\tLR: 0.100000\nTraining Epoch: 2 [8064/60000]\tLoss: 0.5787\tLR: 0.100000\nTraining Epoch: 2 [8192/60000]\tLoss: 0.4798\tLR: 0.100000\nTraining Epoch: 2 [8320/60000]\tLoss: 0.4517\tLR: 0.100000\nTraining Epoch: 2 [8448/60000]\tLoss: 0.4048\tLR: 0.100000\nTraining Epoch: 2 [8576/60000]\tLoss: 0.4571\tLR: 0.100000\nTraining Epoch: 2 [8704/60000]\tLoss: 0.5234\tLR: 0.100000\nTraining Epoch: 2 [8832/60000]\tLoss: 0.5556\tLR: 0.100000\nTraining Epoch: 2 [8960/60000]\tLoss: 0.5159\tLR: 0.100000\nTraining Epoch: 2 [9088/60000]\tLoss: 0.4288\tLR: 0.100000\nTraining Epoch: 2 [9216/60000]\tLoss: 0.4429\tLR: 0.100000\nTraining Epoch: 2 [9344/60000]\tLoss: 0.4788\tLR: 0.100000\nTraining Epoch: 2 [9472/60000]\tLoss: 0.4761\tLR: 0.100000\nTraining Epoch: 2 [9600/60000]\tLoss: 0.4655\tLR: 0.100000\nTraining Epoch: 2 [9728/60000]\tLoss: 0.6262\tLR: 0.100000\nTraining Epoch: 2 [9856/60000]\tLoss: 0.5324\tLR: 0.100000\nTraining Epoch: 2 [9984/60000]\tLoss: 0.5343\tLR: 0.100000\nTraining Epoch: 2 [10112/60000]\tLoss: 0.5827\tLR: 0.100000\nTraining Epoch: 2 [10240/60000]\tLoss: 0.6770\tLR: 0.100000\nTraining Epoch: 2 [10368/60000]\tLoss: 0.4377\tLR: 0.100000\nTraining Epoch: 2 [10496/60000]\tLoss: 0.5040\tLR: 0.100000\nTraining Epoch: 2 [10624/60000]\tLoss: 0.4782\tLR: 0.100000\nTraining Epoch: 2 [10752/60000]\tLoss: 0.3792\tLR: 0.100000\nTraining Epoch: 2 [10880/60000]\tLoss: 0.5286\tLR: 0.100000\nTraining Epoch: 2 [11008/60000]\tLoss: 0.5598\tLR: 0.100000\nTraining Epoch: 2 [11136/60000]\tLoss: 0.4816\tLR: 0.100000\nTraining Epoch: 2 [11264/60000]\tLoss: 0.4200\tLR: 0.100000\nTraining Epoch: 2 [11392/60000]\tLoss: 0.3872\tLR: 0.100000\nTraining Epoch: 2 [11520/60000]\tLoss: 0.4043\tLR: 0.100000\nTraining Epoch: 2 [11648/60000]\tLoss: 0.5901\tLR: 0.100000\nTraining Epoch: 2 [11776/60000]\tLoss: 0.6298\tLR: 0.100000\nTraining Epoch: 2 [11904/60000]\tLoss: 0.5488\tLR: 0.100000\nTraining Epoch: 2 [12032/60000]\tLoss: 0.5286\tLR: 0.100000\nTraining Epoch: 2 [12160/60000]\tLoss: 0.4094\tLR: 0.100000\nTraining Epoch: 2 [12288/60000]\tLoss: 0.4909\tLR: 0.100000\nTraining Epoch: 2 [12416/60000]\tLoss: 0.3815\tLR: 0.100000\nTraining Epoch: 2 [12544/60000]\tLoss: 0.7087\tLR: 0.100000\nTraining Epoch: 2 [12672/60000]\tLoss: 0.5538\tLR: 0.100000\nTraining Epoch: 2 [12800/60000]\tLoss: 0.4340\tLR: 0.100000\nTraining Epoch: 2 [12928/60000]\tLoss: 0.5469\tLR: 0.100000\nTraining Epoch: 2 [13056/60000]\tLoss: 0.4450\tLR: 0.100000\nTraining Epoch: 2 [13184/60000]\tLoss: 0.5203\tLR: 0.100000\nTraining Epoch: 2 [13312/60000]\tLoss: 0.5611\tLR: 0.100000\nTraining Epoch: 2 [13440/60000]\tLoss: 0.4177\tLR: 0.100000\nTraining Epoch: 2 [13568/60000]\tLoss: 0.4420\tLR: 0.100000\nTraining Epoch: 2 [13696/60000]\tLoss: 0.7468\tLR: 0.100000\nTraining Epoch: 2 [13824/60000]\tLoss: 0.4885\tLR: 0.100000\nTraining Epoch: 2 [13952/60000]\tLoss: 0.3860\tLR: 0.100000\nTraining Epoch: 2 [14080/60000]\tLoss: 0.4209\tLR: 0.100000\nTraining Epoch: 2 [14208/60000]\tLoss: 0.6108\tLR: 0.100000\nTraining Epoch: 2 [14336/60000]\tLoss: 0.5594\tLR: 0.100000\nTraining Epoch: 2 [14464/60000]\tLoss: 0.5179\tLR: 0.100000\nTraining Epoch: 2 [14592/60000]\tLoss: 0.4844\tLR: 0.100000\nTraining Epoch: 2 [14720/60000]\tLoss: 0.4883\tLR: 0.100000\nTraining Epoch: 2 [14848/60000]\tLoss: 0.6189\tLR: 0.100000\nTraining Epoch: 2 [14976/60000]\tLoss: 0.3923\tLR: 0.100000\nTraining Epoch: 2 [15104/60000]\tLoss: 0.5414\tLR: 0.100000\nTraining Epoch: 2 [15232/60000]\tLoss: 0.5079\tLR: 0.100000\nTraining Epoch: 2 [15360/60000]\tLoss: 0.4857\tLR: 0.100000\nTraining Epoch: 2 [15488/60000]\tLoss: 0.5052\tLR: 0.100000\nTraining Epoch: 2 [15616/60000]\tLoss: 0.5230\tLR: 0.100000\nTraining Epoch: 2 [15744/60000]\tLoss: 0.3789\tLR: 0.100000\nTraining Epoch: 2 [15872/60000]\tLoss: 0.4740\tLR: 0.100000\nTraining Epoch: 2 [16000/60000]\tLoss: 0.5748\tLR: 0.100000\nTraining Epoch: 2 [16128/60000]\tLoss: 0.4438\tLR: 0.100000\nTraining Epoch: 2 [16256/60000]\tLoss: 0.5193\tLR: 0.100000\nTraining Epoch: 2 [16384/60000]\tLoss: 0.4974\tLR: 0.100000\nTraining Epoch: 2 [16512/60000]\tLoss: 0.6571\tLR: 0.100000\nTraining Epoch: 2 [16640/60000]\tLoss: 0.4175\tLR: 0.100000\nTraining Epoch: 2 [16768/60000]\tLoss: 0.5878\tLR: 0.100000\nTraining Epoch: 2 [16896/60000]\tLoss: 0.5091\tLR: 0.100000\nTraining Epoch: 2 [17024/60000]\tLoss: 0.6876\tLR: 0.100000\nTraining Epoch: 2 [17152/60000]\tLoss: 0.4638\tLR: 0.100000\nTraining Epoch: 2 [17280/60000]\tLoss: 0.5616\tLR: 0.100000\nTraining Epoch: 2 [17408/60000]\tLoss: 0.5486\tLR: 0.100000\nTraining Epoch: 2 [17536/60000]\tLoss: 0.5139\tLR: 0.100000\nTraining Epoch: 2 [17664/60000]\tLoss: 0.5018\tLR: 0.100000\nTraining Epoch: 2 [17792/60000]\tLoss: 0.5085\tLR: 0.100000\nTraining Epoch: 2 [17920/60000]\tLoss: 0.4708\tLR: 0.100000\nTraining Epoch: 2 [18048/60000]\tLoss: 0.4654\tLR: 0.100000\nTraining Epoch: 2 [18176/60000]\tLoss: 0.5206\tLR: 0.100000\nTraining Epoch: 2 [18304/60000]\tLoss: 0.3647\tLR: 0.100000\nTraining Epoch: 2 [18432/60000]\tLoss: 0.6106\tLR: 0.100000\nTraining Epoch: 2 [18560/60000]\tLoss: 0.5835\tLR: 0.100000\nTraining Epoch: 2 [18688/60000]\tLoss: 0.3454\tLR: 0.100000\nTraining Epoch: 2 [18816/60000]\tLoss: 0.6507\tLR: 0.100000\nTraining Epoch: 2 [18944/60000]\tLoss: 0.3997\tLR: 0.100000\nTraining Epoch: 2 [19072/60000]\tLoss: 0.3580\tLR: 0.100000\nTraining Epoch: 2 [19200/60000]\tLoss: 0.4382\tLR: 0.100000\nTraining Epoch: 2 [19328/60000]\tLoss: 0.4232\tLR: 0.100000\nTraining Epoch: 2 [19456/60000]\tLoss: 0.5517\tLR: 0.100000\nTraining Epoch: 2 [19584/60000]\tLoss: 0.5768\tLR: 0.100000\nTraining Epoch: 2 [19712/60000]\tLoss: 0.3611\tLR: 0.100000\nTraining Epoch: 2 [19840/60000]\tLoss: 0.4946\tLR: 0.100000\nTraining Epoch: 2 [19968/60000]\tLoss: 0.4139\tLR: 0.100000\nTraining Epoch: 2 [20096/60000]\tLoss: 0.3959\tLR: 0.100000\nTraining Epoch: 2 [20224/60000]\tLoss: 0.5469\tLR: 0.100000\nTraining Epoch: 2 [20352/60000]\tLoss: 0.3825\tLR: 0.100000\nTraining Epoch: 2 [20480/60000]\tLoss: 0.5336\tLR: 0.100000\nTraining Epoch: 2 [20608/60000]\tLoss: 0.4864\tLR: 0.100000\nTraining Epoch: 2 [20736/60000]\tLoss: 0.5188\tLR: 0.100000\nTraining Epoch: 2 [20864/60000]\tLoss: 0.5146\tLR: 0.100000\nTraining Epoch: 2 [20992/60000]\tLoss: 0.6323\tLR: 0.100000\nTraining Epoch: 2 [21120/60000]\tLoss: 0.3936\tLR: 0.100000\nTraining Epoch: 2 [21248/60000]\tLoss: 0.4963\tLR: 0.100000\nTraining Epoch: 2 [21376/60000]\tLoss: 0.3786\tLR: 0.100000\nTraining Epoch: 2 [21504/60000]\tLoss: 0.5142\tLR: 0.100000\nTraining Epoch: 2 [21632/60000]\tLoss: 0.4618\tLR: 0.100000\nTraining Epoch: 2 [21760/60000]\tLoss: 0.4633\tLR: 0.100000\nTraining Epoch: 2 [21888/60000]\tLoss: 0.5858\tLR: 0.100000\nTraining Epoch: 2 [22016/60000]\tLoss: 0.4486\tLR: 0.100000\nTraining Epoch: 2 [22144/60000]\tLoss: 0.5099\tLR: 0.100000\nTraining Epoch: 2 [22272/60000]\tLoss: 0.5701\tLR: 0.100000\nTraining Epoch: 2 [22400/60000]\tLoss: 0.3536\tLR: 0.100000\nTraining Epoch: 2 [22528/60000]\tLoss: 0.5183\tLR: 0.100000\nTraining Epoch: 2 [22656/60000]\tLoss: 0.5021\tLR: 0.100000\nTraining Epoch: 2 [22784/60000]\tLoss: 0.3835\tLR: 0.100000\nTraining Epoch: 2 [22912/60000]\tLoss: 0.5643\tLR: 0.100000\nTraining Epoch: 2 [23040/60000]\tLoss: 0.3210\tLR: 0.100000\nTraining Epoch: 2 [23168/60000]\tLoss: 0.3133\tLR: 0.100000\nTraining Epoch: 2 [23296/60000]\tLoss: 0.5368\tLR: 0.100000\nTraining Epoch: 2 [23424/60000]\tLoss: 0.4365\tLR: 0.100000\nTraining Epoch: 2 [23552/60000]\tLoss: 0.3358\tLR: 0.100000\nTraining Epoch: 2 [23680/60000]\tLoss: 0.4616\tLR: 0.100000\nTraining Epoch: 2 [23808/60000]\tLoss: 0.4108\tLR: 0.100000\nTraining Epoch: 2 [23936/60000]\tLoss: 0.5242\tLR: 0.100000\nTraining Epoch: 2 [24064/60000]\tLoss: 0.5899\tLR: 0.100000\nTraining Epoch: 2 [24192/60000]\tLoss: 0.5928\tLR: 0.100000\nTraining Epoch: 2 [24320/60000]\tLoss: 0.3785\tLR: 0.100000\nTraining Epoch: 2 [24448/60000]\tLoss: 0.4661\tLR: 0.100000\nTraining Epoch: 2 [24576/60000]\tLoss: 0.4939\tLR: 0.100000\nTraining Epoch: 2 [24704/60000]\tLoss: 0.6079\tLR: 0.100000\nTraining Epoch: 2 [24832/60000]\tLoss: 0.5140\tLR: 0.100000\nTraining Epoch: 2 [24960/60000]\tLoss: 0.4185\tLR: 0.100000\nTraining Epoch: 2 [25088/60000]\tLoss: 0.5314\tLR: 0.100000\nTraining Epoch: 2 [25216/60000]\tLoss: 0.3995\tLR: 0.100000\nTraining Epoch: 2 [25344/60000]\tLoss: 0.4838\tLR: 0.100000\nTraining Epoch: 2 [25472/60000]\tLoss: 0.4095\tLR: 0.100000\nTraining Epoch: 2 [25600/60000]\tLoss: 0.3432\tLR: 0.100000\nTraining Epoch: 2 [25728/60000]\tLoss: 0.4305\tLR: 0.100000\nTraining Epoch: 2 [25856/60000]\tLoss: 0.5004\tLR: 0.100000\nTraining Epoch: 2 [25984/60000]\tLoss: 0.3538\tLR: 0.100000\nTraining Epoch: 2 [26112/60000]\tLoss: 0.3173\tLR: 0.100000\nTraining Epoch: 2 [26240/60000]\tLoss: 0.4841\tLR: 0.100000\nTraining Epoch: 2 [26368/60000]\tLoss: 0.3680\tLR: 0.100000\nTraining Epoch: 2 [26496/60000]\tLoss: 0.4710\tLR: 0.100000\nTraining Epoch: 2 [26624/60000]\tLoss: 0.3360\tLR: 0.100000\nTraining Epoch: 2 [26752/60000]\tLoss: 0.4657\tLR: 0.100000\nTraining Epoch: 2 [26880/60000]\tLoss: 0.5468\tLR: 0.100000\nTraining Epoch: 2 [27008/60000]\tLoss: 0.5049\tLR: 0.100000\nTraining Epoch: 2 [27136/60000]\tLoss: 0.4286\tLR: 0.100000\nTraining Epoch: 2 [27264/60000]\tLoss: 0.4598\tLR: 0.100000\nTraining Epoch: 2 [27392/60000]\tLoss: 0.5388\tLR: 0.100000\nTraining Epoch: 2 [27520/60000]\tLoss: 0.5606\tLR: 0.100000\nTraining Epoch: 2 [27648/60000]\tLoss: 0.4558\tLR: 0.100000\nTraining Epoch: 2 [27776/60000]\tLoss: 0.3879\tLR: 0.100000\nTraining Epoch: 2 [27904/60000]\tLoss: 0.3588\tLR: 0.100000\nTraining Epoch: 2 [28032/60000]\tLoss: 0.4883\tLR: 0.100000\nTraining Epoch: 2 [28160/60000]\tLoss: 0.3702\tLR: 0.100000\nTraining Epoch: 2 [28288/60000]\tLoss: 0.4293\tLR: 0.100000\nTraining Epoch: 2 [28416/60000]\tLoss: 0.5644\tLR: 0.100000\nTraining Epoch: 2 [28544/60000]\tLoss: 0.3640\tLR: 0.100000\nTraining Epoch: 2 [28672/60000]\tLoss: 0.4714\tLR: 0.100000\nTraining Epoch: 2 [28800/60000]\tLoss: 0.5514\tLR: 0.100000\nTraining Epoch: 2 [28928/60000]\tLoss: 0.4570\tLR: 0.100000\nTraining Epoch: 2 [29056/60000]\tLoss: 0.4288\tLR: 0.100000\nTraining Epoch: 2 [29184/60000]\tLoss: 0.4665\tLR: 0.100000\nTraining Epoch: 2 [29312/60000]\tLoss: 0.3925\tLR: 0.100000\nTraining Epoch: 2 [29440/60000]\tLoss: 0.3967\tLR: 0.100000\nTraining Epoch: 2 [29568/60000]\tLoss: 0.4789\tLR: 0.100000\nTraining Epoch: 2 [29696/60000]\tLoss: 0.3459\tLR: 0.100000\nTraining Epoch: 2 [29824/60000]\tLoss: 0.4308\tLR: 0.100000\nTraining Epoch: 2 [29952/60000]\tLoss: 0.5155\tLR: 0.100000\nTraining Epoch: 2 [30080/60000]\tLoss: 0.4901\tLR: 0.100000\nTraining Epoch: 2 [30208/60000]\tLoss: 0.5918\tLR: 0.100000\nTraining Epoch: 2 [30336/60000]\tLoss: 0.4737\tLR: 0.100000\nTraining Epoch: 2 [30464/60000]\tLoss: 0.4657\tLR: 0.100000\nTraining Epoch: 2 [30592/60000]\tLoss: 0.3899\tLR: 0.100000\nTraining Epoch: 2 [30720/60000]\tLoss: 0.5396\tLR: 0.100000\nTraining Epoch: 2 [30848/60000]\tLoss: 0.6081\tLR: 0.100000\nTraining Epoch: 2 [30976/60000]\tLoss: 0.4891\tLR: 0.100000\nTraining Epoch: 2 [31104/60000]\tLoss: 0.5061\tLR: 0.100000\nTraining Epoch: 2 [31232/60000]\tLoss: 0.4539\tLR: 0.100000\nTraining Epoch: 2 [31360/60000]\tLoss: 0.4157\tLR: 0.100000\nTraining Epoch: 2 [31488/60000]\tLoss: 0.3860\tLR: 0.100000\nTraining Epoch: 2 [31616/60000]\tLoss: 0.3859\tLR: 0.100000\nTraining Epoch: 2 [31744/60000]\tLoss: 0.5842\tLR: 0.100000\nTraining Epoch: 2 [31872/60000]\tLoss: 0.4607\tLR: 0.100000\nTraining Epoch: 2 [32000/60000]\tLoss: 0.3234\tLR: 0.100000\nTraining Epoch: 2 [32128/60000]\tLoss: 0.3754\tLR: 0.100000\nTraining Epoch: 2 [32256/60000]\tLoss: 0.5806\tLR: 0.100000\nTraining Epoch: 2 [32384/60000]\tLoss: 0.4286\tLR: 0.100000\nTraining Epoch: 2 [32512/60000]\tLoss: 0.4080\tLR: 0.100000\nTraining Epoch: 2 [32640/60000]\tLoss: 0.3930\tLR: 0.100000\nTraining Epoch: 2 [32768/60000]\tLoss: 0.4505\tLR: 0.100000\nTraining Epoch: 2 [32896/60000]\tLoss: 0.3790\tLR: 0.100000\nTraining Epoch: 2 [33024/60000]\tLoss: 0.4933\tLR: 0.100000\nTraining Epoch: 2 [33152/60000]\tLoss: 0.4307\tLR: 0.100000\nTraining Epoch: 2 [33280/60000]\tLoss: 0.5613\tLR: 0.100000\nTraining Epoch: 2 [33408/60000]\tLoss: 0.4559\tLR: 0.100000\nTraining Epoch: 2 [33536/60000]\tLoss: 0.3169\tLR: 0.100000\nTraining Epoch: 2 [33664/60000]\tLoss: 0.3343\tLR: 0.100000\nTraining Epoch: 2 [33792/60000]\tLoss: 0.3140\tLR: 0.100000\nTraining Epoch: 2 [33920/60000]\tLoss: 0.3775\tLR: 0.100000\nTraining Epoch: 2 [34048/60000]\tLoss: 0.6311\tLR: 0.100000\nTraining Epoch: 2 [34176/60000]\tLoss: 0.6240\tLR: 0.100000\nTraining Epoch: 2 [34304/60000]\tLoss: 0.3358\tLR: 0.100000\nTraining Epoch: 2 [34432/60000]\tLoss: 0.4496\tLR: 0.100000\nTraining Epoch: 2 [34560/60000]\tLoss: 0.4617\tLR: 0.100000\nTraining Epoch: 2 [34688/60000]\tLoss: 0.4331\tLR: 0.100000\nTraining Epoch: 2 [34816/60000]\tLoss: 0.3573\tLR: 0.100000\nTraining Epoch: 2 [34944/60000]\tLoss: 0.4335\tLR: 0.100000\nTraining Epoch: 2 [35072/60000]\tLoss: 0.4530\tLR: 0.100000\nTraining Epoch: 2 [35200/60000]\tLoss: 0.4547\tLR: 0.100000\nTraining Epoch: 2 [35328/60000]\tLoss: 0.3621\tLR: 0.100000\nTraining Epoch: 2 [35456/60000]\tLoss: 0.3657\tLR: 0.100000\nTraining Epoch: 2 [35584/60000]\tLoss: 0.3619\tLR: 0.100000\nTraining Epoch: 2 [35712/60000]\tLoss: 0.4412\tLR: 0.100000\nTraining Epoch: 2 [35840/60000]\tLoss: 0.4481\tLR: 0.100000\nTraining Epoch: 2 [35968/60000]\tLoss: 0.3431\tLR: 0.100000\nTraining Epoch: 2 [36096/60000]\tLoss: 0.3791\tLR: 0.100000\nTraining Epoch: 2 [36224/60000]\tLoss: 0.4421\tLR: 0.100000\nTraining Epoch: 2 [36352/60000]\tLoss: 0.4964\tLR: 0.100000\nTraining Epoch: 2 [36480/60000]\tLoss: 0.3688\tLR: 0.100000\nTraining Epoch: 2 [36608/60000]\tLoss: 0.3679\tLR: 0.100000\nTraining Epoch: 2 [36736/60000]\tLoss: 0.5914\tLR: 0.100000\nTraining Epoch: 2 [36864/60000]\tLoss: 0.4665\tLR: 0.100000\nTraining Epoch: 2 [36992/60000]\tLoss: 0.5153\tLR: 0.100000\nTraining Epoch: 2 [37120/60000]\tLoss: 0.3322\tLR: 0.100000\nTraining Epoch: 2 [37248/60000]\tLoss: 0.3737\tLR: 0.100000\nTraining Epoch: 2 [37376/60000]\tLoss: 0.4374\tLR: 0.100000\nTraining Epoch: 2 [37504/60000]\tLoss: 0.5133\tLR: 0.100000\nTraining Epoch: 2 [37632/60000]\tLoss: 0.4430\tLR: 0.100000\nTraining Epoch: 2 [37760/60000]\tLoss: 0.4546\tLR: 0.100000\nTraining Epoch: 2 [37888/60000]\tLoss: 0.6946\tLR: 0.100000\nTraining Epoch: 2 [38016/60000]\tLoss: 0.3661\tLR: 0.100000\nTraining Epoch: 2 [38144/60000]\tLoss: 0.4556\tLR: 0.100000\nTraining Epoch: 2 [38272/60000]\tLoss: 0.4269\tLR: 0.100000\nTraining Epoch: 2 [38400/60000]\tLoss: 0.5602\tLR: 0.100000\nTraining Epoch: 2 [38528/60000]\tLoss: 0.3686\tLR: 0.100000\nTraining Epoch: 2 [38656/60000]\tLoss: 0.4060\tLR: 0.100000\nTraining Epoch: 2 [38784/60000]\tLoss: 0.3904\tLR: 0.100000\nTraining Epoch: 2 [38912/60000]\tLoss: 0.4325\tLR: 0.100000\nTraining Epoch: 2 [39040/60000]\tLoss: 0.3885\tLR: 0.100000\nTraining Epoch: 2 [39168/60000]\tLoss: 0.4990\tLR: 0.100000\nTraining Epoch: 2 [39296/60000]\tLoss: 0.4202\tLR: 0.100000\nTraining Epoch: 2 [39424/60000]\tLoss: 0.4837\tLR: 0.100000\nTraining Epoch: 2 [39552/60000]\tLoss: 0.4344\tLR: 0.100000\nTraining Epoch: 2 [39680/60000]\tLoss: 0.3968\tLR: 0.100000\nTraining Epoch: 2 [39808/60000]\tLoss: 0.4921\tLR: 0.100000\nTraining Epoch: 2 [39936/60000]\tLoss: 0.4302\tLR: 0.100000\nTraining Epoch: 2 [40064/60000]\tLoss: 0.3956\tLR: 0.100000\nTraining Epoch: 2 [40192/60000]\tLoss: 0.3589\tLR: 0.100000\nTraining Epoch: 2 [40320/60000]\tLoss: 0.4327\tLR: 0.100000\nTraining Epoch: 2 [40448/60000]\tLoss: 0.3251\tLR: 0.100000\nTraining Epoch: 2 [40576/60000]\tLoss: 0.5111\tLR: 0.100000\nTraining Epoch: 2 [40704/60000]\tLoss: 0.4262\tLR: 0.100000\nTraining Epoch: 2 [40832/60000]\tLoss: 0.3467\tLR: 0.100000\nTraining Epoch: 2 [40960/60000]\tLoss: 0.4450\tLR: 0.100000\nTraining Epoch: 2 [41088/60000]\tLoss: 0.4018\tLR: 0.100000\nTraining Epoch: 2 [41216/60000]\tLoss: 0.4669\tLR: 0.100000\nTraining Epoch: 2 [41344/60000]\tLoss: 0.4603\tLR: 0.100000\nTraining Epoch: 2 [41472/60000]\tLoss: 0.3410\tLR: 0.100000\nTraining Epoch: 2 [41600/60000]\tLoss: 0.4806\tLR: 0.100000\nTraining Epoch: 2 [41728/60000]\tLoss: 0.5784\tLR: 0.100000\nTraining Epoch: 2 [41856/60000]\tLoss: 0.5784\tLR: 0.100000\nTraining Epoch: 2 [41984/60000]\tLoss: 0.3557\tLR: 0.100000\nTraining Epoch: 2 [42112/60000]\tLoss: 0.3575\tLR: 0.100000\nTraining Epoch: 2 [42240/60000]\tLoss: 0.3452\tLR: 0.100000\nTraining Epoch: 2 [42368/60000]\tLoss: 0.4818\tLR: 0.100000\nTraining Epoch: 2 [42496/60000]\tLoss: 0.4760\tLR: 0.100000\nTraining Epoch: 2 [42624/60000]\tLoss: 0.5295\tLR: 0.100000\nTraining Epoch: 2 [42752/60000]\tLoss: 0.4652\tLR: 0.100000\nTraining Epoch: 2 [42880/60000]\tLoss: 0.4615\tLR: 0.100000\nTraining Epoch: 2 [43008/60000]\tLoss: 0.4940\tLR: 0.100000\nTraining Epoch: 2 [43136/60000]\tLoss: 0.3758\tLR: 0.100000\nTraining Epoch: 2 [43264/60000]\tLoss: 0.3270\tLR: 0.100000\nTraining Epoch: 2 [43392/60000]\tLoss: 0.3423\tLR: 0.100000\nTraining Epoch: 2 [43520/60000]\tLoss: 0.6110\tLR: 0.100000\nTraining Epoch: 2 [43648/60000]\tLoss: 0.3104\tLR: 0.100000\nTraining Epoch: 2 [43776/60000]\tLoss: 0.3208\tLR: 0.100000\nTraining Epoch: 2 [43904/60000]\tLoss: 0.5286\tLR: 0.100000\nTraining Epoch: 2 [44032/60000]\tLoss: 0.3218\tLR: 0.100000\nTraining Epoch: 2 [44160/60000]\tLoss: 0.3267\tLR: 0.100000\nTraining Epoch: 2 [44288/60000]\tLoss: 0.4917\tLR: 0.100000\nTraining Epoch: 2 [44416/60000]\tLoss: 0.4591\tLR: 0.100000\nTraining Epoch: 2 [44544/60000]\tLoss: 0.3045\tLR: 0.100000\nTraining Epoch: 2 [44672/60000]\tLoss: 0.4567\tLR: 0.100000\nTraining Epoch: 2 [44800/60000]\tLoss: 0.5044\tLR: 0.100000\nTraining Epoch: 2 [44928/60000]\tLoss: 0.4033\tLR: 0.100000\nTraining Epoch: 2 [45056/60000]\tLoss: 0.5932\tLR: 0.100000\nTraining Epoch: 2 [45184/60000]\tLoss: 0.4893\tLR: 0.100000\nTraining Epoch: 2 [45312/60000]\tLoss: 0.4218\tLR: 0.100000\nTraining Epoch: 2 [45440/60000]\tLoss: 0.2814\tLR: 0.100000\nTraining Epoch: 2 [45568/60000]\tLoss: 0.4461\tLR: 0.100000\nTraining Epoch: 2 [45696/60000]\tLoss: 0.3501\tLR: 0.100000\nTraining Epoch: 2 [45824/60000]\tLoss: 0.3942\tLR: 0.100000\nTraining Epoch: 2 [45952/60000]\tLoss: 0.5157\tLR: 0.100000\nTraining Epoch: 2 [46080/60000]\tLoss: 0.4751\tLR: 0.100000\nTraining Epoch: 2 [46208/60000]\tLoss: 0.4539\tLR: 0.100000\nTraining Epoch: 2 [46336/60000]\tLoss: 0.3123\tLR: 0.100000\nTraining Epoch: 2 [46464/60000]\tLoss: 0.4135\tLR: 0.100000\nTraining Epoch: 2 [46592/60000]\tLoss: 0.5825\tLR: 0.100000\nTraining Epoch: 2 [46720/60000]\tLoss: 0.4121\tLR: 0.100000\nTraining Epoch: 2 [46848/60000]\tLoss: 0.4051\tLR: 0.100000\nTraining Epoch: 2 [46976/60000]\tLoss: 0.3477\tLR: 0.100000\nTraining Epoch: 2 [47104/60000]\tLoss: 0.5845\tLR: 0.100000\nTraining Epoch: 2 [47232/60000]\tLoss: 0.5150\tLR: 0.100000\nTraining Epoch: 2 [47360/60000]\tLoss: 0.4491\tLR: 0.100000\nTraining Epoch: 2 [47488/60000]\tLoss: 0.4569\tLR: 0.100000\nTraining Epoch: 2 [47616/60000]\tLoss: 0.3589\tLR: 0.100000\nTraining Epoch: 2 [47744/60000]\tLoss: 0.4739\tLR: 0.100000\nTraining Epoch: 2 [47872/60000]\tLoss: 0.4448\tLR: 0.100000\nTraining Epoch: 2 [48000/60000]\tLoss: 0.5571\tLR: 0.100000\nTraining Epoch: 2 [48128/60000]\tLoss: 0.4486\tLR: 0.100000\nTraining Epoch: 2 [48256/60000]\tLoss: 0.3188\tLR: 0.100000\nTraining Epoch: 2 [48384/60000]\tLoss: 0.4327\tLR: 0.100000\nTraining Epoch: 2 [48512/60000]\tLoss: 0.4374\tLR: 0.100000\nTraining Epoch: 2 [48640/60000]\tLoss: 0.5037\tLR: 0.100000\nTraining Epoch: 2 [48768/60000]\tLoss: 0.5481\tLR: 0.100000\nTraining Epoch: 2 [48896/60000]\tLoss: 0.4622\tLR: 0.100000\nTraining Epoch: 2 [49024/60000]\tLoss: 0.4133\tLR: 0.100000\nTraining Epoch: 2 [49152/60000]\tLoss: 0.3996\tLR: 0.100000\nTraining Epoch: 2 [49280/60000]\tLoss: 0.5139\tLR: 0.100000\nTraining Epoch: 2 [49408/60000]\tLoss: 0.3636\tLR: 0.100000\nTraining Epoch: 2 [49536/60000]\tLoss: 0.3224\tLR: 0.100000\nTraining Epoch: 2 [49664/60000]\tLoss: 0.3091\tLR: 0.100000\nTraining Epoch: 2 [49792/60000]\tLoss: 0.5619\tLR: 0.100000\nTraining Epoch: 2 [49920/60000]\tLoss: 0.4958\tLR: 0.100000\nTraining Epoch: 2 [50048/60000]\tLoss: 0.6283\tLR: 0.100000\nTraining Epoch: 2 [50176/60000]\tLoss: 0.3592\tLR: 0.100000\nTraining Epoch: 2 [50304/60000]\tLoss: 0.5553\tLR: 0.100000\nTraining Epoch: 2 [50432/60000]\tLoss: 0.3969\tLR: 0.100000\nTraining Epoch: 2 [50560/60000]\tLoss: 0.3400\tLR: 0.100000\nTraining Epoch: 2 [50688/60000]\tLoss: 0.3885\tLR: 0.100000\nTraining Epoch: 2 [50816/60000]\tLoss: 0.3996\tLR: 0.100000\nTraining Epoch: 2 [50944/60000]\tLoss: 0.3670\tLR: 0.100000\nTraining Epoch: 2 [51072/60000]\tLoss: 0.3929\tLR: 0.100000\nTraining Epoch: 2 [51200/60000]\tLoss: 0.5123\tLR: 0.100000\nTraining Epoch: 2 [51328/60000]\tLoss: 0.4711\tLR: 0.100000\nTraining Epoch: 2 [51456/60000]\tLoss: 0.2953\tLR: 0.100000\nTraining Epoch: 2 [51584/60000]\tLoss: 0.4749\tLR: 0.100000\nTraining Epoch: 2 [51712/60000]\tLoss: 0.4528\tLR: 0.100000\nTraining Epoch: 2 [51840/60000]\tLoss: 0.5474\tLR: 0.100000\nTraining Epoch: 2 [51968/60000]\tLoss: 0.3379\tLR: 0.100000\nTraining Epoch: 2 [52096/60000]\tLoss: 0.3855\tLR: 0.100000\nTraining Epoch: 2 [52224/60000]\tLoss: 0.3489\tLR: 0.100000\nTraining Epoch: 2 [52352/60000]\tLoss: 0.2674\tLR: 0.100000\nTraining Epoch: 2 [52480/60000]\tLoss: 0.4003\tLR: 0.100000\nTraining Epoch: 2 [52608/60000]\tLoss: 0.3920\tLR: 0.100000\nTraining Epoch: 2 [52736/60000]\tLoss: 0.3270\tLR: 0.100000\nTraining Epoch: 2 [52864/60000]\tLoss: 0.3860\tLR: 0.100000\nTraining Epoch: 2 [52992/60000]\tLoss: 0.4454\tLR: 0.100000\nTraining Epoch: 2 [53120/60000]\tLoss: 0.4072\tLR: 0.100000\nTraining Epoch: 2 [53248/60000]\tLoss: 0.3227\tLR: 0.100000\nTraining Epoch: 2 [53376/60000]\tLoss: 0.4525\tLR: 0.100000\nTraining Epoch: 2 [53504/60000]\tLoss: 0.3157\tLR: 0.100000\nTraining Epoch: 2 [53632/60000]\tLoss: 0.4242\tLR: 0.100000\nTraining Epoch: 2 [53760/60000]\tLoss: 0.4760\tLR: 0.100000\nTraining Epoch: 2 [53888/60000]\tLoss: 0.4622\tLR: 0.100000\nTraining Epoch: 2 [54016/60000]\tLoss: 0.4111\tLR: 0.100000\nTraining Epoch: 2 [54144/60000]\tLoss: 0.4298\tLR: 0.100000\nTraining Epoch: 2 [54272/60000]\tLoss: 0.3091\tLR: 0.100000\nTraining Epoch: 2 [54400/60000]\tLoss: 0.4633\tLR: 0.100000\nTraining Epoch: 2 [54528/60000]\tLoss: 0.2767\tLR: 0.100000\nTraining Epoch: 2 [54656/60000]\tLoss: 0.4804\tLR: 0.100000\nTraining Epoch: 2 [54784/60000]\tLoss: 0.3082\tLR: 0.100000\nTraining Epoch: 2 [54912/60000]\tLoss: 0.4520\tLR: 0.100000\nTraining Epoch: 2 [55040/60000]\tLoss: 0.3143\tLR: 0.100000\nTraining Epoch: 2 [55168/60000]\tLoss: 0.3840\tLR: 0.100000\nTraining Epoch: 2 [55296/60000]\tLoss: 0.3997\tLR: 0.100000\nTraining Epoch: 2 [55424/60000]\tLoss: 0.3780\tLR: 0.100000\nTraining Epoch: 2 [55552/60000]\tLoss: 0.4506\tLR: 0.100000\nTraining Epoch: 2 [55680/60000]\tLoss: 0.5446\tLR: 0.100000\nTraining Epoch: 2 [55808/60000]\tLoss: 0.4314\tLR: 0.100000\nTraining Epoch: 2 [55936/60000]\tLoss: 0.4596\tLR: 0.100000\nTraining Epoch: 2 [56064/60000]\tLoss: 0.3015\tLR: 0.100000\nTraining Epoch: 2 [56192/60000]\tLoss: 0.4870\tLR: 0.100000\nTraining Epoch: 2 [56320/60000]\tLoss: 0.3380\tLR: 0.100000\nTraining Epoch: 2 [56448/60000]\tLoss: 0.3954\tLR: 0.100000\nTraining Epoch: 2 [56576/60000]\tLoss: 0.3559\tLR: 0.100000\nTraining Epoch: 2 [56704/60000]\tLoss: 0.6348\tLR: 0.100000\nTraining Epoch: 2 [56832/60000]\tLoss: 0.3900\tLR: 0.100000\nTraining Epoch: 2 [56960/60000]\tLoss: 0.3442\tLR: 0.100000\nTraining Epoch: 2 [57088/60000]\tLoss: 0.4884\tLR: 0.100000\nTraining Epoch: 2 [57216/60000]\tLoss: 0.4649\tLR: 0.100000\nTraining Epoch: 2 [57344/60000]\tLoss: 0.4486\tLR: 0.100000\nTraining Epoch: 2 [57472/60000]\tLoss: 0.3166\tLR: 0.100000\nTraining Epoch: 2 [57600/60000]\tLoss: 0.4864\tLR: 0.100000\nTraining Epoch: 2 [57728/60000]\tLoss: 0.4248\tLR: 0.100000\nTraining Epoch: 2 [57856/60000]\tLoss: 0.3321\tLR: 0.100000\nTraining Epoch: 2 [57984/60000]\tLoss: 0.2888\tLR: 0.100000\nTraining Epoch: 2 [58112/60000]\tLoss: 0.3679\tLR: 0.100000\nTraining Epoch: 2 [58240/60000]\tLoss: 0.5270\tLR: 0.100000\nTraining Epoch: 2 [58368/60000]\tLoss: 0.3253\tLR: 0.100000\nTraining Epoch: 2 [58496/60000]\tLoss: 0.2991\tLR: 0.100000\nTraining Epoch: 2 [58624/60000]\tLoss: 0.3692\tLR: 0.100000\nTraining Epoch: 2 [58752/60000]\tLoss: 0.5281\tLR: 0.100000\nTraining Epoch: 2 [58880/60000]\tLoss: 0.3113\tLR: 0.100000\nTraining Epoch: 2 [59008/60000]\tLoss: 0.3486\tLR: 0.100000\nTraining Epoch: 2 [59136/60000]\tLoss: 0.4848\tLR: 0.100000\nTraining Epoch: 2 [59264/60000]\tLoss: 0.4163\tLR: 0.100000\nTraining Epoch: 2 [59392/60000]\tLoss: 0.4036\tLR: 0.100000\nTraining Epoch: 2 [59520/60000]\tLoss: 0.4245\tLR: 0.100000\nTraining Epoch: 2 [59648/60000]\tLoss: 0.3953\tLR: 0.100000\nTraining Epoch: 2 [59776/60000]\tLoss: 0.3676\tLR: 0.100000\nTraining Epoch: 2 [59904/60000]\tLoss: 0.3403\tLR: 0.100000\nTraining Epoch: 2 [60000/60000]\tLoss: 0.3986\tLR: 0.100000\nTest set: Average loss: 0.0034, Accuracy: 0.8519\n\nTraining Epoch: 3 [128/60000]\tLoss: 0.3442\tLR: 0.100000\nTraining Epoch: 3 [256/60000]\tLoss: 0.5189\tLR: 0.100000\nTraining Epoch: 3 [384/60000]\tLoss: 0.4197\tLR: 0.100000\nTraining Epoch: 3 [512/60000]\tLoss: 0.3467\tLR: 0.100000\nTraining Epoch: 3 [640/60000]\tLoss: 0.3654\tLR: 0.100000\nTraining Epoch: 3 [768/60000]\tLoss: 0.4448\tLR: 0.100000\nTraining Epoch: 3 [896/60000]\tLoss: 0.3841\tLR: 0.100000\nTraining Epoch: 3 [1024/60000]\tLoss: 0.4517\tLR: 0.100000\nTraining Epoch: 3 [1152/60000]\tLoss: 0.4592\tLR: 0.100000\nTraining Epoch: 3 [1280/60000]\tLoss: 0.3932\tLR: 0.100000\nTraining Epoch: 3 [1408/60000]\tLoss: 0.3957\tLR: 0.100000\nTraining Epoch: 3 [1536/60000]\tLoss: 0.3449\tLR: 0.100000\nTraining Epoch: 3 [1664/60000]\tLoss: 0.3444\tLR: 0.100000\nTraining Epoch: 3 [1792/60000]\tLoss: 0.3896\tLR: 0.100000\nTraining Epoch: 3 [1920/60000]\tLoss: 0.4946\tLR: 0.100000\nTraining Epoch: 3 [2048/60000]\tLoss: 0.3847\tLR: 0.100000\nTraining Epoch: 3 [2176/60000]\tLoss: 0.5182\tLR: 0.100000\nTraining Epoch: 3 [2304/60000]\tLoss: 0.2995\tLR: 0.100000\nTraining Epoch: 3 [2432/60000]\tLoss: 0.5397\tLR: 0.100000\nTraining Epoch: 3 [2560/60000]\tLoss: 0.3749\tLR: 0.100000\nTraining Epoch: 3 [2688/60000]\tLoss: 0.3525\tLR: 0.100000\nTraining Epoch: 3 [2816/60000]\tLoss: 0.3316\tLR: 0.100000\nTraining Epoch: 3 [2944/60000]\tLoss: 0.3348\tLR: 0.100000\nTraining Epoch: 3 [3072/60000]\tLoss: 0.3369\tLR: 0.100000\nTraining Epoch: 3 [3200/60000]\tLoss: 0.3685\tLR: 0.100000\nTraining Epoch: 3 [3328/60000]\tLoss: 0.4368\tLR: 0.100000\nTraining Epoch: 3 [3456/60000]\tLoss: 0.4582\tLR: 0.100000\nTraining Epoch: 3 [3584/60000]\tLoss: 0.3581\tLR: 0.100000\nTraining Epoch: 3 [3712/60000]\tLoss: 0.3984\tLR: 0.100000\nTraining Epoch: 3 [3840/60000]\tLoss: 0.3934\tLR: 0.100000\nTraining Epoch: 3 [3968/60000]\tLoss: 0.3309\tLR: 0.100000\nTraining Epoch: 3 [4096/60000]\tLoss: 0.3575\tLR: 0.100000\nTraining Epoch: 3 [4224/60000]\tLoss: 0.3260\tLR: 0.100000\nTraining Epoch: 3 [4352/60000]\tLoss: 0.4825\tLR: 0.100000\nTraining Epoch: 3 [4480/60000]\tLoss: 0.3187\tLR: 0.100000\nTraining Epoch: 3 [4608/60000]\tLoss: 0.4600\tLR: 0.100000\nTraining Epoch: 3 [4736/60000]\tLoss: 0.4237\tLR: 0.100000\nTraining Epoch: 3 [4864/60000]\tLoss: 0.3754\tLR: 0.100000\nTraining Epoch: 3 [4992/60000]\tLoss: 0.3639\tLR: 0.100000\nTraining Epoch: 3 [5120/60000]\tLoss: 0.5832\tLR: 0.100000\nTraining Epoch: 3 [5248/60000]\tLoss: 0.3469\tLR: 0.100000\nTraining Epoch: 3 [5376/60000]\tLoss: 0.4525\tLR: 0.100000\nTraining Epoch: 3 [5504/60000]\tLoss: 0.4331\tLR: 0.100000\nTraining Epoch: 3 [5632/60000]\tLoss: 0.2984\tLR: 0.100000\nTraining Epoch: 3 [5760/60000]\tLoss: 0.2977\tLR: 0.100000\nTraining Epoch: 3 [5888/60000]\tLoss: 0.4251\tLR: 0.100000\nTraining Epoch: 3 [6016/60000]\tLoss: 0.3157\tLR: 0.100000\nTraining Epoch: 3 [6144/60000]\tLoss: 0.2850\tLR: 0.100000\nTraining Epoch: 3 [6272/60000]\tLoss: 0.4112\tLR: 0.100000\nTraining Epoch: 3 [6400/60000]\tLoss: 0.3511\tLR: 0.100000\nTraining Epoch: 3 [6528/60000]\tLoss: 0.5890\tLR: 0.100000\nTraining Epoch: 3 [6656/60000]\tLoss: 0.2824\tLR: 0.100000\nTraining Epoch: 3 [6784/60000]\tLoss: 0.3457\tLR: 0.100000\nTraining Epoch: 3 [6912/60000]\tLoss: 0.3428\tLR: 0.100000\nTraining Epoch: 3 [7040/60000]\tLoss: 0.4031\tLR: 0.100000\nTraining Epoch: 3 [7168/60000]\tLoss: 0.3971\tLR: 0.100000\nTraining Epoch: 3 [7296/60000]\tLoss: 0.3960\tLR: 0.100000\nTraining Epoch: 3 [7424/60000]\tLoss: 0.3452\tLR: 0.100000\nTraining Epoch: 3 [7552/60000]\tLoss: 0.4740\tLR: 0.100000\nTraining Epoch: 3 [7680/60000]\tLoss: 0.4109\tLR: 0.100000\nTraining Epoch: 3 [7808/60000]\tLoss: 0.4505\tLR: 0.100000\nTraining Epoch: 3 [7936/60000]\tLoss: 0.3980\tLR: 0.100000\nTraining Epoch: 3 [8064/60000]\tLoss: 0.3912\tLR: 0.100000\nTraining Epoch: 3 [8192/60000]\tLoss: 0.4339\tLR: 0.100000\nTraining Epoch: 3 [8320/60000]\tLoss: 0.3442\tLR: 0.100000\nTraining Epoch: 3 [8448/60000]\tLoss: 0.4156\tLR: 0.100000\nTraining Epoch: 3 [8576/60000]\tLoss: 0.4580\tLR: 0.100000\nTraining Epoch: 3 [8704/60000]\tLoss: 0.4107\tLR: 0.100000\nTraining Epoch: 3 [8832/60000]\tLoss: 0.4580\tLR: 0.100000\nTraining Epoch: 3 [8960/60000]\tLoss: 0.3659\tLR: 0.100000\nTraining Epoch: 3 [9088/60000]\tLoss: 0.4733\tLR: 0.100000\nTraining Epoch: 3 [9216/60000]\tLoss: 0.4655\tLR: 0.100000\nTraining Epoch: 3 [9344/60000]\tLoss: 0.3830\tLR: 0.100000\nTraining Epoch: 3 [9472/60000]\tLoss: 0.3435\tLR: 0.100000\nTraining Epoch: 3 [9600/60000]\tLoss: 0.5467\tLR: 0.100000\nTraining Epoch: 3 [9728/60000]\tLoss: 0.3722\tLR: 0.100000\nTraining Epoch: 3 [9856/60000]\tLoss: 0.5158\tLR: 0.100000\nTraining Epoch: 3 [9984/60000]\tLoss: 0.5202\tLR: 0.100000\nTraining Epoch: 3 [10112/60000]\tLoss: 0.4051\tLR: 0.100000\nTraining Epoch: 3 [10240/60000]\tLoss: 0.4504\tLR: 0.100000\nTraining Epoch: 3 [10368/60000]\tLoss: 0.3571\tLR: 0.100000\nTraining Epoch: 3 [10496/60000]\tLoss: 0.4078\tLR: 0.100000\nTraining Epoch: 3 [10624/60000]\tLoss: 0.4431\tLR: 0.100000\nTraining Epoch: 3 [10752/60000]\tLoss: 0.4957\tLR: 0.100000\nTraining Epoch: 3 [10880/60000]\tLoss: 0.3466\tLR: 0.100000\nTraining Epoch: 3 [11008/60000]\tLoss: 0.4550\tLR: 0.100000\nTraining Epoch: 3 [11136/60000]\tLoss: 0.3886\tLR: 0.100000\nTraining Epoch: 3 [11264/60000]\tLoss: 0.2576\tLR: 0.100000\nTraining Epoch: 3 [11392/60000]\tLoss: 0.4603\tLR: 0.100000\nTraining Epoch: 3 [11520/60000]\tLoss: 0.5260\tLR: 0.100000\nTraining Epoch: 3 [11648/60000]\tLoss: 0.4286\tLR: 0.100000\nTraining Epoch: 3 [11776/60000]\tLoss: 0.3206\tLR: 0.100000\nTraining Epoch: 3 [11904/60000]\tLoss: 0.2815\tLR: 0.100000\nTraining Epoch: 3 [12032/60000]\tLoss: 0.3720\tLR: 0.100000\nTraining Epoch: 3 [12160/60000]\tLoss: 0.3723\tLR: 0.100000\nTraining Epoch: 3 [12288/60000]\tLoss: 0.5843\tLR: 0.100000\nTraining Epoch: 3 [12416/60000]\tLoss: 0.4257\tLR: 0.100000\nTraining Epoch: 3 [12544/60000]\tLoss: 0.5979\tLR: 0.100000\nTraining Epoch: 3 [12672/60000]\tLoss: 0.3565\tLR: 0.100000\nTraining Epoch: 3 [12800/60000]\tLoss: 0.3559\tLR: 0.100000\nTraining Epoch: 3 [12928/60000]\tLoss: 0.5069\tLR: 0.100000\nTraining Epoch: 3 [13056/60000]\tLoss: 0.3177\tLR: 0.100000\nTraining Epoch: 3 [13184/60000]\tLoss: 0.4131\tLR: 0.100000\nTraining Epoch: 3 [13312/60000]\tLoss: 0.3924\tLR: 0.100000\nTraining Epoch: 3 [13440/60000]\tLoss: 0.3463\tLR: 0.100000\nTraining Epoch: 3 [13568/60000]\tLoss: 0.3574\tLR: 0.100000\nTraining Epoch: 3 [13696/60000]\tLoss: 0.3160\tLR: 0.100000\nTraining Epoch: 3 [13824/60000]\tLoss: 0.3950\tLR: 0.100000\nTraining Epoch: 3 [13952/60000]\tLoss: 0.2636\tLR: 0.100000\nTraining Epoch: 3 [14080/60000]\tLoss: 0.3754\tLR: 0.100000\nTraining Epoch: 3 [14208/60000]\tLoss: 0.3724\tLR: 0.100000\nTraining Epoch: 3 [14336/60000]\tLoss: 0.5598\tLR: 0.100000\nTraining Epoch: 3 [14464/60000]\tLoss: 0.4048\tLR: 0.100000\nTraining Epoch: 3 [14592/60000]\tLoss: 0.3600\tLR: 0.100000\nTraining Epoch: 3 [14720/60000]\tLoss: 0.3496\tLR: 0.100000\nTraining Epoch: 3 [14848/60000]\tLoss: 0.4456\tLR: 0.100000\nTraining Epoch: 3 [14976/60000]\tLoss: 0.4864\tLR: 0.100000\nTraining Epoch: 3 [15104/60000]\tLoss: 0.4830\tLR: 0.100000\nTraining Epoch: 3 [15232/60000]\tLoss: 0.4881\tLR: 0.100000\nTraining Epoch: 3 [15360/60000]\tLoss: 0.3667\tLR: 0.100000\nTraining Epoch: 3 [15488/60000]\tLoss: 0.4134\tLR: 0.100000\nTraining Epoch: 3 [15616/60000]\tLoss: 0.4607\tLR: 0.100000\nTraining Epoch: 3 [15744/60000]\tLoss: 0.3359\tLR: 0.100000\nTraining Epoch: 3 [15872/60000]\tLoss: 0.3678\tLR: 0.100000\nTraining Epoch: 3 [16000/60000]\tLoss: 0.4362\tLR: 0.100000\nTraining Epoch: 3 [16128/60000]\tLoss: 0.4490\tLR: 0.100000\nTraining Epoch: 3 [16256/60000]\tLoss: 0.4834\tLR: 0.100000\nTraining Epoch: 3 [16384/60000]\tLoss: 0.6143\tLR: 0.100000\nTraining Epoch: 3 [16512/60000]\tLoss: 0.4738\tLR: 0.100000\nTraining Epoch: 3 [16640/60000]\tLoss: 0.5545\tLR: 0.100000\nTraining Epoch: 3 [16768/60000]\tLoss: 0.3872\tLR: 0.100000\nTraining Epoch: 3 [16896/60000]\tLoss: 0.5362\tLR: 0.100000\nTraining Epoch: 3 [17024/60000]\tLoss: 0.2945\tLR: 0.100000\nTraining Epoch: 3 [17152/60000]\tLoss: 0.3580\tLR: 0.100000\nTraining Epoch: 3 [17280/60000]\tLoss: 0.4233\tLR: 0.100000\nTraining Epoch: 3 [17408/60000]\tLoss: 0.2752\tLR: 0.100000\nTraining Epoch: 3 [17536/60000]\tLoss: 0.3683\tLR: 0.100000\nTraining Epoch: 3 [17664/60000]\tLoss: 0.3998\tLR: 0.100000\nTraining Epoch: 3 [17792/60000]\tLoss: 0.4564\tLR: 0.100000\nTraining Epoch: 3 [17920/60000]\tLoss: 0.3847\tLR: 0.100000\nTraining Epoch: 3 [18048/60000]\tLoss: 0.3617\tLR: 0.100000\nTraining Epoch: 3 [18176/60000]\tLoss: 0.3929\tLR: 0.100000\nTraining Epoch: 3 [18304/60000]\tLoss: 0.3857\tLR: 0.100000\nTraining Epoch: 3 [18432/60000]\tLoss: 0.3117\tLR: 0.100000\nTraining Epoch: 3 [18560/60000]\tLoss: 0.3522\tLR: 0.100000\nTraining Epoch: 3 [18688/60000]\tLoss: 0.3593\tLR: 0.100000\nTraining Epoch: 3 [18816/60000]\tLoss: 0.2788\tLR: 0.100000\nTraining Epoch: 3 [18944/60000]\tLoss: 0.4911\tLR: 0.100000\nTraining Epoch: 3 [19072/60000]\tLoss: 0.2847\tLR: 0.100000\nTraining Epoch: 3 [19200/60000]\tLoss: 0.3719\tLR: 0.100000\nTraining Epoch: 3 [19328/60000]\tLoss: 0.2902\tLR: 0.100000\nTraining Epoch: 3 [19456/60000]\tLoss: 0.3951\tLR: 0.100000\nTraining Epoch: 3 [19584/60000]\tLoss: 0.4389\tLR: 0.100000\nTraining Epoch: 3 [19712/60000]\tLoss: 0.2753\tLR: 0.100000\nTraining Epoch: 3 [19840/60000]\tLoss: 0.2463\tLR: 0.100000\nTraining Epoch: 3 [19968/60000]\tLoss: 0.4079\tLR: 0.100000\nTraining Epoch: 3 [20096/60000]\tLoss: 0.3222\tLR: 0.100000\nTraining Epoch: 3 [20224/60000]\tLoss: 0.3047\tLR: 0.100000\nTraining Epoch: 3 [20352/60000]\tLoss: 0.4968\tLR: 0.100000\nTraining Epoch: 3 [20480/60000]\tLoss: 0.4335\tLR: 0.100000\nTraining Epoch: 3 [20608/60000]\tLoss: 0.3984\tLR: 0.100000\nTraining Epoch: 3 [20736/60000]\tLoss: 0.2947\tLR: 0.100000\nTraining Epoch: 3 [20864/60000]\tLoss: 0.4149\tLR: 0.100000\nTraining Epoch: 3 [20992/60000]\tLoss: 0.3586\tLR: 0.100000\nTraining Epoch: 3 [21120/60000]\tLoss: 0.3280\tLR: 0.100000\nTraining Epoch: 3 [21248/60000]\tLoss: 0.5000\tLR: 0.100000\nTraining Epoch: 3 [21376/60000]\tLoss: 0.3991\tLR: 0.100000\nTraining Epoch: 3 [21504/60000]\tLoss: 0.4603\tLR: 0.100000\nTraining Epoch: 3 [21632/60000]\tLoss: 0.5131\tLR: 0.100000\nTraining Epoch: 3 [21760/60000]\tLoss: 0.4107\tLR: 0.100000\nTraining Epoch: 3 [21888/60000]\tLoss: 0.3508\tLR: 0.100000\nTraining Epoch: 3 [22016/60000]\tLoss: 0.4776\tLR: 0.100000\nTraining Epoch: 3 [22144/60000]\tLoss: 0.3355\tLR: 0.100000\nTraining Epoch: 3 [22272/60000]\tLoss: 0.3139\tLR: 0.100000\nTraining Epoch: 3 [22400/60000]\tLoss: 0.4486\tLR: 0.100000\nTraining Epoch: 3 [22528/60000]\tLoss: 0.4189\tLR: 0.100000\nTraining Epoch: 3 [22656/60000]\tLoss: 0.4009\tLR: 0.100000\nTraining Epoch: 3 [22784/60000]\tLoss: 0.3646\tLR: 0.100000\nTraining Epoch: 3 [22912/60000]\tLoss: 0.3271\tLR: 0.100000\nTraining Epoch: 3 [23040/60000]\tLoss: 0.4800\tLR: 0.100000\nTraining Epoch: 3 [23168/60000]\tLoss: 0.4038\tLR: 0.100000\nTraining Epoch: 3 [23296/60000]\tLoss: 0.3305\tLR: 0.100000\nTraining Epoch: 3 [23424/60000]\tLoss: 0.5408\tLR: 0.100000\nTraining Epoch: 3 [23552/60000]\tLoss: 0.3191\tLR: 0.100000\nTraining Epoch: 3 [23680/60000]\tLoss: 0.4791\tLR: 0.100000\nTraining Epoch: 3 [23808/60000]\tLoss: 0.5343\tLR: 0.100000\nTraining Epoch: 3 [23936/60000]\tLoss: 0.4350\tLR: 0.100000\nTraining Epoch: 3 [24064/60000]\tLoss: 0.3593\tLR: 0.100000\nTraining Epoch: 3 [24192/60000]\tLoss: 0.4365\tLR: 0.100000\nTraining Epoch: 3 [24320/60000]\tLoss: 0.3829\tLR: 0.100000\nTraining Epoch: 3 [24448/60000]\tLoss: 0.3654\tLR: 0.100000\nTraining Epoch: 3 [24576/60000]\tLoss: 0.6268\tLR: 0.100000\nTraining Epoch: 3 [24704/60000]\tLoss: 0.3160\tLR: 0.100000\nTraining Epoch: 3 [24832/60000]\tLoss: 0.3480\tLR: 0.100000\nTraining Epoch: 3 [24960/60000]\tLoss: 0.3257\tLR: 0.100000\nTraining Epoch: 3 [25088/60000]\tLoss: 0.3361\tLR: 0.100000\nTraining Epoch: 3 [25216/60000]\tLoss: 0.2879\tLR: 0.100000\nTraining Epoch: 3 [25344/60000]\tLoss: 0.2457\tLR: 0.100000\nTraining Epoch: 3 [25472/60000]\tLoss: 0.3234\tLR: 0.100000\nTraining Epoch: 3 [25600/60000]\tLoss: 0.5652\tLR: 0.100000\nTraining Epoch: 3 [25728/60000]\tLoss: 0.3142\tLR: 0.100000\nTraining Epoch: 3 [25856/60000]\tLoss: 0.5634\tLR: 0.100000\nTraining Epoch: 3 [25984/60000]\tLoss: 0.3429\tLR: 0.100000\nTraining Epoch: 3 [26112/60000]\tLoss: 0.3321\tLR: 0.100000\nTraining Epoch: 3 [26240/60000]\tLoss: 0.4485\tLR: 0.100000\nTraining Epoch: 3 [26368/60000]\tLoss: 0.4433\tLR: 0.100000\nTraining Epoch: 3 [26496/60000]\tLoss: 0.3185\tLR: 0.100000\nTraining Epoch: 3 [26624/60000]\tLoss: 0.4478\tLR: 0.100000\nTraining Epoch: 3 [26752/60000]\tLoss: 0.3129\tLR: 0.100000\nTraining Epoch: 3 [26880/60000]\tLoss: 0.3366\tLR: 0.100000\nTraining Epoch: 3 [27008/60000]\tLoss: 0.3009\tLR: 0.100000\nTraining Epoch: 3 [27136/60000]\tLoss: 0.3131\tLR: 0.100000\nTraining Epoch: 3 [27264/60000]\tLoss: 0.3868\tLR: 0.100000\nTraining Epoch: 3 [27392/60000]\tLoss: 0.4268\tLR: 0.100000\nTraining Epoch: 3 [27520/60000]\tLoss: 0.4486\tLR: 0.100000\nTraining Epoch: 3 [27648/60000]\tLoss: 0.4345\tLR: 0.100000\nTraining Epoch: 3 [27776/60000]\tLoss: 0.4771\tLR: 0.100000\nTraining Epoch: 3 [27904/60000]\tLoss: 0.2924\tLR: 0.100000\nTraining Epoch: 3 [28032/60000]\tLoss: 0.3452\tLR: 0.100000\nTraining Epoch: 3 [28160/60000]\tLoss: 0.3541\tLR: 0.100000\nTraining Epoch: 3 [28288/60000]\tLoss: 0.3690\tLR: 0.100000\nTraining Epoch: 3 [28416/60000]\tLoss: 0.3882\tLR: 0.100000\nTraining Epoch: 3 [28544/60000]\tLoss: 0.5064\tLR: 0.100000\nTraining Epoch: 3 [28672/60000]\tLoss: 0.3277\tLR: 0.100000\nTraining Epoch: 3 [28800/60000]\tLoss: 0.3302\tLR: 0.100000\nTraining Epoch: 3 [28928/60000]\tLoss: 0.4607\tLR: 0.100000\nTraining Epoch: 3 [29056/60000]\tLoss: 0.4669\tLR: 0.100000\nTraining Epoch: 3 [29184/60000]\tLoss: 0.3684\tLR: 0.100000\nTraining Epoch: 3 [29312/60000]\tLoss: 0.2153\tLR: 0.100000\nTraining Epoch: 3 [29440/60000]\tLoss: 0.3265\tLR: 0.100000\nTraining Epoch: 3 [29568/60000]\tLoss: 0.4575\tLR: 0.100000\nTraining Epoch: 3 [29696/60000]\tLoss: 0.5009\tLR: 0.100000\nTraining Epoch: 3 [29824/60000]\tLoss: 0.4569\tLR: 0.100000\nTraining Epoch: 3 [29952/60000]\tLoss: 0.3467\tLR: 0.100000\nTraining Epoch: 3 [30080/60000]\tLoss: 0.3292\tLR: 0.100000\nTraining Epoch: 3 [30208/60000]\tLoss: 0.5319\tLR: 0.100000\nTraining Epoch: 3 [30336/60000]\tLoss: 0.3681\tLR: 0.100000\nTraining Epoch: 3 [30464/60000]\tLoss: 0.4409\tLR: 0.100000\nTraining Epoch: 3 [30592/60000]\tLoss: 0.2040\tLR: 0.100000\nTraining Epoch: 3 [30720/60000]\tLoss: 0.4311\tLR: 0.100000\nTraining Epoch: 3 [30848/60000]\tLoss: 0.2766\tLR: 0.100000\nTraining Epoch: 3 [30976/60000]\tLoss: 0.4027\tLR: 0.100000\nTraining Epoch: 3 [31104/60000]\tLoss: 0.4054\tLR: 0.100000\nTraining Epoch: 3 [31232/60000]\tLoss: 0.5054\tLR: 0.100000\nTraining Epoch: 3 [31360/60000]\tLoss: 0.2639\tLR: 0.100000\nTraining Epoch: 3 [31488/60000]\tLoss: 0.4020\tLR: 0.100000\nTraining Epoch: 3 [31616/60000]\tLoss: 0.3151\tLR: 0.100000\nTraining Epoch: 3 [31744/60000]\tLoss: 0.3713\tLR: 0.100000\nTraining Epoch: 3 [31872/60000]\tLoss: 0.3573\tLR: 0.100000\nTraining Epoch: 3 [32000/60000]\tLoss: 0.3355\tLR: 0.100000\nTraining Epoch: 3 [32128/60000]\tLoss: 0.2715\tLR: 0.100000\nTraining Epoch: 3 [32256/60000]\tLoss: 0.4123\tLR: 0.100000\nTraining Epoch: 3 [32384/60000]\tLoss: 0.3935\tLR: 0.100000\nTraining Epoch: 3 [32512/60000]\tLoss: 0.2820\tLR: 0.100000\nTraining Epoch: 3 [32640/60000]\tLoss: 0.3624\tLR: 0.100000\nTraining Epoch: 3 [32768/60000]\tLoss: 0.3643\tLR: 0.100000\nTraining Epoch: 3 [32896/60000]\tLoss: 0.3397\tLR: 0.100000\nTraining Epoch: 3 [33024/60000]\tLoss: 0.3609\tLR: 0.100000\nTraining Epoch: 3 [33152/60000]\tLoss: 0.3881\tLR: 0.100000\nTraining Epoch: 3 [33280/60000]\tLoss: 0.2749\tLR: 0.100000\nTraining Epoch: 3 [33408/60000]\tLoss: 0.3009\tLR: 0.100000\nTraining Epoch: 3 [33536/60000]\tLoss: 0.4870\tLR: 0.100000\nTraining Epoch: 3 [33664/60000]\tLoss: 0.2705\tLR: 0.100000\nTraining Epoch: 3 [33792/60000]\tLoss: 0.5467\tLR: 0.100000\nTraining Epoch: 3 [33920/60000]\tLoss: 0.4056\tLR: 0.100000\nTraining Epoch: 3 [34048/60000]\tLoss: 0.3048\tLR: 0.100000\nTraining Epoch: 3 [34176/60000]\tLoss: 0.4650\tLR: 0.100000\nTraining Epoch: 3 [34304/60000]\tLoss: 0.3521\tLR: 0.100000\nTraining Epoch: 3 [34432/60000]\tLoss: 0.3402\tLR: 0.100000\nTraining Epoch: 3 [34560/60000]\tLoss: 0.3657\tLR: 0.100000\nTraining Epoch: 3 [34688/60000]\tLoss: 0.3655\tLR: 0.100000\nTraining Epoch: 3 [34816/60000]\tLoss: 0.3743\tLR: 0.100000\nTraining Epoch: 3 [34944/60000]\tLoss: 0.4175\tLR: 0.100000\nTraining Epoch: 3 [35072/60000]\tLoss: 0.3238\tLR: 0.100000\nTraining Epoch: 3 [35200/60000]\tLoss: 0.2920\tLR: 0.100000\nTraining Epoch: 3 [35328/60000]\tLoss: 0.3176\tLR: 0.100000\nTraining Epoch: 3 [35456/60000]\tLoss: 0.3709\tLR: 0.100000\nTraining Epoch: 3 [35584/60000]\tLoss: 0.3018\tLR: 0.100000\nTraining Epoch: 3 [35712/60000]\tLoss: 0.3783\tLR: 0.100000\nTraining Epoch: 3 [35840/60000]\tLoss: 0.3414\tLR: 0.100000\nTraining Epoch: 3 [35968/60000]\tLoss: 0.3176\tLR: 0.100000\nTraining Epoch: 3 [36096/60000]\tLoss: 0.4251\tLR: 0.100000\nTraining Epoch: 3 [36224/60000]\tLoss: 0.3969\tLR: 0.100000\nTraining Epoch: 3 [36352/60000]\tLoss: 0.4965\tLR: 0.100000\nTraining Epoch: 3 [36480/60000]\tLoss: 0.3870\tLR: 0.100000\nTraining Epoch: 3 [36608/60000]\tLoss: 0.3104\tLR: 0.100000\nTraining Epoch: 3 [36736/60000]\tLoss: 0.4054\tLR: 0.100000\nTraining Epoch: 3 [36864/60000]\tLoss: 0.4454\tLR: 0.100000\nTraining Epoch: 3 [36992/60000]\tLoss: 0.4336\tLR: 0.100000\nTraining Epoch: 3 [37120/60000]\tLoss: 0.4119\tLR: 0.100000\nTraining Epoch: 3 [37248/60000]\tLoss: 0.3890\tLR: 0.100000\nTraining Epoch: 3 [37376/60000]\tLoss: 0.3425\tLR: 0.100000\nTraining Epoch: 3 [37504/60000]\tLoss: 0.3982\tLR: 0.100000\nTraining Epoch: 3 [37632/60000]\tLoss: 0.2417\tLR: 0.100000\nTraining Epoch: 3 [37760/60000]\tLoss: 0.3820\tLR: 0.100000\nTraining Epoch: 3 [37888/60000]\tLoss: 0.2931\tLR: 0.100000\nTraining Epoch: 3 [38016/60000]\tLoss: 0.3937\tLR: 0.100000\nTraining Epoch: 3 [38144/60000]\tLoss: 0.5061\tLR: 0.100000\nTraining Epoch: 3 [38272/60000]\tLoss: 0.3688\tLR: 0.100000\nTraining Epoch: 3 [38400/60000]\tLoss: 0.4680\tLR: 0.100000\nTraining Epoch: 3 [38528/60000]\tLoss: 0.3079\tLR: 0.100000\nTraining Epoch: 3 [38656/60000]\tLoss: 0.4207\tLR: 0.100000\nTraining Epoch: 3 [38784/60000]\tLoss: 0.4477\tLR: 0.100000\nTraining Epoch: 3 [38912/60000]\tLoss: 0.4700\tLR: 0.100000\nTraining Epoch: 3 [39040/60000]\tLoss: 0.2130\tLR: 0.100000\nTraining Epoch: 3 [39168/60000]\tLoss: 0.4845\tLR: 0.100000\nTraining Epoch: 3 [39296/60000]\tLoss: 0.3973\tLR: 0.100000\nTraining Epoch: 3 [39424/60000]\tLoss: 0.4083\tLR: 0.100000\nTraining Epoch: 3 [39552/60000]\tLoss: 0.3746\tLR: 0.100000\nTraining Epoch: 3 [39680/60000]\tLoss: 0.3790\tLR: 0.100000\nTraining Epoch: 3 [39808/60000]\tLoss: 0.2907\tLR: 0.100000\nTraining Epoch: 3 [39936/60000]\tLoss: 0.3918\tLR: 0.100000\nTraining Epoch: 3 [40064/60000]\tLoss: 0.4200\tLR: 0.100000\nTraining Epoch: 3 [40192/60000]\tLoss: 0.3673\tLR: 0.100000\nTraining Epoch: 3 [40320/60000]\tLoss: 0.3702\tLR: 0.100000\nTraining Epoch: 3 [40448/60000]\tLoss: 0.5709\tLR: 0.100000\nTraining Epoch: 3 [40576/60000]\tLoss: 0.4812\tLR: 0.100000\nTraining Epoch: 3 [40704/60000]\tLoss: 0.5232\tLR: 0.100000\nTraining Epoch: 3 [40832/60000]\tLoss: 0.5048\tLR: 0.100000\nTraining Epoch: 3 [40960/60000]\tLoss: 0.3027\tLR: 0.100000\nTraining Epoch: 3 [41088/60000]\tLoss: 0.3469\tLR: 0.100000\nTraining Epoch: 3 [41216/60000]\tLoss: 0.5001\tLR: 0.100000\nTraining Epoch: 3 [41344/60000]\tLoss: 0.4229\tLR: 0.100000\nTraining Epoch: 3 [41472/60000]\tLoss: 0.2961\tLR: 0.100000\nTraining Epoch: 3 [41600/60000]\tLoss: 0.3619\tLR: 0.100000\nTraining Epoch: 3 [41728/60000]\tLoss: 0.4610\tLR: 0.100000\nTraining Epoch: 3 [41856/60000]\tLoss: 0.3467\tLR: 0.100000\nTraining Epoch: 3 [41984/60000]\tLoss: 0.3612\tLR: 0.100000\nTraining Epoch: 3 [42112/60000]\tLoss: 0.4756\tLR: 0.100000\nTraining Epoch: 3 [42240/60000]\tLoss: 0.3671\tLR: 0.100000\nTraining Epoch: 3 [42368/60000]\tLoss: 0.3432\tLR: 0.100000\nTraining Epoch: 3 [42496/60000]\tLoss: 0.5392\tLR: 0.100000\nTraining Epoch: 3 [42624/60000]\tLoss: 0.3266\tLR: 0.100000\nTraining Epoch: 3 [42752/60000]\tLoss: 0.3428\tLR: 0.100000\nTraining Epoch: 3 [42880/60000]\tLoss: 0.3682\tLR: 0.100000\nTraining Epoch: 3 [43008/60000]\tLoss: 0.4841\tLR: 0.100000\nTraining Epoch: 3 [43136/60000]\tLoss: 0.5487\tLR: 0.100000\nTraining Epoch: 3 [43264/60000]\tLoss: 0.2675\tLR: 0.100000\nTraining Epoch: 3 [43392/60000]\tLoss: 0.2631\tLR: 0.100000\nTraining Epoch: 3 [43520/60000]\tLoss: 0.3007\tLR: 0.100000\nTraining Epoch: 3 [43648/60000]\tLoss: 0.4695\tLR: 0.100000\nTraining Epoch: 3 [43776/60000]\tLoss: 0.3201\tLR: 0.100000\nTraining Epoch: 3 [43904/60000]\tLoss: 0.2699\tLR: 0.100000\nTraining Epoch: 3 [44032/60000]\tLoss: 0.3504\tLR: 0.100000\nTraining Epoch: 3 [44160/60000]\tLoss: 0.4502\tLR: 0.100000\nTraining Epoch: 3 [44288/60000]\tLoss: 0.4637\tLR: 0.100000\nTraining Epoch: 3 [44416/60000]\tLoss: 0.3328\tLR: 0.100000\nTraining Epoch: 3 [44544/60000]\tLoss: 0.3198\tLR: 0.100000\nTraining Epoch: 3 [44672/60000]\tLoss: 0.3712\tLR: 0.100000\nTraining Epoch: 3 [44800/60000]\tLoss: 0.2775\tLR: 0.100000\nTraining Epoch: 3 [44928/60000]\tLoss: 0.3404\tLR: 0.100000\nTraining Epoch: 3 [45056/60000]\tLoss: 0.4338\tLR: 0.100000\nTraining Epoch: 3 [45184/60000]\tLoss: 0.3797\tLR: 0.100000\nTraining Epoch: 3 [45312/60000]\tLoss: 0.4465\tLR: 0.100000\nTraining Epoch: 3 [45440/60000]\tLoss: 0.2538\tLR: 0.100000\nTraining Epoch: 3 [45568/60000]\tLoss: 0.3473\tLR: 0.100000\nTraining Epoch: 3 [45696/60000]\tLoss: 0.3127\tLR: 0.100000\nTraining Epoch: 3 [45824/60000]\tLoss: 0.4053\tLR: 0.100000\nTraining Epoch: 3 [45952/60000]\tLoss: 0.2986\tLR: 0.100000\nTraining Epoch: 3 [46080/60000]\tLoss: 0.2645\tLR: 0.100000\nTraining Epoch: 3 [46208/60000]\tLoss: 0.4440\tLR: 0.100000\nTraining Epoch: 3 [46336/60000]\tLoss: 0.3628\tLR: 0.100000\nTraining Epoch: 3 [46464/60000]\tLoss: 0.3416\tLR: 0.100000\nTraining Epoch: 3 [46592/60000]\tLoss: 0.3396\tLR: 0.100000\nTraining Epoch: 3 [46720/60000]\tLoss: 0.3958\tLR: 0.100000\nTraining Epoch: 3 [46848/60000]\tLoss: 0.4260\tLR: 0.100000\nTraining Epoch: 3 [46976/60000]\tLoss: 0.3690\tLR: 0.100000\nTraining Epoch: 3 [47104/60000]\tLoss: 0.3990\tLR: 0.100000\nTraining Epoch: 3 [47232/60000]\tLoss: 0.3110\tLR: 0.100000\nTraining Epoch: 3 [47360/60000]\tLoss: 0.3637\tLR: 0.100000\nTraining Epoch: 3 [47488/60000]\tLoss: 0.4049\tLR: 0.100000\nTraining Epoch: 3 [47616/60000]\tLoss: 0.3395\tLR: 0.100000\nTraining Epoch: 3 [47744/60000]\tLoss: 0.4032\tLR: 0.100000\nTraining Epoch: 3 [47872/60000]\tLoss: 0.3177\tLR: 0.100000\nTraining Epoch: 3 [48000/60000]\tLoss: 0.3896\tLR: 0.100000\nTraining Epoch: 3 [48128/60000]\tLoss: 0.4219\tLR: 0.100000\nTraining Epoch: 3 [48256/60000]\tLoss: 0.4475\tLR: 0.100000\nTraining Epoch: 3 [48384/60000]\tLoss: 0.3331\tLR: 0.100000\nTraining Epoch: 3 [48512/60000]\tLoss: 0.5812\tLR: 0.100000\nTraining Epoch: 3 [48640/60000]\tLoss: 0.2476\tLR: 0.100000\nTraining Epoch: 3 [48768/60000]\tLoss: 0.4443\tLR: 0.100000\nTraining Epoch: 3 [48896/60000]\tLoss: 0.4135\tLR: 0.100000\nTraining Epoch: 3 [49024/60000]\tLoss: 0.4936\tLR: 0.100000\nTraining Epoch: 3 [49152/60000]\tLoss: 0.4017\tLR: 0.100000\nTraining Epoch: 3 [49280/60000]\tLoss: 0.2983\tLR: 0.100000\nTraining Epoch: 3 [49408/60000]\tLoss: 0.4092\tLR: 0.100000\nTraining Epoch: 3 [49536/60000]\tLoss: 0.3914\tLR: 0.100000\nTraining Epoch: 3 [49664/60000]\tLoss: 0.3332\tLR: 0.100000\nTraining Epoch: 3 [49792/60000]\tLoss: 0.3824\tLR: 0.100000\nTraining Epoch: 3 [49920/60000]\tLoss: 0.4135\tLR: 0.100000\nTraining Epoch: 3 [50048/60000]\tLoss: 0.3186\tLR: 0.100000\nTraining Epoch: 3 [50176/60000]\tLoss: 0.3976\tLR: 0.100000\nTraining Epoch: 3 [50304/60000]\tLoss: 0.3508\tLR: 0.100000\nTraining Epoch: 3 [50432/60000]\tLoss: 0.3502\tLR: 0.100000\nTraining Epoch: 3 [50560/60000]\tLoss: 0.3957\tLR: 0.100000\nTraining Epoch: 3 [50688/60000]\tLoss: 0.5568\tLR: 0.100000\nTraining Epoch: 3 [50816/60000]\tLoss: 0.4706\tLR: 0.100000\nTraining Epoch: 3 [50944/60000]\tLoss: 0.3087\tLR: 0.100000\nTraining Epoch: 3 [51072/60000]\tLoss: 0.4232\tLR: 0.100000\nTraining Epoch: 3 [51200/60000]\tLoss: 0.3982\tLR: 0.100000\nTraining Epoch: 3 [51328/60000]\tLoss: 0.3322\tLR: 0.100000\nTraining Epoch: 3 [51456/60000]\tLoss: 0.2843\tLR: 0.100000\nTraining Epoch: 3 [51584/60000]\tLoss: 0.3081\tLR: 0.100000\nTraining Epoch: 3 [51712/60000]\tLoss: 0.3266\tLR: 0.100000\nTraining Epoch: 3 [51840/60000]\tLoss: 0.2732\tLR: 0.100000\nTraining Epoch: 3 [51968/60000]\tLoss: 0.3439\tLR: 0.100000\nTraining Epoch: 3 [52096/60000]\tLoss: 0.3506\tLR: 0.100000\nTraining Epoch: 3 [52224/60000]\tLoss: 0.2527\tLR: 0.100000\nTraining Epoch: 3 [52352/60000]\tLoss: 0.3475\tLR: 0.100000\nTraining Epoch: 3 [52480/60000]\tLoss: 0.2945\tLR: 0.100000\nTraining Epoch: 3 [52608/60000]\tLoss: 0.2886\tLR: 0.100000\nTraining Epoch: 3 [52736/60000]\tLoss: 0.3583\tLR: 0.100000\nTraining Epoch: 3 [52864/60000]\tLoss: 0.3755\tLR: 0.100000\nTraining Epoch: 3 [52992/60000]\tLoss: 0.2591\tLR: 0.100000\nTraining Epoch: 3 [53120/60000]\tLoss: 0.3373\tLR: 0.100000\nTraining Epoch: 3 [53248/60000]\tLoss: 0.3743\tLR: 0.100000\nTraining Epoch: 3 [53376/60000]\tLoss: 0.3512\tLR: 0.100000\nTraining Epoch: 3 [53504/60000]\tLoss: 0.2791\tLR: 0.100000\nTraining Epoch: 3 [53632/60000]\tLoss: 0.4562\tLR: 0.100000\nTraining Epoch: 3 [53760/60000]\tLoss: 0.3055\tLR: 0.100000\nTraining Epoch: 3 [53888/60000]\tLoss: 0.3427\tLR: 0.100000\nTraining Epoch: 3 [54016/60000]\tLoss: 0.3643\tLR: 0.100000\nTraining Epoch: 3 [54144/60000]\tLoss: 0.3513\tLR: 0.100000\nTraining Epoch: 3 [54272/60000]\tLoss: 0.4682\tLR: 0.100000\nTraining Epoch: 3 [54400/60000]\tLoss: 0.4743\tLR: 0.100000\nTraining Epoch: 3 [54528/60000]\tLoss: 0.4160\tLR: 0.100000\nTraining Epoch: 3 [54656/60000]\tLoss: 0.5100\tLR: 0.100000\nTraining Epoch: 3 [54784/60000]\tLoss: 0.4535\tLR: 0.100000\nTraining Epoch: 3 [54912/60000]\tLoss: 0.5033\tLR: 0.100000\nTraining Epoch: 3 [55040/60000]\tLoss: 0.4305\tLR: 0.100000\nTraining Epoch: 3 [55168/60000]\tLoss: 0.3729\tLR: 0.100000\nTraining Epoch: 3 [55296/60000]\tLoss: 0.3109\tLR: 0.100000\nTraining Epoch: 3 [55424/60000]\tLoss: 0.3373\tLR: 0.100000\nTraining Epoch: 3 [55552/60000]\tLoss: 0.3722\tLR: 0.100000\nTraining Epoch: 3 [55680/60000]\tLoss: 0.4203\tLR: 0.100000\nTraining Epoch: 3 [55808/60000]\tLoss: 0.3750\tLR: 0.100000\nTraining Epoch: 3 [55936/60000]\tLoss: 0.3709\tLR: 0.100000\nTraining Epoch: 3 [56064/60000]\tLoss: 0.4388\tLR: 0.100000\nTraining Epoch: 3 [56192/60000]\tLoss: 0.4510\tLR: 0.100000\nTraining Epoch: 3 [56320/60000]\tLoss: 0.3438\tLR: 0.100000\nTraining Epoch: 3 [56448/60000]\tLoss: 0.4659\tLR: 0.100000\nTraining Epoch: 3 [56576/60000]\tLoss: 0.3086\tLR: 0.100000\nTraining Epoch: 3 [56704/60000]\tLoss: 0.2914\tLR: 0.100000\nTraining Epoch: 3 [56832/60000]\tLoss: 0.4690\tLR: 0.100000\nTraining Epoch: 3 [56960/60000]\tLoss: 0.3225\tLR: 0.100000\nTraining Epoch: 3 [57088/60000]\tLoss: 0.2901\tLR: 0.100000\nTraining Epoch: 3 [57216/60000]\tLoss: 0.2127\tLR: 0.100000\nTraining Epoch: 3 [57344/60000]\tLoss: 0.3920\tLR: 0.100000\nTraining Epoch: 3 [57472/60000]\tLoss: 0.3471\tLR: 0.100000\nTraining Epoch: 3 [57600/60000]\tLoss: 0.3408\tLR: 0.100000\nTraining Epoch: 3 [57728/60000]\tLoss: 0.3402\tLR: 0.100000\nTraining Epoch: 3 [57856/60000]\tLoss: 0.2712\tLR: 0.100000\nTraining Epoch: 3 [57984/60000]\tLoss: 0.3813\tLR: 0.100000\nTraining Epoch: 3 [58112/60000]\tLoss: 0.4044\tLR: 0.100000\nTraining Epoch: 3 [58240/60000]\tLoss: 0.2813\tLR: 0.100000\nTraining Epoch: 3 [58368/60000]\tLoss: 0.4692\tLR: 0.100000\nTraining Epoch: 3 [58496/60000]\tLoss: 0.3593\tLR: 0.100000\nTraining Epoch: 3 [58624/60000]\tLoss: 0.3354\tLR: 0.100000\nTraining Epoch: 3 [58752/60000]\tLoss: 0.4278\tLR: 0.100000\nTraining Epoch: 3 [58880/60000]\tLoss: 0.3154\tLR: 0.100000\nTraining Epoch: 3 [59008/60000]\tLoss: 0.1923\tLR: 0.100000\nTraining Epoch: 3 [59136/60000]\tLoss: 0.3465\tLR: 0.100000\nTraining Epoch: 3 [59264/60000]\tLoss: 0.3942\tLR: 0.100000\nTraining Epoch: 3 [59392/60000]\tLoss: 0.3454\tLR: 0.100000\nTraining Epoch: 3 [59520/60000]\tLoss: 0.4455\tLR: 0.100000\nTraining Epoch: 3 [59648/60000]\tLoss: 0.3505\tLR: 0.100000\nTraining Epoch: 3 [59776/60000]\tLoss: 0.3293\tLR: 0.100000\nTraining Epoch: 3 [59904/60000]\tLoss: 0.2604\tLR: 0.100000\nTraining Epoch: 3 [60000/60000]\tLoss: 0.5019\tLR: 0.100000\nTest set: Average loss: 0.0030, Accuracy: 0.8649\n\nTraining Epoch: 4 [128/60000]\tLoss: 0.4101\tLR: 0.100000\nTraining Epoch: 4 [256/60000]\tLoss: 0.2614\tLR: 0.100000\nTraining Epoch: 4 [384/60000]\tLoss: 0.3806\tLR: 0.100000\nTraining Epoch: 4 [512/60000]\tLoss: 0.3228\tLR: 0.100000\nTraining Epoch: 4 [640/60000]\tLoss: 0.4657\tLR: 0.100000\nTraining Epoch: 4 [768/60000]\tLoss: 0.4495\tLR: 0.100000\nTraining Epoch: 4 [896/60000]\tLoss: 0.3738\tLR: 0.100000\nTraining Epoch: 4 [1024/60000]\tLoss: 0.5051\tLR: 0.100000\nTraining Epoch: 4 [1152/60000]\tLoss: 0.4380\tLR: 0.100000\nTraining Epoch: 4 [1280/60000]\tLoss: 0.3877\tLR: 0.100000\nTraining Epoch: 4 [1408/60000]\tLoss: 0.3057\tLR: 0.100000\nTraining Epoch: 4 [1536/60000]\tLoss: 0.3420\tLR: 0.100000\nTraining Epoch: 4 [1664/60000]\tLoss: 0.3044\tLR: 0.100000\nTraining Epoch: 4 [1792/60000]\tLoss: 0.2863\tLR: 0.100000\nTraining Epoch: 4 [1920/60000]\tLoss: 0.2366\tLR: 0.100000\nTraining Epoch: 4 [2048/60000]\tLoss: 0.3261\tLR: 0.100000\nTraining Epoch: 4 [2176/60000]\tLoss: 0.3955\tLR: 0.100000\nTraining Epoch: 4 [2304/60000]\tLoss: 0.2885\tLR: 0.100000\nTraining Epoch: 4 [2432/60000]\tLoss: 0.2259\tLR: 0.100000\nTraining Epoch: 4 [2560/60000]\tLoss: 0.4840\tLR: 0.100000\nTraining Epoch: 4 [2688/60000]\tLoss: 0.2420\tLR: 0.100000\nTraining Epoch: 4 [2816/60000]\tLoss: 0.3108\tLR: 0.100000\nTraining Epoch: 4 [2944/60000]\tLoss: 0.4319\tLR: 0.100000\nTraining Epoch: 4 [3072/60000]\tLoss: 0.3418\tLR: 0.100000\nTraining Epoch: 4 [3200/60000]\tLoss: 0.3635\tLR: 0.100000\nTraining Epoch: 4 [3328/60000]\tLoss: 0.4686\tLR: 0.100000\nTraining Epoch: 4 [3456/60000]\tLoss: 0.3046\tLR: 0.100000\nTraining Epoch: 4 [3584/60000]\tLoss: 0.3472\tLR: 0.100000\nTraining Epoch: 4 [3712/60000]\tLoss: 0.4026\tLR: 0.100000\nTraining Epoch: 4 [3840/60000]\tLoss: 0.2952\tLR: 0.100000\nTraining Epoch: 4 [3968/60000]\tLoss: 0.4012\tLR: 0.100000\nTraining Epoch: 4 [4096/60000]\tLoss: 0.5588\tLR: 0.100000\nTraining Epoch: 4 [4224/60000]\tLoss: 0.2720\tLR: 0.100000\nTraining Epoch: 4 [4352/60000]\tLoss: 0.3220\tLR: 0.100000\nTraining Epoch: 4 [4480/60000]\tLoss: 0.5308\tLR: 0.100000\nTraining Epoch: 4 [4608/60000]\tLoss: 0.4346\tLR: 0.100000\nTraining Epoch: 4 [4736/60000]\tLoss: 0.3420\tLR: 0.100000\nTraining Epoch: 4 [4864/60000]\tLoss: 0.4134\tLR: 0.100000\nTraining Epoch: 4 [4992/60000]\tLoss: 0.2252\tLR: 0.100000\nTraining Epoch: 4 [5120/60000]\tLoss: 0.2942\tLR: 0.100000\nTraining Epoch: 4 [5248/60000]\tLoss: 0.3912\tLR: 0.100000\nTraining Epoch: 4 [5376/60000]\tLoss: 0.3852\tLR: 0.100000\nTraining Epoch: 4 [5504/60000]\tLoss: 0.3624\tLR: 0.100000\nTraining Epoch: 4 [5632/60000]\tLoss: 0.4428\tLR: 0.100000\nTraining Epoch: 4 [5760/60000]\tLoss: 0.2507\tLR: 0.100000\nTraining Epoch: 4 [5888/60000]\tLoss: 0.2666\tLR: 0.100000\nTraining Epoch: 4 [6016/60000]\tLoss: 0.3804\tLR: 0.100000\nTraining Epoch: 4 [6144/60000]\tLoss: 0.3457\tLR: 0.100000\nTraining Epoch: 4 [6272/60000]\tLoss: 0.4117\tLR: 0.100000\nTraining Epoch: 4 [6400/60000]\tLoss: 0.3829\tLR: 0.100000\nTraining Epoch: 4 [6528/60000]\tLoss: 0.2717\tLR: 0.100000\nTraining Epoch: 4 [6656/60000]\tLoss: 0.3564\tLR: 0.100000\nTraining Epoch: 4 [6784/60000]\tLoss: 0.3350\tLR: 0.100000\nTraining Epoch: 4 [6912/60000]\tLoss: 0.3109\tLR: 0.100000\nTraining Epoch: 4 [7040/60000]\tLoss: 0.5116\tLR: 0.100000\nTraining Epoch: 4 [7168/60000]\tLoss: 0.3482\tLR: 0.100000\nTraining Epoch: 4 [7296/60000]\tLoss: 0.2498\tLR: 0.100000\nTraining Epoch: 4 [7424/60000]\tLoss: 0.3252\tLR: 0.100000\nTraining Epoch: 4 [7552/60000]\tLoss: 0.3652\tLR: 0.100000\nTraining Epoch: 4 [7680/60000]\tLoss: 0.2229\tLR: 0.100000\nTraining Epoch: 4 [7808/60000]\tLoss: 0.4628\tLR: 0.100000\nTraining Epoch: 4 [7936/60000]\tLoss: 0.3140\tLR: 0.100000\nTraining Epoch: 4 [8064/60000]\tLoss: 0.2475\tLR: 0.100000\nTraining Epoch: 4 [8192/60000]\tLoss: 0.1890\tLR: 0.100000\nTraining Epoch: 4 [8320/60000]\tLoss: 0.4521\tLR: 0.100000\nTraining Epoch: 4 [8448/60000]\tLoss: 0.4282\tLR: 0.100000\nTraining Epoch: 4 [8576/60000]\tLoss: 0.3020\tLR: 0.100000\nTraining Epoch: 4 [8704/60000]\tLoss: 0.2118\tLR: 0.100000\nTraining Epoch: 4 [8832/60000]\tLoss: 0.3883\tLR: 0.100000\nTraining Epoch: 4 [8960/60000]\tLoss: 0.2959\tLR: 0.100000\nTraining Epoch: 4 [9088/60000]\tLoss: 0.2957\tLR: 0.100000\nTraining Epoch: 4 [9216/60000]\tLoss: 0.2821\tLR: 0.100000\nTraining Epoch: 4 [9344/60000]\tLoss: 0.3070\tLR: 0.100000\nTraining Epoch: 4 [9472/60000]\tLoss: 0.3391\tLR: 0.100000\nTraining Epoch: 4 [9600/60000]\tLoss: 0.2476\tLR: 0.100000\nTraining Epoch: 4 [9728/60000]\tLoss: 0.3439\tLR: 0.100000\nTraining Epoch: 4 [9856/60000]\tLoss: 0.3702\tLR: 0.100000\nTraining Epoch: 4 [9984/60000]\tLoss: 0.2809\tLR: 0.100000\nTraining Epoch: 4 [10112/60000]\tLoss: 0.2288\tLR: 0.100000\nTraining Epoch: 4 [10240/60000]\tLoss: 0.3937\tLR: 0.100000\nTraining Epoch: 4 [10368/60000]\tLoss: 0.4869\tLR: 0.100000\nTraining Epoch: 4 [10496/60000]\tLoss: 0.3750\tLR: 0.100000\nTraining Epoch: 4 [10624/60000]\tLoss: 0.3816\tLR: 0.100000\nTraining Epoch: 4 [10752/60000]\tLoss: 0.2595\tLR: 0.100000\nTraining Epoch: 4 [10880/60000]\tLoss: 0.2818\tLR: 0.100000\nTraining Epoch: 4 [11008/60000]\tLoss: 0.3469\tLR: 0.100000\nTraining Epoch: 4 [11136/60000]\tLoss: 0.5826\tLR: 0.100000\nTraining Epoch: 4 [11264/60000]\tLoss: 0.2809\tLR: 0.100000\nTraining Epoch: 4 [11392/60000]\tLoss: 0.4679\tLR: 0.100000\nTraining Epoch: 4 [11520/60000]\tLoss: 0.4201\tLR: 0.100000\nTraining Epoch: 4 [11648/60000]\tLoss: 0.1910\tLR: 0.100000\nTraining Epoch: 4 [11776/60000]\tLoss: 0.4411\tLR: 0.100000\nTraining Epoch: 4 [11904/60000]\tLoss: 0.3035\tLR: 0.100000\nTraining Epoch: 4 [12032/60000]\tLoss: 0.4224\tLR: 0.100000\nTraining Epoch: 4 [12160/60000]\tLoss: 0.4131\tLR: 0.100000\nTraining Epoch: 4 [12288/60000]\tLoss: 0.3547\tLR: 0.100000\nTraining Epoch: 4 [12416/60000]\tLoss: 0.4403\tLR: 0.100000\nTraining Epoch: 4 [12544/60000]\tLoss: 0.2611\tLR: 0.100000\nTraining Epoch: 4 [12672/60000]\tLoss: 0.4249\tLR: 0.100000\nTraining Epoch: 4 [12800/60000]\tLoss: 0.3895\tLR: 0.100000\nTraining Epoch: 4 [12928/60000]\tLoss: 0.3156\tLR: 0.100000\nTraining Epoch: 4 [13056/60000]\tLoss: 0.4912\tLR: 0.100000\nTraining Epoch: 4 [13184/60000]\tLoss: 0.3679\tLR: 0.100000\nTraining Epoch: 4 [13312/60000]\tLoss: 0.2294\tLR: 0.100000\nTraining Epoch: 4 [13440/60000]\tLoss: 0.3073\tLR: 0.100000\nTraining Epoch: 4 [13568/60000]\tLoss: 0.3785\tLR: 0.100000\nTraining Epoch: 4 [13696/60000]\tLoss: 0.2993\tLR: 0.100000\nTraining Epoch: 4 [13824/60000]\tLoss: 0.2897\tLR: 0.100000\nTraining Epoch: 4 [13952/60000]\tLoss: 0.5030\tLR: 0.100000\nTraining Epoch: 4 [14080/60000]\tLoss: 0.3264\tLR: 0.100000\nTraining Epoch: 4 [14208/60000]\tLoss: 0.2843\tLR: 0.100000\nTraining Epoch: 4 [14336/60000]\tLoss: 0.2654\tLR: 0.100000\nTraining Epoch: 4 [14464/60000]\tLoss: 0.3838\tLR: 0.100000\nTraining Epoch: 4 [14592/60000]\tLoss: 0.3476\tLR: 0.100000\nTraining Epoch: 4 [14720/60000]\tLoss: 0.3934\tLR: 0.100000\nTraining Epoch: 4 [14848/60000]\tLoss: 0.2619\tLR: 0.100000\nTraining Epoch: 4 [14976/60000]\tLoss: 0.4637\tLR: 0.100000\nTraining Epoch: 4 [15104/60000]\tLoss: 0.3062\tLR: 0.100000\nTraining Epoch: 4 [15232/60000]\tLoss: 0.2093\tLR: 0.100000\nTraining Epoch: 4 [15360/60000]\tLoss: 0.2861\tLR: 0.100000\nTraining Epoch: 4 [15488/60000]\tLoss: 0.4127\tLR: 0.100000\nTraining Epoch: 4 [15616/60000]\tLoss: 0.3155\tLR: 0.100000\nTraining Epoch: 4 [15744/60000]\tLoss: 0.3419\tLR: 0.100000\nTraining Epoch: 4 [15872/60000]\tLoss: 0.4098\tLR: 0.100000\nTraining Epoch: 4 [16000/60000]\tLoss: 0.3311\tLR: 0.100000\nTraining Epoch: 4 [16128/60000]\tLoss: 0.2002\tLR: 0.100000\nTraining Epoch: 4 [16256/60000]\tLoss: 0.4465\tLR: 0.100000\nTraining Epoch: 4 [16384/60000]\tLoss: 0.4241\tLR: 0.100000\nTraining Epoch: 4 [16512/60000]\tLoss: 0.3296\tLR: 0.100000\nTraining Epoch: 4 [16640/60000]\tLoss: 0.3919\tLR: 0.100000\nTraining Epoch: 4 [16768/60000]\tLoss: 0.3753\tLR: 0.100000\nTraining Epoch: 4 [16896/60000]\tLoss: 0.4468\tLR: 0.100000\nTraining Epoch: 4 [17024/60000]\tLoss: 0.4303\tLR: 0.100000\nTraining Epoch: 4 [17152/60000]\tLoss: 0.3528\tLR: 0.100000\nTraining Epoch: 4 [17280/60000]\tLoss: 0.3691\tLR: 0.100000\nTraining Epoch: 4 [17408/60000]\tLoss: 0.4247\tLR: 0.100000\nTraining Epoch: 4 [17536/60000]\tLoss: 0.2659\tLR: 0.100000\nTraining Epoch: 4 [17664/60000]\tLoss: 0.2304\tLR: 0.100000\nTraining Epoch: 4 [17792/60000]\tLoss: 0.3552\tLR: 0.100000\nTraining Epoch: 4 [17920/60000]\tLoss: 0.3773\tLR: 0.100000\nTraining Epoch: 4 [18048/60000]\tLoss: 0.3550\tLR: 0.100000\nTraining Epoch: 4 [18176/60000]\tLoss: 0.3404\tLR: 0.100000\nTraining Epoch: 4 [18304/60000]\tLoss: 0.4092\tLR: 0.100000\nTraining Epoch: 4 [18432/60000]\tLoss: 0.4248\tLR: 0.100000\nTraining Epoch: 4 [18560/60000]\tLoss: 0.3689\tLR: 0.100000\nTraining Epoch: 4 [18688/60000]\tLoss: 0.2742\tLR: 0.100000\nTraining Epoch: 4 [18816/60000]\tLoss: 0.2399\tLR: 0.100000\nTraining Epoch: 4 [18944/60000]\tLoss: 0.3156\tLR: 0.100000\nTraining Epoch: 4 [19072/60000]\tLoss: 0.4812\tLR: 0.100000\nTraining Epoch: 4 [19200/60000]\tLoss: 0.3098\tLR: 0.100000\nTraining Epoch: 4 [19328/60000]\tLoss: 0.3648\tLR: 0.100000\nTraining Epoch: 4 [19456/60000]\tLoss: 0.3753\tLR: 0.100000\nTraining Epoch: 4 [19584/60000]\tLoss: 0.2923\tLR: 0.100000\nTraining Epoch: 4 [19712/60000]\tLoss: 0.2369\tLR: 0.100000\nTraining Epoch: 4 [19840/60000]\tLoss: 0.2776\tLR: 0.100000\nTraining Epoch: 4 [19968/60000]\tLoss: 0.3061\tLR: 0.100000\nTraining Epoch: 4 [20096/60000]\tLoss: 0.2925\tLR: 0.100000\nTraining Epoch: 4 [20224/60000]\tLoss: 0.2200\tLR: 0.100000\nTraining Epoch: 4 [20352/60000]\tLoss: 0.4289\tLR: 0.100000\nTraining Epoch: 4 [20480/60000]\tLoss: 0.3782\tLR: 0.100000\nTraining Epoch: 4 [20608/60000]\tLoss: 0.2698\tLR: 0.100000\nTraining Epoch: 4 [20736/60000]\tLoss: 0.3314\tLR: 0.100000\nTraining Epoch: 4 [20864/60000]\tLoss: 0.3519\tLR: 0.100000\nTraining Epoch: 4 [20992/60000]\tLoss: 0.2693\tLR: 0.100000\nTraining Epoch: 4 [21120/60000]\tLoss: 0.3110\tLR: 0.100000\nTraining Epoch: 4 [21248/60000]\tLoss: 0.4677\tLR: 0.100000\nTraining Epoch: 4 [21376/60000]\tLoss: 0.3615\tLR: 0.100000\nTraining Epoch: 4 [21504/60000]\tLoss: 0.3051\tLR: 0.100000\nTraining Epoch: 4 [21632/60000]\tLoss: 0.4207\tLR: 0.100000\nTraining Epoch: 4 [21760/60000]\tLoss: 0.4030\tLR: 0.100000\nTraining Epoch: 4 [21888/60000]\tLoss: 0.3311\tLR: 0.100000\nTraining Epoch: 4 [22016/60000]\tLoss: 0.2396\tLR: 0.100000\nTraining Epoch: 4 [22144/60000]\tLoss: 0.3028\tLR: 0.100000\nTraining Epoch: 4 [22272/60000]\tLoss: 0.3937\tLR: 0.100000\nTraining Epoch: 4 [22400/60000]\tLoss: 0.3489\tLR: 0.100000\nTraining Epoch: 4 [22528/60000]\tLoss: 0.3110\tLR: 0.100000\nTraining Epoch: 4 [22656/60000]\tLoss: 0.2406\tLR: 0.100000\nTraining Epoch: 4 [22784/60000]\tLoss: 0.3311\tLR: 0.100000\nTraining Epoch: 4 [22912/60000]\tLoss: 0.4518\tLR: 0.100000\nTraining Epoch: 4 [23040/60000]\tLoss: 0.3740\tLR: 0.100000\nTraining Epoch: 4 [23168/60000]\tLoss: 0.4504\tLR: 0.100000\nTraining Epoch: 4 [23296/60000]\tLoss: 0.2712\tLR: 0.100000\nTraining Epoch: 4 [23424/60000]\tLoss: 0.2834\tLR: 0.100000\nTraining Epoch: 4 [23552/60000]\tLoss: 0.3296\tLR: 0.100000\nTraining Epoch: 4 [23680/60000]\tLoss: 0.3999\tLR: 0.100000\nTraining Epoch: 4 [23808/60000]\tLoss: 0.3761\tLR: 0.100000\nTraining Epoch: 4 [23936/60000]\tLoss: 0.2301\tLR: 0.100000\nTraining Epoch: 4 [24064/60000]\tLoss: 0.2913\tLR: 0.100000\nTraining Epoch: 4 [24192/60000]\tLoss: 0.3832\tLR: 0.100000\nTraining Epoch: 4 [24320/60000]\tLoss: 0.3366\tLR: 0.100000\nTraining Epoch: 4 [24448/60000]\tLoss: 0.3700\tLR: 0.100000\nTraining Epoch: 4 [24576/60000]\tLoss: 0.4000\tLR: 0.100000\nTraining Epoch: 4 [24704/60000]\tLoss: 0.2588\tLR: 0.100000\nTraining Epoch: 4 [24832/60000]\tLoss: 0.3922\tLR: 0.100000\nTraining Epoch: 4 [24960/60000]\tLoss: 0.2979\tLR: 0.100000\nTraining Epoch: 4 [25088/60000]\tLoss: 0.2998\tLR: 0.100000\nTraining Epoch: 4 [25216/60000]\tLoss: 0.2767\tLR: 0.100000\nTraining Epoch: 4 [25344/60000]\tLoss: 0.3656\tLR: 0.100000\nTraining Epoch: 4 [25472/60000]\tLoss: 0.3487\tLR: 0.100000\nTraining Epoch: 4 [25600/60000]\tLoss: 0.4602\tLR: 0.100000\nTraining Epoch: 4 [25728/60000]\tLoss: 0.2890\tLR: 0.100000\nTraining Epoch: 4 [25856/60000]\tLoss: 0.3781\tLR: 0.100000\nTraining Epoch: 4 [25984/60000]\tLoss: 0.4578\tLR: 0.100000\nTraining Epoch: 4 [26112/60000]\tLoss: 0.2994\tLR: 0.100000\nTraining Epoch: 4 [26240/60000]\tLoss: 0.2494\tLR: 0.100000\nTraining Epoch: 4 [26368/60000]\tLoss: 0.4921\tLR: 0.100000\nTraining Epoch: 4 [26496/60000]\tLoss: 0.2866\tLR: 0.100000\nTraining Epoch: 4 [26624/60000]\tLoss: 0.3355\tLR: 0.100000\nTraining Epoch: 4 [26752/60000]\tLoss: 0.3895\tLR: 0.100000\nTraining Epoch: 4 [26880/60000]\tLoss: 0.2652\tLR: 0.100000\nTraining Epoch: 4 [27008/60000]\tLoss: 0.3608\tLR: 0.100000\nTraining Epoch: 4 [27136/60000]\tLoss: 0.3186\tLR: 0.100000\nTraining Epoch: 4 [27264/60000]\tLoss: 0.2960\tLR: 0.100000\nTraining Epoch: 4 [27392/60000]\tLoss: 0.3153\tLR: 0.100000\nTraining Epoch: 4 [27520/60000]\tLoss: 0.3697\tLR: 0.100000\nTraining Epoch: 4 [27648/60000]\tLoss: 0.4638\tLR: 0.100000\nTraining Epoch: 4 [27776/60000]\tLoss: 0.3089\tLR: 0.100000\nTraining Epoch: 4 [27904/60000]\tLoss: 0.2763\tLR: 0.100000\nTraining Epoch: 4 [28032/60000]\tLoss: 0.4803\tLR: 0.100000\nTraining Epoch: 4 [28160/60000]\tLoss: 0.4060\tLR: 0.100000\nTraining Epoch: 4 [28288/60000]\tLoss: 0.5142\tLR: 0.100000\nTraining Epoch: 4 [28416/60000]\tLoss: 0.3678\tLR: 0.100000\nTraining Epoch: 4 [28544/60000]\tLoss: 0.2963\tLR: 0.100000\nTraining Epoch: 4 [28672/60000]\tLoss: 0.4573\tLR: 0.100000\nTraining Epoch: 4 [28800/60000]\tLoss: 0.4035\tLR: 0.100000\nTraining Epoch: 4 [28928/60000]\tLoss: 0.3700\tLR: 0.100000\nTraining Epoch: 4 [29056/60000]\tLoss: 0.3388\tLR: 0.100000\nTraining Epoch: 4 [29184/60000]\tLoss: 0.3198\tLR: 0.100000\nTraining Epoch: 4 [29312/60000]\tLoss: 0.4140\tLR: 0.100000\nTraining Epoch: 4 [29440/60000]\tLoss: 0.3898\tLR: 0.100000\nTraining Epoch: 4 [29568/60000]\tLoss: 0.3870\tLR: 0.100000\nTraining Epoch: 4 [29696/60000]\tLoss: 0.3356\tLR: 0.100000\nTraining Epoch: 4 [29824/60000]\tLoss: 0.4370\tLR: 0.100000\nTraining Epoch: 4 [29952/60000]\tLoss: 0.4264\tLR: 0.100000\nTraining Epoch: 4 [30080/60000]\tLoss: 0.3800\tLR: 0.100000\nTraining Epoch: 4 [30208/60000]\tLoss: 0.3463\tLR: 0.100000\nTraining Epoch: 4 [30336/60000]\tLoss: 0.3622\tLR: 0.100000\nTraining Epoch: 4 [30464/60000]\tLoss: 0.5305\tLR: 0.100000\nTraining Epoch: 4 [30592/60000]\tLoss: 0.2627\tLR: 0.100000\nTraining Epoch: 4 [30720/60000]\tLoss: 0.3249\tLR: 0.100000\nTraining Epoch: 4 [30848/60000]\tLoss: 0.4837\tLR: 0.100000\nTraining Epoch: 4 [30976/60000]\tLoss: 0.3087\tLR: 0.100000\nTraining Epoch: 4 [31104/60000]\tLoss: 0.3490\tLR: 0.100000\nTraining Epoch: 4 [31232/60000]\tLoss: 0.4541\tLR: 0.100000\nTraining Epoch: 4 [31360/60000]\tLoss: 0.2316\tLR: 0.100000\nTraining Epoch: 4 [31488/60000]\tLoss: 0.4036\tLR: 0.100000\nTraining Epoch: 4 [31616/60000]\tLoss: 0.3046\tLR: 0.100000\nTraining Epoch: 4 [31744/60000]\tLoss: 0.3142\tLR: 0.100000\nTraining Epoch: 4 [31872/60000]\tLoss: 0.4447\tLR: 0.100000\nTraining Epoch: 4 [32000/60000]\tLoss: 0.2209\tLR: 0.100000\nTraining Epoch: 4 [32128/60000]\tLoss: 0.2896\tLR: 0.100000\nTraining Epoch: 4 [32256/60000]\tLoss: 0.3279\tLR: 0.100000\nTraining Epoch: 4 [32384/60000]\tLoss: 0.2949\tLR: 0.100000\nTraining Epoch: 4 [32512/60000]\tLoss: 0.3203\tLR: 0.100000\nTraining Epoch: 4 [32640/60000]\tLoss: 0.4624\tLR: 0.100000\nTraining Epoch: 4 [32768/60000]\tLoss: 0.3777\tLR: 0.100000\nTraining Epoch: 4 [32896/60000]\tLoss: 0.3289\tLR: 0.100000\nTraining Epoch: 4 [33024/60000]\tLoss: 0.3986\tLR: 0.100000\nTraining Epoch: 4 [33152/60000]\tLoss: 0.4345\tLR: 0.100000\nTraining Epoch: 4 [33280/60000]\tLoss: 0.2821\tLR: 0.100000\nTraining Epoch: 4 [33408/60000]\tLoss: 0.2871\tLR: 0.100000\nTraining Epoch: 4 [33536/60000]\tLoss: 0.4909\tLR: 0.100000\nTraining Epoch: 4 [33664/60000]\tLoss: 0.3847\tLR: 0.100000\nTraining Epoch: 4 [33792/60000]\tLoss: 0.3728\tLR: 0.100000\nTraining Epoch: 4 [33920/60000]\tLoss: 0.4236\tLR: 0.100000\nTraining Epoch: 4 [34048/60000]\tLoss: 0.3616\tLR: 0.100000\nTraining Epoch: 4 [34176/60000]\tLoss: 0.3297\tLR: 0.100000\nTraining Epoch: 4 [34304/60000]\tLoss: 0.4438\tLR: 0.100000\nTraining Epoch: 4 [34432/60000]\tLoss: 0.4124\tLR: 0.100000\nTraining Epoch: 4 [34560/60000]\tLoss: 0.3949\tLR: 0.100000\nTraining Epoch: 4 [34688/60000]\tLoss: 0.3467\tLR: 0.100000\nTraining Epoch: 4 [34816/60000]\tLoss: 0.2946\tLR: 0.100000\nTraining Epoch: 4 [34944/60000]\tLoss: 0.3374\tLR: 0.100000\nTraining Epoch: 4 [35072/60000]\tLoss: 0.2342\tLR: 0.100000\nTraining Epoch: 4 [35200/60000]\tLoss: 0.4161\tLR: 0.100000\nTraining Epoch: 4 [35328/60000]\tLoss: 0.3629\tLR: 0.100000\nTraining Epoch: 4 [35456/60000]\tLoss: 0.2940\tLR: 0.100000\nTraining Epoch: 4 [35584/60000]\tLoss: 0.1860\tLR: 0.100000\nTraining Epoch: 4 [35712/60000]\tLoss: 0.4213\tLR: 0.100000\nTraining Epoch: 4 [35840/60000]\tLoss: 0.4003\tLR: 0.100000\nTraining Epoch: 4 [35968/60000]\tLoss: 0.2832\tLR: 0.100000\nTraining Epoch: 4 [36096/60000]\tLoss: 0.3685\tLR: 0.100000\nTraining Epoch: 4 [36224/60000]\tLoss: 0.3709\tLR: 0.100000\nTraining Epoch: 4 [36352/60000]\tLoss: 0.2907\tLR: 0.100000\nTraining Epoch: 4 [36480/60000]\tLoss: 0.2429\tLR: 0.100000\nTraining Epoch: 4 [36608/60000]\tLoss: 0.4493\tLR: 0.100000\nTraining Epoch: 4 [36736/60000]\tLoss: 0.4541\tLR: 0.100000\nTraining Epoch: 4 [36864/60000]\tLoss: 0.2925\tLR: 0.100000\nTraining Epoch: 4 [36992/60000]\tLoss: 0.3105\tLR: 0.100000\nTraining Epoch: 4 [37120/60000]\tLoss: 0.3223\tLR: 0.100000\nTraining Epoch: 4 [37248/60000]\tLoss: 0.2705\tLR: 0.100000\nTraining Epoch: 4 [37376/60000]\tLoss: 0.3914\tLR: 0.100000\nTraining Epoch: 4 [37504/60000]\tLoss: 0.4225\tLR: 0.100000\nTraining Epoch: 4 [37632/60000]\tLoss: 0.3269\tLR: 0.100000\nTraining Epoch: 4 [37760/60000]\tLoss: 0.4153\tLR: 0.100000\nTraining Epoch: 4 [37888/60000]\tLoss: 0.2835\tLR: 0.100000\nTraining Epoch: 4 [38016/60000]\tLoss: 0.3661\tLR: 0.100000\nTraining Epoch: 4 [38144/60000]\tLoss: 0.2689\tLR: 0.100000\nTraining Epoch: 4 [38272/60000]\tLoss: 0.4323\tLR: 0.100000\nTraining Epoch: 4 [38400/60000]\tLoss: 0.4703\tLR: 0.100000\nTraining Epoch: 4 [38528/60000]\tLoss: 0.4558\tLR: 0.100000\nTraining Epoch: 4 [38656/60000]\tLoss: 0.4148\tLR: 0.100000\nTraining Epoch: 4 [38784/60000]\tLoss: 0.3307\tLR: 0.100000\nTraining Epoch: 4 [38912/60000]\tLoss: 0.2554\tLR: 0.100000\nTraining Epoch: 4 [39040/60000]\tLoss: 0.3434\tLR: 0.100000\nTraining Epoch: 4 [39168/60000]\tLoss: 0.4287\tLR: 0.100000\nTraining Epoch: 4 [39296/60000]\tLoss: 0.3422\tLR: 0.100000\nTraining Epoch: 4 [39424/60000]\tLoss: 0.3193\tLR: 0.100000\nTraining Epoch: 4 [39552/60000]\tLoss: 0.3405\tLR: 0.100000\nTraining Epoch: 4 [39680/60000]\tLoss: 0.3147\tLR: 0.100000\nTraining Epoch: 4 [39808/60000]\tLoss: 0.3809\tLR: 0.100000\nTraining Epoch: 4 [39936/60000]\tLoss: 0.2887\tLR: 0.100000\nTraining Epoch: 4 [40064/60000]\tLoss: 0.2358\tLR: 0.100000\nTraining Epoch: 4 [40192/60000]\tLoss: 0.2662\tLR: 0.100000\nTraining Epoch: 4 [40320/60000]\tLoss: 0.4203\tLR: 0.100000\nTraining Epoch: 4 [40448/60000]\tLoss: 0.3945\tLR: 0.100000\nTraining Epoch: 4 [40576/60000]\tLoss: 0.3396\tLR: 0.100000\nTraining Epoch: 4 [40704/60000]\tLoss: 0.3961\tLR: 0.100000\nTraining Epoch: 4 [40832/60000]\tLoss: 0.2370\tLR: 0.100000\nTraining Epoch: 4 [40960/60000]\tLoss: 0.2457\tLR: 0.100000\nTraining Epoch: 4 [41088/60000]\tLoss: 0.3419\tLR: 0.100000\nTraining Epoch: 4 [41216/60000]\tLoss: 0.3477\tLR: 0.100000\nTraining Epoch: 4 [41344/60000]\tLoss: 0.2897\tLR: 0.100000\nTraining Epoch: 4 [41472/60000]\tLoss: 0.3540\tLR: 0.100000\nTraining Epoch: 4 [41600/60000]\tLoss: 0.3220\tLR: 0.100000\nTraining Epoch: 4 [41728/60000]\tLoss: 0.4920\tLR: 0.100000\nTraining Epoch: 4 [41856/60000]\tLoss: 0.4651\tLR: 0.100000\nTraining Epoch: 4 [41984/60000]\tLoss: 0.3364\tLR: 0.100000\nTraining Epoch: 4 [42112/60000]\tLoss: 0.2276\tLR: 0.100000\nTraining Epoch: 4 [42240/60000]\tLoss: 0.3935\tLR: 0.100000\nTraining Epoch: 4 [42368/60000]\tLoss: 0.2791\tLR: 0.100000\nTraining Epoch: 4 [42496/60000]\tLoss: 0.5553\tLR: 0.100000\nTraining Epoch: 4 [42624/60000]\tLoss: 0.3001\tLR: 0.100000\nTraining Epoch: 4 [42752/60000]\tLoss: 0.4994\tLR: 0.100000\nTraining Epoch: 4 [42880/60000]\tLoss: 0.4187\tLR: 0.100000\nTraining Epoch: 4 [43008/60000]\tLoss: 0.3609\tLR: 0.100000\nTraining Epoch: 4 [43136/60000]\tLoss: 0.2801\tLR: 0.100000\nTraining Epoch: 4 [43264/60000]\tLoss: 0.3422\tLR: 0.100000\nTraining Epoch: 4 [43392/60000]\tLoss: 0.5560\tLR: 0.100000\nTraining Epoch: 4 [43520/60000]\tLoss: 0.4502\tLR: 0.100000\nTraining Epoch: 4 [43648/60000]\tLoss: 0.3661\tLR: 0.100000\nTraining Epoch: 4 [43776/60000]\tLoss: 0.5154\tLR: 0.100000\nTraining Epoch: 4 [43904/60000]\tLoss: 0.3506\tLR: 0.100000\nTraining Epoch: 4 [44032/60000]\tLoss: 0.3893\tLR: 0.100000\nTraining Epoch: 4 [44160/60000]\tLoss: 0.4104\tLR: 0.100000\nTraining Epoch: 4 [44288/60000]\tLoss: 0.4439\tLR: 0.100000\nTraining Epoch: 4 [44416/60000]\tLoss: 0.3826\tLR: 0.100000\nTraining Epoch: 4 [44544/60000]\tLoss: 0.2788\tLR: 0.100000\nTraining Epoch: 4 [44672/60000]\tLoss: 0.3311\tLR: 0.100000\nTraining Epoch: 4 [44800/60000]\tLoss: 0.3238\tLR: 0.100000\nTraining Epoch: 4 [44928/60000]\tLoss: 0.5570\tLR: 0.100000\nTraining Epoch: 4 [45056/60000]\tLoss: 0.3647\tLR: 0.100000\nTraining Epoch: 4 [45184/60000]\tLoss: 0.2232\tLR: 0.100000\nTraining Epoch: 4 [45312/60000]\tLoss: 0.3298\tLR: 0.100000\nTraining Epoch: 4 [45440/60000]\tLoss: 0.3835\tLR: 0.100000\nTraining Epoch: 4 [45568/60000]\tLoss: 0.3424\tLR: 0.100000\nTraining Epoch: 4 [45696/60000]\tLoss: 0.3330\tLR: 0.100000\nTraining Epoch: 4 [45824/60000]\tLoss: 0.2753\tLR: 0.100000\nTraining Epoch: 4 [45952/60000]\tLoss: 0.3097\tLR: 0.100000\nTraining Epoch: 4 [46080/60000]\tLoss: 0.4038\tLR: 0.100000\nTraining Epoch: 4 [46208/60000]\tLoss: 0.2615\tLR: 0.100000\nTraining Epoch: 4 [46336/60000]\tLoss: 0.3550\tLR: 0.100000\nTraining Epoch: 4 [46464/60000]\tLoss: 0.3894\tLR: 0.100000\nTraining Epoch: 4 [46592/60000]\tLoss: 0.2287\tLR: 0.100000\nTraining Epoch: 4 [46720/60000]\tLoss: 0.2960\tLR: 0.100000\nTraining Epoch: 4 [46848/60000]\tLoss: 0.4133\tLR: 0.100000\nTraining Epoch: 4 [46976/60000]\tLoss: 0.2584\tLR: 0.100000\nTraining Epoch: 4 [47104/60000]\tLoss: 0.3012\tLR: 0.100000\nTraining Epoch: 4 [47232/60000]\tLoss: 0.2942\tLR: 0.100000\nTraining Epoch: 4 [47360/60000]\tLoss: 0.2723\tLR: 0.100000\nTraining Epoch: 4 [47488/60000]\tLoss: 0.2932\tLR: 0.100000\nTraining Epoch: 4 [47616/60000]\tLoss: 0.3548\tLR: 0.100000\nTraining Epoch: 4 [47744/60000]\tLoss: 0.2405\tLR: 0.100000\nTraining Epoch: 4 [47872/60000]\tLoss: 0.2302\tLR: 0.100000\nTraining Epoch: 4 [48000/60000]\tLoss: 0.2503\tLR: 0.100000\nTraining Epoch: 4 [48128/60000]\tLoss: 0.3075\tLR: 0.100000\nTraining Epoch: 4 [48256/60000]\tLoss: 0.3089\tLR: 0.100000\nTraining Epoch: 4 [48384/60000]\tLoss: 0.3473\tLR: 0.100000\nTraining Epoch: 4 [48512/60000]\tLoss: 0.2872\tLR: 0.100000\nTraining Epoch: 4 [48640/60000]\tLoss: 0.3340\tLR: 0.100000\nTraining Epoch: 4 [48768/60000]\tLoss: 0.3213\tLR: 0.100000\nTraining Epoch: 4 [48896/60000]\tLoss: 0.2980\tLR: 0.100000\nTraining Epoch: 4 [49024/60000]\tLoss: 0.2372\tLR: 0.100000\nTraining Epoch: 4 [49152/60000]\tLoss: 0.3904\tLR: 0.100000\nTraining Epoch: 4 [49280/60000]\tLoss: 0.3990\tLR: 0.100000\nTraining Epoch: 4 [49408/60000]\tLoss: 0.3409\tLR: 0.100000\nTraining Epoch: 4 [49536/60000]\tLoss: 0.4263\tLR: 0.100000\nTraining Epoch: 4 [49664/60000]\tLoss: 0.2921\tLR: 0.100000\nTraining Epoch: 4 [49792/60000]\tLoss: 0.2965\tLR: 0.100000\nTraining Epoch: 4 [49920/60000]\tLoss: 0.3645\tLR: 0.100000\nTraining Epoch: 4 [50048/60000]\tLoss: 0.2544\tLR: 0.100000\nTraining Epoch: 4 [50176/60000]\tLoss: 0.2982\tLR: 0.100000\nTraining Epoch: 4 [50304/60000]\tLoss: 0.4140\tLR: 0.100000\nTraining Epoch: 4 [50432/60000]\tLoss: 0.3926\tLR: 0.100000\nTraining Epoch: 4 [50560/60000]\tLoss: 0.4222\tLR: 0.100000\nTraining Epoch: 4 [50688/60000]\tLoss: 0.3799\tLR: 0.100000\nTraining Epoch: 4 [50816/60000]\tLoss: 0.2895\tLR: 0.100000\nTraining Epoch: 4 [50944/60000]\tLoss: 0.4136\tLR: 0.100000\nTraining Epoch: 4 [51072/60000]\tLoss: 0.3996\tLR: 0.100000\nTraining Epoch: 4 [51200/60000]\tLoss: 0.4329\tLR: 0.100000\nTraining Epoch: 4 [51328/60000]\tLoss: 0.3725\tLR: 0.100000\nTraining Epoch: 4 [51456/60000]\tLoss: 0.2476\tLR: 0.100000\nTraining Epoch: 4 [51584/60000]\tLoss: 0.3799\tLR: 0.100000\nTraining Epoch: 4 [51712/60000]\tLoss: 0.3328\tLR: 0.100000\nTraining Epoch: 4 [51840/60000]\tLoss: 0.4309\tLR: 0.100000\nTraining Epoch: 4 [51968/60000]\tLoss: 0.3746\tLR: 0.100000\nTraining Epoch: 4 [52096/60000]\tLoss: 0.4989\tLR: 0.100000\nTraining Epoch: 4 [52224/60000]\tLoss: 0.4088\tLR: 0.100000\nTraining Epoch: 4 [52352/60000]\tLoss: 0.4623\tLR: 0.100000\nTraining Epoch: 4 [52480/60000]\tLoss: 0.3685\tLR: 0.100000\nTraining Epoch: 4 [52608/60000]\tLoss: 0.2737\tLR: 0.100000\nTraining Epoch: 4 [52736/60000]\tLoss: 0.4145\tLR: 0.100000\nTraining Epoch: 4 [52864/60000]\tLoss: 0.4166\tLR: 0.100000\nTraining Epoch: 4 [52992/60000]\tLoss: 0.2936\tLR: 0.100000\nTraining Epoch: 4 [53120/60000]\tLoss: 0.2820\tLR: 0.100000\nTraining Epoch: 4 [53248/60000]\tLoss: 0.3397\tLR: 0.100000\nTraining Epoch: 4 [53376/60000]\tLoss: 0.2415\tLR: 0.100000\nTraining Epoch: 4 [53504/60000]\tLoss: 0.3630\tLR: 0.100000\nTraining Epoch: 4 [53632/60000]\tLoss: 0.3461\tLR: 0.100000\nTraining Epoch: 4 [53760/60000]\tLoss: 0.4238\tLR: 0.100000\nTraining Epoch: 4 [53888/60000]\tLoss: 0.2827\tLR: 0.100000\nTraining Epoch: 4 [54016/60000]\tLoss: 0.3947\tLR: 0.100000\nTraining Epoch: 4 [54144/60000]\tLoss: 0.4453\tLR: 0.100000\nTraining Epoch: 4 [54272/60000]\tLoss: 0.2955\tLR: 0.100000\nTraining Epoch: 4 [54400/60000]\tLoss: 0.3738\tLR: 0.100000\nTraining Epoch: 4 [54528/60000]\tLoss: 0.2739\tLR: 0.100000\nTraining Epoch: 4 [54656/60000]\tLoss: 0.2974\tLR: 0.100000\nTraining Epoch: 4 [54784/60000]\tLoss: 0.3544\tLR: 0.100000\nTraining Epoch: 4 [54912/60000]\tLoss: 0.2538\tLR: 0.100000\nTraining Epoch: 4 [55040/60000]\tLoss: 0.4038\tLR: 0.100000\nTraining Epoch: 4 [55168/60000]\tLoss: 0.2862\tLR: 0.100000\nTraining Epoch: 4 [55296/60000]\tLoss: 0.2885\tLR: 0.100000\nTraining Epoch: 4 [55424/60000]\tLoss: 0.3188\tLR: 0.100000\nTraining Epoch: 4 [55552/60000]\tLoss: 0.3583\tLR: 0.100000\nTraining Epoch: 4 [55680/60000]\tLoss: 0.3063\tLR: 0.100000\nTraining Epoch: 4 [55808/60000]\tLoss: 0.3748\tLR: 0.100000\nTraining Epoch: 4 [55936/60000]\tLoss: 0.2886\tLR: 0.100000\nTraining Epoch: 4 [56064/60000]\tLoss: 0.2590\tLR: 0.100000\nTraining Epoch: 4 [56192/60000]\tLoss: 0.4673\tLR: 0.100000\nTraining Epoch: 4 [56320/60000]\tLoss: 0.4015\tLR: 0.100000\nTraining Epoch: 4 [56448/60000]\tLoss: 0.3077\tLR: 0.100000\nTraining Epoch: 4 [56576/60000]\tLoss: 0.2711\tLR: 0.100000\nTraining Epoch: 4 [56704/60000]\tLoss: 0.3339\tLR: 0.100000\nTraining Epoch: 4 [56832/60000]\tLoss: 0.3756\tLR: 0.100000\nTraining Epoch: 4 [56960/60000]\tLoss: 0.3369\tLR: 0.100000\nTraining Epoch: 4 [57088/60000]\tLoss: 0.3441\tLR: 0.100000\nTraining Epoch: 4 [57216/60000]\tLoss: 0.2769\tLR: 0.100000\nTraining Epoch: 4 [57344/60000]\tLoss: 0.3436\tLR: 0.100000\nTraining Epoch: 4 [57472/60000]\tLoss: 0.4177\tLR: 0.100000\nTraining Epoch: 4 [57600/60000]\tLoss: 0.3554\tLR: 0.100000\nTraining Epoch: 4 [57728/60000]\tLoss: 0.3842\tLR: 0.100000\nTraining Epoch: 4 [57856/60000]\tLoss: 0.4504\tLR: 0.100000\nTraining Epoch: 4 [57984/60000]\tLoss: 0.3457\tLR: 0.100000\nTraining Epoch: 4 [58112/60000]\tLoss: 0.3044\tLR: 0.100000\nTraining Epoch: 4 [58240/60000]\tLoss: 0.3613\tLR: 0.100000\nTraining Epoch: 4 [58368/60000]\tLoss: 0.3007\tLR: 0.100000\nTraining Epoch: 4 [58496/60000]\tLoss: 0.3235\tLR: 0.100000\nTraining Epoch: 4 [58624/60000]\tLoss: 0.3524\tLR: 0.100000\nTraining Epoch: 4 [58752/60000]\tLoss: 0.4806\tLR: 0.100000\nTraining Epoch: 4 [58880/60000]\tLoss: 0.2544\tLR: 0.100000\nTraining Epoch: 4 [59008/60000]\tLoss: 0.2991\tLR: 0.100000\nTraining Epoch: 4 [59136/60000]\tLoss: 0.2935\tLR: 0.100000\nTraining Epoch: 4 [59264/60000]\tLoss: 0.2981\tLR: 0.100000\nTraining Epoch: 4 [59392/60000]\tLoss: 0.3040\tLR: 0.100000\nTraining Epoch: 4 [59520/60000]\tLoss: 0.3252\tLR: 0.100000\nTraining Epoch: 4 [59648/60000]\tLoss: 0.3052\tLR: 0.100000\nTraining Epoch: 4 [59776/60000]\tLoss: 0.2671\tLR: 0.100000\nTraining Epoch: 4 [59904/60000]\tLoss: 0.3131\tLR: 0.100000\nTraining Epoch: 4 [60000/60000]\tLoss: 0.4061\tLR: 0.100000\nTest set: Average loss: 0.0030, Accuracy: 0.8648\n\n\nbest_acc:  0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing the Model\n","metadata":{}},{"cell_type":"code","source":"weights_file=\"./resnetcbam18.pth\"\ntorch.save(net.state_dict(), weights_file)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:22:13.991237Z","iopub.execute_input":"2024-03-31T08:22:13.991560Z","iopub.status.idle":"2024-03-31T08:22:14.092772Z","shell.execute_reply.started":"2024-03-31T08:22:13.991528Z","shell.execute_reply":"2024-03-31T08:22:14.091949Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    net = get_network()\n    FashionMNIST_test_loader = get_test_dataloader(\n        num_workers=2,\n        batch_size=128,\n        shuffle=True\n    )\n\n    net.load_state_dict(torch.load(weights_file), True)\n    print(net)\n    net.eval()\n\n    correct_1 = 0.0\n    correct_5 = 0.0\n    total = 0\n\n    for n_iter, (image, label) in enumerate(FashionMNIST_test_loader):\n        print(\"iteration: {}\\ttotal {} iterations\".format(n_iter + 1, len(FashionMNIST_test_loader)))\n        image = Variable(image).cuda()\n        label = Variable(label).cuda()\n        output = net(image)\n        _, pred = output.topk(5, 1, largest=True, sorted=True)\n\n        label = label.view(label.size(0), -1).expand_as(pred)\n        correct = pred.eq(label).float()\n\n        #compute top 5\n        correct_5 += correct[:, :5].sum()\n\n        #compute top1 \n        correct_1 += correct[:, :1].sum()\n\n\n    print()\n    print(\"Top 1 err: \", 1 - correct_1 / len(FashionMNIST_test_loader.dataset))\n    print(\"Top 5 err: \", 1 - correct_5 / len(FashionMNIST_test_loader.dataset))\n    print(\"Parameter numbers: {}\".format(sum(p.numel() for p in net.parameters())))","metadata":{"execution":{"iopub.status.busy":"2024-03-31T08:22:14.093979Z","iopub.execute_input":"2024-03-31T08:22:14.094313Z","iopub.status.idle":"2024-03-31T08:22:17.107823Z","shell.execute_reply.started":"2024-03-31T08:22:14.094284Z","shell.execute_reply":"2024-03-31T08:22:17.106641Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ResNetCBAM(\n  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n      (cbam): CBAM(\n        (ca): ChannelAttention(\n          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n          (max_pool): AdaptiveMaxPool2d(output_size=1)\n          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (relu1): ReLU()\n          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n        (sa): SpatialAttention(\n          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (sigmoid): Sigmoid()\n        )\n      )\n    )\n  )\n  (linear): Linear(in_features=512, out_features=10, bias=True)\n)\niteration: 1\ttotal 79 iterations\niteration: 2\ttotal 79 iterations\niteration: 3\ttotal 79 iterations\niteration: 4\ttotal 79 iterations\niteration: 5\ttotal 79 iterations\niteration: 6\ttotal 79 iterations\niteration: 7\ttotal 79 iterations\niteration: 8\ttotal 79 iterations\niteration: 9\ttotal 79 iterations\niteration: 10\ttotal 79 iterations\niteration: 11\ttotal 79 iterations\niteration: 12\ttotal 79 iterations\niteration: 13\ttotal 79 iterations\niteration: 14\ttotal 79 iterations\niteration: 15\ttotal 79 iterations\niteration: 16\ttotal 79 iterations\niteration: 17\ttotal 79 iterations\niteration: 18\ttotal 79 iterations\niteration: 19\ttotal 79 iterations\niteration: 20\ttotal 79 iterations\niteration: 21\ttotal 79 iterations\niteration: 22\ttotal 79 iterations\niteration: 23\ttotal 79 iterations\niteration: 24\ttotal 79 iterations\niteration: 25\ttotal 79 iterations\niteration: 26\ttotal 79 iterations\niteration: 27\ttotal 79 iterations\niteration: 28\ttotal 79 iterations\niteration: 29\ttotal 79 iterations\niteration: 30\ttotal 79 iterations\niteration: 31\ttotal 79 iterations\niteration: 32\ttotal 79 iterations\niteration: 33\ttotal 79 iterations\niteration: 34\ttotal 79 iterations\niteration: 35\ttotal 79 iterations\niteration: 36\ttotal 79 iterations\niteration: 37\ttotal 79 iterations\niteration: 38\ttotal 79 iterations\niteration: 39\ttotal 79 iterations\niteration: 40\ttotal 79 iterations\niteration: 41\ttotal 79 iterations\niteration: 42\ttotal 79 iterations\niteration: 43\ttotal 79 iterations\niteration: 44\ttotal 79 iterations\niteration: 45\ttotal 79 iterations\niteration: 46\ttotal 79 iterations\niteration: 47\ttotal 79 iterations\niteration: 48\ttotal 79 iterations\niteration: 49\ttotal 79 iterations\niteration: 50\ttotal 79 iterations\niteration: 51\ttotal 79 iterations\niteration: 52\ttotal 79 iterations\niteration: 53\ttotal 79 iterations\niteration: 54\ttotal 79 iterations\niteration: 55\ttotal 79 iterations\niteration: 56\ttotal 79 iterations\niteration: 57\ttotal 79 iterations\niteration: 58\ttotal 79 iterations\niteration: 59\ttotal 79 iterations\niteration: 60\ttotal 79 iterations\niteration: 61\ttotal 79 iterations\niteration: 62\ttotal 79 iterations\niteration: 63\ttotal 79 iterations\niteration: 64\ttotal 79 iterations\niteration: 65\ttotal 79 iterations\niteration: 66\ttotal 79 iterations\niteration: 67\ttotal 79 iterations\niteration: 68\ttotal 79 iterations\niteration: 69\ttotal 79 iterations\niteration: 70\ttotal 79 iterations\niteration: 71\ttotal 79 iterations\niteration: 72\ttotal 79 iterations\niteration: 73\ttotal 79 iterations\niteration: 74\ttotal 79 iterations\niteration: 75\ttotal 79 iterations\niteration: 76\ttotal 79 iterations\niteration: 77\ttotal 79 iterations\niteration: 78\ttotal 79 iterations\niteration: 79\ttotal 79 iterations\n\nTop 1 err:  tensor(0.1352, device='cuda:0')\nTop 5 err:  tensor(0.0011, device='cuda:0')\nParameter numbers: 11259994\n","output_type":"stream"}]}]}