{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:38.213630Z",
     "iopub.status.busy": "2024-03-25T18:42:38.213332Z",
     "iopub.status.idle": "2024-03-25T18:42:46.954212Z",
     "shell.execute_reply": "2024-03-25T18:42:46.953348Z",
     "shell.execute_reply.started": "2024-03-25T18:42:38.213602Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import glob\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data (CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:46.956266Z",
     "iopub.status.busy": "2024-03-25T18:42:46.955838Z",
     "iopub.status.idle": "2024-03-25T18:42:46.970660Z",
     "shell.execute_reply": "2024-03-25T18:42:46.969605Z",
     "shell.execute_reply.started": "2024-03-25T18:42:46.956240Z"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR100Train(Dataset):\n",
    "    \"\"\"CIFAR100 test dataset, derived from\n",
    "    torch.utils.data.DataSet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        #if transform is given, we transoform data using\n",
    "        with open(os.path.join(path, 'train'), 'rb') as CIFAR100:\n",
    "            self.data = pickle.load(CIFAR100, encoding='bytes')\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data['fine_labels'.encode()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.data['fine_labels'.encode()][index]\n",
    "        r = self.data['data'.encode()][index, :1024].reshape(32, 32)\n",
    "        g = self.data['data'.encode()][index, 1024:2048].reshape(32, 32)\n",
    "        b = self.data['data'.encode()][index, 2048:].reshape(32, 32)\n",
    "        image = numpy.dstack((r, g, b))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return label, image\n",
    "\n",
    "class CIFAR100Test(Dataset):\n",
    "    \"\"\"CIFAR100 test dataset, derived from\n",
    "    torch.utils.data.DataSet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        with open(os.path.join(path, 'test'), 'rb') as CIFAR100:\n",
    "            self.data = pickle.load(CIFAR100, encoding='bytes')\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['data'.encode()])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.data['fine_labels'.encode()][index]\n",
    "        r = self.data['data'.encode()][index, :1024].reshape(32, 32)\n",
    "        g = self.data['data'.encode()][index, 1024:2048].reshape(32, 32)\n",
    "        b = self.data['data'.encode()][index, 2048:].reshape(32, 32)\n",
    "        image = numpy.dstack((r, g, b))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return label, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Modules (CAM,SAM,CBAM,ZAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:46.972701Z",
     "iopub.status.busy": "2024-03-25T18:42:46.972064Z",
     "iopub.status.idle": "2024-03-25T18:42:46.993507Z",
     "shell.execute_reply": "2024-03-25T18:42:46.992576Z",
     "shell.execute_reply.started": "2024-03-25T18:42:46.972668Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes):\n",
    "        super(CBAM, self).__init__()\n",
    "\n",
    "        self.ca = ChannelAttention(in_planes)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x * (self.ca(x))\n",
    "        out = out * (self.sa(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "## ZAM ##    \n",
    "class ZeroChannelAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ZeroChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "    \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.avg_pool(x) + self.max_pool(x))\n",
    "\n",
    "class ZeroSpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ZeroSpatialAttention, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "    \n",
    "class ZAM(nn.Module):\n",
    "    def __init__(self, use_skip_connection = False):\n",
    "        super(ZAM, self).__init__()\n",
    "\n",
    "        self.ca = ZeroChannelAttention()\n",
    "        self.sa = ZeroSpatialAttention()\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x + x * self.ca(x) if self.use_skip_connection else x * self.ca(x)\n",
    "        out = out + out * self.sa(out) if self.use_skip_connection else out * self.sa(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models (ResNet,MobileNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:46.996721Z",
     "iopub.status.busy": "2024-03-25T18:42:46.996356Z",
     "iopub.status.idle": "2024-03-25T18:42:47.027963Z",
     "shell.execute_reply": "2024-03-25T18:42:47.027250Z",
     "shell.execute_reply.started": "2024-03-25T18:42:46.996690Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "        \n",
    "        self.cbam = CBAM(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.cbam(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "        \n",
    "        self.cbam = CBAM(self.expansion*planes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.cbam(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetCBAM(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNetCBAM, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNetCBAM18():\n",
    "    return ResNetCBAM(BasicBlock, [2,2,2,2])\n",
    "\n",
    "class MobileNetCBAM(nn.Module):\n",
    "    def __init__(self, classes = 100):\n",
    "        super(MobileNetCBAM, self).__init__()\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                CBAM(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32, 1), \n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 1),\n",
    "            conv_dw(128, 128, 1),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 256, 1),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 1024, 2),\n",
    "            conv_dw(1024, 1024, 1),\n",
    "            nn.AvgPool2d(4),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:47.029629Z",
     "iopub.status.busy": "2024-03-25T18:42:47.029260Z",
     "iopub.status.idle": "2024-03-25T18:42:47.045590Z",
     "shell.execute_reply": "2024-03-25T18:42:47.044602Z",
     "shell.execute_reply.started": "2024-03-25T18:42:47.029601Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_network():\n",
    "    #net=ResNetCBAM18().cuda()\n",
    "    net=MobileNetCBAM().cuda()\n",
    "    \"\"\" return given network\n",
    "    \"\"\"\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of CIFAR100 training dataset\n",
    "        std: std of CIFAR100 training dataset\n",
    "        path: path to CIFAR100 training python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle \n",
    "    Returns: train_data_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #CIFAR100_training = CIFAR100Train(path, transform=transform_train)\n",
    "    CIFAR100_training = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    CIFAR100_training_loader = DataLoader(\n",
    "        CIFAR100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return CIFAR100_training_loader\n",
    "\n",
    "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of CIFAR100 test dataset\n",
    "        std: std of CIFAR100 test dataset\n",
    "        path: path to CIFAR100 test python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle \n",
    "    Returns: CIFAR100_test_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #CIFAR100_test = CIFAR100Test(path, transform=transform_test)\n",
    "    CIFAR100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    CIFAR100_test_loader = DataLoader(\n",
    "        CIFAR100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return CIFAR100_test_loader\n",
    "\n",
    "def compute_mean_std(CIFAR100_dataset):\n",
    "    \"\"\"compute the mean and std of CIFAR100 dataset\n",
    "    Args:\n",
    "        CIFAR100_training_dataset or CIFAR100_test_dataset\n",
    "        witch derived from class torch.utils.data\n",
    "    \n",
    "    Returns:\n",
    "        a tuple contains mean, std value of entire dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data_r = numpy.dstack([CIFAR100_dataset[i][1][:, :, 0] for i in range(len(CIFAR100_dataset))])\n",
    "    data_g = numpy.dstack([CIFAR100_dataset[i][1][:, :, 1] for i in range(len(CIFAR100_dataset))])\n",
    "    data_b = numpy.dstack([CIFAR100_dataset[i][1][:, :, 2] for i in range(len(CIFAR100_dataset))])\n",
    "    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n",
    "    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:47.047073Z",
     "iopub.status.busy": "2024-03-25T18:42:47.046718Z",
     "iopub.status.idle": "2024-03-25T18:42:47.058982Z",
     "shell.execute_reply": "2024-03-25T18:42:47.058165Z",
     "shell.execute_reply.started": "2024-03-25T18:42:47.047043Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "CIFAR10_TRAIN_MEAN = (0.49139968, 0.48215827 ,0.44653124)\n",
    "CIFAR10_TRAIN_STD = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "CIFAR100_TRAIN_MEAN = (0.5088964127604166, 0.48739301317401956, 0.44194221124387256)\n",
    "CIFAR100_TRAIN_STD = (0.2682515741720801, 0.2573637364478126, 0.2770957707973042)\n",
    "\n",
    "#directory to save weights file\n",
    "CHECKPOINT_PATH = 'checkpoint'\n",
    "\n",
    "#total training epoches\n",
    "EPOCH = 25\n",
    "MILESTONES = [6, 12, 16]\n",
    "\n",
    "#initial learning rate\n",
    "#INIT_LR = 0.1\n",
    "\n",
    "#time of we run the script\n",
    "TIME_NOW = datetime.now().isoformat()\n",
    "\n",
    "#tensorboard log dir\n",
    "LOG_DIR = 'runs'\n",
    "\n",
    "#save weights file per SAVE_EPOCH epoch\n",
    "SAVE_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T18:42:47.060525Z",
     "iopub.status.busy": "2024-03-25T18:42:47.060280Z",
     "iopub.status.idle": "2024-03-25T19:11:41.751152Z",
     "shell.execute_reply": "2024-03-25T19:11:41.749998Z",
     "shell.execute_reply.started": "2024-03-25T18:42:47.060503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:01<00:00, 93426649.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [128/50000]\tLoss: 4.6019\tLR: 0.000256\n",
      "Training Epoch: 1 [256/50000]\tLoss: 4.5958\tLR: 0.000512\n",
      "Training Epoch: 1 [384/50000]\tLoss: 4.6056\tLR: 0.000767\n",
      "Training Epoch: 1 [512/50000]\tLoss: 4.6070\tLR: 0.001023\n",
      "Training Epoch: 1 [640/50000]\tLoss: 4.6042\tLR: 0.001279\n",
      "Training Epoch: 1 [768/50000]\tLoss: 4.6119\tLR: 0.001535\n",
      "Training Epoch: 1 [896/50000]\tLoss: 4.5993\tLR: 0.001790\n",
      "Training Epoch: 1 [1024/50000]\tLoss: 4.6116\tLR: 0.002046\n",
      "Training Epoch: 1 [1152/50000]\tLoss: 4.6050\tLR: 0.002302\n",
      "Training Epoch: 1 [1280/50000]\tLoss: 4.6134\tLR: 0.002558\n",
      "Training Epoch: 1 [1408/50000]\tLoss: 4.6048\tLR: 0.002813\n",
      "Training Epoch: 1 [1536/50000]\tLoss: 4.6076\tLR: 0.003069\n",
      "Training Epoch: 1 [1664/50000]\tLoss: 4.6059\tLR: 0.003325\n",
      "Training Epoch: 1 [1792/50000]\tLoss: 4.6029\tLR: 0.003581\n",
      "Training Epoch: 1 [1920/50000]\tLoss: 4.6038\tLR: 0.003836\n",
      "Training Epoch: 1 [2048/50000]\tLoss: 4.6042\tLR: 0.004092\n",
      "Training Epoch: 1 [2176/50000]\tLoss: 4.6085\tLR: 0.004348\n",
      "Training Epoch: 1 [2304/50000]\tLoss: 4.6042\tLR: 0.004604\n",
      "Training Epoch: 1 [2432/50000]\tLoss: 4.6105\tLR: 0.004859\n",
      "Training Epoch: 1 [2560/50000]\tLoss: 4.6120\tLR: 0.005115\n",
      "Training Epoch: 1 [2688/50000]\tLoss: 4.6012\tLR: 0.005371\n",
      "Training Epoch: 1 [2816/50000]\tLoss: 4.6063\tLR: 0.005627\n",
      "Training Epoch: 1 [2944/50000]\tLoss: 4.6143\tLR: 0.005882\n",
      "Training Epoch: 1 [3072/50000]\tLoss: 4.6136\tLR: 0.006138\n",
      "Training Epoch: 1 [3200/50000]\tLoss: 4.6073\tLR: 0.006394\n",
      "Training Epoch: 1 [3328/50000]\tLoss: 4.6022\tLR: 0.006650\n",
      "Training Epoch: 1 [3456/50000]\tLoss: 4.6050\tLR: 0.006905\n",
      "Training Epoch: 1 [3584/50000]\tLoss: 4.6014\tLR: 0.007161\n",
      "Training Epoch: 1 [3712/50000]\tLoss: 4.6012\tLR: 0.007417\n",
      "Training Epoch: 1 [3840/50000]\tLoss: 4.6095\tLR: 0.007673\n",
      "Training Epoch: 1 [3968/50000]\tLoss: 4.6031\tLR: 0.007928\n",
      "Training Epoch: 1 [4096/50000]\tLoss: 4.6124\tLR: 0.008184\n",
      "Training Epoch: 1 [4224/50000]\tLoss: 4.6072\tLR: 0.008440\n",
      "Training Epoch: 1 [4352/50000]\tLoss: 4.6047\tLR: 0.008696\n",
      "Training Epoch: 1 [4480/50000]\tLoss: 4.6046\tLR: 0.008951\n",
      "Training Epoch: 1 [4608/50000]\tLoss: 4.6028\tLR: 0.009207\n",
      "Training Epoch: 1 [4736/50000]\tLoss: 4.6008\tLR: 0.009463\n",
      "Training Epoch: 1 [4864/50000]\tLoss: 4.6096\tLR: 0.009719\n",
      "Training Epoch: 1 [4992/50000]\tLoss: 4.6021\tLR: 0.009974\n",
      "Training Epoch: 1 [5120/50000]\tLoss: 4.6052\tLR: 0.010230\n",
      "Training Epoch: 1 [5248/50000]\tLoss: 4.6108\tLR: 0.010486\n",
      "Training Epoch: 1 [5376/50000]\tLoss: 4.6120\tLR: 0.010742\n",
      "Training Epoch: 1 [5504/50000]\tLoss: 4.6157\tLR: 0.010997\n",
      "Training Epoch: 1 [5632/50000]\tLoss: 4.5985\tLR: 0.011253\n",
      "Training Epoch: 1 [5760/50000]\tLoss: 4.6073\tLR: 0.011509\n",
      "Training Epoch: 1 [5888/50000]\tLoss: 4.6101\tLR: 0.011765\n",
      "Training Epoch: 1 [6016/50000]\tLoss: 4.6070\tLR: 0.012020\n",
      "Training Epoch: 1 [6144/50000]\tLoss: 4.6079\tLR: 0.012276\n",
      "Training Epoch: 1 [6272/50000]\tLoss: 4.6100\tLR: 0.012532\n",
      "Training Epoch: 1 [6400/50000]\tLoss: 4.6105\tLR: 0.012788\n",
      "Training Epoch: 1 [6528/50000]\tLoss: 4.6113\tLR: 0.013043\n",
      "Training Epoch: 1 [6656/50000]\tLoss: 4.6111\tLR: 0.013299\n",
      "Training Epoch: 1 [6784/50000]\tLoss: 4.6068\tLR: 0.013555\n",
      "Training Epoch: 1 [6912/50000]\tLoss: 4.6121\tLR: 0.013811\n",
      "Training Epoch: 1 [7040/50000]\tLoss: 4.6170\tLR: 0.014066\n",
      "Training Epoch: 1 [7168/50000]\tLoss: 4.6080\tLR: 0.014322\n",
      "Training Epoch: 1 [7296/50000]\tLoss: 4.6090\tLR: 0.014578\n",
      "Training Epoch: 1 [7424/50000]\tLoss: 4.6110\tLR: 0.014834\n",
      "Training Epoch: 1 [7552/50000]\tLoss: 4.6027\tLR: 0.015090\n",
      "Training Epoch: 1 [7680/50000]\tLoss: 4.6068\tLR: 0.015345\n",
      "Training Epoch: 1 [7808/50000]\tLoss: 4.6049\tLR: 0.015601\n",
      "Training Epoch: 1 [7936/50000]\tLoss: 4.6121\tLR: 0.015857\n",
      "Training Epoch: 1 [8064/50000]\tLoss: 4.6122\tLR: 0.016113\n",
      "Training Epoch: 1 [8192/50000]\tLoss: 4.6189\tLR: 0.016368\n",
      "Training Epoch: 1 [8320/50000]\tLoss: 4.6092\tLR: 0.016624\n",
      "Training Epoch: 1 [8448/50000]\tLoss: 4.6088\tLR: 0.016880\n",
      "Training Epoch: 1 [8576/50000]\tLoss: 4.6051\tLR: 0.017136\n",
      "Training Epoch: 1 [8704/50000]\tLoss: 4.6115\tLR: 0.017391\n",
      "Training Epoch: 1 [8832/50000]\tLoss: 4.6105\tLR: 0.017647\n",
      "Training Epoch: 1 [8960/50000]\tLoss: 4.6199\tLR: 0.017903\n",
      "Training Epoch: 1 [9088/50000]\tLoss: 4.6091\tLR: 0.018159\n",
      "Training Epoch: 1 [9216/50000]\tLoss: 4.6065\tLR: 0.018414\n",
      "Training Epoch: 1 [9344/50000]\tLoss: 4.6023\tLR: 0.018670\n",
      "Training Epoch: 1 [9472/50000]\tLoss: 4.6019\tLR: 0.018926\n",
      "Training Epoch: 1 [9600/50000]\tLoss: 4.6141\tLR: 0.019182\n",
      "Training Epoch: 1 [9728/50000]\tLoss: 4.6104\tLR: 0.019437\n",
      "Training Epoch: 1 [9856/50000]\tLoss: 4.6041\tLR: 0.019693\n",
      "Training Epoch: 1 [9984/50000]\tLoss: 4.6058\tLR: 0.019949\n",
      "Training Epoch: 1 [10112/50000]\tLoss: 4.6073\tLR: 0.020205\n",
      "Training Epoch: 1 [10240/50000]\tLoss: 4.6084\tLR: 0.020460\n",
      "Training Epoch: 1 [10368/50000]\tLoss: 4.6048\tLR: 0.020716\n",
      "Training Epoch: 1 [10496/50000]\tLoss: 4.6081\tLR: 0.020972\n",
      "Training Epoch: 1 [10624/50000]\tLoss: 4.6046\tLR: 0.021228\n",
      "Training Epoch: 1 [10752/50000]\tLoss: 4.6185\tLR: 0.021483\n",
      "Training Epoch: 1 [10880/50000]\tLoss: 4.6112\tLR: 0.021739\n",
      "Training Epoch: 1 [11008/50000]\tLoss: 4.6145\tLR: 0.021995\n",
      "Training Epoch: 1 [11136/50000]\tLoss: 4.6072\tLR: 0.022251\n",
      "Training Epoch: 1 [11264/50000]\tLoss: 4.6043\tLR: 0.022506\n",
      "Training Epoch: 1 [11392/50000]\tLoss: 4.6027\tLR: 0.022762\n",
      "Training Epoch: 1 [11520/50000]\tLoss: 4.6028\tLR: 0.023018\n",
      "Training Epoch: 1 [11648/50000]\tLoss: 4.6157\tLR: 0.023274\n",
      "Training Epoch: 1 [11776/50000]\tLoss: 4.6055\tLR: 0.023529\n",
      "Training Epoch: 1 [11904/50000]\tLoss: 4.6098\tLR: 0.023785\n",
      "Training Epoch: 1 [12032/50000]\tLoss: 4.6080\tLR: 0.024041\n",
      "Training Epoch: 1 [12160/50000]\tLoss: 4.6090\tLR: 0.024297\n",
      "Training Epoch: 1 [12288/50000]\tLoss: 4.6095\tLR: 0.024552\n",
      "Training Epoch: 1 [12416/50000]\tLoss: 4.6077\tLR: 0.024808\n",
      "Training Epoch: 1 [12544/50000]\tLoss: 4.6102\tLR: 0.025064\n",
      "Training Epoch: 1 [12672/50000]\tLoss: 4.6022\tLR: 0.025320\n",
      "Training Epoch: 1 [12800/50000]\tLoss: 4.6103\tLR: 0.025575\n",
      "Training Epoch: 1 [12928/50000]\tLoss: 4.6149\tLR: 0.025831\n",
      "Training Epoch: 1 [13056/50000]\tLoss: 4.6122\tLR: 0.026087\n",
      "Training Epoch: 1 [13184/50000]\tLoss: 4.6081\tLR: 0.026343\n",
      "Training Epoch: 1 [13312/50000]\tLoss: 4.6113\tLR: 0.026598\n",
      "Training Epoch: 1 [13440/50000]\tLoss: 4.6115\tLR: 0.026854\n",
      "Training Epoch: 1 [13568/50000]\tLoss: 4.6003\tLR: 0.027110\n",
      "Training Epoch: 1 [13696/50000]\tLoss: 4.6108\tLR: 0.027366\n",
      "Training Epoch: 1 [13824/50000]\tLoss: 4.6060\tLR: 0.027621\n",
      "Training Epoch: 1 [13952/50000]\tLoss: 4.6049\tLR: 0.027877\n",
      "Training Epoch: 1 [14080/50000]\tLoss: 4.6071\tLR: 0.028133\n",
      "Training Epoch: 1 [14208/50000]\tLoss: 4.6023\tLR: 0.028389\n",
      "Training Epoch: 1 [14336/50000]\tLoss: 4.6120\tLR: 0.028645\n",
      "Training Epoch: 1 [14464/50000]\tLoss: 4.6105\tLR: 0.028900\n",
      "Training Epoch: 1 [14592/50000]\tLoss: 4.6110\tLR: 0.029156\n",
      "Training Epoch: 1 [14720/50000]\tLoss: 4.6001\tLR: 0.029412\n",
      "Training Epoch: 1 [14848/50000]\tLoss: 4.6018\tLR: 0.029668\n",
      "Training Epoch: 1 [14976/50000]\tLoss: 4.6036\tLR: 0.029923\n",
      "Training Epoch: 1 [15104/50000]\tLoss: 4.6058\tLR: 0.030179\n",
      "Training Epoch: 1 [15232/50000]\tLoss: 4.6161\tLR: 0.030435\n",
      "Training Epoch: 1 [15360/50000]\tLoss: 4.6053\tLR: 0.030691\n",
      "Training Epoch: 1 [15488/50000]\tLoss: 4.6066\tLR: 0.030946\n",
      "Training Epoch: 1 [15616/50000]\tLoss: 4.6083\tLR: 0.031202\n",
      "Training Epoch: 1 [15744/50000]\tLoss: 4.6030\tLR: 0.031458\n",
      "Training Epoch: 1 [15872/50000]\tLoss: 4.6114\tLR: 0.031714\n",
      "Training Epoch: 1 [16000/50000]\tLoss: 4.6125\tLR: 0.031969\n",
      "Training Epoch: 1 [16128/50000]\tLoss: 4.6110\tLR: 0.032225\n",
      "Training Epoch: 1 [16256/50000]\tLoss: 4.6088\tLR: 0.032481\n",
      "Training Epoch: 1 [16384/50000]\tLoss: 4.6013\tLR: 0.032737\n",
      "Training Epoch: 1 [16512/50000]\tLoss: 4.6089\tLR: 0.032992\n",
      "Training Epoch: 1 [16640/50000]\tLoss: 4.5991\tLR: 0.033248\n",
      "Training Epoch: 1 [16768/50000]\tLoss: 4.6067\tLR: 0.033504\n",
      "Training Epoch: 1 [16896/50000]\tLoss: 4.6029\tLR: 0.033760\n",
      "Training Epoch: 1 [17024/50000]\tLoss: 4.6055\tLR: 0.034015\n",
      "Training Epoch: 1 [17152/50000]\tLoss: 4.6044\tLR: 0.034271\n",
      "Training Epoch: 1 [17280/50000]\tLoss: 4.5976\tLR: 0.034527\n",
      "Training Epoch: 1 [17408/50000]\tLoss: 4.6044\tLR: 0.034783\n",
      "Training Epoch: 1 [17536/50000]\tLoss: 4.6101\tLR: 0.035038\n",
      "Training Epoch: 1 [17664/50000]\tLoss: 4.6061\tLR: 0.035294\n",
      "Training Epoch: 1 [17792/50000]\tLoss: 4.6085\tLR: 0.035550\n",
      "Training Epoch: 1 [17920/50000]\tLoss: 4.6030\tLR: 0.035806\n",
      "Training Epoch: 1 [18048/50000]\tLoss: 4.6034\tLR: 0.036061\n",
      "Training Epoch: 1 [18176/50000]\tLoss: 4.5974\tLR: 0.036317\n",
      "Training Epoch: 1 [18304/50000]\tLoss: 4.6171\tLR: 0.036573\n",
      "Training Epoch: 1 [18432/50000]\tLoss: 4.6160\tLR: 0.036829\n",
      "Training Epoch: 1 [18560/50000]\tLoss: 4.6034\tLR: 0.037084\n",
      "Training Epoch: 1 [18688/50000]\tLoss: 4.6065\tLR: 0.037340\n",
      "Training Epoch: 1 [18816/50000]\tLoss: 4.6063\tLR: 0.037596\n",
      "Training Epoch: 1 [18944/50000]\tLoss: 4.6075\tLR: 0.037852\n",
      "Training Epoch: 1 [19072/50000]\tLoss: 4.5986\tLR: 0.038107\n",
      "Training Epoch: 1 [19200/50000]\tLoss: 4.6086\tLR: 0.038363\n",
      "Training Epoch: 1 [19328/50000]\tLoss: 4.6070\tLR: 0.038619\n",
      "Training Epoch: 1 [19456/50000]\tLoss: 4.6073\tLR: 0.038875\n",
      "Training Epoch: 1 [19584/50000]\tLoss: 4.6004\tLR: 0.039130\n",
      "Training Epoch: 1 [19712/50000]\tLoss: 4.5997\tLR: 0.039386\n",
      "Training Epoch: 1 [19840/50000]\tLoss: 4.6047\tLR: 0.039642\n",
      "Training Epoch: 1 [19968/50000]\tLoss: 4.6151\tLR: 0.039898\n",
      "Training Epoch: 1 [20096/50000]\tLoss: 4.6030\tLR: 0.040153\n",
      "Training Epoch: 1 [20224/50000]\tLoss: 4.6040\tLR: 0.040409\n",
      "Training Epoch: 1 [20352/50000]\tLoss: 4.6131\tLR: 0.040665\n",
      "Training Epoch: 1 [20480/50000]\tLoss: 4.5949\tLR: 0.040921\n",
      "Training Epoch: 1 [20608/50000]\tLoss: 4.6026\tLR: 0.041176\n",
      "Training Epoch: 1 [20736/50000]\tLoss: 4.5962\tLR: 0.041432\n",
      "Training Epoch: 1 [20864/50000]\tLoss: 4.6023\tLR: 0.041688\n",
      "Training Epoch: 1 [20992/50000]\tLoss: 4.5964\tLR: 0.041944\n",
      "Training Epoch: 1 [21120/50000]\tLoss: 4.6129\tLR: 0.042199\n",
      "Training Epoch: 1 [21248/50000]\tLoss: 4.5902\tLR: 0.042455\n",
      "Training Epoch: 1 [21376/50000]\tLoss: 4.6069\tLR: 0.042711\n",
      "Training Epoch: 1 [21504/50000]\tLoss: 4.6147\tLR: 0.042967\n",
      "Training Epoch: 1 [21632/50000]\tLoss: 4.6003\tLR: 0.043223\n",
      "Training Epoch: 1 [21760/50000]\tLoss: 4.6052\tLR: 0.043478\n",
      "Training Epoch: 1 [21888/50000]\tLoss: 4.6040\tLR: 0.043734\n",
      "Training Epoch: 1 [22016/50000]\tLoss: 4.5952\tLR: 0.043990\n",
      "Training Epoch: 1 [22144/50000]\tLoss: 4.5969\tLR: 0.044246\n",
      "Training Epoch: 1 [22272/50000]\tLoss: 4.6071\tLR: 0.044501\n",
      "Training Epoch: 1 [22400/50000]\tLoss: 4.5987\tLR: 0.044757\n",
      "Training Epoch: 1 [22528/50000]\tLoss: 4.5999\tLR: 0.045013\n",
      "Training Epoch: 1 [22656/50000]\tLoss: 4.5831\tLR: 0.045269\n",
      "Training Epoch: 1 [22784/50000]\tLoss: 4.6063\tLR: 0.045524\n",
      "Training Epoch: 1 [22912/50000]\tLoss: 4.5895\tLR: 0.045780\n",
      "Training Epoch: 1 [23040/50000]\tLoss: 4.6019\tLR: 0.046036\n",
      "Training Epoch: 1 [23168/50000]\tLoss: 4.5980\tLR: 0.046292\n",
      "Training Epoch: 1 [23296/50000]\tLoss: 4.5832\tLR: 0.046547\n",
      "Training Epoch: 1 [23424/50000]\tLoss: 4.5872\tLR: 0.046803\n",
      "Training Epoch: 1 [23552/50000]\tLoss: 4.5960\tLR: 0.047059\n",
      "Training Epoch: 1 [23680/50000]\tLoss: 4.6052\tLR: 0.047315\n",
      "Training Epoch: 1 [23808/50000]\tLoss: 4.5809\tLR: 0.047570\n",
      "Training Epoch: 1 [23936/50000]\tLoss: 4.5526\tLR: 0.047826\n",
      "Training Epoch: 1 [24064/50000]\tLoss: 4.5927\tLR: 0.048082\n",
      "Training Epoch: 1 [24192/50000]\tLoss: 4.6093\tLR: 0.048338\n",
      "Training Epoch: 1 [24320/50000]\tLoss: 4.5562\tLR: 0.048593\n",
      "Training Epoch: 1 [24448/50000]\tLoss: 4.5581\tLR: 0.048849\n",
      "Training Epoch: 1 [24576/50000]\tLoss: 4.5864\tLR: 0.049105\n",
      "Training Epoch: 1 [24704/50000]\tLoss: 4.5895\tLR: 0.049361\n",
      "Training Epoch: 1 [24832/50000]\tLoss: 4.5709\tLR: 0.049616\n",
      "Training Epoch: 1 [24960/50000]\tLoss: 4.5342\tLR: 0.049872\n",
      "Training Epoch: 1 [25088/50000]\tLoss: 4.5215\tLR: 0.050128\n",
      "Training Epoch: 1 [25216/50000]\tLoss: 4.5711\tLR: 0.050384\n",
      "Training Epoch: 1 [25344/50000]\tLoss: 4.5630\tLR: 0.050639\n",
      "Training Epoch: 1 [25472/50000]\tLoss: 4.5277\tLR: 0.050895\n",
      "Training Epoch: 1 [25600/50000]\tLoss: 4.5798\tLR: 0.051151\n",
      "Training Epoch: 1 [25728/50000]\tLoss: 4.5850\tLR: 0.051407\n",
      "Training Epoch: 1 [25856/50000]\tLoss: 4.6130\tLR: 0.051662\n",
      "Training Epoch: 1 [25984/50000]\tLoss: 4.6069\tLR: 0.051918\n",
      "Training Epoch: 1 [26112/50000]\tLoss: 4.5766\tLR: 0.052174\n",
      "Training Epoch: 1 [26240/50000]\tLoss: 4.5763\tLR: 0.052430\n",
      "Training Epoch: 1 [26368/50000]\tLoss: 4.5415\tLR: 0.052685\n",
      "Training Epoch: 1 [26496/50000]\tLoss: 4.6164\tLR: 0.052941\n",
      "Training Epoch: 1 [26624/50000]\tLoss: 4.5653\tLR: 0.053197\n",
      "Training Epoch: 1 [26752/50000]\tLoss: 4.5774\tLR: 0.053453\n",
      "Training Epoch: 1 [26880/50000]\tLoss: 4.6137\tLR: 0.053708\n",
      "Training Epoch: 1 [27008/50000]\tLoss: 4.5656\tLR: 0.053964\n",
      "Training Epoch: 1 [27136/50000]\tLoss: 4.5407\tLR: 0.054220\n",
      "Training Epoch: 1 [27264/50000]\tLoss: 4.5566\tLR: 0.054476\n",
      "Training Epoch: 1 [27392/50000]\tLoss: 4.5605\tLR: 0.054731\n",
      "Training Epoch: 1 [27520/50000]\tLoss: 4.5545\tLR: 0.054987\n",
      "Training Epoch: 1 [27648/50000]\tLoss: 4.5329\tLR: 0.055243\n",
      "Training Epoch: 1 [27776/50000]\tLoss: 4.6047\tLR: 0.055499\n",
      "Training Epoch: 1 [27904/50000]\tLoss: 4.5102\tLR: 0.055754\n",
      "Training Epoch: 1 [28032/50000]\tLoss: 4.5677\tLR: 0.056010\n",
      "Training Epoch: 1 [28160/50000]\tLoss: 4.5893\tLR: 0.056266\n",
      "Training Epoch: 1 [28288/50000]\tLoss: 4.6331\tLR: 0.056522\n",
      "Training Epoch: 1 [28416/50000]\tLoss: 4.6121\tLR: 0.056777\n",
      "Training Epoch: 1 [28544/50000]\tLoss: 4.6089\tLR: 0.057033\n",
      "Training Epoch: 1 [28672/50000]\tLoss: 4.5734\tLR: 0.057289\n",
      "Training Epoch: 1 [28800/50000]\tLoss: 4.5907\tLR: 0.057545\n",
      "Training Epoch: 1 [28928/50000]\tLoss: 4.5522\tLR: 0.057801\n",
      "Training Epoch: 1 [29056/50000]\tLoss: 4.5801\tLR: 0.058056\n",
      "Training Epoch: 1 [29184/50000]\tLoss: 4.5812\tLR: 0.058312\n",
      "Training Epoch: 1 [29312/50000]\tLoss: 4.5673\tLR: 0.058568\n",
      "Training Epoch: 1 [29440/50000]\tLoss: 4.5976\tLR: 0.058824\n",
      "Training Epoch: 1 [29568/50000]\tLoss: 4.5723\tLR: 0.059079\n",
      "Training Epoch: 1 [29696/50000]\tLoss: 4.5501\tLR: 0.059335\n",
      "Training Epoch: 1 [29824/50000]\tLoss: 4.5905\tLR: 0.059591\n",
      "Training Epoch: 1 [29952/50000]\tLoss: 4.5590\tLR: 0.059847\n",
      "Training Epoch: 1 [30080/50000]\tLoss: 4.5581\tLR: 0.060102\n",
      "Training Epoch: 1 [30208/50000]\tLoss: 4.5570\tLR: 0.060358\n",
      "Training Epoch: 1 [30336/50000]\tLoss: 4.5474\tLR: 0.060614\n",
      "Training Epoch: 1 [30464/50000]\tLoss: 4.5678\tLR: 0.060870\n",
      "Training Epoch: 1 [30592/50000]\tLoss: 4.5701\tLR: 0.061125\n",
      "Training Epoch: 1 [30720/50000]\tLoss: 4.5047\tLR: 0.061381\n",
      "Training Epoch: 1 [30848/50000]\tLoss: 4.5734\tLR: 0.061637\n",
      "Training Epoch: 1 [30976/50000]\tLoss: 4.5649\tLR: 0.061893\n",
      "Training Epoch: 1 [31104/50000]\tLoss: 4.5651\tLR: 0.062148\n",
      "Training Epoch: 1 [31232/50000]\tLoss: 4.6568\tLR: 0.062404\n",
      "Training Epoch: 1 [31360/50000]\tLoss: 4.5094\tLR: 0.062660\n",
      "Training Epoch: 1 [31488/50000]\tLoss: 4.5860\tLR: 0.062916\n",
      "Training Epoch: 1 [31616/50000]\tLoss: 4.5085\tLR: 0.063171\n",
      "Training Epoch: 1 [31744/50000]\tLoss: 4.5186\tLR: 0.063427\n",
      "Training Epoch: 1 [31872/50000]\tLoss: 4.5677\tLR: 0.063683\n",
      "Training Epoch: 1 [32000/50000]\tLoss: 4.5659\tLR: 0.063939\n",
      "Training Epoch: 1 [32128/50000]\tLoss: 4.5261\tLR: 0.064194\n",
      "Training Epoch: 1 [32256/50000]\tLoss: 4.5153\tLR: 0.064450\n",
      "Training Epoch: 1 [32384/50000]\tLoss: 4.5655\tLR: 0.064706\n",
      "Training Epoch: 1 [32512/50000]\tLoss: 4.5708\tLR: 0.064962\n",
      "Training Epoch: 1 [32640/50000]\tLoss: 4.5073\tLR: 0.065217\n",
      "Training Epoch: 1 [32768/50000]\tLoss: 4.5692\tLR: 0.065473\n",
      "Training Epoch: 1 [32896/50000]\tLoss: 4.5503\tLR: 0.065729\n",
      "Training Epoch: 1 [33024/50000]\tLoss: 4.5082\tLR: 0.065985\n",
      "Training Epoch: 1 [33152/50000]\tLoss: 4.5092\tLR: 0.066240\n",
      "Training Epoch: 1 [33280/50000]\tLoss: 4.5244\tLR: 0.066496\n",
      "Training Epoch: 1 [33408/50000]\tLoss: 4.5500\tLR: 0.066752\n",
      "Training Epoch: 1 [33536/50000]\tLoss: 4.4945\tLR: 0.067008\n",
      "Training Epoch: 1 [33664/50000]\tLoss: 4.5590\tLR: 0.067263\n",
      "Training Epoch: 1 [33792/50000]\tLoss: 4.5954\tLR: 0.067519\n",
      "Training Epoch: 1 [33920/50000]\tLoss: 4.5294\tLR: 0.067775\n",
      "Training Epoch: 1 [34048/50000]\tLoss: 4.5178\tLR: 0.068031\n",
      "Training Epoch: 1 [34176/50000]\tLoss: 4.5667\tLR: 0.068286\n",
      "Training Epoch: 1 [34304/50000]\tLoss: 4.5719\tLR: 0.068542\n",
      "Training Epoch: 1 [34432/50000]\tLoss: 4.4978\tLR: 0.068798\n",
      "Training Epoch: 1 [34560/50000]\tLoss: 4.5271\tLR: 0.069054\n",
      "Training Epoch: 1 [34688/50000]\tLoss: 4.5194\tLR: 0.069309\n",
      "Training Epoch: 1 [34816/50000]\tLoss: 4.5340\tLR: 0.069565\n",
      "Training Epoch: 1 [34944/50000]\tLoss: 4.4975\tLR: 0.069821\n",
      "Training Epoch: 1 [35072/50000]\tLoss: 4.4993\tLR: 0.070077\n",
      "Training Epoch: 1 [35200/50000]\tLoss: 4.5252\tLR: 0.070332\n",
      "Training Epoch: 1 [35328/50000]\tLoss: 4.5564\tLR: 0.070588\n",
      "Training Epoch: 1 [35456/50000]\tLoss: 4.4873\tLR: 0.070844\n",
      "Training Epoch: 1 [35584/50000]\tLoss: 4.4807\tLR: 0.071100\n",
      "Training Epoch: 1 [35712/50000]\tLoss: 4.5097\tLR: 0.071355\n",
      "Training Epoch: 1 [35840/50000]\tLoss: 4.4912\tLR: 0.071611\n",
      "Training Epoch: 1 [35968/50000]\tLoss: 4.5444\tLR: 0.071867\n",
      "Training Epoch: 1 [36096/50000]\tLoss: 4.5484\tLR: 0.072123\n",
      "Training Epoch: 1 [36224/50000]\tLoss: 4.5210\tLR: 0.072379\n",
      "Training Epoch: 1 [36352/50000]\tLoss: 4.5025\tLR: 0.072634\n",
      "Training Epoch: 1 [36480/50000]\tLoss: 4.4723\tLR: 0.072890\n",
      "Training Epoch: 1 [36608/50000]\tLoss: 4.4981\tLR: 0.073146\n",
      "Training Epoch: 1 [36736/50000]\tLoss: 4.5056\tLR: 0.073402\n",
      "Training Epoch: 1 [36864/50000]\tLoss: 4.5222\tLR: 0.073657\n",
      "Training Epoch: 1 [36992/50000]\tLoss: 4.5457\tLR: 0.073913\n",
      "Training Epoch: 1 [37120/50000]\tLoss: 4.4917\tLR: 0.074169\n",
      "Training Epoch: 1 [37248/50000]\tLoss: 4.4948\tLR: 0.074425\n",
      "Training Epoch: 1 [37376/50000]\tLoss: 4.4936\tLR: 0.074680\n",
      "Training Epoch: 1 [37504/50000]\tLoss: 4.4434\tLR: 0.074936\n",
      "Training Epoch: 1 [37632/50000]\tLoss: 4.4941\tLR: 0.075192\n",
      "Training Epoch: 1 [37760/50000]\tLoss: 4.5116\tLR: 0.075448\n",
      "Training Epoch: 1 [37888/50000]\tLoss: 4.5332\tLR: 0.075703\n",
      "Training Epoch: 1 [38016/50000]\tLoss: 4.5230\tLR: 0.075959\n",
      "Training Epoch: 1 [38144/50000]\tLoss: 4.4264\tLR: 0.076215\n",
      "Training Epoch: 1 [38272/50000]\tLoss: 4.4166\tLR: 0.076471\n",
      "Training Epoch: 1 [38400/50000]\tLoss: 4.3669\tLR: 0.076726\n",
      "Training Epoch: 1 [38528/50000]\tLoss: 4.4397\tLR: 0.076982\n",
      "Training Epoch: 1 [38656/50000]\tLoss: 4.4123\tLR: 0.077238\n",
      "Training Epoch: 1 [38784/50000]\tLoss: 4.4407\tLR: 0.077494\n",
      "Training Epoch: 1 [38912/50000]\tLoss: 4.3554\tLR: 0.077749\n",
      "Training Epoch: 1 [39040/50000]\tLoss: 4.4356\tLR: 0.078005\n",
      "Training Epoch: 1 [39168/50000]\tLoss: 4.5231\tLR: 0.078261\n",
      "Training Epoch: 1 [39296/50000]\tLoss: 4.4629\tLR: 0.078517\n",
      "Training Epoch: 1 [39424/50000]\tLoss: 4.4858\tLR: 0.078772\n",
      "Training Epoch: 1 [39552/50000]\tLoss: 4.3310\tLR: 0.079028\n",
      "Training Epoch: 1 [39680/50000]\tLoss: 4.4135\tLR: 0.079284\n",
      "Training Epoch: 1 [39808/50000]\tLoss: 4.4356\tLR: 0.079540\n",
      "Training Epoch: 1 [39936/50000]\tLoss: 4.4616\tLR: 0.079795\n",
      "Training Epoch: 1 [40064/50000]\tLoss: 4.3723\tLR: 0.080051\n",
      "Training Epoch: 1 [40192/50000]\tLoss: 4.5322\tLR: 0.080307\n",
      "Training Epoch: 1 [40320/50000]\tLoss: 4.3791\tLR: 0.080563\n",
      "Training Epoch: 1 [40448/50000]\tLoss: 4.4364\tLR: 0.080818\n",
      "Training Epoch: 1 [40576/50000]\tLoss: 4.4857\tLR: 0.081074\n",
      "Training Epoch: 1 [40704/50000]\tLoss: 4.4841\tLR: 0.081330\n",
      "Training Epoch: 1 [40832/50000]\tLoss: 4.3806\tLR: 0.081586\n",
      "Training Epoch: 1 [40960/50000]\tLoss: 4.4354\tLR: 0.081841\n",
      "Training Epoch: 1 [41088/50000]\tLoss: 4.4830\tLR: 0.082097\n",
      "Training Epoch: 1 [41216/50000]\tLoss: 4.4102\tLR: 0.082353\n",
      "Training Epoch: 1 [41344/50000]\tLoss: 4.3678\tLR: 0.082609\n",
      "Training Epoch: 1 [41472/50000]\tLoss: 4.4609\tLR: 0.082864\n",
      "Training Epoch: 1 [41600/50000]\tLoss: 4.4946\tLR: 0.083120\n",
      "Training Epoch: 1 [41728/50000]\tLoss: 4.4129\tLR: 0.083376\n",
      "Training Epoch: 1 [41856/50000]\tLoss: 4.4362\tLR: 0.083632\n",
      "Training Epoch: 1 [41984/50000]\tLoss: 4.4239\tLR: 0.083887\n",
      "Training Epoch: 1 [42112/50000]\tLoss: 4.3869\tLR: 0.084143\n",
      "Training Epoch: 1 [42240/50000]\tLoss: 4.4317\tLR: 0.084399\n",
      "Training Epoch: 1 [42368/50000]\tLoss: 4.3909\tLR: 0.084655\n",
      "Training Epoch: 1 [42496/50000]\tLoss: 4.3955\tLR: 0.084910\n",
      "Training Epoch: 1 [42624/50000]\tLoss: 4.4152\tLR: 0.085166\n",
      "Training Epoch: 1 [42752/50000]\tLoss: 4.2687\tLR: 0.085422\n",
      "Training Epoch: 1 [42880/50000]\tLoss: 4.1879\tLR: 0.085678\n",
      "Training Epoch: 1 [43008/50000]\tLoss: 4.3808\tLR: 0.085934\n",
      "Training Epoch: 1 [43136/50000]\tLoss: 4.3078\tLR: 0.086189\n",
      "Training Epoch: 1 [43264/50000]\tLoss: 4.2828\tLR: 0.086445\n",
      "Training Epoch: 1 [43392/50000]\tLoss: 4.2266\tLR: 0.086701\n",
      "Training Epoch: 1 [43520/50000]\tLoss: 4.3264\tLR: 0.086957\n",
      "Training Epoch: 1 [43648/50000]\tLoss: 4.3790\tLR: 0.087212\n",
      "Training Epoch: 1 [43776/50000]\tLoss: 4.2984\tLR: 0.087468\n",
      "Training Epoch: 1 [43904/50000]\tLoss: 4.4046\tLR: 0.087724\n",
      "Training Epoch: 1 [44032/50000]\tLoss: 4.3725\tLR: 0.087980\n",
      "Training Epoch: 1 [44160/50000]\tLoss: 4.3461\tLR: 0.088235\n",
      "Training Epoch: 1 [44288/50000]\tLoss: 4.4047\tLR: 0.088491\n",
      "Training Epoch: 1 [44416/50000]\tLoss: 4.2703\tLR: 0.088747\n",
      "Training Epoch: 1 [44544/50000]\tLoss: 4.3572\tLR: 0.089003\n",
      "Training Epoch: 1 [44672/50000]\tLoss: 4.3820\tLR: 0.089258\n",
      "Training Epoch: 1 [44800/50000]\tLoss: 4.3611\tLR: 0.089514\n",
      "Training Epoch: 1 [44928/50000]\tLoss: 4.4247\tLR: 0.089770\n",
      "Training Epoch: 1 [45056/50000]\tLoss: 4.3183\tLR: 0.090026\n",
      "Training Epoch: 1 [45184/50000]\tLoss: 4.3128\tLR: 0.090281\n",
      "Training Epoch: 1 [45312/50000]\tLoss: 4.2934\tLR: 0.090537\n",
      "Training Epoch: 1 [45440/50000]\tLoss: 4.3184\tLR: 0.090793\n",
      "Training Epoch: 1 [45568/50000]\tLoss: 4.2467\tLR: 0.091049\n",
      "Training Epoch: 1 [45696/50000]\tLoss: 4.3364\tLR: 0.091304\n",
      "Training Epoch: 1 [45824/50000]\tLoss: 4.3206\tLR: 0.091560\n",
      "Training Epoch: 1 [45952/50000]\tLoss: 4.2264\tLR: 0.091816\n",
      "Training Epoch: 1 [46080/50000]\tLoss: 4.3368\tLR: 0.092072\n",
      "Training Epoch: 1 [46208/50000]\tLoss: 4.3340\tLR: 0.092327\n",
      "Training Epoch: 1 [46336/50000]\tLoss: 4.2934\tLR: 0.092583\n",
      "Training Epoch: 1 [46464/50000]\tLoss: 4.1720\tLR: 0.092839\n",
      "Training Epoch: 1 [46592/50000]\tLoss: 4.0942\tLR: 0.093095\n",
      "Training Epoch: 1 [46720/50000]\tLoss: 4.2894\tLR: 0.093350\n",
      "Training Epoch: 1 [46848/50000]\tLoss: 4.1918\tLR: 0.093606\n",
      "Training Epoch: 1 [46976/50000]\tLoss: 4.1624\tLR: 0.093862\n",
      "Training Epoch: 1 [47104/50000]\tLoss: 4.2489\tLR: 0.094118\n",
      "Training Epoch: 1 [47232/50000]\tLoss: 4.1640\tLR: 0.094373\n",
      "Training Epoch: 1 [47360/50000]\tLoss: 4.2698\tLR: 0.094629\n",
      "Training Epoch: 1 [47488/50000]\tLoss: 4.1138\tLR: 0.094885\n",
      "Training Epoch: 1 [47616/50000]\tLoss: 4.1567\tLR: 0.095141\n",
      "Training Epoch: 1 [47744/50000]\tLoss: 4.2069\tLR: 0.095396\n",
      "Training Epoch: 1 [47872/50000]\tLoss: 4.1344\tLR: 0.095652\n",
      "Training Epoch: 1 [48000/50000]\tLoss: 4.2577\tLR: 0.095908\n",
      "Training Epoch: 1 [48128/50000]\tLoss: 4.1965\tLR: 0.096164\n",
      "Training Epoch: 1 [48256/50000]\tLoss: 4.1912\tLR: 0.096419\n",
      "Training Epoch: 1 [48384/50000]\tLoss: 4.2361\tLR: 0.096675\n",
      "Training Epoch: 1 [48512/50000]\tLoss: 4.3006\tLR: 0.096931\n",
      "Training Epoch: 1 [48640/50000]\tLoss: 4.1855\tLR: 0.097187\n",
      "Training Epoch: 1 [48768/50000]\tLoss: 4.1476\tLR: 0.097442\n",
      "Training Epoch: 1 [48896/50000]\tLoss: 4.2869\tLR: 0.097698\n",
      "Training Epoch: 1 [49024/50000]\tLoss: 4.2435\tLR: 0.097954\n",
      "Training Epoch: 1 [49152/50000]\tLoss: 4.2425\tLR: 0.098210\n",
      "Training Epoch: 1 [49280/50000]\tLoss: 4.1658\tLR: 0.098465\n",
      "Training Epoch: 1 [49408/50000]\tLoss: 4.0864\tLR: 0.098721\n",
      "Training Epoch: 1 [49536/50000]\tLoss: 4.1321\tLR: 0.098977\n",
      "Training Epoch: 1 [49664/50000]\tLoss: 4.0606\tLR: 0.099233\n",
      "Training Epoch: 1 [49792/50000]\tLoss: 4.1202\tLR: 0.099488\n",
      "Training Epoch: 1 [49920/50000]\tLoss: 4.1005\tLR: 0.099744\n",
      "Training Epoch: 1 [50000/50000]\tLoss: 4.2083\tLR: 0.100000\n",
      "Test set: Average loss: 0.0338, Accuracy: 0.0340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [128/50000]\tLoss: 4.0966\tLR: 0.100000\n",
      "Training Epoch: 2 [256/50000]\tLoss: 4.1079\tLR: 0.100000\n",
      "Training Epoch: 2 [384/50000]\tLoss: 4.1327\tLR: 0.100000\n",
      "Training Epoch: 2 [512/50000]\tLoss: 4.1637\tLR: 0.100000\n",
      "Training Epoch: 2 [640/50000]\tLoss: 4.0621\tLR: 0.100000\n",
      "Training Epoch: 2 [768/50000]\tLoss: 4.1621\tLR: 0.100000\n",
      "Training Epoch: 2 [896/50000]\tLoss: 4.2108\tLR: 0.100000\n",
      "Training Epoch: 2 [1024/50000]\tLoss: 4.0706\tLR: 0.100000\n",
      "Training Epoch: 2 [1152/50000]\tLoss: 4.1268\tLR: 0.100000\n",
      "Training Epoch: 2 [1280/50000]\tLoss: 4.2095\tLR: 0.100000\n",
      "Training Epoch: 2 [1408/50000]\tLoss: 4.2186\tLR: 0.100000\n",
      "Training Epoch: 2 [1536/50000]\tLoss: 4.0874\tLR: 0.100000\n",
      "Training Epoch: 2 [1664/50000]\tLoss: 4.0525\tLR: 0.100000\n",
      "Training Epoch: 2 [1792/50000]\tLoss: 4.1406\tLR: 0.100000\n",
      "Training Epoch: 2 [1920/50000]\tLoss: 4.1529\tLR: 0.100000\n",
      "Training Epoch: 2 [2048/50000]\tLoss: 4.0466\tLR: 0.100000\n",
      "Training Epoch: 2 [2176/50000]\tLoss: 4.1613\tLR: 0.100000\n",
      "Training Epoch: 2 [2304/50000]\tLoss: 4.0657\tLR: 0.100000\n",
      "Training Epoch: 2 [2432/50000]\tLoss: 4.1783\tLR: 0.100000\n",
      "Training Epoch: 2 [2560/50000]\tLoss: 4.1318\tLR: 0.100000\n",
      "Training Epoch: 2 [2688/50000]\tLoss: 4.0777\tLR: 0.100000\n",
      "Training Epoch: 2 [2816/50000]\tLoss: 4.1963\tLR: 0.100000\n",
      "Training Epoch: 2 [2944/50000]\tLoss: 4.1649\tLR: 0.100000\n",
      "Training Epoch: 2 [3072/50000]\tLoss: 4.0827\tLR: 0.100000\n",
      "Training Epoch: 2 [3200/50000]\tLoss: 4.0088\tLR: 0.100000\n",
      "Training Epoch: 2 [3328/50000]\tLoss: 4.1983\tLR: 0.100000\n",
      "Training Epoch: 2 [3456/50000]\tLoss: 4.1523\tLR: 0.100000\n",
      "Training Epoch: 2 [3584/50000]\tLoss: 4.0801\tLR: 0.100000\n",
      "Training Epoch: 2 [3712/50000]\tLoss: 4.1359\tLR: 0.100000\n",
      "Training Epoch: 2 [3840/50000]\tLoss: 4.1289\tLR: 0.100000\n",
      "Training Epoch: 2 [3968/50000]\tLoss: 3.9920\tLR: 0.100000\n",
      "Training Epoch: 2 [4096/50000]\tLoss: 4.0218\tLR: 0.100000\n",
      "Training Epoch: 2 [4224/50000]\tLoss: 4.2299\tLR: 0.100000\n",
      "Training Epoch: 2 [4352/50000]\tLoss: 4.2044\tLR: 0.100000\n",
      "Training Epoch: 2 [4480/50000]\tLoss: 3.9655\tLR: 0.100000\n",
      "Training Epoch: 2 [4608/50000]\tLoss: 4.0896\tLR: 0.100000\n",
      "Training Epoch: 2 [4736/50000]\tLoss: 3.9502\tLR: 0.100000\n",
      "Training Epoch: 2 [4864/50000]\tLoss: 4.1692\tLR: 0.100000\n",
      "Training Epoch: 2 [4992/50000]\tLoss: 4.1111\tLR: 0.100000\n",
      "Training Epoch: 2 [5120/50000]\tLoss: 4.0350\tLR: 0.100000\n",
      "Training Epoch: 2 [5248/50000]\tLoss: 4.0248\tLR: 0.100000\n",
      "Training Epoch: 2 [5376/50000]\tLoss: 4.0596\tLR: 0.100000\n",
      "Training Epoch: 2 [5504/50000]\tLoss: 4.1177\tLR: 0.100000\n",
      "Training Epoch: 2 [5632/50000]\tLoss: 4.0976\tLR: 0.100000\n",
      "Training Epoch: 2 [5760/50000]\tLoss: 4.1881\tLR: 0.100000\n",
      "Training Epoch: 2 [5888/50000]\tLoss: 4.2136\tLR: 0.100000\n",
      "Training Epoch: 2 [6016/50000]\tLoss: 4.0270\tLR: 0.100000\n",
      "Training Epoch: 2 [6144/50000]\tLoss: 4.1482\tLR: 0.100000\n",
      "Training Epoch: 2 [6272/50000]\tLoss: 4.1165\tLR: 0.100000\n",
      "Training Epoch: 2 [6400/50000]\tLoss: 4.0539\tLR: 0.100000\n",
      "Training Epoch: 2 [6528/50000]\tLoss: 3.9822\tLR: 0.100000\n",
      "Training Epoch: 2 [6656/50000]\tLoss: 4.2705\tLR: 0.100000\n",
      "Training Epoch: 2 [6784/50000]\tLoss: 4.0591\tLR: 0.100000\n",
      "Training Epoch: 2 [6912/50000]\tLoss: 3.9995\tLR: 0.100000\n",
      "Training Epoch: 2 [7040/50000]\tLoss: 4.1160\tLR: 0.100000\n",
      "Training Epoch: 2 [7168/50000]\tLoss: 4.1258\tLR: 0.100000\n",
      "Training Epoch: 2 [7296/50000]\tLoss: 4.1688\tLR: 0.100000\n",
      "Training Epoch: 2 [7424/50000]\tLoss: 4.0315\tLR: 0.100000\n",
      "Training Epoch: 2 [7552/50000]\tLoss: 3.9684\tLR: 0.100000\n",
      "Training Epoch: 2 [7680/50000]\tLoss: 4.1722\tLR: 0.100000\n",
      "Training Epoch: 2 [7808/50000]\tLoss: 4.0160\tLR: 0.100000\n",
      "Training Epoch: 2 [7936/50000]\tLoss: 4.2690\tLR: 0.100000\n",
      "Training Epoch: 2 [8064/50000]\tLoss: 4.0107\tLR: 0.100000\n",
      "Training Epoch: 2 [8192/50000]\tLoss: 3.9992\tLR: 0.100000\n",
      "Training Epoch: 2 [8320/50000]\tLoss: 4.0421\tLR: 0.100000\n",
      "Training Epoch: 2 [8448/50000]\tLoss: 4.1510\tLR: 0.100000\n",
      "Training Epoch: 2 [8576/50000]\tLoss: 3.9452\tLR: 0.100000\n",
      "Training Epoch: 2 [8704/50000]\tLoss: 4.1114\tLR: 0.100000\n",
      "Training Epoch: 2 [8832/50000]\tLoss: 4.1525\tLR: 0.100000\n",
      "Training Epoch: 2 [8960/50000]\tLoss: 4.1597\tLR: 0.100000\n",
      "Training Epoch: 2 [9088/50000]\tLoss: 3.9623\tLR: 0.100000\n",
      "Training Epoch: 2 [9216/50000]\tLoss: 4.1179\tLR: 0.100000\n",
      "Training Epoch: 2 [9344/50000]\tLoss: 4.2231\tLR: 0.100000\n",
      "Training Epoch: 2 [9472/50000]\tLoss: 3.9826\tLR: 0.100000\n",
      "Training Epoch: 2 [9600/50000]\tLoss: 4.1084\tLR: 0.100000\n",
      "Training Epoch: 2 [9728/50000]\tLoss: 4.1088\tLR: 0.100000\n",
      "Training Epoch: 2 [9856/50000]\tLoss: 4.0262\tLR: 0.100000\n",
      "Training Epoch: 2 [9984/50000]\tLoss: 4.0148\tLR: 0.100000\n",
      "Training Epoch: 2 [10112/50000]\tLoss: 3.9319\tLR: 0.100000\n",
      "Training Epoch: 2 [10240/50000]\tLoss: 4.0587\tLR: 0.100000\n",
      "Training Epoch: 2 [10368/50000]\tLoss: 3.9580\tLR: 0.100000\n",
      "Training Epoch: 2 [10496/50000]\tLoss: 3.7956\tLR: 0.100000\n",
      "Training Epoch: 2 [10624/50000]\tLoss: 3.9341\tLR: 0.100000\n",
      "Training Epoch: 2 [10752/50000]\tLoss: 3.9760\tLR: 0.100000\n",
      "Training Epoch: 2 [10880/50000]\tLoss: 4.0368\tLR: 0.100000\n",
      "Training Epoch: 2 [11008/50000]\tLoss: 3.9248\tLR: 0.100000\n",
      "Training Epoch: 2 [11136/50000]\tLoss: 3.9237\tLR: 0.100000\n",
      "Training Epoch: 2 [11264/50000]\tLoss: 3.8207\tLR: 0.100000\n",
      "Training Epoch: 2 [11392/50000]\tLoss: 3.9452\tLR: 0.100000\n",
      "Training Epoch: 2 [11520/50000]\tLoss: 3.8411\tLR: 0.100000\n",
      "Training Epoch: 2 [11648/50000]\tLoss: 3.8542\tLR: 0.100000\n",
      "Training Epoch: 2 [11776/50000]\tLoss: 3.8181\tLR: 0.100000\n",
      "Training Epoch: 2 [11904/50000]\tLoss: 4.2613\tLR: 0.100000\n",
      "Training Epoch: 2 [12032/50000]\tLoss: 3.9492\tLR: 0.100000\n",
      "Training Epoch: 2 [12160/50000]\tLoss: 4.0045\tLR: 0.100000\n",
      "Training Epoch: 2 [12288/50000]\tLoss: 4.0476\tLR: 0.100000\n",
      "Training Epoch: 2 [12416/50000]\tLoss: 4.1927\tLR: 0.100000\n",
      "Training Epoch: 2 [12544/50000]\tLoss: 4.1263\tLR: 0.100000\n",
      "Training Epoch: 2 [12672/50000]\tLoss: 3.8727\tLR: 0.100000\n",
      "Training Epoch: 2 [12800/50000]\tLoss: 4.1117\tLR: 0.100000\n",
      "Training Epoch: 2 [12928/50000]\tLoss: 4.0732\tLR: 0.100000\n",
      "Training Epoch: 2 [13056/50000]\tLoss: 3.8357\tLR: 0.100000\n",
      "Training Epoch: 2 [13184/50000]\tLoss: 3.8732\tLR: 0.100000\n",
      "Training Epoch: 2 [13312/50000]\tLoss: 3.9532\tLR: 0.100000\n",
      "Training Epoch: 2 [13440/50000]\tLoss: 3.9527\tLR: 0.100000\n",
      "Training Epoch: 2 [13568/50000]\tLoss: 3.9791\tLR: 0.100000\n",
      "Training Epoch: 2 [13696/50000]\tLoss: 3.9293\tLR: 0.100000\n",
      "Training Epoch: 2 [13824/50000]\tLoss: 3.8967\tLR: 0.100000\n",
      "Training Epoch: 2 [13952/50000]\tLoss: 3.8385\tLR: 0.100000\n",
      "Training Epoch: 2 [14080/50000]\tLoss: 4.0868\tLR: 0.100000\n",
      "Training Epoch: 2 [14208/50000]\tLoss: 4.2356\tLR: 0.100000\n",
      "Training Epoch: 2 [14336/50000]\tLoss: 4.1134\tLR: 0.100000\n",
      "Training Epoch: 2 [14464/50000]\tLoss: 3.9166\tLR: 0.100000\n",
      "Training Epoch: 2 [14592/50000]\tLoss: 4.1012\tLR: 0.100000\n",
      "Training Epoch: 2 [14720/50000]\tLoss: 4.1092\tLR: 0.100000\n",
      "Training Epoch: 2 [14848/50000]\tLoss: 4.0545\tLR: 0.100000\n",
      "Training Epoch: 2 [14976/50000]\tLoss: 4.0199\tLR: 0.100000\n",
      "Training Epoch: 2 [15104/50000]\tLoss: 4.0241\tLR: 0.100000\n",
      "Training Epoch: 2 [15232/50000]\tLoss: 3.9890\tLR: 0.100000\n",
      "Training Epoch: 2 [15360/50000]\tLoss: 4.0457\tLR: 0.100000\n",
      "Training Epoch: 2 [15488/50000]\tLoss: 4.0476\tLR: 0.100000\n",
      "Training Epoch: 2 [15616/50000]\tLoss: 4.0175\tLR: 0.100000\n",
      "Training Epoch: 2 [15744/50000]\tLoss: 4.1460\tLR: 0.100000\n",
      "Training Epoch: 2 [15872/50000]\tLoss: 3.9503\tLR: 0.100000\n",
      "Training Epoch: 2 [16000/50000]\tLoss: 3.9956\tLR: 0.100000\n",
      "Training Epoch: 2 [16128/50000]\tLoss: 4.0924\tLR: 0.100000\n",
      "Training Epoch: 2 [16256/50000]\tLoss: 3.9914\tLR: 0.100000\n",
      "Training Epoch: 2 [16384/50000]\tLoss: 3.8773\tLR: 0.100000\n",
      "Training Epoch: 2 [16512/50000]\tLoss: 4.0202\tLR: 0.100000\n",
      "Training Epoch: 2 [16640/50000]\tLoss: 3.8895\tLR: 0.100000\n",
      "Training Epoch: 2 [16768/50000]\tLoss: 3.9308\tLR: 0.100000\n",
      "Training Epoch: 2 [16896/50000]\tLoss: 3.9753\tLR: 0.100000\n",
      "Training Epoch: 2 [17024/50000]\tLoss: 3.9882\tLR: 0.100000\n",
      "Training Epoch: 2 [17152/50000]\tLoss: 3.9216\tLR: 0.100000\n",
      "Training Epoch: 2 [17280/50000]\tLoss: 3.7908\tLR: 0.100000\n",
      "Training Epoch: 2 [17408/50000]\tLoss: 3.7756\tLR: 0.100000\n",
      "Training Epoch: 2 [17536/50000]\tLoss: 4.0219\tLR: 0.100000\n",
      "Training Epoch: 2 [17664/50000]\tLoss: 4.0048\tLR: 0.100000\n",
      "Training Epoch: 2 [17792/50000]\tLoss: 3.8780\tLR: 0.100000\n",
      "Training Epoch: 2 [17920/50000]\tLoss: 3.9659\tLR: 0.100000\n",
      "Training Epoch: 2 [18048/50000]\tLoss: 3.9327\tLR: 0.100000\n",
      "Training Epoch: 2 [18176/50000]\tLoss: 3.9898\tLR: 0.100000\n",
      "Training Epoch: 2 [18304/50000]\tLoss: 3.9187\tLR: 0.100000\n",
      "Training Epoch: 2 [18432/50000]\tLoss: 3.7837\tLR: 0.100000\n",
      "Training Epoch: 2 [18560/50000]\tLoss: 3.9681\tLR: 0.100000\n",
      "Training Epoch: 2 [18688/50000]\tLoss: 3.8822\tLR: 0.100000\n",
      "Training Epoch: 2 [18816/50000]\tLoss: 3.9749\tLR: 0.100000\n",
      "Training Epoch: 2 [18944/50000]\tLoss: 4.0061\tLR: 0.100000\n",
      "Training Epoch: 2 [19072/50000]\tLoss: 3.7989\tLR: 0.100000\n",
      "Training Epoch: 2 [19200/50000]\tLoss: 3.9586\tLR: 0.100000\n",
      "Training Epoch: 2 [19328/50000]\tLoss: 3.7570\tLR: 0.100000\n",
      "Training Epoch: 2 [19456/50000]\tLoss: 3.8229\tLR: 0.100000\n",
      "Training Epoch: 2 [19584/50000]\tLoss: 4.0281\tLR: 0.100000\n",
      "Training Epoch: 2 [19712/50000]\tLoss: 4.0317\tLR: 0.100000\n",
      "Training Epoch: 2 [19840/50000]\tLoss: 3.7148\tLR: 0.100000\n",
      "Training Epoch: 2 [19968/50000]\tLoss: 3.8634\tLR: 0.100000\n",
      "Training Epoch: 2 [20096/50000]\tLoss: 3.8701\tLR: 0.100000\n",
      "Training Epoch: 2 [20224/50000]\tLoss: 4.0699\tLR: 0.100000\n",
      "Training Epoch: 2 [20352/50000]\tLoss: 3.9194\tLR: 0.100000\n",
      "Training Epoch: 2 [20480/50000]\tLoss: 3.9218\tLR: 0.100000\n",
      "Training Epoch: 2 [20608/50000]\tLoss: 3.8264\tLR: 0.100000\n",
      "Training Epoch: 2 [20736/50000]\tLoss: 3.9795\tLR: 0.100000\n",
      "Training Epoch: 2 [20864/50000]\tLoss: 3.8213\tLR: 0.100000\n",
      "Training Epoch: 2 [20992/50000]\tLoss: 4.0611\tLR: 0.100000\n",
      "Training Epoch: 2 [21120/50000]\tLoss: 3.9661\tLR: 0.100000\n",
      "Training Epoch: 2 [21248/50000]\tLoss: 3.8388\tLR: 0.100000\n",
      "Training Epoch: 2 [21376/50000]\tLoss: 3.8700\tLR: 0.100000\n",
      "Training Epoch: 2 [21504/50000]\tLoss: 3.9541\tLR: 0.100000\n",
      "Training Epoch: 2 [21632/50000]\tLoss: 4.0072\tLR: 0.100000\n",
      "Training Epoch: 2 [21760/50000]\tLoss: 3.9348\tLR: 0.100000\n",
      "Training Epoch: 2 [21888/50000]\tLoss: 4.0782\tLR: 0.100000\n",
      "Training Epoch: 2 [22016/50000]\tLoss: 3.8676\tLR: 0.100000\n",
      "Training Epoch: 2 [22144/50000]\tLoss: 3.9940\tLR: 0.100000\n",
      "Training Epoch: 2 [22272/50000]\tLoss: 3.8683\tLR: 0.100000\n",
      "Training Epoch: 2 [22400/50000]\tLoss: 4.1024\tLR: 0.100000\n",
      "Training Epoch: 2 [22528/50000]\tLoss: 3.9693\tLR: 0.100000\n",
      "Training Epoch: 2 [22656/50000]\tLoss: 3.9390\tLR: 0.100000\n",
      "Training Epoch: 2 [22784/50000]\tLoss: 3.8974\tLR: 0.100000\n",
      "Training Epoch: 2 [22912/50000]\tLoss: 3.9418\tLR: 0.100000\n",
      "Training Epoch: 2 [23040/50000]\tLoss: 3.7645\tLR: 0.100000\n",
      "Training Epoch: 2 [23168/50000]\tLoss: 3.8260\tLR: 0.100000\n",
      "Training Epoch: 2 [23296/50000]\tLoss: 3.8098\tLR: 0.100000\n",
      "Training Epoch: 2 [23424/50000]\tLoss: 3.9620\tLR: 0.100000\n",
      "Training Epoch: 2 [23552/50000]\tLoss: 3.8816\tLR: 0.100000\n",
      "Training Epoch: 2 [23680/50000]\tLoss: 3.8774\tLR: 0.100000\n",
      "Training Epoch: 2 [23808/50000]\tLoss: 3.9274\tLR: 0.100000\n",
      "Training Epoch: 2 [23936/50000]\tLoss: 3.8634\tLR: 0.100000\n",
      "Training Epoch: 2 [24064/50000]\tLoss: 3.7506\tLR: 0.100000\n",
      "Training Epoch: 2 [24192/50000]\tLoss: 3.8324\tLR: 0.100000\n",
      "Training Epoch: 2 [24320/50000]\tLoss: 3.7597\tLR: 0.100000\n",
      "Training Epoch: 2 [24448/50000]\tLoss: 3.8994\tLR: 0.100000\n",
      "Training Epoch: 2 [24576/50000]\tLoss: 3.9064\tLR: 0.100000\n",
      "Training Epoch: 2 [24704/50000]\tLoss: 3.8767\tLR: 0.100000\n",
      "Training Epoch: 2 [24832/50000]\tLoss: 3.8598\tLR: 0.100000\n",
      "Training Epoch: 2 [24960/50000]\tLoss: 3.9087\tLR: 0.100000\n",
      "Training Epoch: 2 [25088/50000]\tLoss: 3.9936\tLR: 0.100000\n",
      "Training Epoch: 2 [25216/50000]\tLoss: 3.7022\tLR: 0.100000\n",
      "Training Epoch: 2 [25344/50000]\tLoss: 3.8092\tLR: 0.100000\n",
      "Training Epoch: 2 [25472/50000]\tLoss: 3.7631\tLR: 0.100000\n",
      "Training Epoch: 2 [25600/50000]\tLoss: 3.8429\tLR: 0.100000\n",
      "Training Epoch: 2 [25728/50000]\tLoss: 3.8808\tLR: 0.100000\n",
      "Training Epoch: 2 [25856/50000]\tLoss: 3.7595\tLR: 0.100000\n",
      "Training Epoch: 2 [25984/50000]\tLoss: 3.8335\tLR: 0.100000\n",
      "Training Epoch: 2 [26112/50000]\tLoss: 3.7885\tLR: 0.100000\n",
      "Training Epoch: 2 [26240/50000]\tLoss: 3.9264\tLR: 0.100000\n",
      "Training Epoch: 2 [26368/50000]\tLoss: 3.9627\tLR: 0.100000\n",
      "Training Epoch: 2 [26496/50000]\tLoss: 3.9804\tLR: 0.100000\n",
      "Training Epoch: 2 [26624/50000]\tLoss: 3.7960\tLR: 0.100000\n",
      "Training Epoch: 2 [26752/50000]\tLoss: 3.7919\tLR: 0.100000\n",
      "Training Epoch: 2 [26880/50000]\tLoss: 4.0035\tLR: 0.100000\n",
      "Training Epoch: 2 [27008/50000]\tLoss: 3.8361\tLR: 0.100000\n",
      "Training Epoch: 2 [27136/50000]\tLoss: 3.7548\tLR: 0.100000\n",
      "Training Epoch: 2 [27264/50000]\tLoss: 3.7615\tLR: 0.100000\n",
      "Training Epoch: 2 [27392/50000]\tLoss: 3.6724\tLR: 0.100000\n",
      "Training Epoch: 2 [27520/50000]\tLoss: 3.8960\tLR: 0.100000\n",
      "Training Epoch: 2 [27648/50000]\tLoss: 3.9995\tLR: 0.100000\n",
      "Training Epoch: 2 [27776/50000]\tLoss: 3.9841\tLR: 0.100000\n",
      "Training Epoch: 2 [27904/50000]\tLoss: 3.7261\tLR: 0.100000\n",
      "Training Epoch: 2 [28032/50000]\tLoss: 3.7971\tLR: 0.100000\n",
      "Training Epoch: 2 [28160/50000]\tLoss: 3.7144\tLR: 0.100000\n",
      "Training Epoch: 2 [28288/50000]\tLoss: 3.6408\tLR: 0.100000\n",
      "Training Epoch: 2 [28416/50000]\tLoss: 3.8282\tLR: 0.100000\n",
      "Training Epoch: 2 [28544/50000]\tLoss: 3.7443\tLR: 0.100000\n",
      "Training Epoch: 2 [28672/50000]\tLoss: 3.8430\tLR: 0.100000\n",
      "Training Epoch: 2 [28800/50000]\tLoss: 3.9601\tLR: 0.100000\n",
      "Training Epoch: 2 [28928/50000]\tLoss: 3.7786\tLR: 0.100000\n",
      "Training Epoch: 2 [29056/50000]\tLoss: 3.8981\tLR: 0.100000\n",
      "Training Epoch: 2 [29184/50000]\tLoss: 3.7257\tLR: 0.100000\n",
      "Training Epoch: 2 [29312/50000]\tLoss: 3.7707\tLR: 0.100000\n",
      "Training Epoch: 2 [29440/50000]\tLoss: 3.9386\tLR: 0.100000\n",
      "Training Epoch: 2 [29568/50000]\tLoss: 3.8795\tLR: 0.100000\n",
      "Training Epoch: 2 [29696/50000]\tLoss: 3.6213\tLR: 0.100000\n",
      "Training Epoch: 2 [29824/50000]\tLoss: 3.6461\tLR: 0.100000\n",
      "Training Epoch: 2 [29952/50000]\tLoss: 3.5714\tLR: 0.100000\n",
      "Training Epoch: 2 [30080/50000]\tLoss: 3.7430\tLR: 0.100000\n",
      "Training Epoch: 2 [30208/50000]\tLoss: 3.7748\tLR: 0.100000\n",
      "Training Epoch: 2 [30336/50000]\tLoss: 3.5733\tLR: 0.100000\n",
      "Training Epoch: 2 [30464/50000]\tLoss: 3.5776\tLR: 0.100000\n",
      "Training Epoch: 2 [30592/50000]\tLoss: 3.5433\tLR: 0.100000\n",
      "Training Epoch: 2 [30720/50000]\tLoss: 3.7911\tLR: 0.100000\n",
      "Training Epoch: 2 [30848/50000]\tLoss: 3.7305\tLR: 0.100000\n",
      "Training Epoch: 2 [30976/50000]\tLoss: 3.7963\tLR: 0.100000\n",
      "Training Epoch: 2 [31104/50000]\tLoss: 3.6391\tLR: 0.100000\n",
      "Training Epoch: 2 [31232/50000]\tLoss: 3.6482\tLR: 0.100000\n",
      "Training Epoch: 2 [31360/50000]\tLoss: 3.5985\tLR: 0.100000\n",
      "Training Epoch: 2 [31488/50000]\tLoss: 3.7225\tLR: 0.100000\n",
      "Training Epoch: 2 [31616/50000]\tLoss: 3.5819\tLR: 0.100000\n",
      "Training Epoch: 2 [31744/50000]\tLoss: 3.7615\tLR: 0.100000\n",
      "Training Epoch: 2 [31872/50000]\tLoss: 4.0224\tLR: 0.100000\n",
      "Training Epoch: 2 [32000/50000]\tLoss: 3.8727\tLR: 0.100000\n",
      "Training Epoch: 2 [32128/50000]\tLoss: 3.7189\tLR: 0.100000\n",
      "Training Epoch: 2 [32256/50000]\tLoss: 3.7451\tLR: 0.100000\n",
      "Training Epoch: 2 [32384/50000]\tLoss: 3.7240\tLR: 0.100000\n",
      "Training Epoch: 2 [32512/50000]\tLoss: 3.7787\tLR: 0.100000\n",
      "Training Epoch: 2 [32640/50000]\tLoss: 3.5089\tLR: 0.100000\n",
      "Training Epoch: 2 [32768/50000]\tLoss: 3.8787\tLR: 0.100000\n",
      "Training Epoch: 2 [32896/50000]\tLoss: 4.0087\tLR: 0.100000\n",
      "Training Epoch: 2 [33024/50000]\tLoss: 3.7709\tLR: 0.100000\n",
      "Training Epoch: 2 [33152/50000]\tLoss: 3.9191\tLR: 0.100000\n",
      "Training Epoch: 2 [33280/50000]\tLoss: 3.9406\tLR: 0.100000\n",
      "Training Epoch: 2 [33408/50000]\tLoss: 3.7363\tLR: 0.100000\n",
      "Training Epoch: 2 [33536/50000]\tLoss: 3.5170\tLR: 0.100000\n",
      "Training Epoch: 2 [33664/50000]\tLoss: 3.6986\tLR: 0.100000\n",
      "Training Epoch: 2 [33792/50000]\tLoss: 3.7333\tLR: 0.100000\n",
      "Training Epoch: 2 [33920/50000]\tLoss: 3.8553\tLR: 0.100000\n",
      "Training Epoch: 2 [34048/50000]\tLoss: 3.7197\tLR: 0.100000\n",
      "Training Epoch: 2 [34176/50000]\tLoss: 3.6232\tLR: 0.100000\n",
      "Training Epoch: 2 [34304/50000]\tLoss: 3.6257\tLR: 0.100000\n",
      "Training Epoch: 2 [34432/50000]\tLoss: 3.8357\tLR: 0.100000\n",
      "Training Epoch: 2 [34560/50000]\tLoss: 3.7359\tLR: 0.100000\n",
      "Training Epoch: 2 [34688/50000]\tLoss: 3.7895\tLR: 0.100000\n",
      "Training Epoch: 2 [34816/50000]\tLoss: 3.7133\tLR: 0.100000\n",
      "Training Epoch: 2 [34944/50000]\tLoss: 3.6734\tLR: 0.100000\n",
      "Training Epoch: 2 [35072/50000]\tLoss: 3.7849\tLR: 0.100000\n",
      "Training Epoch: 2 [35200/50000]\tLoss: 3.6444\tLR: 0.100000\n",
      "Training Epoch: 2 [35328/50000]\tLoss: 3.8743\tLR: 0.100000\n",
      "Training Epoch: 2 [35456/50000]\tLoss: 3.6051\tLR: 0.100000\n",
      "Training Epoch: 2 [35584/50000]\tLoss: 3.5518\tLR: 0.100000\n",
      "Training Epoch: 2 [35712/50000]\tLoss: 3.6911\tLR: 0.100000\n",
      "Training Epoch: 2 [35840/50000]\tLoss: 3.6504\tLR: 0.100000\n",
      "Training Epoch: 2 [35968/50000]\tLoss: 3.7517\tLR: 0.100000\n",
      "Training Epoch: 2 [36096/50000]\tLoss: 3.6811\tLR: 0.100000\n",
      "Training Epoch: 2 [36224/50000]\tLoss: 3.6537\tLR: 0.100000\n",
      "Training Epoch: 2 [36352/50000]\tLoss: 3.7970\tLR: 0.100000\n",
      "Training Epoch: 2 [36480/50000]\tLoss: 3.9611\tLR: 0.100000\n",
      "Training Epoch: 2 [36608/50000]\tLoss: 3.7352\tLR: 0.100000\n",
      "Training Epoch: 2 [36736/50000]\tLoss: 3.5758\tLR: 0.100000\n",
      "Training Epoch: 2 [36864/50000]\tLoss: 3.7256\tLR: 0.100000\n",
      "Training Epoch: 2 [36992/50000]\tLoss: 3.6682\tLR: 0.100000\n",
      "Training Epoch: 2 [37120/50000]\tLoss: 3.7241\tLR: 0.100000\n",
      "Training Epoch: 2 [37248/50000]\tLoss: 3.9658\tLR: 0.100000\n",
      "Training Epoch: 2 [37376/50000]\tLoss: 3.7018\tLR: 0.100000\n",
      "Training Epoch: 2 [37504/50000]\tLoss: 3.5742\tLR: 0.100000\n",
      "Training Epoch: 2 [37632/50000]\tLoss: 3.6945\tLR: 0.100000\n",
      "Training Epoch: 2 [37760/50000]\tLoss: 3.8071\tLR: 0.100000\n",
      "Training Epoch: 2 [37888/50000]\tLoss: 3.7400\tLR: 0.100000\n",
      "Training Epoch: 2 [38016/50000]\tLoss: 3.9010\tLR: 0.100000\n",
      "Training Epoch: 2 [38144/50000]\tLoss: 3.8560\tLR: 0.100000\n",
      "Training Epoch: 2 [38272/50000]\tLoss: 3.6691\tLR: 0.100000\n",
      "Training Epoch: 2 [38400/50000]\tLoss: 3.8128\tLR: 0.100000\n",
      "Training Epoch: 2 [38528/50000]\tLoss: 3.8281\tLR: 0.100000\n",
      "Training Epoch: 2 [38656/50000]\tLoss: 3.8086\tLR: 0.100000\n",
      "Training Epoch: 2 [38784/50000]\tLoss: 3.7333\tLR: 0.100000\n",
      "Training Epoch: 2 [38912/50000]\tLoss: 3.7303\tLR: 0.100000\n",
      "Training Epoch: 2 [39040/50000]\tLoss: 3.6578\tLR: 0.100000\n",
      "Training Epoch: 2 [39168/50000]\tLoss: 3.7016\tLR: 0.100000\n",
      "Training Epoch: 2 [39296/50000]\tLoss: 3.6536\tLR: 0.100000\n",
      "Training Epoch: 2 [39424/50000]\tLoss: 3.7549\tLR: 0.100000\n",
      "Training Epoch: 2 [39552/50000]\tLoss: 3.8110\tLR: 0.100000\n",
      "Training Epoch: 2 [39680/50000]\tLoss: 3.7442\tLR: 0.100000\n",
      "Training Epoch: 2 [39808/50000]\tLoss: 3.9805\tLR: 0.100000\n",
      "Training Epoch: 2 [39936/50000]\tLoss: 3.6846\tLR: 0.100000\n",
      "Training Epoch: 2 [40064/50000]\tLoss: 3.9975\tLR: 0.100000\n",
      "Training Epoch: 2 [40192/50000]\tLoss: 3.5822\tLR: 0.100000\n",
      "Training Epoch: 2 [40320/50000]\tLoss: 3.7204\tLR: 0.100000\n",
      "Training Epoch: 2 [40448/50000]\tLoss: 3.5528\tLR: 0.100000\n",
      "Training Epoch: 2 [40576/50000]\tLoss: 3.5078\tLR: 0.100000\n",
      "Training Epoch: 2 [40704/50000]\tLoss: 3.7096\tLR: 0.100000\n",
      "Training Epoch: 2 [40832/50000]\tLoss: 3.6347\tLR: 0.100000\n",
      "Training Epoch: 2 [40960/50000]\tLoss: 3.6573\tLR: 0.100000\n",
      "Training Epoch: 2 [41088/50000]\tLoss: 3.6760\tLR: 0.100000\n",
      "Training Epoch: 2 [41216/50000]\tLoss: 3.6690\tLR: 0.100000\n",
      "Training Epoch: 2 [41344/50000]\tLoss: 3.5594\tLR: 0.100000\n",
      "Training Epoch: 2 [41472/50000]\tLoss: 3.9207\tLR: 0.100000\n",
      "Training Epoch: 2 [41600/50000]\tLoss: 3.7018\tLR: 0.100000\n",
      "Training Epoch: 2 [41728/50000]\tLoss: 3.6045\tLR: 0.100000\n",
      "Training Epoch: 2 [41856/50000]\tLoss: 3.8495\tLR: 0.100000\n",
      "Training Epoch: 2 [41984/50000]\tLoss: 3.7797\tLR: 0.100000\n",
      "Training Epoch: 2 [42112/50000]\tLoss: 3.4721\tLR: 0.100000\n",
      "Training Epoch: 2 [42240/50000]\tLoss: 3.6048\tLR: 0.100000\n",
      "Training Epoch: 2 [42368/50000]\tLoss: 3.7573\tLR: 0.100000\n",
      "Training Epoch: 2 [42496/50000]\tLoss: 3.6811\tLR: 0.100000\n",
      "Training Epoch: 2 [42624/50000]\tLoss: 3.8417\tLR: 0.100000\n",
      "Training Epoch: 2 [42752/50000]\tLoss: 3.5945\tLR: 0.100000\n",
      "Training Epoch: 2 [42880/50000]\tLoss: 3.5919\tLR: 0.100000\n",
      "Training Epoch: 2 [43008/50000]\tLoss: 3.6075\tLR: 0.100000\n",
      "Training Epoch: 2 [43136/50000]\tLoss: 3.6284\tLR: 0.100000\n",
      "Training Epoch: 2 [43264/50000]\tLoss: 3.6354\tLR: 0.100000\n",
      "Training Epoch: 2 [43392/50000]\tLoss: 3.8185\tLR: 0.100000\n",
      "Training Epoch: 2 [43520/50000]\tLoss: 3.5883\tLR: 0.100000\n",
      "Training Epoch: 2 [43648/50000]\tLoss: 3.5497\tLR: 0.100000\n",
      "Training Epoch: 2 [43776/50000]\tLoss: 3.5626\tLR: 0.100000\n",
      "Training Epoch: 2 [43904/50000]\tLoss: 3.6437\tLR: 0.100000\n",
      "Training Epoch: 2 [44032/50000]\tLoss: 3.7768\tLR: 0.100000\n",
      "Training Epoch: 2 [44160/50000]\tLoss: 3.6266\tLR: 0.100000\n",
      "Training Epoch: 2 [44288/50000]\tLoss: 3.5099\tLR: 0.100000\n",
      "Training Epoch: 2 [44416/50000]\tLoss: 3.5279\tLR: 0.100000\n",
      "Training Epoch: 2 [44544/50000]\tLoss: 3.7757\tLR: 0.100000\n",
      "Training Epoch: 2 [44672/50000]\tLoss: 3.5827\tLR: 0.100000\n",
      "Training Epoch: 2 [44800/50000]\tLoss: 3.3498\tLR: 0.100000\n",
      "Training Epoch: 2 [44928/50000]\tLoss: 3.7688\tLR: 0.100000\n",
      "Training Epoch: 2 [45056/50000]\tLoss: 3.7334\tLR: 0.100000\n",
      "Training Epoch: 2 [45184/50000]\tLoss: 3.5093\tLR: 0.100000\n",
      "Training Epoch: 2 [45312/50000]\tLoss: 3.5575\tLR: 0.100000\n",
      "Training Epoch: 2 [45440/50000]\tLoss: 3.7258\tLR: 0.100000\n",
      "Training Epoch: 2 [45568/50000]\tLoss: 3.5165\tLR: 0.100000\n",
      "Training Epoch: 2 [45696/50000]\tLoss: 3.8330\tLR: 0.100000\n",
      "Training Epoch: 2 [45824/50000]\tLoss: 3.5595\tLR: 0.100000\n",
      "Training Epoch: 2 [45952/50000]\tLoss: 3.6145\tLR: 0.100000\n",
      "Training Epoch: 2 [46080/50000]\tLoss: 3.6657\tLR: 0.100000\n",
      "Training Epoch: 2 [46208/50000]\tLoss: 3.6145\tLR: 0.100000\n",
      "Training Epoch: 2 [46336/50000]\tLoss: 3.5999\tLR: 0.100000\n",
      "Training Epoch: 2 [46464/50000]\tLoss: 3.7225\tLR: 0.100000\n",
      "Training Epoch: 2 [46592/50000]\tLoss: 3.7027\tLR: 0.100000\n",
      "Training Epoch: 2 [46720/50000]\tLoss: 3.5809\tLR: 0.100000\n",
      "Training Epoch: 2 [46848/50000]\tLoss: 3.5380\tLR: 0.100000\n",
      "Training Epoch: 2 [46976/50000]\tLoss: 3.5392\tLR: 0.100000\n",
      "Training Epoch: 2 [47104/50000]\tLoss: 3.4736\tLR: 0.100000\n",
      "Training Epoch: 2 [47232/50000]\tLoss: 3.3651\tLR: 0.100000\n",
      "Training Epoch: 2 [47360/50000]\tLoss: 3.8570\tLR: 0.100000\n",
      "Training Epoch: 2 [47488/50000]\tLoss: 3.7348\tLR: 0.100000\n",
      "Training Epoch: 2 [47616/50000]\tLoss: 3.8231\tLR: 0.100000\n",
      "Training Epoch: 2 [47744/50000]\tLoss: 3.5508\tLR: 0.100000\n",
      "Training Epoch: 2 [47872/50000]\tLoss: 3.6370\tLR: 0.100000\n",
      "Training Epoch: 2 [48000/50000]\tLoss: 3.6609\tLR: 0.100000\n",
      "Training Epoch: 2 [48128/50000]\tLoss: 3.4339\tLR: 0.100000\n",
      "Training Epoch: 2 [48256/50000]\tLoss: 3.4954\tLR: 0.100000\n",
      "Training Epoch: 2 [48384/50000]\tLoss: 3.8076\tLR: 0.100000\n",
      "Training Epoch: 2 [48512/50000]\tLoss: 3.8253\tLR: 0.100000\n",
      "Training Epoch: 2 [48640/50000]\tLoss: 3.5283\tLR: 0.100000\n",
      "Training Epoch: 2 [48768/50000]\tLoss: 3.5212\tLR: 0.100000\n",
      "Training Epoch: 2 [48896/50000]\tLoss: 3.4974\tLR: 0.100000\n",
      "Training Epoch: 2 [49024/50000]\tLoss: 3.6712\tLR: 0.100000\n",
      "Training Epoch: 2 [49152/50000]\tLoss: 3.5135\tLR: 0.100000\n",
      "Training Epoch: 2 [49280/50000]\tLoss: 3.6745\tLR: 0.100000\n",
      "Training Epoch: 2 [49408/50000]\tLoss: 3.6001\tLR: 0.100000\n",
      "Training Epoch: 2 [49536/50000]\tLoss: 3.4724\tLR: 0.100000\n",
      "Training Epoch: 2 [49664/50000]\tLoss: 3.6586\tLR: 0.100000\n",
      "Training Epoch: 2 [49792/50000]\tLoss: 3.6581\tLR: 0.100000\n",
      "Training Epoch: 2 [49920/50000]\tLoss: 3.6625\tLR: 0.100000\n",
      "Training Epoch: 2 [50000/50000]\tLoss: 3.2837\tLR: 0.100000\n",
      "Test set: Average loss: 0.0281, Accuracy: 0.1450\n",
      "\n",
      "Training Epoch: 3 [128/50000]\tLoss: 3.2982\tLR: 0.100000\n",
      "Training Epoch: 3 [256/50000]\tLoss: 3.5700\tLR: 0.100000\n",
      "Training Epoch: 3 [384/50000]\tLoss: 3.6184\tLR: 0.100000\n",
      "Training Epoch: 3 [512/50000]\tLoss: 3.5676\tLR: 0.100000\n",
      "Training Epoch: 3 [640/50000]\tLoss: 3.6203\tLR: 0.100000\n",
      "Training Epoch: 3 [768/50000]\tLoss: 3.5318\tLR: 0.100000\n",
      "Training Epoch: 3 [896/50000]\tLoss: 3.5273\tLR: 0.100000\n",
      "Training Epoch: 3 [1024/50000]\tLoss: 3.4992\tLR: 0.100000\n",
      "Training Epoch: 3 [1152/50000]\tLoss: 3.5673\tLR: 0.100000\n",
      "Training Epoch: 3 [1280/50000]\tLoss: 3.6102\tLR: 0.100000\n",
      "Training Epoch: 3 [1408/50000]\tLoss: 3.5996\tLR: 0.100000\n",
      "Training Epoch: 3 [1536/50000]\tLoss: 3.4887\tLR: 0.100000\n",
      "Training Epoch: 3 [1664/50000]\tLoss: 3.5496\tLR: 0.100000\n",
      "Training Epoch: 3 [1792/50000]\tLoss: 3.2973\tLR: 0.100000\n",
      "Training Epoch: 3 [1920/50000]\tLoss: 3.5797\tLR: 0.100000\n",
      "Training Epoch: 3 [2048/50000]\tLoss: 3.4622\tLR: 0.100000\n",
      "Training Epoch: 3 [2176/50000]\tLoss: 3.4498\tLR: 0.100000\n",
      "Training Epoch: 3 [2304/50000]\tLoss: 3.4292\tLR: 0.100000\n",
      "Training Epoch: 3 [2432/50000]\tLoss: 3.6653\tLR: 0.100000\n",
      "Training Epoch: 3 [2560/50000]\tLoss: 3.5346\tLR: 0.100000\n",
      "Training Epoch: 3 [2688/50000]\tLoss: 3.7196\tLR: 0.100000\n",
      "Training Epoch: 3 [2816/50000]\tLoss: 3.4855\tLR: 0.100000\n",
      "Training Epoch: 3 [2944/50000]\tLoss: 3.5056\tLR: 0.100000\n",
      "Training Epoch: 3 [3072/50000]\tLoss: 3.4748\tLR: 0.100000\n",
      "Training Epoch: 3 [3200/50000]\tLoss: 3.4957\tLR: 0.100000\n",
      "Training Epoch: 3 [3328/50000]\tLoss: 3.4142\tLR: 0.100000\n",
      "Training Epoch: 3 [3456/50000]\tLoss: 3.6735\tLR: 0.100000\n",
      "Training Epoch: 3 [3584/50000]\tLoss: 3.5915\tLR: 0.100000\n",
      "Training Epoch: 3 [3712/50000]\tLoss: 3.6765\tLR: 0.100000\n",
      "Training Epoch: 3 [3840/50000]\tLoss: 3.6756\tLR: 0.100000\n",
      "Training Epoch: 3 [3968/50000]\tLoss: 3.7531\tLR: 0.100000\n",
      "Training Epoch: 3 [4096/50000]\tLoss: 3.4880\tLR: 0.100000\n",
      "Training Epoch: 3 [4224/50000]\tLoss: 3.4878\tLR: 0.100000\n",
      "Training Epoch: 3 [4352/50000]\tLoss: 3.5136\tLR: 0.100000\n",
      "Training Epoch: 3 [4480/50000]\tLoss: 3.6848\tLR: 0.100000\n",
      "Training Epoch: 3 [4608/50000]\tLoss: 3.5119\tLR: 0.100000\n",
      "Training Epoch: 3 [4736/50000]\tLoss: 3.5783\tLR: 0.100000\n",
      "Training Epoch: 3 [4864/50000]\tLoss: 3.5486\tLR: 0.100000\n",
      "Training Epoch: 3 [4992/50000]\tLoss: 3.4705\tLR: 0.100000\n",
      "Training Epoch: 3 [5120/50000]\tLoss: 3.4069\tLR: 0.100000\n",
      "Training Epoch: 3 [5248/50000]\tLoss: 3.5511\tLR: 0.100000\n",
      "Training Epoch: 3 [5376/50000]\tLoss: 3.3306\tLR: 0.100000\n",
      "Training Epoch: 3 [5504/50000]\tLoss: 3.4643\tLR: 0.100000\n",
      "Training Epoch: 3 [5632/50000]\tLoss: 3.6394\tLR: 0.100000\n",
      "Training Epoch: 3 [5760/50000]\tLoss: 3.3429\tLR: 0.100000\n",
      "Training Epoch: 3 [5888/50000]\tLoss: 3.5248\tLR: 0.100000\n",
      "Training Epoch: 3 [6016/50000]\tLoss: 3.3566\tLR: 0.100000\n",
      "Training Epoch: 3 [6144/50000]\tLoss: 3.5615\tLR: 0.100000\n",
      "Training Epoch: 3 [6272/50000]\tLoss: 3.3069\tLR: 0.100000\n",
      "Training Epoch: 3 [6400/50000]\tLoss: 3.3574\tLR: 0.100000\n",
      "Training Epoch: 3 [6528/50000]\tLoss: 3.4143\tLR: 0.100000\n",
      "Training Epoch: 3 [6656/50000]\tLoss: 3.3307\tLR: 0.100000\n",
      "Training Epoch: 3 [6784/50000]\tLoss: 3.5338\tLR: 0.100000\n",
      "Training Epoch: 3 [6912/50000]\tLoss: 3.4663\tLR: 0.100000\n",
      "Training Epoch: 3 [7040/50000]\tLoss: 3.5472\tLR: 0.100000\n",
      "Training Epoch: 3 [7168/50000]\tLoss: 3.6839\tLR: 0.100000\n",
      "Training Epoch: 3 [7296/50000]\tLoss: 3.4572\tLR: 0.100000\n",
      "Training Epoch: 3 [7424/50000]\tLoss: 3.5705\tLR: 0.100000\n",
      "Training Epoch: 3 [7552/50000]\tLoss: 3.5179\tLR: 0.100000\n",
      "Training Epoch: 3 [7680/50000]\tLoss: 3.3882\tLR: 0.100000\n",
      "Training Epoch: 3 [7808/50000]\tLoss: 3.6432\tLR: 0.100000\n",
      "Training Epoch: 3 [7936/50000]\tLoss: 3.5594\tLR: 0.100000\n",
      "Training Epoch: 3 [8064/50000]\tLoss: 3.3996\tLR: 0.100000\n",
      "Training Epoch: 3 [8192/50000]\tLoss: 3.3904\tLR: 0.100000\n",
      "Training Epoch: 3 [8320/50000]\tLoss: 3.5492\tLR: 0.100000\n",
      "Training Epoch: 3 [8448/50000]\tLoss: 3.4633\tLR: 0.100000\n",
      "Training Epoch: 3 [8576/50000]\tLoss: 3.4142\tLR: 0.100000\n",
      "Training Epoch: 3 [8704/50000]\tLoss: 3.6058\tLR: 0.100000\n",
      "Training Epoch: 3 [8832/50000]\tLoss: 3.6375\tLR: 0.100000\n",
      "Training Epoch: 3 [8960/50000]\tLoss: 3.5178\tLR: 0.100000\n",
      "Training Epoch: 3 [9088/50000]\tLoss: 3.6304\tLR: 0.100000\n",
      "Training Epoch: 3 [9216/50000]\tLoss: 3.5537\tLR: 0.100000\n",
      "Training Epoch: 3 [9344/50000]\tLoss: 3.4990\tLR: 0.100000\n",
      "Training Epoch: 3 [9472/50000]\tLoss: 3.4737\tLR: 0.100000\n",
      "Training Epoch: 3 [9600/50000]\tLoss: 3.4473\tLR: 0.100000\n",
      "Training Epoch: 3 [9728/50000]\tLoss: 3.6098\tLR: 0.100000\n",
      "Training Epoch: 3 [9856/50000]\tLoss: 3.5773\tLR: 0.100000\n",
      "Training Epoch: 3 [9984/50000]\tLoss: 3.5561\tLR: 0.100000\n",
      "Training Epoch: 3 [10112/50000]\tLoss: 3.5991\tLR: 0.100000\n",
      "Training Epoch: 3 [10240/50000]\tLoss: 3.5611\tLR: 0.100000\n",
      "Training Epoch: 3 [10368/50000]\tLoss: 3.5374\tLR: 0.100000\n",
      "Training Epoch: 3 [10496/50000]\tLoss: 3.6624\tLR: 0.100000\n",
      "Training Epoch: 3 [10624/50000]\tLoss: 3.2731\tLR: 0.100000\n",
      "Training Epoch: 3 [10752/50000]\tLoss: 3.8236\tLR: 0.100000\n",
      "Training Epoch: 3 [10880/50000]\tLoss: 3.4131\tLR: 0.100000\n",
      "Training Epoch: 3 [11008/50000]\tLoss: 3.4948\tLR: 0.100000\n",
      "Training Epoch: 3 [11136/50000]\tLoss: 3.5629\tLR: 0.100000\n",
      "Training Epoch: 3 [11264/50000]\tLoss: 3.5041\tLR: 0.100000\n",
      "Training Epoch: 3 [11392/50000]\tLoss: 3.5158\tLR: 0.100000\n",
      "Training Epoch: 3 [11520/50000]\tLoss: 3.3068\tLR: 0.100000\n",
      "Training Epoch: 3 [11648/50000]\tLoss: 3.4902\tLR: 0.100000\n",
      "Training Epoch: 3 [11776/50000]\tLoss: 3.4328\tLR: 0.100000\n",
      "Training Epoch: 3 [11904/50000]\tLoss: 3.4971\tLR: 0.100000\n",
      "Training Epoch: 3 [12032/50000]\tLoss: 3.2715\tLR: 0.100000\n",
      "Training Epoch: 3 [12160/50000]\tLoss: 3.3071\tLR: 0.100000\n",
      "Training Epoch: 3 [12288/50000]\tLoss: 3.4094\tLR: 0.100000\n",
      "Training Epoch: 3 [12416/50000]\tLoss: 3.3772\tLR: 0.100000\n",
      "Training Epoch: 3 [12544/50000]\tLoss: 3.4032\tLR: 0.100000\n",
      "Training Epoch: 3 [12672/50000]\tLoss: 3.4740\tLR: 0.100000\n",
      "Training Epoch: 3 [12800/50000]\tLoss: 3.6024\tLR: 0.100000\n",
      "Training Epoch: 3 [12928/50000]\tLoss: 3.3693\tLR: 0.100000\n",
      "Training Epoch: 3 [13056/50000]\tLoss: 3.4912\tLR: 0.100000\n",
      "Training Epoch: 3 [13184/50000]\tLoss: 3.3590\tLR: 0.100000\n",
      "Training Epoch: 3 [13312/50000]\tLoss: 3.6073\tLR: 0.100000\n",
      "Training Epoch: 3 [13440/50000]\tLoss: 3.5103\tLR: 0.100000\n",
      "Training Epoch: 3 [13568/50000]\tLoss: 3.4844\tLR: 0.100000\n",
      "Training Epoch: 3 [13696/50000]\tLoss: 3.8579\tLR: 0.100000\n",
      "Training Epoch: 3 [13824/50000]\tLoss: 3.4322\tLR: 0.100000\n",
      "Training Epoch: 3 [13952/50000]\tLoss: 3.4303\tLR: 0.100000\n",
      "Training Epoch: 3 [14080/50000]\tLoss: 3.3384\tLR: 0.100000\n",
      "Training Epoch: 3 [14208/50000]\tLoss: 3.4201\tLR: 0.100000\n",
      "Training Epoch: 3 [14336/50000]\tLoss: 3.4281\tLR: 0.100000\n",
      "Training Epoch: 3 [14464/50000]\tLoss: 3.5612\tLR: 0.100000\n",
      "Training Epoch: 3 [14592/50000]\tLoss: 3.4409\tLR: 0.100000\n",
      "Training Epoch: 3 [14720/50000]\tLoss: 3.2741\tLR: 0.100000\n",
      "Training Epoch: 3 [14848/50000]\tLoss: 3.3960\tLR: 0.100000\n",
      "Training Epoch: 3 [14976/50000]\tLoss: 3.5381\tLR: 0.100000\n",
      "Training Epoch: 3 [15104/50000]\tLoss: 3.3511\tLR: 0.100000\n",
      "Training Epoch: 3 [15232/50000]\tLoss: 3.4586\tLR: 0.100000\n",
      "Training Epoch: 3 [15360/50000]\tLoss: 3.2575\tLR: 0.100000\n",
      "Training Epoch: 3 [15488/50000]\tLoss: 3.3782\tLR: 0.100000\n",
      "Training Epoch: 3 [15616/50000]\tLoss: 3.4540\tLR: 0.100000\n",
      "Training Epoch: 3 [15744/50000]\tLoss: 3.4331\tLR: 0.100000\n",
      "Training Epoch: 3 [15872/50000]\tLoss: 3.4585\tLR: 0.100000\n",
      "Training Epoch: 3 [16000/50000]\tLoss: 3.4996\tLR: 0.100000\n",
      "Training Epoch: 3 [16128/50000]\tLoss: 3.3771\tLR: 0.100000\n",
      "Training Epoch: 3 [16256/50000]\tLoss: 3.3918\tLR: 0.100000\n",
      "Training Epoch: 3 [16384/50000]\tLoss: 3.4825\tLR: 0.100000\n",
      "Training Epoch: 3 [16512/50000]\tLoss: 3.4391\tLR: 0.100000\n",
      "Training Epoch: 3 [16640/50000]\tLoss: 3.4362\tLR: 0.100000\n",
      "Training Epoch: 3 [16768/50000]\tLoss: 3.4107\tLR: 0.100000\n",
      "Training Epoch: 3 [16896/50000]\tLoss: 3.7493\tLR: 0.100000\n",
      "Training Epoch: 3 [17024/50000]\tLoss: 3.4483\tLR: 0.100000\n",
      "Training Epoch: 3 [17152/50000]\tLoss: 3.4433\tLR: 0.100000\n",
      "Training Epoch: 3 [17280/50000]\tLoss: 3.4768\tLR: 0.100000\n",
      "Training Epoch: 3 [17408/50000]\tLoss: 3.3824\tLR: 0.100000\n",
      "Training Epoch: 3 [17536/50000]\tLoss: 3.3002\tLR: 0.100000\n",
      "Training Epoch: 3 [17664/50000]\tLoss: 3.5305\tLR: 0.100000\n",
      "Training Epoch: 3 [17792/50000]\tLoss: 3.3485\tLR: 0.100000\n",
      "Training Epoch: 3 [17920/50000]\tLoss: 3.4250\tLR: 0.100000\n",
      "Training Epoch: 3 [18048/50000]\tLoss: 3.4398\tLR: 0.100000\n",
      "Training Epoch: 3 [18176/50000]\tLoss: 3.5282\tLR: 0.100000\n",
      "Training Epoch: 3 [18304/50000]\tLoss: 3.5314\tLR: 0.100000\n",
      "Training Epoch: 3 [18432/50000]\tLoss: 3.6040\tLR: 0.100000\n",
      "Training Epoch: 3 [18560/50000]\tLoss: 3.2634\tLR: 0.100000\n",
      "Training Epoch: 3 [18688/50000]\tLoss: 3.5402\tLR: 0.100000\n",
      "Training Epoch: 3 [18816/50000]\tLoss: 3.2724\tLR: 0.100000\n",
      "Training Epoch: 3 [18944/50000]\tLoss: 3.2415\tLR: 0.100000\n",
      "Training Epoch: 3 [19072/50000]\tLoss: 3.1932\tLR: 0.100000\n",
      "Training Epoch: 3 [19200/50000]\tLoss: 3.2538\tLR: 0.100000\n",
      "Training Epoch: 3 [19328/50000]\tLoss: 3.5316\tLR: 0.100000\n",
      "Training Epoch: 3 [19456/50000]\tLoss: 3.2279\tLR: 0.100000\n",
      "Training Epoch: 3 [19584/50000]\tLoss: 3.4948\tLR: 0.100000\n",
      "Training Epoch: 3 [19712/50000]\tLoss: 3.4633\tLR: 0.100000\n",
      "Training Epoch: 3 [19840/50000]\tLoss: 3.5415\tLR: 0.100000\n",
      "Training Epoch: 3 [19968/50000]\tLoss: 3.5153\tLR: 0.100000\n",
      "Training Epoch: 3 [20096/50000]\tLoss: 3.6521\tLR: 0.100000\n",
      "Training Epoch: 3 [20224/50000]\tLoss: 3.4104\tLR: 0.100000\n",
      "Training Epoch: 3 [20352/50000]\tLoss: 3.4318\tLR: 0.100000\n",
      "Training Epoch: 3 [20480/50000]\tLoss: 3.3362\tLR: 0.100000\n",
      "Training Epoch: 3 [20608/50000]\tLoss: 3.2778\tLR: 0.100000\n",
      "Training Epoch: 3 [20736/50000]\tLoss: 3.2715\tLR: 0.100000\n",
      "Training Epoch: 3 [20864/50000]\tLoss: 3.3561\tLR: 0.100000\n",
      "Training Epoch: 3 [20992/50000]\tLoss: 3.5601\tLR: 0.100000\n",
      "Training Epoch: 3 [21120/50000]\tLoss: 3.2561\tLR: 0.100000\n",
      "Training Epoch: 3 [21248/50000]\tLoss: 3.3206\tLR: 0.100000\n",
      "Training Epoch: 3 [21376/50000]\tLoss: 3.3389\tLR: 0.100000\n",
      "Training Epoch: 3 [21504/50000]\tLoss: 3.1819\tLR: 0.100000\n",
      "Training Epoch: 3 [21632/50000]\tLoss: 3.1778\tLR: 0.100000\n",
      "Training Epoch: 3 [21760/50000]\tLoss: 3.3369\tLR: 0.100000\n",
      "Training Epoch: 3 [21888/50000]\tLoss: 3.3057\tLR: 0.100000\n",
      "Training Epoch: 3 [22016/50000]\tLoss: 3.1803\tLR: 0.100000\n",
      "Training Epoch: 3 [22144/50000]\tLoss: 3.4893\tLR: 0.100000\n",
      "Training Epoch: 3 [22272/50000]\tLoss: 3.2717\tLR: 0.100000\n",
      "Training Epoch: 3 [22400/50000]\tLoss: 3.5065\tLR: 0.100000\n",
      "Training Epoch: 3 [22528/50000]\tLoss: 3.4363\tLR: 0.100000\n",
      "Training Epoch: 3 [22656/50000]\tLoss: 3.1601\tLR: 0.100000\n",
      "Training Epoch: 3 [22784/50000]\tLoss: 3.6933\tLR: 0.100000\n",
      "Training Epoch: 3 [22912/50000]\tLoss: 3.2327\tLR: 0.100000\n",
      "Training Epoch: 3 [23040/50000]\tLoss: 3.5820\tLR: 0.100000\n",
      "Training Epoch: 3 [23168/50000]\tLoss: 3.2080\tLR: 0.100000\n",
      "Training Epoch: 3 [23296/50000]\tLoss: 3.2953\tLR: 0.100000\n",
      "Training Epoch: 3 [23424/50000]\tLoss: 3.3516\tLR: 0.100000\n",
      "Training Epoch: 3 [23552/50000]\tLoss: 3.3215\tLR: 0.100000\n",
      "Training Epoch: 3 [23680/50000]\tLoss: 3.1111\tLR: 0.100000\n",
      "Training Epoch: 3 [23808/50000]\tLoss: 3.2808\tLR: 0.100000\n",
      "Training Epoch: 3 [23936/50000]\tLoss: 3.3432\tLR: 0.100000\n",
      "Training Epoch: 3 [24064/50000]\tLoss: 3.3447\tLR: 0.100000\n",
      "Training Epoch: 3 [24192/50000]\tLoss: 3.1681\tLR: 0.100000\n",
      "Training Epoch: 3 [24320/50000]\tLoss: 3.2293\tLR: 0.100000\n",
      "Training Epoch: 3 [24448/50000]\tLoss: 3.2818\tLR: 0.100000\n",
      "Training Epoch: 3 [24576/50000]\tLoss: 3.2814\tLR: 0.100000\n",
      "Training Epoch: 3 [24704/50000]\tLoss: 3.2206\tLR: 0.100000\n",
      "Training Epoch: 3 [24832/50000]\tLoss: 3.3823\tLR: 0.100000\n",
      "Training Epoch: 3 [24960/50000]\tLoss: 3.4283\tLR: 0.100000\n",
      "Training Epoch: 3 [25088/50000]\tLoss: 3.2841\tLR: 0.100000\n",
      "Training Epoch: 3 [25216/50000]\tLoss: 3.2941\tLR: 0.100000\n",
      "Training Epoch: 3 [25344/50000]\tLoss: 3.2313\tLR: 0.100000\n",
      "Training Epoch: 3 [25472/50000]\tLoss: 3.3072\tLR: 0.100000\n",
      "Training Epoch: 3 [25600/50000]\tLoss: 3.0625\tLR: 0.100000\n",
      "Training Epoch: 3 [25728/50000]\tLoss: 3.2148\tLR: 0.100000\n",
      "Training Epoch: 3 [25856/50000]\tLoss: 3.2197\tLR: 0.100000\n",
      "Training Epoch: 3 [25984/50000]\tLoss: 3.2436\tLR: 0.100000\n",
      "Training Epoch: 3 [26112/50000]\tLoss: 3.2873\tLR: 0.100000\n",
      "Training Epoch: 3 [26240/50000]\tLoss: 3.2631\tLR: 0.100000\n",
      "Training Epoch: 3 [26368/50000]\tLoss: 3.5281\tLR: 0.100000\n",
      "Training Epoch: 3 [26496/50000]\tLoss: 3.3740\tLR: 0.100000\n",
      "Training Epoch: 3 [26624/50000]\tLoss: 3.3938\tLR: 0.100000\n",
      "Training Epoch: 3 [26752/50000]\tLoss: 3.3059\tLR: 0.100000\n",
      "Training Epoch: 3 [26880/50000]\tLoss: 3.2239\tLR: 0.100000\n",
      "Training Epoch: 3 [27008/50000]\tLoss: 3.2981\tLR: 0.100000\n",
      "Training Epoch: 3 [27136/50000]\tLoss: 3.5395\tLR: 0.100000\n",
      "Training Epoch: 3 [27264/50000]\tLoss: 3.3386\tLR: 0.100000\n",
      "Training Epoch: 3 [27392/50000]\tLoss: 3.1600\tLR: 0.100000\n",
      "Training Epoch: 3 [27520/50000]\tLoss: 3.2601\tLR: 0.100000\n",
      "Training Epoch: 3 [27648/50000]\tLoss: 3.3284\tLR: 0.100000\n",
      "Training Epoch: 3 [27776/50000]\tLoss: 3.1186\tLR: 0.100000\n",
      "Training Epoch: 3 [27904/50000]\tLoss: 3.3071\tLR: 0.100000\n",
      "Training Epoch: 3 [28032/50000]\tLoss: 3.3161\tLR: 0.100000\n",
      "Training Epoch: 3 [28160/50000]\tLoss: 3.2022\tLR: 0.100000\n",
      "Training Epoch: 3 [28288/50000]\tLoss: 3.3012\tLR: 0.100000\n",
      "Training Epoch: 3 [28416/50000]\tLoss: 3.4382\tLR: 0.100000\n",
      "Training Epoch: 3 [28544/50000]\tLoss: 3.2615\tLR: 0.100000\n",
      "Training Epoch: 3 [28672/50000]\tLoss: 3.1109\tLR: 0.100000\n",
      "Training Epoch: 3 [28800/50000]\tLoss: 3.3547\tLR: 0.100000\n",
      "Training Epoch: 3 [28928/50000]\tLoss: 3.4588\tLR: 0.100000\n",
      "Training Epoch: 3 [29056/50000]\tLoss: 3.3056\tLR: 0.100000\n",
      "Training Epoch: 3 [29184/50000]\tLoss: 3.2533\tLR: 0.100000\n",
      "Training Epoch: 3 [29312/50000]\tLoss: 3.4658\tLR: 0.100000\n",
      "Training Epoch: 3 [29440/50000]\tLoss: 3.1733\tLR: 0.100000\n",
      "Training Epoch: 3 [29568/50000]\tLoss: 3.2410\tLR: 0.100000\n",
      "Training Epoch: 3 [29696/50000]\tLoss: 3.5058\tLR: 0.100000\n",
      "Training Epoch: 3 [29824/50000]\tLoss: 3.3301\tLR: 0.100000\n",
      "Training Epoch: 3 [29952/50000]\tLoss: 3.3652\tLR: 0.100000\n",
      "Training Epoch: 3 [30080/50000]\tLoss: 3.3044\tLR: 0.100000\n",
      "Training Epoch: 3 [30208/50000]\tLoss: 3.4863\tLR: 0.100000\n",
      "Training Epoch: 3 [30336/50000]\tLoss: 3.2293\tLR: 0.100000\n",
      "Training Epoch: 3 [30464/50000]\tLoss: 3.3404\tLR: 0.100000\n",
      "Training Epoch: 3 [30592/50000]\tLoss: 3.3027\tLR: 0.100000\n",
      "Training Epoch: 3 [30720/50000]\tLoss: 3.1639\tLR: 0.100000\n",
      "Training Epoch: 3 [30848/50000]\tLoss: 3.3675\tLR: 0.100000\n",
      "Training Epoch: 3 [30976/50000]\tLoss: 3.2552\tLR: 0.100000\n",
      "Training Epoch: 3 [31104/50000]\tLoss: 3.4079\tLR: 0.100000\n",
      "Training Epoch: 3 [31232/50000]\tLoss: 3.3711\tLR: 0.100000\n",
      "Training Epoch: 3 [31360/50000]\tLoss: 3.2584\tLR: 0.100000\n",
      "Training Epoch: 3 [31488/50000]\tLoss: 3.4052\tLR: 0.100000\n",
      "Training Epoch: 3 [31616/50000]\tLoss: 3.5123\tLR: 0.100000\n",
      "Training Epoch: 3 [31744/50000]\tLoss: 3.1405\tLR: 0.100000\n",
      "Training Epoch: 3 [31872/50000]\tLoss: 3.3964\tLR: 0.100000\n",
      "Training Epoch: 3 [32000/50000]\tLoss: 3.2572\tLR: 0.100000\n",
      "Training Epoch: 3 [32128/50000]\tLoss: 3.2617\tLR: 0.100000\n",
      "Training Epoch: 3 [32256/50000]\tLoss: 3.1473\tLR: 0.100000\n",
      "Training Epoch: 3 [32384/50000]\tLoss: 3.0442\tLR: 0.100000\n",
      "Training Epoch: 3 [32512/50000]\tLoss: 3.3984\tLR: 0.100000\n",
      "Training Epoch: 3 [32640/50000]\tLoss: 3.1941\tLR: 0.100000\n",
      "Training Epoch: 3 [32768/50000]\tLoss: 3.2201\tLR: 0.100000\n",
      "Training Epoch: 3 [32896/50000]\tLoss: 3.1019\tLR: 0.100000\n",
      "Training Epoch: 3 [33024/50000]\tLoss: 3.1334\tLR: 0.100000\n",
      "Training Epoch: 3 [33152/50000]\tLoss: 2.9763\tLR: 0.100000\n",
      "Training Epoch: 3 [33280/50000]\tLoss: 3.0460\tLR: 0.100000\n",
      "Training Epoch: 3 [33408/50000]\tLoss: 3.1908\tLR: 0.100000\n",
      "Training Epoch: 3 [33536/50000]\tLoss: 3.2048\tLR: 0.100000\n",
      "Training Epoch: 3 [33664/50000]\tLoss: 3.1373\tLR: 0.100000\n",
      "Training Epoch: 3 [33792/50000]\tLoss: 3.0464\tLR: 0.100000\n",
      "Training Epoch: 3 [33920/50000]\tLoss: 3.2849\tLR: 0.100000\n",
      "Training Epoch: 3 [34048/50000]\tLoss: 3.2532\tLR: 0.100000\n",
      "Training Epoch: 3 [34176/50000]\tLoss: 3.3998\tLR: 0.100000\n",
      "Training Epoch: 3 [34304/50000]\tLoss: 3.1395\tLR: 0.100000\n",
      "Training Epoch: 3 [34432/50000]\tLoss: 3.0354\tLR: 0.100000\n",
      "Training Epoch: 3 [34560/50000]\tLoss: 2.9468\tLR: 0.100000\n",
      "Training Epoch: 3 [34688/50000]\tLoss: 3.0993\tLR: 0.100000\n",
      "Training Epoch: 3 [34816/50000]\tLoss: 3.1192\tLR: 0.100000\n",
      "Training Epoch: 3 [34944/50000]\tLoss: 3.2091\tLR: 0.100000\n",
      "Training Epoch: 3 [35072/50000]\tLoss: 3.1311\tLR: 0.100000\n",
      "Training Epoch: 3 [35200/50000]\tLoss: 3.1020\tLR: 0.100000\n",
      "Training Epoch: 3 [35328/50000]\tLoss: 3.1500\tLR: 0.100000\n",
      "Training Epoch: 3 [35456/50000]\tLoss: 3.0166\tLR: 0.100000\n",
      "Training Epoch: 3 [35584/50000]\tLoss: 3.3673\tLR: 0.100000\n",
      "Training Epoch: 3 [35712/50000]\tLoss: 3.4413\tLR: 0.100000\n",
      "Training Epoch: 3 [35840/50000]\tLoss: 3.3070\tLR: 0.100000\n",
      "Training Epoch: 3 [35968/50000]\tLoss: 3.3946\tLR: 0.100000\n",
      "Training Epoch: 3 [36096/50000]\tLoss: 3.3835\tLR: 0.100000\n",
      "Training Epoch: 3 [36224/50000]\tLoss: 2.9855\tLR: 0.100000\n",
      "Training Epoch: 3 [36352/50000]\tLoss: 3.1618\tLR: 0.100000\n",
      "Training Epoch: 3 [36480/50000]\tLoss: 3.2861\tLR: 0.100000\n",
      "Training Epoch: 3 [36608/50000]\tLoss: 3.2181\tLR: 0.100000\n",
      "Training Epoch: 3 [36736/50000]\tLoss: 3.0599\tLR: 0.100000\n",
      "Training Epoch: 3 [36864/50000]\tLoss: 3.3218\tLR: 0.100000\n",
      "Training Epoch: 3 [36992/50000]\tLoss: 3.2042\tLR: 0.100000\n",
      "Training Epoch: 3 [37120/50000]\tLoss: 3.4946\tLR: 0.100000\n",
      "Training Epoch: 3 [37248/50000]\tLoss: 3.1978\tLR: 0.100000\n",
      "Training Epoch: 3 [37376/50000]\tLoss: 3.1058\tLR: 0.100000\n",
      "Training Epoch: 3 [37504/50000]\tLoss: 3.1092\tLR: 0.100000\n",
      "Training Epoch: 3 [37632/50000]\tLoss: 3.1483\tLR: 0.100000\n",
      "Training Epoch: 3 [37760/50000]\tLoss: 3.2058\tLR: 0.100000\n",
      "Training Epoch: 3 [37888/50000]\tLoss: 3.3576\tLR: 0.100000\n",
      "Training Epoch: 3 [38016/50000]\tLoss: 3.0594\tLR: 0.100000\n",
      "Training Epoch: 3 [38144/50000]\tLoss: 3.1516\tLR: 0.100000\n",
      "Training Epoch: 3 [38272/50000]\tLoss: 3.1343\tLR: 0.100000\n",
      "Training Epoch: 3 [38400/50000]\tLoss: 3.3027\tLR: 0.100000\n",
      "Training Epoch: 3 [38528/50000]\tLoss: 3.2796\tLR: 0.100000\n",
      "Training Epoch: 3 [38656/50000]\tLoss: 3.2015\tLR: 0.100000\n",
      "Training Epoch: 3 [38784/50000]\tLoss: 3.2207\tLR: 0.100000\n",
      "Training Epoch: 3 [38912/50000]\tLoss: 3.2207\tLR: 0.100000\n",
      "Training Epoch: 3 [39040/50000]\tLoss: 3.2556\tLR: 0.100000\n",
      "Training Epoch: 3 [39168/50000]\tLoss: 3.2433\tLR: 0.100000\n",
      "Training Epoch: 3 [39296/50000]\tLoss: 3.2163\tLR: 0.100000\n",
      "Training Epoch: 3 [39424/50000]\tLoss: 3.1652\tLR: 0.100000\n",
      "Training Epoch: 3 [39552/50000]\tLoss: 3.2192\tLR: 0.100000\n",
      "Training Epoch: 3 [39680/50000]\tLoss: 3.2101\tLR: 0.100000\n",
      "Training Epoch: 3 [39808/50000]\tLoss: 3.1415\tLR: 0.100000\n",
      "Training Epoch: 3 [39936/50000]\tLoss: 3.2515\tLR: 0.100000\n",
      "Training Epoch: 3 [40064/50000]\tLoss: 3.2700\tLR: 0.100000\n",
      "Training Epoch: 3 [40192/50000]\tLoss: 3.2272\tLR: 0.100000\n",
      "Training Epoch: 3 [40320/50000]\tLoss: 3.2877\tLR: 0.100000\n",
      "Training Epoch: 3 [40448/50000]\tLoss: 3.2842\tLR: 0.100000\n",
      "Training Epoch: 3 [40576/50000]\tLoss: 3.1260\tLR: 0.100000\n",
      "Training Epoch: 3 [40704/50000]\tLoss: 3.0737\tLR: 0.100000\n",
      "Training Epoch: 3 [40832/50000]\tLoss: 3.3054\tLR: 0.100000\n",
      "Training Epoch: 3 [40960/50000]\tLoss: 3.0965\tLR: 0.100000\n",
      "Training Epoch: 3 [41088/50000]\tLoss: 3.2815\tLR: 0.100000\n",
      "Training Epoch: 3 [41216/50000]\tLoss: 2.9672\tLR: 0.100000\n",
      "Training Epoch: 3 [41344/50000]\tLoss: 3.2997\tLR: 0.100000\n",
      "Training Epoch: 3 [41472/50000]\tLoss: 3.2262\tLR: 0.100000\n",
      "Training Epoch: 3 [41600/50000]\tLoss: 3.0910\tLR: 0.100000\n",
      "Training Epoch: 3 [41728/50000]\tLoss: 3.2918\tLR: 0.100000\n",
      "Training Epoch: 3 [41856/50000]\tLoss: 3.0162\tLR: 0.100000\n",
      "Training Epoch: 3 [41984/50000]\tLoss: 3.2011\tLR: 0.100000\n",
      "Training Epoch: 3 [42112/50000]\tLoss: 3.1264\tLR: 0.100000\n",
      "Training Epoch: 3 [42240/50000]\tLoss: 3.2930\tLR: 0.100000\n",
      "Training Epoch: 3 [42368/50000]\tLoss: 3.1038\tLR: 0.100000\n",
      "Training Epoch: 3 [42496/50000]\tLoss: 3.4371\tLR: 0.100000\n",
      "Training Epoch: 3 [42624/50000]\tLoss: 3.1831\tLR: 0.100000\n",
      "Training Epoch: 3 [42752/50000]\tLoss: 3.0517\tLR: 0.100000\n",
      "Training Epoch: 3 [42880/50000]\tLoss: 2.9328\tLR: 0.100000\n",
      "Training Epoch: 3 [43008/50000]\tLoss: 3.0799\tLR: 0.100000\n",
      "Training Epoch: 3 [43136/50000]\tLoss: 3.3630\tLR: 0.100000\n",
      "Training Epoch: 3 [43264/50000]\tLoss: 3.2983\tLR: 0.100000\n",
      "Training Epoch: 3 [43392/50000]\tLoss: 2.9816\tLR: 0.100000\n",
      "Training Epoch: 3 [43520/50000]\tLoss: 3.2975\tLR: 0.100000\n",
      "Training Epoch: 3 [43648/50000]\tLoss: 3.3910\tLR: 0.100000\n",
      "Training Epoch: 3 [43776/50000]\tLoss: 3.2276\tLR: 0.100000\n",
      "Training Epoch: 3 [43904/50000]\tLoss: 3.2267\tLR: 0.100000\n",
      "Training Epoch: 3 [44032/50000]\tLoss: 3.3168\tLR: 0.100000\n",
      "Training Epoch: 3 [44160/50000]\tLoss: 3.3394\tLR: 0.100000\n",
      "Training Epoch: 3 [44288/50000]\tLoss: 3.0900\tLR: 0.100000\n",
      "Training Epoch: 3 [44416/50000]\tLoss: 3.1601\tLR: 0.100000\n",
      "Training Epoch: 3 [44544/50000]\tLoss: 3.1168\tLR: 0.100000\n",
      "Training Epoch: 3 [44672/50000]\tLoss: 3.3002\tLR: 0.100000\n",
      "Training Epoch: 3 [44800/50000]\tLoss: 2.8384\tLR: 0.100000\n",
      "Training Epoch: 3 [44928/50000]\tLoss: 3.0667\tLR: 0.100000\n",
      "Training Epoch: 3 [45056/50000]\tLoss: 3.1173\tLR: 0.100000\n",
      "Training Epoch: 3 [45184/50000]\tLoss: 3.0123\tLR: 0.100000\n",
      "Training Epoch: 3 [45312/50000]\tLoss: 3.2518\tLR: 0.100000\n",
      "Training Epoch: 3 [45440/50000]\tLoss: 2.9783\tLR: 0.100000\n",
      "Training Epoch: 3 [45568/50000]\tLoss: 3.2677\tLR: 0.100000\n",
      "Training Epoch: 3 [45696/50000]\tLoss: 3.0479\tLR: 0.100000\n",
      "Training Epoch: 3 [45824/50000]\tLoss: 3.2631\tLR: 0.100000\n",
      "Training Epoch: 3 [45952/50000]\tLoss: 3.0016\tLR: 0.100000\n",
      "Training Epoch: 3 [46080/50000]\tLoss: 3.2742\tLR: 0.100000\n",
      "Training Epoch: 3 [46208/50000]\tLoss: 3.3283\tLR: 0.100000\n",
      "Training Epoch: 3 [46336/50000]\tLoss: 3.1138\tLR: 0.100000\n",
      "Training Epoch: 3 [46464/50000]\tLoss: 3.3213\tLR: 0.100000\n",
      "Training Epoch: 3 [46592/50000]\tLoss: 3.2357\tLR: 0.100000\n",
      "Training Epoch: 3 [46720/50000]\tLoss: 3.0900\tLR: 0.100000\n",
      "Training Epoch: 3 [46848/50000]\tLoss: 3.2328\tLR: 0.100000\n",
      "Training Epoch: 3 [46976/50000]\tLoss: 3.1377\tLR: 0.100000\n",
      "Training Epoch: 3 [47104/50000]\tLoss: 3.1701\tLR: 0.100000\n",
      "Training Epoch: 3 [47232/50000]\tLoss: 3.1322\tLR: 0.100000\n",
      "Training Epoch: 3 [47360/50000]\tLoss: 3.3000\tLR: 0.100000\n",
      "Training Epoch: 3 [47488/50000]\tLoss: 2.9429\tLR: 0.100000\n",
      "Training Epoch: 3 [47616/50000]\tLoss: 3.0389\tLR: 0.100000\n",
      "Training Epoch: 3 [47744/50000]\tLoss: 3.0307\tLR: 0.100000\n",
      "Training Epoch: 3 [47872/50000]\tLoss: 3.1681\tLR: 0.100000\n",
      "Training Epoch: 3 [48000/50000]\tLoss: 2.9616\tLR: 0.100000\n",
      "Training Epoch: 3 [48128/50000]\tLoss: 3.2071\tLR: 0.100000\n",
      "Training Epoch: 3 [48256/50000]\tLoss: 3.0265\tLR: 0.100000\n",
      "Training Epoch: 3 [48384/50000]\tLoss: 2.8896\tLR: 0.100000\n",
      "Training Epoch: 3 [48512/50000]\tLoss: 3.1161\tLR: 0.100000\n",
      "Training Epoch: 3 [48640/50000]\tLoss: 3.2378\tLR: 0.100000\n",
      "Training Epoch: 3 [48768/50000]\tLoss: 3.3221\tLR: 0.100000\n",
      "Training Epoch: 3 [48896/50000]\tLoss: 3.2072\tLR: 0.100000\n",
      "Training Epoch: 3 [49024/50000]\tLoss: 3.1043\tLR: 0.100000\n",
      "Training Epoch: 3 [49152/50000]\tLoss: 3.0150\tLR: 0.100000\n",
      "Training Epoch: 3 [49280/50000]\tLoss: 3.0568\tLR: 0.100000\n",
      "Training Epoch: 3 [49408/50000]\tLoss: 3.0977\tLR: 0.100000\n",
      "Training Epoch: 3 [49536/50000]\tLoss: 3.0493\tLR: 0.100000\n",
      "Training Epoch: 3 [49664/50000]\tLoss: 3.1752\tLR: 0.100000\n",
      "Training Epoch: 3 [49792/50000]\tLoss: 3.0586\tLR: 0.100000\n",
      "Training Epoch: 3 [49920/50000]\tLoss: 3.1560\tLR: 0.100000\n",
      "Training Epoch: 3 [50000/50000]\tLoss: 3.0691\tLR: 0.100000\n",
      "Test set: Average loss: 0.0256, Accuracy: 0.1982\n",
      "\n",
      "Training Epoch: 4 [128/50000]\tLoss: 3.1094\tLR: 0.100000\n",
      "Training Epoch: 4 [256/50000]\tLoss: 3.2545\tLR: 0.100000\n",
      "Training Epoch: 4 [384/50000]\tLoss: 3.0353\tLR: 0.100000\n",
      "Training Epoch: 4 [512/50000]\tLoss: 3.0524\tLR: 0.100000\n",
      "Training Epoch: 4 [640/50000]\tLoss: 3.1449\tLR: 0.100000\n",
      "Training Epoch: 4 [768/50000]\tLoss: 3.0968\tLR: 0.100000\n",
      "Training Epoch: 4 [896/50000]\tLoss: 3.0773\tLR: 0.100000\n",
      "Training Epoch: 4 [1024/50000]\tLoss: 3.0150\tLR: 0.100000\n",
      "Training Epoch: 4 [1152/50000]\tLoss: 3.0692\tLR: 0.100000\n",
      "Training Epoch: 4 [1280/50000]\tLoss: 3.0428\tLR: 0.100000\n",
      "Training Epoch: 4 [1408/50000]\tLoss: 3.2503\tLR: 0.100000\n",
      "Training Epoch: 4 [1536/50000]\tLoss: 3.0491\tLR: 0.100000\n",
      "Training Epoch: 4 [1664/50000]\tLoss: 2.8793\tLR: 0.100000\n",
      "Training Epoch: 4 [1792/50000]\tLoss: 3.1377\tLR: 0.100000\n",
      "Training Epoch: 4 [1920/50000]\tLoss: 3.1066\tLR: 0.100000\n",
      "Training Epoch: 4 [2048/50000]\tLoss: 3.1545\tLR: 0.100000\n",
      "Training Epoch: 4 [2176/50000]\tLoss: 2.9748\tLR: 0.100000\n",
      "Training Epoch: 4 [2304/50000]\tLoss: 3.2350\tLR: 0.100000\n",
      "Training Epoch: 4 [2432/50000]\tLoss: 2.8968\tLR: 0.100000\n",
      "Training Epoch: 4 [2560/50000]\tLoss: 3.1549\tLR: 0.100000\n",
      "Training Epoch: 4 [2688/50000]\tLoss: 2.9124\tLR: 0.100000\n",
      "Training Epoch: 4 [2816/50000]\tLoss: 2.8486\tLR: 0.100000\n",
      "Training Epoch: 4 [2944/50000]\tLoss: 3.0360\tLR: 0.100000\n",
      "Training Epoch: 4 [3072/50000]\tLoss: 2.9086\tLR: 0.100000\n",
      "Training Epoch: 4 [3200/50000]\tLoss: 2.8486\tLR: 0.100000\n",
      "Training Epoch: 4 [3328/50000]\tLoss: 2.8551\tLR: 0.100000\n",
      "Training Epoch: 4 [3456/50000]\tLoss: 2.7513\tLR: 0.100000\n",
      "Training Epoch: 4 [3584/50000]\tLoss: 3.0764\tLR: 0.100000\n",
      "Training Epoch: 4 [3712/50000]\tLoss: 3.0427\tLR: 0.100000\n",
      "Training Epoch: 4 [3840/50000]\tLoss: 3.0113\tLR: 0.100000\n",
      "Training Epoch: 4 [3968/50000]\tLoss: 2.8864\tLR: 0.100000\n",
      "Training Epoch: 4 [4096/50000]\tLoss: 2.8862\tLR: 0.100000\n",
      "Training Epoch: 4 [4224/50000]\tLoss: 3.2309\tLR: 0.100000\n",
      "Training Epoch: 4 [4352/50000]\tLoss: 3.0729\tLR: 0.100000\n",
      "Training Epoch: 4 [4480/50000]\tLoss: 3.2585\tLR: 0.100000\n",
      "Training Epoch: 4 [4608/50000]\tLoss: 2.8707\tLR: 0.100000\n",
      "Training Epoch: 4 [4736/50000]\tLoss: 3.3234\tLR: 0.100000\n",
      "Training Epoch: 4 [4864/50000]\tLoss: 3.0399\tLR: 0.100000\n",
      "Training Epoch: 4 [4992/50000]\tLoss: 2.9532\tLR: 0.100000\n",
      "Training Epoch: 4 [5120/50000]\tLoss: 3.0327\tLR: 0.100000\n",
      "Training Epoch: 4 [5248/50000]\tLoss: 3.0059\tLR: 0.100000\n",
      "Training Epoch: 4 [5376/50000]\tLoss: 2.8350\tLR: 0.100000\n",
      "Training Epoch: 4 [5504/50000]\tLoss: 2.9798\tLR: 0.100000\n",
      "Training Epoch: 4 [5632/50000]\tLoss: 2.9286\tLR: 0.100000\n",
      "Training Epoch: 4 [5760/50000]\tLoss: 3.1781\tLR: 0.100000\n",
      "Training Epoch: 4 [5888/50000]\tLoss: 3.1802\tLR: 0.100000\n",
      "Training Epoch: 4 [6016/50000]\tLoss: 2.9110\tLR: 0.100000\n",
      "Training Epoch: 4 [6144/50000]\tLoss: 2.7961\tLR: 0.100000\n",
      "Training Epoch: 4 [6272/50000]\tLoss: 2.9359\tLR: 0.100000\n",
      "Training Epoch: 4 [6400/50000]\tLoss: 3.1395\tLR: 0.100000\n",
      "Training Epoch: 4 [6528/50000]\tLoss: 2.9509\tLR: 0.100000\n",
      "Training Epoch: 4 [6656/50000]\tLoss: 2.8181\tLR: 0.100000\n",
      "Training Epoch: 4 [6784/50000]\tLoss: 3.0521\tLR: 0.100000\n",
      "Training Epoch: 4 [6912/50000]\tLoss: 3.1794\tLR: 0.100000\n",
      "Training Epoch: 4 [7040/50000]\tLoss: 2.8563\tLR: 0.100000\n",
      "Training Epoch: 4 [7168/50000]\tLoss: 2.9044\tLR: 0.100000\n",
      "Training Epoch: 4 [7296/50000]\tLoss: 3.0050\tLR: 0.100000\n",
      "Training Epoch: 4 [7424/50000]\tLoss: 3.0234\tLR: 0.100000\n",
      "Training Epoch: 4 [7552/50000]\tLoss: 2.8785\tLR: 0.100000\n",
      "Training Epoch: 4 [7680/50000]\tLoss: 3.2275\tLR: 0.100000\n",
      "Training Epoch: 4 [7808/50000]\tLoss: 2.9304\tLR: 0.100000\n",
      "Training Epoch: 4 [7936/50000]\tLoss: 3.0182\tLR: 0.100000\n",
      "Training Epoch: 4 [8064/50000]\tLoss: 3.0645\tLR: 0.100000\n",
      "Training Epoch: 4 [8192/50000]\tLoss: 2.9397\tLR: 0.100000\n",
      "Training Epoch: 4 [8320/50000]\tLoss: 3.0517\tLR: 0.100000\n",
      "Training Epoch: 4 [8448/50000]\tLoss: 2.9675\tLR: 0.100000\n",
      "Training Epoch: 4 [8576/50000]\tLoss: 3.2762\tLR: 0.100000\n",
      "Training Epoch: 4 [8704/50000]\tLoss: 3.0758\tLR: 0.100000\n",
      "Training Epoch: 4 [8832/50000]\tLoss: 3.0505\tLR: 0.100000\n",
      "Training Epoch: 4 [8960/50000]\tLoss: 2.9429\tLR: 0.100000\n",
      "Training Epoch: 4 [9088/50000]\tLoss: 3.0252\tLR: 0.100000\n",
      "Training Epoch: 4 [9216/50000]\tLoss: 2.9843\tLR: 0.100000\n",
      "Training Epoch: 4 [9344/50000]\tLoss: 2.8059\tLR: 0.100000\n",
      "Training Epoch: 4 [9472/50000]\tLoss: 2.8336\tLR: 0.100000\n",
      "Training Epoch: 4 [9600/50000]\tLoss: 2.8793\tLR: 0.100000\n",
      "Training Epoch: 4 [9728/50000]\tLoss: 2.9481\tLR: 0.100000\n",
      "Training Epoch: 4 [9856/50000]\tLoss: 2.7392\tLR: 0.100000\n",
      "Training Epoch: 4 [9984/50000]\tLoss: 3.0497\tLR: 0.100000\n",
      "Training Epoch: 4 [10112/50000]\tLoss: 2.8048\tLR: 0.100000\n",
      "Training Epoch: 4 [10240/50000]\tLoss: 3.2292\tLR: 0.100000\n",
      "Training Epoch: 4 [10368/50000]\tLoss: 2.7358\tLR: 0.100000\n",
      "Training Epoch: 4 [10496/50000]\tLoss: 3.0390\tLR: 0.100000\n",
      "Training Epoch: 4 [10624/50000]\tLoss: 2.9485\tLR: 0.100000\n",
      "Training Epoch: 4 [10752/50000]\tLoss: 2.8527\tLR: 0.100000\n",
      "Training Epoch: 4 [10880/50000]\tLoss: 2.7911\tLR: 0.100000\n",
      "Training Epoch: 4 [11008/50000]\tLoss: 3.2179\tLR: 0.100000\n",
      "Training Epoch: 4 [11136/50000]\tLoss: 3.1021\tLR: 0.100000\n",
      "Training Epoch: 4 [11264/50000]\tLoss: 2.8810\tLR: 0.100000\n",
      "Training Epoch: 4 [11392/50000]\tLoss: 2.7982\tLR: 0.100000\n",
      "Training Epoch: 4 [11520/50000]\tLoss: 3.0812\tLR: 0.100000\n",
      "Training Epoch: 4 [11648/50000]\tLoss: 2.8381\tLR: 0.100000\n",
      "Training Epoch: 4 [11776/50000]\tLoss: 3.0213\tLR: 0.100000\n",
      "Training Epoch: 4 [11904/50000]\tLoss: 3.0420\tLR: 0.100000\n",
      "Training Epoch: 4 [12032/50000]\tLoss: 3.1448\tLR: 0.100000\n",
      "Training Epoch: 4 [12160/50000]\tLoss: 3.1427\tLR: 0.100000\n",
      "Training Epoch: 4 [12288/50000]\tLoss: 2.9898\tLR: 0.100000\n",
      "Training Epoch: 4 [12416/50000]\tLoss: 2.9798\tLR: 0.100000\n",
      "Training Epoch: 4 [12544/50000]\tLoss: 3.0344\tLR: 0.100000\n",
      "Training Epoch: 4 [12672/50000]\tLoss: 3.1564\tLR: 0.100000\n",
      "Training Epoch: 4 [12800/50000]\tLoss: 2.7174\tLR: 0.100000\n",
      "Training Epoch: 4 [12928/50000]\tLoss: 2.9780\tLR: 0.100000\n",
      "Training Epoch: 4 [13056/50000]\tLoss: 2.9460\tLR: 0.100000\n",
      "Training Epoch: 4 [13184/50000]\tLoss: 2.9273\tLR: 0.100000\n",
      "Training Epoch: 4 [13312/50000]\tLoss: 2.9076\tLR: 0.100000\n",
      "Training Epoch: 4 [13440/50000]\tLoss: 2.9306\tLR: 0.100000\n",
      "Training Epoch: 4 [13568/50000]\tLoss: 2.8825\tLR: 0.100000\n",
      "Training Epoch: 4 [13696/50000]\tLoss: 2.7705\tLR: 0.100000\n",
      "Training Epoch: 4 [13824/50000]\tLoss: 2.8890\tLR: 0.100000\n",
      "Training Epoch: 4 [13952/50000]\tLoss: 3.0271\tLR: 0.100000\n",
      "Training Epoch: 4 [14080/50000]\tLoss: 2.7635\tLR: 0.100000\n",
      "Training Epoch: 4 [14208/50000]\tLoss: 2.8886\tLR: 0.100000\n",
      "Training Epoch: 4 [14336/50000]\tLoss: 2.9425\tLR: 0.100000\n",
      "Training Epoch: 4 [14464/50000]\tLoss: 3.1169\tLR: 0.100000\n",
      "Training Epoch: 4 [14592/50000]\tLoss: 2.8644\tLR: 0.100000\n",
      "Training Epoch: 4 [14720/50000]\tLoss: 3.0389\tLR: 0.100000\n",
      "Training Epoch: 4 [14848/50000]\tLoss: 3.0414\tLR: 0.100000\n",
      "Training Epoch: 4 [14976/50000]\tLoss: 3.1657\tLR: 0.100000\n",
      "Training Epoch: 4 [15104/50000]\tLoss: 3.1388\tLR: 0.100000\n",
      "Training Epoch: 4 [15232/50000]\tLoss: 2.9525\tLR: 0.100000\n",
      "Training Epoch: 4 [15360/50000]\tLoss: 2.7029\tLR: 0.100000\n",
      "Training Epoch: 4 [15488/50000]\tLoss: 2.8740\tLR: 0.100000\n",
      "Training Epoch: 4 [15616/50000]\tLoss: 3.0319\tLR: 0.100000\n",
      "Training Epoch: 4 [15744/50000]\tLoss: 3.2258\tLR: 0.100000\n",
      "Training Epoch: 4 [15872/50000]\tLoss: 3.0287\tLR: 0.100000\n",
      "Training Epoch: 4 [16000/50000]\tLoss: 2.9305\tLR: 0.100000\n",
      "Training Epoch: 4 [16128/50000]\tLoss: 2.9138\tLR: 0.100000\n",
      "Training Epoch: 4 [16256/50000]\tLoss: 3.1854\tLR: 0.100000\n",
      "Training Epoch: 4 [16384/50000]\tLoss: 2.8816\tLR: 0.100000\n",
      "Training Epoch: 4 [16512/50000]\tLoss: 2.7582\tLR: 0.100000\n",
      "Training Epoch: 4 [16640/50000]\tLoss: 3.0544\tLR: 0.100000\n",
      "Training Epoch: 4 [16768/50000]\tLoss: 3.0255\tLR: 0.100000\n",
      "Training Epoch: 4 [16896/50000]\tLoss: 2.9543\tLR: 0.100000\n",
      "Training Epoch: 4 [17024/50000]\tLoss: 2.9633\tLR: 0.100000\n",
      "Training Epoch: 4 [17152/50000]\tLoss: 3.0805\tLR: 0.100000\n",
      "Training Epoch: 4 [17280/50000]\tLoss: 3.0083\tLR: 0.100000\n",
      "Training Epoch: 4 [17408/50000]\tLoss: 3.0078\tLR: 0.100000\n",
      "Training Epoch: 4 [17536/50000]\tLoss: 3.1579\tLR: 0.100000\n",
      "Training Epoch: 4 [17664/50000]\tLoss: 3.2050\tLR: 0.100000\n",
      "Training Epoch: 4 [17792/50000]\tLoss: 2.7762\tLR: 0.100000\n",
      "Training Epoch: 4 [17920/50000]\tLoss: 2.9096\tLR: 0.100000\n",
      "Training Epoch: 4 [18048/50000]\tLoss: 3.1084\tLR: 0.100000\n",
      "Training Epoch: 4 [18176/50000]\tLoss: 3.0009\tLR: 0.100000\n",
      "Training Epoch: 4 [18304/50000]\tLoss: 3.0262\tLR: 0.100000\n",
      "Training Epoch: 4 [18432/50000]\tLoss: 2.8653\tLR: 0.100000\n",
      "Training Epoch: 4 [18560/50000]\tLoss: 2.9061\tLR: 0.100000\n",
      "Training Epoch: 4 [18688/50000]\tLoss: 2.9911\tLR: 0.100000\n",
      "Training Epoch: 4 [18816/50000]\tLoss: 2.6933\tLR: 0.100000\n",
      "Training Epoch: 4 [18944/50000]\tLoss: 2.9363\tLR: 0.100000\n",
      "Training Epoch: 4 [19072/50000]\tLoss: 2.9577\tLR: 0.100000\n",
      "Training Epoch: 4 [19200/50000]\tLoss: 2.8812\tLR: 0.100000\n",
      "Training Epoch: 4 [19328/50000]\tLoss: 3.0200\tLR: 0.100000\n",
      "Training Epoch: 4 [19456/50000]\tLoss: 2.7973\tLR: 0.100000\n",
      "Training Epoch: 4 [19584/50000]\tLoss: 2.7246\tLR: 0.100000\n",
      "Training Epoch: 4 [19712/50000]\tLoss: 2.7579\tLR: 0.100000\n",
      "Training Epoch: 4 [19840/50000]\tLoss: 2.9575\tLR: 0.100000\n",
      "Training Epoch: 4 [19968/50000]\tLoss: 2.8724\tLR: 0.100000\n",
      "Training Epoch: 4 [20096/50000]\tLoss: 2.8377\tLR: 0.100000\n",
      "Training Epoch: 4 [20224/50000]\tLoss: 2.8254\tLR: 0.100000\n",
      "Training Epoch: 4 [20352/50000]\tLoss: 3.0170\tLR: 0.100000\n",
      "Training Epoch: 4 [20480/50000]\tLoss: 2.7890\tLR: 0.100000\n",
      "Training Epoch: 4 [20608/50000]\tLoss: 3.0491\tLR: 0.100000\n",
      "Training Epoch: 4 [20736/50000]\tLoss: 2.8444\tLR: 0.100000\n",
      "Training Epoch: 4 [20864/50000]\tLoss: 2.9782\tLR: 0.100000\n",
      "Training Epoch: 4 [20992/50000]\tLoss: 2.9360\tLR: 0.100000\n",
      "Training Epoch: 4 [21120/50000]\tLoss: 3.0001\tLR: 0.100000\n",
      "Training Epoch: 4 [21248/50000]\tLoss: 2.7621\tLR: 0.100000\n",
      "Training Epoch: 4 [21376/50000]\tLoss: 2.9545\tLR: 0.100000\n",
      "Training Epoch: 4 [21504/50000]\tLoss: 2.9306\tLR: 0.100000\n",
      "Training Epoch: 4 [21632/50000]\tLoss: 2.9241\tLR: 0.100000\n",
      "Training Epoch: 4 [21760/50000]\tLoss: 2.8831\tLR: 0.100000\n",
      "Training Epoch: 4 [21888/50000]\tLoss: 2.7126\tLR: 0.100000\n",
      "Training Epoch: 4 [22016/50000]\tLoss: 2.9264\tLR: 0.100000\n",
      "Training Epoch: 4 [22144/50000]\tLoss: 3.0138\tLR: 0.100000\n",
      "Training Epoch: 4 [22272/50000]\tLoss: 3.0087\tLR: 0.100000\n",
      "Training Epoch: 4 [22400/50000]\tLoss: 3.1733\tLR: 0.100000\n",
      "Training Epoch: 4 [22528/50000]\tLoss: 3.0260\tLR: 0.100000\n",
      "Training Epoch: 4 [22656/50000]\tLoss: 3.1243\tLR: 0.100000\n",
      "Training Epoch: 4 [22784/50000]\tLoss: 2.8756\tLR: 0.100000\n",
      "Training Epoch: 4 [22912/50000]\tLoss: 2.7698\tLR: 0.100000\n",
      "Training Epoch: 4 [23040/50000]\tLoss: 2.8480\tLR: 0.100000\n",
      "Training Epoch: 4 [23168/50000]\tLoss: 2.9217\tLR: 0.100000\n",
      "Training Epoch: 4 [23296/50000]\tLoss: 2.9370\tLR: 0.100000\n",
      "Training Epoch: 4 [23424/50000]\tLoss: 2.8732\tLR: 0.100000\n",
      "Training Epoch: 4 [23552/50000]\tLoss: 3.1367\tLR: 0.100000\n",
      "Training Epoch: 4 [23680/50000]\tLoss: 3.0027\tLR: 0.100000\n",
      "Training Epoch: 4 [23808/50000]\tLoss: 2.9479\tLR: 0.100000\n",
      "Training Epoch: 4 [23936/50000]\tLoss: 2.6714\tLR: 0.100000\n",
      "Training Epoch: 4 [24064/50000]\tLoss: 2.9145\tLR: 0.100000\n",
      "Training Epoch: 4 [24192/50000]\tLoss: 2.9450\tLR: 0.100000\n",
      "Training Epoch: 4 [24320/50000]\tLoss: 2.8342\tLR: 0.100000\n",
      "Training Epoch: 4 [24448/50000]\tLoss: 2.6572\tLR: 0.100000\n",
      "Training Epoch: 4 [24576/50000]\tLoss: 2.9078\tLR: 0.100000\n",
      "Training Epoch: 4 [24704/50000]\tLoss: 2.8422\tLR: 0.100000\n",
      "Training Epoch: 4 [24832/50000]\tLoss: 2.6831\tLR: 0.100000\n",
      "Training Epoch: 4 [24960/50000]\tLoss: 3.2407\tLR: 0.100000\n",
      "Training Epoch: 4 [25088/50000]\tLoss: 2.8510\tLR: 0.100000\n",
      "Training Epoch: 4 [25216/50000]\tLoss: 2.9820\tLR: 0.100000\n",
      "Training Epoch: 4 [25344/50000]\tLoss: 2.6182\tLR: 0.100000\n",
      "Training Epoch: 4 [25472/50000]\tLoss: 2.8439\tLR: 0.100000\n",
      "Training Epoch: 4 [25600/50000]\tLoss: 3.0235\tLR: 0.100000\n",
      "Training Epoch: 4 [25728/50000]\tLoss: 2.6976\tLR: 0.100000\n",
      "Training Epoch: 4 [25856/50000]\tLoss: 3.0022\tLR: 0.100000\n",
      "Training Epoch: 4 [25984/50000]\tLoss: 2.8162\tLR: 0.100000\n",
      "Training Epoch: 4 [26112/50000]\tLoss: 3.0015\tLR: 0.100000\n",
      "Training Epoch: 4 [26240/50000]\tLoss: 2.8271\tLR: 0.100000\n",
      "Training Epoch: 4 [26368/50000]\tLoss: 2.9366\tLR: 0.100000\n",
      "Training Epoch: 4 [26496/50000]\tLoss: 2.5995\tLR: 0.100000\n",
      "Training Epoch: 4 [26624/50000]\tLoss: 2.8367\tLR: 0.100000\n",
      "Training Epoch: 4 [26752/50000]\tLoss: 2.9405\tLR: 0.100000\n",
      "Training Epoch: 4 [26880/50000]\tLoss: 2.8195\tLR: 0.100000\n",
      "Training Epoch: 4 [27008/50000]\tLoss: 2.8784\tLR: 0.100000\n",
      "Training Epoch: 4 [27136/50000]\tLoss: 2.8039\tLR: 0.100000\n",
      "Training Epoch: 4 [27264/50000]\tLoss: 2.8464\tLR: 0.100000\n",
      "Training Epoch: 4 [27392/50000]\tLoss: 3.0204\tLR: 0.100000\n",
      "Training Epoch: 4 [27520/50000]\tLoss: 2.8351\tLR: 0.100000\n",
      "Training Epoch: 4 [27648/50000]\tLoss: 2.6108\tLR: 0.100000\n",
      "Training Epoch: 4 [27776/50000]\tLoss: 2.8822\tLR: 0.100000\n",
      "Training Epoch: 4 [27904/50000]\tLoss: 2.6792\tLR: 0.100000\n",
      "Training Epoch: 4 [28032/50000]\tLoss: 2.9442\tLR: 0.100000\n",
      "Training Epoch: 4 [28160/50000]\tLoss: 2.7057\tLR: 0.100000\n",
      "Training Epoch: 4 [28288/50000]\tLoss: 2.6952\tLR: 0.100000\n",
      "Training Epoch: 4 [28416/50000]\tLoss: 2.8632\tLR: 0.100000\n",
      "Training Epoch: 4 [28544/50000]\tLoss: 2.9947\tLR: 0.100000\n",
      "Training Epoch: 4 [28672/50000]\tLoss: 2.8377\tLR: 0.100000\n",
      "Training Epoch: 4 [28800/50000]\tLoss: 2.8053\tLR: 0.100000\n",
      "Training Epoch: 4 [28928/50000]\tLoss: 3.1972\tLR: 0.100000\n",
      "Training Epoch: 4 [29056/50000]\tLoss: 2.5915\tLR: 0.100000\n",
      "Training Epoch: 4 [29184/50000]\tLoss: 2.8342\tLR: 0.100000\n",
      "Training Epoch: 4 [29312/50000]\tLoss: 2.8625\tLR: 0.100000\n",
      "Training Epoch: 4 [29440/50000]\tLoss: 2.8354\tLR: 0.100000\n",
      "Training Epoch: 4 [29568/50000]\tLoss: 2.7550\tLR: 0.100000\n",
      "Training Epoch: 4 [29696/50000]\tLoss: 2.8791\tLR: 0.100000\n",
      "Training Epoch: 4 [29824/50000]\tLoss: 2.9970\tLR: 0.100000\n",
      "Training Epoch: 4 [29952/50000]\tLoss: 2.9606\tLR: 0.100000\n",
      "Training Epoch: 4 [30080/50000]\tLoss: 2.8402\tLR: 0.100000\n",
      "Training Epoch: 4 [30208/50000]\tLoss: 2.9314\tLR: 0.100000\n",
      "Training Epoch: 4 [30336/50000]\tLoss: 2.7669\tLR: 0.100000\n",
      "Training Epoch: 4 [30464/50000]\tLoss: 2.9398\tLR: 0.100000\n",
      "Training Epoch: 4 [30592/50000]\tLoss: 3.0323\tLR: 0.100000\n",
      "Training Epoch: 4 [30720/50000]\tLoss: 3.0697\tLR: 0.100000\n",
      "Training Epoch: 4 [30848/50000]\tLoss: 2.7509\tLR: 0.100000\n",
      "Training Epoch: 4 [30976/50000]\tLoss: 2.6556\tLR: 0.100000\n",
      "Training Epoch: 4 [31104/50000]\tLoss: 2.8467\tLR: 0.100000\n",
      "Training Epoch: 4 [31232/50000]\tLoss: 2.6916\tLR: 0.100000\n",
      "Training Epoch: 4 [31360/50000]\tLoss: 2.9009\tLR: 0.100000\n",
      "Training Epoch: 4 [31488/50000]\tLoss: 2.8775\tLR: 0.100000\n",
      "Training Epoch: 4 [31616/50000]\tLoss: 2.9379\tLR: 0.100000\n",
      "Training Epoch: 4 [31744/50000]\tLoss: 2.7485\tLR: 0.100000\n",
      "Training Epoch: 4 [31872/50000]\tLoss: 2.9321\tLR: 0.100000\n",
      "Training Epoch: 4 [32000/50000]\tLoss: 2.6880\tLR: 0.100000\n",
      "Training Epoch: 4 [32128/50000]\tLoss: 2.7230\tLR: 0.100000\n",
      "Training Epoch: 4 [32256/50000]\tLoss: 2.9879\tLR: 0.100000\n",
      "Training Epoch: 4 [32384/50000]\tLoss: 2.6044\tLR: 0.100000\n",
      "Training Epoch: 4 [32512/50000]\tLoss: 2.6519\tLR: 0.100000\n",
      "Training Epoch: 4 [32640/50000]\tLoss: 2.9517\tLR: 0.100000\n",
      "Training Epoch: 4 [32768/50000]\tLoss: 2.7812\tLR: 0.100000\n",
      "Training Epoch: 4 [32896/50000]\tLoss: 2.8989\tLR: 0.100000\n",
      "Training Epoch: 4 [33024/50000]\tLoss: 2.8558\tLR: 0.100000\n",
      "Training Epoch: 4 [33152/50000]\tLoss: 3.0136\tLR: 0.100000\n",
      "Training Epoch: 4 [33280/50000]\tLoss: 2.9176\tLR: 0.100000\n",
      "Training Epoch: 4 [33408/50000]\tLoss: 2.8833\tLR: 0.100000\n",
      "Training Epoch: 4 [33536/50000]\tLoss: 2.8836\tLR: 0.100000\n",
      "Training Epoch: 4 [33664/50000]\tLoss: 3.2466\tLR: 0.100000\n",
      "Training Epoch: 4 [33792/50000]\tLoss: 2.9167\tLR: 0.100000\n",
      "Training Epoch: 4 [33920/50000]\tLoss: 2.6918\tLR: 0.100000\n",
      "Training Epoch: 4 [34048/50000]\tLoss: 2.7879\tLR: 0.100000\n",
      "Training Epoch: 4 [34176/50000]\tLoss: 2.9965\tLR: 0.100000\n",
      "Training Epoch: 4 [34304/50000]\tLoss: 2.7953\tLR: 0.100000\n",
      "Training Epoch: 4 [34432/50000]\tLoss: 2.7179\tLR: 0.100000\n",
      "Training Epoch: 4 [34560/50000]\tLoss: 2.8363\tLR: 0.100000\n",
      "Training Epoch: 4 [34688/50000]\tLoss: 2.5210\tLR: 0.100000\n",
      "Training Epoch: 4 [34816/50000]\tLoss: 2.8371\tLR: 0.100000\n",
      "Training Epoch: 4 [34944/50000]\tLoss: 2.6789\tLR: 0.100000\n",
      "Training Epoch: 4 [35072/50000]\tLoss: 2.7743\tLR: 0.100000\n",
      "Training Epoch: 4 [35200/50000]\tLoss: 2.8271\tLR: 0.100000\n",
      "Training Epoch: 4 [35328/50000]\tLoss: 2.7269\tLR: 0.100000\n",
      "Training Epoch: 4 [35456/50000]\tLoss: 2.8232\tLR: 0.100000\n",
      "Training Epoch: 4 [35584/50000]\tLoss: 2.4925\tLR: 0.100000\n",
      "Training Epoch: 4 [35712/50000]\tLoss: 2.7038\tLR: 0.100000\n",
      "Training Epoch: 4 [35840/50000]\tLoss: 3.1184\tLR: 0.100000\n",
      "Training Epoch: 4 [35968/50000]\tLoss: 2.8431\tLR: 0.100000\n",
      "Training Epoch: 4 [36096/50000]\tLoss: 2.8326\tLR: 0.100000\n",
      "Training Epoch: 4 [36224/50000]\tLoss: 2.6313\tLR: 0.100000\n",
      "Training Epoch: 4 [36352/50000]\tLoss: 2.8385\tLR: 0.100000\n",
      "Training Epoch: 4 [36480/50000]\tLoss: 2.8494\tLR: 0.100000\n",
      "Training Epoch: 4 [36608/50000]\tLoss: 2.7651\tLR: 0.100000\n",
      "Training Epoch: 4 [36736/50000]\tLoss: 2.9291\tLR: 0.100000\n",
      "Training Epoch: 4 [36864/50000]\tLoss: 3.0103\tLR: 0.100000\n",
      "Training Epoch: 4 [36992/50000]\tLoss: 2.7894\tLR: 0.100000\n",
      "Training Epoch: 4 [37120/50000]\tLoss: 2.9488\tLR: 0.100000\n",
      "Training Epoch: 4 [37248/50000]\tLoss: 2.8486\tLR: 0.100000\n",
      "Training Epoch: 4 [37376/50000]\tLoss: 2.6349\tLR: 0.100000\n",
      "Training Epoch: 4 [37504/50000]\tLoss: 2.6879\tLR: 0.100000\n",
      "Training Epoch: 4 [37632/50000]\tLoss: 2.8155\tLR: 0.100000\n",
      "Training Epoch: 4 [37760/50000]\tLoss: 2.8822\tLR: 0.100000\n",
      "Training Epoch: 4 [37888/50000]\tLoss: 2.7358\tLR: 0.100000\n",
      "Training Epoch: 4 [38016/50000]\tLoss: 2.8769\tLR: 0.100000\n",
      "Training Epoch: 4 [38144/50000]\tLoss: 2.6583\tLR: 0.100000\n",
      "Training Epoch: 4 [38272/50000]\tLoss: 2.8752\tLR: 0.100000\n",
      "Training Epoch: 4 [38400/50000]\tLoss: 2.7734\tLR: 0.100000\n",
      "Training Epoch: 4 [38528/50000]\tLoss: 2.9418\tLR: 0.100000\n",
      "Training Epoch: 4 [38656/50000]\tLoss: 2.4918\tLR: 0.100000\n",
      "Training Epoch: 4 [38784/50000]\tLoss: 2.6613\tLR: 0.100000\n",
      "Training Epoch: 4 [38912/50000]\tLoss: 2.9369\tLR: 0.100000\n",
      "Training Epoch: 4 [39040/50000]\tLoss: 2.8881\tLR: 0.100000\n",
      "Training Epoch: 4 [39168/50000]\tLoss: 2.5741\tLR: 0.100000\n",
      "Training Epoch: 4 [39296/50000]\tLoss: 2.7801\tLR: 0.100000\n",
      "Training Epoch: 4 [39424/50000]\tLoss: 2.8628\tLR: 0.100000\n",
      "Training Epoch: 4 [39552/50000]\tLoss: 3.2019\tLR: 0.100000\n",
      "Training Epoch: 4 [39680/50000]\tLoss: 2.7154\tLR: 0.100000\n",
      "Training Epoch: 4 [39808/50000]\tLoss: 2.7951\tLR: 0.100000\n",
      "Training Epoch: 4 [39936/50000]\tLoss: 2.7458\tLR: 0.100000\n",
      "Training Epoch: 4 [40064/50000]\tLoss: 2.8235\tLR: 0.100000\n",
      "Training Epoch: 4 [40192/50000]\tLoss: 2.8140\tLR: 0.100000\n",
      "Training Epoch: 4 [40320/50000]\tLoss: 2.5706\tLR: 0.100000\n",
      "Training Epoch: 4 [40448/50000]\tLoss: 2.7869\tLR: 0.100000\n",
      "Training Epoch: 4 [40576/50000]\tLoss: 2.7224\tLR: 0.100000\n",
      "Training Epoch: 4 [40704/50000]\tLoss: 3.0326\tLR: 0.100000\n",
      "Training Epoch: 4 [40832/50000]\tLoss: 2.9450\tLR: 0.100000\n",
      "Training Epoch: 4 [40960/50000]\tLoss: 3.1157\tLR: 0.100000\n",
      "Training Epoch: 4 [41088/50000]\tLoss: 2.8398\tLR: 0.100000\n",
      "Training Epoch: 4 [41216/50000]\tLoss: 3.0228\tLR: 0.100000\n",
      "Training Epoch: 4 [41344/50000]\tLoss: 2.7810\tLR: 0.100000\n",
      "Training Epoch: 4 [41472/50000]\tLoss: 2.6994\tLR: 0.100000\n",
      "Training Epoch: 4 [41600/50000]\tLoss: 2.6429\tLR: 0.100000\n",
      "Training Epoch: 4 [41728/50000]\tLoss: 2.5361\tLR: 0.100000\n",
      "Training Epoch: 4 [41856/50000]\tLoss: 2.6446\tLR: 0.100000\n",
      "Training Epoch: 4 [41984/50000]\tLoss: 2.5772\tLR: 0.100000\n",
      "Training Epoch: 4 [42112/50000]\tLoss: 2.4119\tLR: 0.100000\n",
      "Training Epoch: 4 [42240/50000]\tLoss: 2.8747\tLR: 0.100000\n",
      "Training Epoch: 4 [42368/50000]\tLoss: 2.6660\tLR: 0.100000\n",
      "Training Epoch: 4 [42496/50000]\tLoss: 2.6122\tLR: 0.100000\n",
      "Training Epoch: 4 [42624/50000]\tLoss: 2.5320\tLR: 0.100000\n",
      "Training Epoch: 4 [42752/50000]\tLoss: 2.7124\tLR: 0.100000\n",
      "Training Epoch: 4 [42880/50000]\tLoss: 2.7334\tLR: 0.100000\n",
      "Training Epoch: 4 [43008/50000]\tLoss: 2.4286\tLR: 0.100000\n",
      "Training Epoch: 4 [43136/50000]\tLoss: 2.9704\tLR: 0.100000\n",
      "Training Epoch: 4 [43264/50000]\tLoss: 2.7496\tLR: 0.100000\n",
      "Training Epoch: 4 [43392/50000]\tLoss: 2.4890\tLR: 0.100000\n",
      "Training Epoch: 4 [43520/50000]\tLoss: 2.5320\tLR: 0.100000\n",
      "Training Epoch: 4 [43648/50000]\tLoss: 2.6720\tLR: 0.100000\n",
      "Training Epoch: 4 [43776/50000]\tLoss: 2.6265\tLR: 0.100000\n",
      "Training Epoch: 4 [43904/50000]\tLoss: 2.6464\tLR: 0.100000\n",
      "Training Epoch: 4 [44032/50000]\tLoss: 2.4805\tLR: 0.100000\n",
      "Training Epoch: 4 [44160/50000]\tLoss: 2.8330\tLR: 0.100000\n",
      "Training Epoch: 4 [44288/50000]\tLoss: 2.4126\tLR: 0.100000\n",
      "Training Epoch: 4 [44416/50000]\tLoss: 2.9622\tLR: 0.100000\n",
      "Training Epoch: 4 [44544/50000]\tLoss: 2.5370\tLR: 0.100000\n",
      "Training Epoch: 4 [44672/50000]\tLoss: 3.0235\tLR: 0.100000\n",
      "Training Epoch: 4 [44800/50000]\tLoss: 2.6953\tLR: 0.100000\n",
      "Training Epoch: 4 [44928/50000]\tLoss: 2.7940\tLR: 0.100000\n",
      "Training Epoch: 4 [45056/50000]\tLoss: 2.7067\tLR: 0.100000\n",
      "Training Epoch: 4 [45184/50000]\tLoss: 2.8078\tLR: 0.100000\n",
      "Training Epoch: 4 [45312/50000]\tLoss: 2.7604\tLR: 0.100000\n",
      "Training Epoch: 4 [45440/50000]\tLoss: 2.5332\tLR: 0.100000\n",
      "Training Epoch: 4 [45568/50000]\tLoss: 2.9070\tLR: 0.100000\n",
      "Training Epoch: 4 [45696/50000]\tLoss: 2.8252\tLR: 0.100000\n",
      "Training Epoch: 4 [45824/50000]\tLoss: 2.8822\tLR: 0.100000\n",
      "Training Epoch: 4 [45952/50000]\tLoss: 2.6364\tLR: 0.100000\n",
      "Training Epoch: 4 [46080/50000]\tLoss: 2.7996\tLR: 0.100000\n",
      "Training Epoch: 4 [46208/50000]\tLoss: 2.8211\tLR: 0.100000\n",
      "Training Epoch: 4 [46336/50000]\tLoss: 2.7221\tLR: 0.100000\n",
      "Training Epoch: 4 [46464/50000]\tLoss: 2.7549\tLR: 0.100000\n",
      "Training Epoch: 4 [46592/50000]\tLoss: 2.7586\tLR: 0.100000\n",
      "Training Epoch: 4 [46720/50000]\tLoss: 2.5215\tLR: 0.100000\n",
      "Training Epoch: 4 [46848/50000]\tLoss: 2.6443\tLR: 0.100000\n",
      "Training Epoch: 4 [46976/50000]\tLoss: 2.8764\tLR: 0.100000\n",
      "Training Epoch: 4 [47104/50000]\tLoss: 2.8586\tLR: 0.100000\n",
      "Training Epoch: 4 [47232/50000]\tLoss: 2.7036\tLR: 0.100000\n",
      "Training Epoch: 4 [47360/50000]\tLoss: 2.9266\tLR: 0.100000\n",
      "Training Epoch: 4 [47488/50000]\tLoss: 2.6282\tLR: 0.100000\n",
      "Training Epoch: 4 [47616/50000]\tLoss: 2.6943\tLR: 0.100000\n",
      "Training Epoch: 4 [47744/50000]\tLoss: 2.6390\tLR: 0.100000\n",
      "Training Epoch: 4 [47872/50000]\tLoss: 2.7234\tLR: 0.100000\n",
      "Training Epoch: 4 [48000/50000]\tLoss: 2.7314\tLR: 0.100000\n",
      "Training Epoch: 4 [48128/50000]\tLoss: 2.6882\tLR: 0.100000\n",
      "Training Epoch: 4 [48256/50000]\tLoss: 3.0419\tLR: 0.100000\n",
      "Training Epoch: 4 [48384/50000]\tLoss: 2.8007\tLR: 0.100000\n",
      "Training Epoch: 4 [48512/50000]\tLoss: 2.7520\tLR: 0.100000\n",
      "Training Epoch: 4 [48640/50000]\tLoss: 2.6969\tLR: 0.100000\n",
      "Training Epoch: 4 [48768/50000]\tLoss: 2.6970\tLR: 0.100000\n",
      "Training Epoch: 4 [48896/50000]\tLoss: 2.8528\tLR: 0.100000\n",
      "Training Epoch: 4 [49024/50000]\tLoss: 2.4754\tLR: 0.100000\n",
      "Training Epoch: 4 [49152/50000]\tLoss: 2.5734\tLR: 0.100000\n",
      "Training Epoch: 4 [49280/50000]\tLoss: 2.9491\tLR: 0.100000\n",
      "Training Epoch: 4 [49408/50000]\tLoss: 2.7012\tLR: 0.100000\n",
      "Training Epoch: 4 [49536/50000]\tLoss: 2.5719\tLR: 0.100000\n",
      "Training Epoch: 4 [49664/50000]\tLoss: 2.3774\tLR: 0.100000\n",
      "Training Epoch: 4 [49792/50000]\tLoss: 2.7025\tLR: 0.100000\n",
      "Training Epoch: 4 [49920/50000]\tLoss: 2.4693\tLR: 0.100000\n",
      "Training Epoch: 4 [50000/50000]\tLoss: 2.7094\tLR: 0.100000\n",
      "Test set: Average loss: 0.0258, Accuracy: 0.2228\n",
      "\n",
      "Training Epoch: 5 [128/50000]\tLoss: 2.5978\tLR: 0.100000\n",
      "Training Epoch: 5 [256/50000]\tLoss: 2.7955\tLR: 0.100000\n",
      "Training Epoch: 5 [384/50000]\tLoss: 2.6746\tLR: 0.100000\n",
      "Training Epoch: 5 [512/50000]\tLoss: 2.7816\tLR: 0.100000\n",
      "Training Epoch: 5 [640/50000]\tLoss: 2.6570\tLR: 0.100000\n",
      "Training Epoch: 5 [768/50000]\tLoss: 2.5396\tLR: 0.100000\n",
      "Training Epoch: 5 [896/50000]\tLoss: 2.8270\tLR: 0.100000\n",
      "Training Epoch: 5 [1024/50000]\tLoss: 2.7467\tLR: 0.100000\n",
      "Training Epoch: 5 [1152/50000]\tLoss: 2.6155\tLR: 0.100000\n",
      "Training Epoch: 5 [1280/50000]\tLoss: 2.6343\tLR: 0.100000\n",
      "Training Epoch: 5 [1408/50000]\tLoss: 2.7380\tLR: 0.100000\n",
      "Training Epoch: 5 [1536/50000]\tLoss: 2.7161\tLR: 0.100000\n",
      "Training Epoch: 5 [1664/50000]\tLoss: 2.8101\tLR: 0.100000\n",
      "Training Epoch: 5 [1792/50000]\tLoss: 2.5874\tLR: 0.100000\n",
      "Training Epoch: 5 [1920/50000]\tLoss: 2.6655\tLR: 0.100000\n",
      "Training Epoch: 5 [2048/50000]\tLoss: 2.9623\tLR: 0.100000\n",
      "Training Epoch: 5 [2176/50000]\tLoss: 2.6032\tLR: 0.100000\n",
      "Training Epoch: 5 [2304/50000]\tLoss: 2.8140\tLR: 0.100000\n",
      "Training Epoch: 5 [2432/50000]\tLoss: 2.7259\tLR: 0.100000\n",
      "Training Epoch: 5 [2560/50000]\tLoss: 2.5110\tLR: 0.100000\n",
      "Training Epoch: 5 [2688/50000]\tLoss: 2.6858\tLR: 0.100000\n",
      "Training Epoch: 5 [2816/50000]\tLoss: 2.7558\tLR: 0.100000\n",
      "Training Epoch: 5 [2944/50000]\tLoss: 2.6089\tLR: 0.100000\n",
      "Training Epoch: 5 [3072/50000]\tLoss: 2.5590\tLR: 0.100000\n",
      "Training Epoch: 5 [3200/50000]\tLoss: 2.6974\tLR: 0.100000\n",
      "Training Epoch: 5 [3328/50000]\tLoss: 2.6345\tLR: 0.100000\n",
      "Training Epoch: 5 [3456/50000]\tLoss: 2.8287\tLR: 0.100000\n",
      "Training Epoch: 5 [3584/50000]\tLoss: 2.6376\tLR: 0.100000\n",
      "Training Epoch: 5 [3712/50000]\tLoss: 2.4222\tLR: 0.100000\n",
      "Training Epoch: 5 [3840/50000]\tLoss: 2.4570\tLR: 0.100000\n",
      "Training Epoch: 5 [3968/50000]\tLoss: 2.8058\tLR: 0.100000\n",
      "Training Epoch: 5 [4096/50000]\tLoss: 2.9084\tLR: 0.100000\n",
      "Training Epoch: 5 [4224/50000]\tLoss: 2.4200\tLR: 0.100000\n",
      "Training Epoch: 5 [4352/50000]\tLoss: 2.6922\tLR: 0.100000\n",
      "Training Epoch: 5 [4480/50000]\tLoss: 2.6973\tLR: 0.100000\n",
      "Training Epoch: 5 [4608/50000]\tLoss: 2.6171\tLR: 0.100000\n",
      "Training Epoch: 5 [4736/50000]\tLoss: 2.2992\tLR: 0.100000\n",
      "Training Epoch: 5 [4864/50000]\tLoss: 2.6985\tLR: 0.100000\n",
      "Training Epoch: 5 [4992/50000]\tLoss: 2.8581\tLR: 0.100000\n",
      "Training Epoch: 5 [5120/50000]\tLoss: 2.7929\tLR: 0.100000\n",
      "Training Epoch: 5 [5248/50000]\tLoss: 2.6361\tLR: 0.100000\n",
      "Training Epoch: 5 [5376/50000]\tLoss: 2.7391\tLR: 0.100000\n",
      "Training Epoch: 5 [5504/50000]\tLoss: 2.8601\tLR: 0.100000\n",
      "Training Epoch: 5 [5632/50000]\tLoss: 2.4277\tLR: 0.100000\n",
      "Training Epoch: 5 [5760/50000]\tLoss: 2.5379\tLR: 0.100000\n",
      "Training Epoch: 5 [5888/50000]\tLoss: 2.4805\tLR: 0.100000\n",
      "Training Epoch: 5 [6016/50000]\tLoss: 2.6009\tLR: 0.100000\n",
      "Training Epoch: 5 [6144/50000]\tLoss: 2.4514\tLR: 0.100000\n",
      "Training Epoch: 5 [6272/50000]\tLoss: 2.6361\tLR: 0.100000\n",
      "Training Epoch: 5 [6400/50000]\tLoss: 2.5561\tLR: 0.100000\n",
      "Training Epoch: 5 [6528/50000]\tLoss: 2.5197\tLR: 0.100000\n",
      "Training Epoch: 5 [6656/50000]\tLoss: 2.5179\tLR: 0.100000\n",
      "Training Epoch: 5 [6784/50000]\tLoss: 2.6030\tLR: 0.100000\n",
      "Training Epoch: 5 [6912/50000]\tLoss: 2.8122\tLR: 0.100000\n",
      "Training Epoch: 5 [7040/50000]\tLoss: 2.7960\tLR: 0.100000\n",
      "Training Epoch: 5 [7168/50000]\tLoss: 2.4293\tLR: 0.100000\n",
      "Training Epoch: 5 [7296/50000]\tLoss: 2.6432\tLR: 0.100000\n",
      "Training Epoch: 5 [7424/50000]\tLoss: 2.9048\tLR: 0.100000\n",
      "Training Epoch: 5 [7552/50000]\tLoss: 2.6176\tLR: 0.100000\n",
      "Training Epoch: 5 [7680/50000]\tLoss: 2.5656\tLR: 0.100000\n",
      "Training Epoch: 5 [7808/50000]\tLoss: 2.5320\tLR: 0.100000\n",
      "Training Epoch: 5 [7936/50000]\tLoss: 2.8655\tLR: 0.100000\n",
      "Training Epoch: 5 [8064/50000]\tLoss: 2.7383\tLR: 0.100000\n",
      "Training Epoch: 5 [8192/50000]\tLoss: 2.5834\tLR: 0.100000\n",
      "Training Epoch: 5 [8320/50000]\tLoss: 2.7539\tLR: 0.100000\n",
      "Training Epoch: 5 [8448/50000]\tLoss: 2.7410\tLR: 0.100000\n",
      "Training Epoch: 5 [8576/50000]\tLoss: 2.7051\tLR: 0.100000\n",
      "Training Epoch: 5 [8704/50000]\tLoss: 2.4797\tLR: 0.100000\n",
      "Training Epoch: 5 [8832/50000]\tLoss: 2.7144\tLR: 0.100000\n",
      "Training Epoch: 5 [8960/50000]\tLoss: 2.5325\tLR: 0.100000\n",
      "Training Epoch: 5 [9088/50000]\tLoss: 2.4355\tLR: 0.100000\n",
      "Training Epoch: 5 [9216/50000]\tLoss: 2.5629\tLR: 0.100000\n",
      "Training Epoch: 5 [9344/50000]\tLoss: 2.6764\tLR: 0.100000\n",
      "Training Epoch: 5 [9472/50000]\tLoss: 2.5203\tLR: 0.100000\n",
      "Training Epoch: 5 [9600/50000]\tLoss: 2.4317\tLR: 0.100000\n",
      "Training Epoch: 5 [9728/50000]\tLoss: 2.8032\tLR: 0.100000\n",
      "Training Epoch: 5 [9856/50000]\tLoss: 2.4699\tLR: 0.100000\n",
      "Training Epoch: 5 [9984/50000]\tLoss: 2.6905\tLR: 0.100000\n",
      "Training Epoch: 5 [10112/50000]\tLoss: 2.7194\tLR: 0.100000\n",
      "Training Epoch: 5 [10240/50000]\tLoss: 2.6176\tLR: 0.100000\n",
      "Training Epoch: 5 [10368/50000]\tLoss: 2.5629\tLR: 0.100000\n",
      "Training Epoch: 5 [10496/50000]\tLoss: 2.5293\tLR: 0.100000\n",
      "Training Epoch: 5 [10624/50000]\tLoss: 2.8497\tLR: 0.100000\n",
      "Training Epoch: 5 [10752/50000]\tLoss: 2.7799\tLR: 0.100000\n",
      "Training Epoch: 5 [10880/50000]\tLoss: 2.5860\tLR: 0.100000\n",
      "Training Epoch: 5 [11008/50000]\tLoss: 2.3608\tLR: 0.100000\n",
      "Training Epoch: 5 [11136/50000]\tLoss: 2.6785\tLR: 0.100000\n",
      "Training Epoch: 5 [11264/50000]\tLoss: 2.2299\tLR: 0.100000\n",
      "Training Epoch: 5 [11392/50000]\tLoss: 2.5593\tLR: 0.100000\n",
      "Training Epoch: 5 [11520/50000]\tLoss: 2.4594\tLR: 0.100000\n",
      "Training Epoch: 5 [11648/50000]\tLoss: 2.5882\tLR: 0.100000\n",
      "Training Epoch: 5 [11776/50000]\tLoss: 2.3489\tLR: 0.100000\n",
      "Training Epoch: 5 [11904/50000]\tLoss: 2.5689\tLR: 0.100000\n",
      "Training Epoch: 5 [12032/50000]\tLoss: 2.6743\tLR: 0.100000\n",
      "Training Epoch: 5 [12160/50000]\tLoss: 2.7896\tLR: 0.100000\n",
      "Training Epoch: 5 [12288/50000]\tLoss: 2.3495\tLR: 0.100000\n",
      "Training Epoch: 5 [12416/50000]\tLoss: 2.7465\tLR: 0.100000\n",
      "Training Epoch: 5 [12544/50000]\tLoss: 2.7029\tLR: 0.100000\n",
      "Training Epoch: 5 [12672/50000]\tLoss: 2.6205\tLR: 0.100000\n",
      "Training Epoch: 5 [12800/50000]\tLoss: 2.5366\tLR: 0.100000\n",
      "Training Epoch: 5 [12928/50000]\tLoss: 2.5788\tLR: 0.100000\n",
      "Training Epoch: 5 [13056/50000]\tLoss: 2.8947\tLR: 0.100000\n",
      "Training Epoch: 5 [13184/50000]\tLoss: 2.5357\tLR: 0.100000\n",
      "Training Epoch: 5 [13312/50000]\tLoss: 2.4586\tLR: 0.100000\n",
      "Training Epoch: 5 [13440/50000]\tLoss: 2.3968\tLR: 0.100000\n",
      "Training Epoch: 5 [13568/50000]\tLoss: 2.6858\tLR: 0.100000\n",
      "Training Epoch: 5 [13696/50000]\tLoss: 2.2358\tLR: 0.100000\n",
      "Training Epoch: 5 [13824/50000]\tLoss: 2.7334\tLR: 0.100000\n",
      "Training Epoch: 5 [13952/50000]\tLoss: 2.5872\tLR: 0.100000\n",
      "Training Epoch: 5 [14080/50000]\tLoss: 2.5455\tLR: 0.100000\n",
      "Training Epoch: 5 [14208/50000]\tLoss: 2.5671\tLR: 0.100000\n",
      "Training Epoch: 5 [14336/50000]\tLoss: 2.8271\tLR: 0.100000\n",
      "Training Epoch: 5 [14464/50000]\tLoss: 2.5887\tLR: 0.100000\n",
      "Training Epoch: 5 [14592/50000]\tLoss: 2.6147\tLR: 0.100000\n",
      "Training Epoch: 5 [14720/50000]\tLoss: 2.6026\tLR: 0.100000\n",
      "Training Epoch: 5 [14848/50000]\tLoss: 2.7568\tLR: 0.100000\n",
      "Training Epoch: 5 [14976/50000]\tLoss: 2.6028\tLR: 0.100000\n",
      "Training Epoch: 5 [15104/50000]\tLoss: 2.5914\tLR: 0.100000\n",
      "Training Epoch: 5 [15232/50000]\tLoss: 2.8327\tLR: 0.100000\n",
      "Training Epoch: 5 [15360/50000]\tLoss: 2.6074\tLR: 0.100000\n",
      "Training Epoch: 5 [15488/50000]\tLoss: 2.6743\tLR: 0.100000\n",
      "Training Epoch: 5 [15616/50000]\tLoss: 2.5826\tLR: 0.100000\n",
      "Training Epoch: 5 [15744/50000]\tLoss: 2.4185\tLR: 0.100000\n",
      "Training Epoch: 5 [15872/50000]\tLoss: 2.5611\tLR: 0.100000\n",
      "Training Epoch: 5 [16000/50000]\tLoss: 2.8684\tLR: 0.100000\n",
      "Training Epoch: 5 [16128/50000]\tLoss: 2.6951\tLR: 0.100000\n",
      "Training Epoch: 5 [16256/50000]\tLoss: 2.4897\tLR: 0.100000\n",
      "Training Epoch: 5 [16384/50000]\tLoss: 2.6155\tLR: 0.100000\n",
      "Training Epoch: 5 [16512/50000]\tLoss: 2.4180\tLR: 0.100000\n",
      "Training Epoch: 5 [16640/50000]\tLoss: 2.7735\tLR: 0.100000\n",
      "Training Epoch: 5 [16768/50000]\tLoss: 2.5733\tLR: 0.100000\n",
      "Training Epoch: 5 [16896/50000]\tLoss: 2.5728\tLR: 0.100000\n",
      "Training Epoch: 5 [17024/50000]\tLoss: 2.6205\tLR: 0.100000\n",
      "Training Epoch: 5 [17152/50000]\tLoss: 2.7586\tLR: 0.100000\n",
      "Training Epoch: 5 [17280/50000]\tLoss: 2.6259\tLR: 0.100000\n",
      "Training Epoch: 5 [17408/50000]\tLoss: 2.3030\tLR: 0.100000\n",
      "Training Epoch: 5 [17536/50000]\tLoss: 2.6115\tLR: 0.100000\n",
      "Training Epoch: 5 [17664/50000]\tLoss: 2.6989\tLR: 0.100000\n",
      "Training Epoch: 5 [17792/50000]\tLoss: 2.7020\tLR: 0.100000\n",
      "Training Epoch: 5 [17920/50000]\tLoss: 2.8926\tLR: 0.100000\n",
      "Training Epoch: 5 [18048/50000]\tLoss: 2.4587\tLR: 0.100000\n",
      "Training Epoch: 5 [18176/50000]\tLoss: 2.3932\tLR: 0.100000\n",
      "Training Epoch: 5 [18304/50000]\tLoss: 2.6323\tLR: 0.100000\n",
      "Training Epoch: 5 [18432/50000]\tLoss: 2.4932\tLR: 0.100000\n",
      "Training Epoch: 5 [18560/50000]\tLoss: 2.6675\tLR: 0.100000\n",
      "Training Epoch: 5 [18688/50000]\tLoss: 2.4548\tLR: 0.100000\n",
      "Training Epoch: 5 [18816/50000]\tLoss: 2.2313\tLR: 0.100000\n",
      "Training Epoch: 5 [18944/50000]\tLoss: 2.3740\tLR: 0.100000\n",
      "Training Epoch: 5 [19072/50000]\tLoss: 2.4613\tLR: 0.100000\n",
      "Training Epoch: 5 [19200/50000]\tLoss: 2.4721\tLR: 0.100000\n",
      "Training Epoch: 5 [19328/50000]\tLoss: 2.4858\tLR: 0.100000\n",
      "Training Epoch: 5 [19456/50000]\tLoss: 2.6550\tLR: 0.100000\n",
      "Training Epoch: 5 [19584/50000]\tLoss: 2.5478\tLR: 0.100000\n",
      "Training Epoch: 5 [19712/50000]\tLoss: 2.6489\tLR: 0.100000\n",
      "Training Epoch: 5 [19840/50000]\tLoss: 2.3602\tLR: 0.100000\n",
      "Training Epoch: 5 [19968/50000]\tLoss: 2.6008\tLR: 0.100000\n",
      "Training Epoch: 5 [20096/50000]\tLoss: 2.7311\tLR: 0.100000\n",
      "Training Epoch: 5 [20224/50000]\tLoss: 2.6604\tLR: 0.100000\n",
      "Training Epoch: 5 [20352/50000]\tLoss: 2.6175\tLR: 0.100000\n",
      "Training Epoch: 5 [20480/50000]\tLoss: 2.4211\tLR: 0.100000\n",
      "Training Epoch: 5 [20608/50000]\tLoss: 2.5968\tLR: 0.100000\n",
      "Training Epoch: 5 [20736/50000]\tLoss: 2.7497\tLR: 0.100000\n",
      "Training Epoch: 5 [20864/50000]\tLoss: 2.6374\tLR: 0.100000\n",
      "Training Epoch: 5 [20992/50000]\tLoss: 2.5035\tLR: 0.100000\n",
      "Training Epoch: 5 [21120/50000]\tLoss: 2.6468\tLR: 0.100000\n",
      "Training Epoch: 5 [21248/50000]\tLoss: 2.5308\tLR: 0.100000\n",
      "Training Epoch: 5 [21376/50000]\tLoss: 2.3136\tLR: 0.100000\n",
      "Training Epoch: 5 [21504/50000]\tLoss: 2.5954\tLR: 0.100000\n",
      "Training Epoch: 5 [21632/50000]\tLoss: 2.7938\tLR: 0.100000\n",
      "Training Epoch: 5 [21760/50000]\tLoss: 2.8239\tLR: 0.100000\n",
      "Training Epoch: 5 [21888/50000]\tLoss: 2.9449\tLR: 0.100000\n",
      "Training Epoch: 5 [22016/50000]\tLoss: 2.3878\tLR: 0.100000\n",
      "Training Epoch: 5 [22144/50000]\tLoss: 2.6361\tLR: 0.100000\n",
      "Training Epoch: 5 [22272/50000]\tLoss: 2.4312\tLR: 0.100000\n",
      "Training Epoch: 5 [22400/50000]\tLoss: 2.5765\tLR: 0.100000\n",
      "Training Epoch: 5 [22528/50000]\tLoss: 2.4948\tLR: 0.100000\n",
      "Training Epoch: 5 [22656/50000]\tLoss: 2.6833\tLR: 0.100000\n",
      "Training Epoch: 5 [22784/50000]\tLoss: 2.5461\tLR: 0.100000\n",
      "Training Epoch: 5 [22912/50000]\tLoss: 2.5174\tLR: 0.100000\n",
      "Training Epoch: 5 [23040/50000]\tLoss: 2.5136\tLR: 0.100000\n",
      "Training Epoch: 5 [23168/50000]\tLoss: 2.7002\tLR: 0.100000\n",
      "Training Epoch: 5 [23296/50000]\tLoss: 2.4570\tLR: 0.100000\n",
      "Training Epoch: 5 [23424/50000]\tLoss: 2.6102\tLR: 0.100000\n",
      "Training Epoch: 5 [23552/50000]\tLoss: 2.4650\tLR: 0.100000\n",
      "Training Epoch: 5 [23680/50000]\tLoss: 2.6768\tLR: 0.100000\n",
      "Training Epoch: 5 [23808/50000]\tLoss: 2.5542\tLR: 0.100000\n",
      "Training Epoch: 5 [23936/50000]\tLoss: 2.2937\tLR: 0.100000\n",
      "Training Epoch: 5 [24064/50000]\tLoss: 2.4639\tLR: 0.100000\n",
      "Training Epoch: 5 [24192/50000]\tLoss: 2.5681\tLR: 0.100000\n",
      "Training Epoch: 5 [24320/50000]\tLoss: 2.8304\tLR: 0.100000\n",
      "Training Epoch: 5 [24448/50000]\tLoss: 2.4191\tLR: 0.100000\n",
      "Training Epoch: 5 [24576/50000]\tLoss: 2.3654\tLR: 0.100000\n",
      "Training Epoch: 5 [24704/50000]\tLoss: 2.4636\tLR: 0.100000\n",
      "Training Epoch: 5 [24832/50000]\tLoss: 2.6667\tLR: 0.100000\n",
      "Training Epoch: 5 [24960/50000]\tLoss: 2.6038\tLR: 0.100000\n",
      "Training Epoch: 5 [25088/50000]\tLoss: 2.4048\tLR: 0.100000\n",
      "Training Epoch: 5 [25216/50000]\tLoss: 2.6147\tLR: 0.100000\n",
      "Training Epoch: 5 [25344/50000]\tLoss: 2.3386\tLR: 0.100000\n",
      "Training Epoch: 5 [25472/50000]\tLoss: 2.7251\tLR: 0.100000\n",
      "Training Epoch: 5 [25600/50000]\tLoss: 2.5266\tLR: 0.100000\n",
      "Training Epoch: 5 [25728/50000]\tLoss: 2.5380\tLR: 0.100000\n",
      "Training Epoch: 5 [25856/50000]\tLoss: 2.4857\tLR: 0.100000\n",
      "Training Epoch: 5 [25984/50000]\tLoss: 2.5129\tLR: 0.100000\n",
      "Training Epoch: 5 [26112/50000]\tLoss: 2.4889\tLR: 0.100000\n",
      "Training Epoch: 5 [26240/50000]\tLoss: 2.5299\tLR: 0.100000\n",
      "Training Epoch: 5 [26368/50000]\tLoss: 2.1407\tLR: 0.100000\n",
      "Training Epoch: 5 [26496/50000]\tLoss: 2.3927\tLR: 0.100000\n",
      "Training Epoch: 5 [26624/50000]\tLoss: 2.4032\tLR: 0.100000\n",
      "Training Epoch: 5 [26752/50000]\tLoss: 2.3982\tLR: 0.100000\n",
      "Training Epoch: 5 [26880/50000]\tLoss: 2.6305\tLR: 0.100000\n",
      "Training Epoch: 5 [27008/50000]\tLoss: 2.5141\tLR: 0.100000\n",
      "Training Epoch: 5 [27136/50000]\tLoss: 2.3258\tLR: 0.100000\n",
      "Training Epoch: 5 [27264/50000]\tLoss: 2.6392\tLR: 0.100000\n",
      "Training Epoch: 5 [27392/50000]\tLoss: 2.5255\tLR: 0.100000\n",
      "Training Epoch: 5 [27520/50000]\tLoss: 2.8984\tLR: 0.100000\n",
      "Training Epoch: 5 [27648/50000]\tLoss: 2.5062\tLR: 0.100000\n",
      "Training Epoch: 5 [27776/50000]\tLoss: 2.5504\tLR: 0.100000\n",
      "Training Epoch: 5 [27904/50000]\tLoss: 2.4157\tLR: 0.100000\n",
      "Training Epoch: 5 [28032/50000]\tLoss: 2.4071\tLR: 0.100000\n",
      "Training Epoch: 5 [28160/50000]\tLoss: 2.4666\tLR: 0.100000\n",
      "Training Epoch: 5 [28288/50000]\tLoss: 2.1520\tLR: 0.100000\n",
      "Training Epoch: 5 [28416/50000]\tLoss: 2.6479\tLR: 0.100000\n",
      "Training Epoch: 5 [28544/50000]\tLoss: 2.6894\tLR: 0.100000\n",
      "Training Epoch: 5 [28672/50000]\tLoss: 2.3438\tLR: 0.100000\n",
      "Training Epoch: 5 [28800/50000]\tLoss: 2.4953\tLR: 0.100000\n",
      "Training Epoch: 5 [28928/50000]\tLoss: 2.6365\tLR: 0.100000\n",
      "Training Epoch: 5 [29056/50000]\tLoss: 2.6902\tLR: 0.100000\n",
      "Training Epoch: 5 [29184/50000]\tLoss: 2.3363\tLR: 0.100000\n",
      "Training Epoch: 5 [29312/50000]\tLoss: 2.6031\tLR: 0.100000\n",
      "Training Epoch: 5 [29440/50000]\tLoss: 2.5301\tLR: 0.100000\n",
      "Training Epoch: 5 [29568/50000]\tLoss: 2.5138\tLR: 0.100000\n",
      "Training Epoch: 5 [29696/50000]\tLoss: 2.4266\tLR: 0.100000\n",
      "Training Epoch: 5 [29824/50000]\tLoss: 2.3444\tLR: 0.100000\n",
      "Training Epoch: 5 [29952/50000]\tLoss: 2.6779\tLR: 0.100000\n",
      "Training Epoch: 5 [30080/50000]\tLoss: 2.4553\tLR: 0.100000\n",
      "Training Epoch: 5 [30208/50000]\tLoss: 2.2626\tLR: 0.100000\n",
      "Training Epoch: 5 [30336/50000]\tLoss: 2.4263\tLR: 0.100000\n",
      "Training Epoch: 5 [30464/50000]\tLoss: 2.5600\tLR: 0.100000\n",
      "Training Epoch: 5 [30592/50000]\tLoss: 2.1867\tLR: 0.100000\n",
      "Training Epoch: 5 [30720/50000]\tLoss: 2.5325\tLR: 0.100000\n",
      "Training Epoch: 5 [30848/50000]\tLoss: 2.6163\tLR: 0.100000\n",
      "Training Epoch: 5 [30976/50000]\tLoss: 2.3233\tLR: 0.100000\n",
      "Training Epoch: 5 [31104/50000]\tLoss: 2.3180\tLR: 0.100000\n",
      "Training Epoch: 5 [31232/50000]\tLoss: 2.5849\tLR: 0.100000\n",
      "Training Epoch: 5 [31360/50000]\tLoss: 2.2256\tLR: 0.100000\n",
      "Training Epoch: 5 [31488/50000]\tLoss: 2.5390\tLR: 0.100000\n",
      "Training Epoch: 5 [31616/50000]\tLoss: 2.4662\tLR: 0.100000\n",
      "Training Epoch: 5 [31744/50000]\tLoss: 2.4629\tLR: 0.100000\n",
      "Training Epoch: 5 [31872/50000]\tLoss: 2.7191\tLR: 0.100000\n",
      "Training Epoch: 5 [32000/50000]\tLoss: 2.6927\tLR: 0.100000\n",
      "Training Epoch: 5 [32128/50000]\tLoss: 2.3339\tLR: 0.100000\n",
      "Training Epoch: 5 [32256/50000]\tLoss: 2.2519\tLR: 0.100000\n",
      "Training Epoch: 5 [32384/50000]\tLoss: 2.3758\tLR: 0.100000\n",
      "Training Epoch: 5 [32512/50000]\tLoss: 2.4696\tLR: 0.100000\n",
      "Training Epoch: 5 [32640/50000]\tLoss: 2.2803\tLR: 0.100000\n",
      "Training Epoch: 5 [32768/50000]\tLoss: 2.4380\tLR: 0.100000\n",
      "Training Epoch: 5 [32896/50000]\tLoss: 2.4187\tLR: 0.100000\n",
      "Training Epoch: 5 [33024/50000]\tLoss: 2.4277\tLR: 0.100000\n",
      "Training Epoch: 5 [33152/50000]\tLoss: 2.7064\tLR: 0.100000\n",
      "Training Epoch: 5 [33280/50000]\tLoss: 2.5157\tLR: 0.100000\n",
      "Training Epoch: 5 [33408/50000]\tLoss: 2.3169\tLR: 0.100000\n",
      "Training Epoch: 5 [33536/50000]\tLoss: 2.4701\tLR: 0.100000\n",
      "Training Epoch: 5 [33664/50000]\tLoss: 2.5396\tLR: 0.100000\n",
      "Training Epoch: 5 [33792/50000]\tLoss: 2.7061\tLR: 0.100000\n",
      "Training Epoch: 5 [33920/50000]\tLoss: 2.1838\tLR: 0.100000\n",
      "Training Epoch: 5 [34048/50000]\tLoss: 2.4512\tLR: 0.100000\n",
      "Training Epoch: 5 [34176/50000]\tLoss: 2.7129\tLR: 0.100000\n",
      "Training Epoch: 5 [34304/50000]\tLoss: 2.3507\tLR: 0.100000\n",
      "Training Epoch: 5 [34432/50000]\tLoss: 2.7063\tLR: 0.100000\n",
      "Training Epoch: 5 [34560/50000]\tLoss: 2.3061\tLR: 0.100000\n",
      "Training Epoch: 5 [34688/50000]\tLoss: 2.5935\tLR: 0.100000\n",
      "Training Epoch: 5 [34816/50000]\tLoss: 2.6362\tLR: 0.100000\n",
      "Training Epoch: 5 [34944/50000]\tLoss: 2.3111\tLR: 0.100000\n",
      "Training Epoch: 5 [35072/50000]\tLoss: 2.5690\tLR: 0.100000\n",
      "Training Epoch: 5 [35200/50000]\tLoss: 2.5542\tLR: 0.100000\n",
      "Training Epoch: 5 [35328/50000]\tLoss: 2.8248\tLR: 0.100000\n",
      "Training Epoch: 5 [35456/50000]\tLoss: 2.3888\tLR: 0.100000\n",
      "Training Epoch: 5 [35584/50000]\tLoss: 2.3665\tLR: 0.100000\n",
      "Training Epoch: 5 [35712/50000]\tLoss: 2.3073\tLR: 0.100000\n",
      "Training Epoch: 5 [35840/50000]\tLoss: 2.7259\tLR: 0.100000\n",
      "Training Epoch: 5 [35968/50000]\tLoss: 2.4704\tLR: 0.100000\n",
      "Training Epoch: 5 [36096/50000]\tLoss: 2.3219\tLR: 0.100000\n",
      "Training Epoch: 5 [36224/50000]\tLoss: 2.3444\tLR: 0.100000\n",
      "Training Epoch: 5 [36352/50000]\tLoss: 2.4552\tLR: 0.100000\n",
      "Training Epoch: 5 [36480/50000]\tLoss: 2.4088\tLR: 0.100000\n",
      "Training Epoch: 5 [36608/50000]\tLoss: 2.5960\tLR: 0.100000\n",
      "Training Epoch: 5 [36736/50000]\tLoss: 2.5761\tLR: 0.100000\n",
      "Training Epoch: 5 [36864/50000]\tLoss: 2.5928\tLR: 0.100000\n",
      "Training Epoch: 5 [36992/50000]\tLoss: 2.4622\tLR: 0.100000\n",
      "Training Epoch: 5 [37120/50000]\tLoss: 2.5696\tLR: 0.100000\n",
      "Training Epoch: 5 [37248/50000]\tLoss: 2.3708\tLR: 0.100000\n",
      "Training Epoch: 5 [37376/50000]\tLoss: 2.5087\tLR: 0.100000\n",
      "Training Epoch: 5 [37504/50000]\tLoss: 2.5841\tLR: 0.100000\n",
      "Training Epoch: 5 [37632/50000]\tLoss: 2.3458\tLR: 0.100000\n",
      "Training Epoch: 5 [37760/50000]\tLoss: 2.2269\tLR: 0.100000\n",
      "Training Epoch: 5 [37888/50000]\tLoss: 2.4778\tLR: 0.100000\n",
      "Training Epoch: 5 [38016/50000]\tLoss: 2.4384\tLR: 0.100000\n",
      "Training Epoch: 5 [38144/50000]\tLoss: 2.4385\tLR: 0.100000\n",
      "Training Epoch: 5 [38272/50000]\tLoss: 2.5804\tLR: 0.100000\n",
      "Training Epoch: 5 [38400/50000]\tLoss: 2.5618\tLR: 0.100000\n",
      "Training Epoch: 5 [38528/50000]\tLoss: 2.5344\tLR: 0.100000\n",
      "Training Epoch: 5 [38656/50000]\tLoss: 2.3655\tLR: 0.100000\n",
      "Training Epoch: 5 [38784/50000]\tLoss: 2.2298\tLR: 0.100000\n",
      "Training Epoch: 5 [38912/50000]\tLoss: 2.5147\tLR: 0.100000\n",
      "Training Epoch: 5 [39040/50000]\tLoss: 2.2269\tLR: 0.100000\n",
      "Training Epoch: 5 [39168/50000]\tLoss: 2.3060\tLR: 0.100000\n",
      "Training Epoch: 5 [39296/50000]\tLoss: 2.5846\tLR: 0.100000\n",
      "Training Epoch: 5 [39424/50000]\tLoss: 2.4915\tLR: 0.100000\n",
      "Training Epoch: 5 [39552/50000]\tLoss: 2.4355\tLR: 0.100000\n",
      "Training Epoch: 5 [39680/50000]\tLoss: 2.4395\tLR: 0.100000\n",
      "Training Epoch: 5 [39808/50000]\tLoss: 2.3290\tLR: 0.100000\n",
      "Training Epoch: 5 [39936/50000]\tLoss: 2.3253\tLR: 0.100000\n",
      "Training Epoch: 5 [40064/50000]\tLoss: 2.4588\tLR: 0.100000\n",
      "Training Epoch: 5 [40192/50000]\tLoss: 2.6411\tLR: 0.100000\n",
      "Training Epoch: 5 [40320/50000]\tLoss: 2.4777\tLR: 0.100000\n",
      "Training Epoch: 5 [40448/50000]\tLoss: 2.2819\tLR: 0.100000\n",
      "Training Epoch: 5 [40576/50000]\tLoss: 2.2666\tLR: 0.100000\n",
      "Training Epoch: 5 [40704/50000]\tLoss: 2.4579\tLR: 0.100000\n",
      "Training Epoch: 5 [40832/50000]\tLoss: 2.2710\tLR: 0.100000\n",
      "Training Epoch: 5 [40960/50000]\tLoss: 2.3205\tLR: 0.100000\n",
      "Training Epoch: 5 [41088/50000]\tLoss: 2.6211\tLR: 0.100000\n",
      "Training Epoch: 5 [41216/50000]\tLoss: 2.7736\tLR: 0.100000\n",
      "Training Epoch: 5 [41344/50000]\tLoss: 2.2405\tLR: 0.100000\n",
      "Training Epoch: 5 [41472/50000]\tLoss: 2.6869\tLR: 0.100000\n",
      "Training Epoch: 5 [41600/50000]\tLoss: 2.4443\tLR: 0.100000\n",
      "Training Epoch: 5 [41728/50000]\tLoss: 2.4363\tLR: 0.100000\n",
      "Training Epoch: 5 [41856/50000]\tLoss: 2.5150\tLR: 0.100000\n",
      "Training Epoch: 5 [41984/50000]\tLoss: 2.5149\tLR: 0.100000\n",
      "Training Epoch: 5 [42112/50000]\tLoss: 2.1975\tLR: 0.100000\n",
      "Training Epoch: 5 [42240/50000]\tLoss: 2.5996\tLR: 0.100000\n",
      "Training Epoch: 5 [42368/50000]\tLoss: 2.6867\tLR: 0.100000\n",
      "Training Epoch: 5 [42496/50000]\tLoss: 2.3640\tLR: 0.100000\n",
      "Training Epoch: 5 [42624/50000]\tLoss: 2.4345\tLR: 0.100000\n",
      "Training Epoch: 5 [42752/50000]\tLoss: 2.5096\tLR: 0.100000\n",
      "Training Epoch: 5 [42880/50000]\tLoss: 2.4466\tLR: 0.100000\n",
      "Training Epoch: 5 [43008/50000]\tLoss: 2.4410\tLR: 0.100000\n",
      "Training Epoch: 5 [43136/50000]\tLoss: 2.3308\tLR: 0.100000\n",
      "Training Epoch: 5 [43264/50000]\tLoss: 2.4512\tLR: 0.100000\n",
      "Training Epoch: 5 [43392/50000]\tLoss: 2.5811\tLR: 0.100000\n",
      "Training Epoch: 5 [43520/50000]\tLoss: 2.5301\tLR: 0.100000\n",
      "Training Epoch: 5 [43648/50000]\tLoss: 2.4636\tLR: 0.100000\n",
      "Training Epoch: 5 [43776/50000]\tLoss: 1.9938\tLR: 0.100000\n",
      "Training Epoch: 5 [43904/50000]\tLoss: 2.3716\tLR: 0.100000\n",
      "Training Epoch: 5 [44032/50000]\tLoss: 2.4916\tLR: 0.100000\n",
      "Training Epoch: 5 [44160/50000]\tLoss: 2.3733\tLR: 0.100000\n",
      "Training Epoch: 5 [44288/50000]\tLoss: 2.5075\tLR: 0.100000\n",
      "Training Epoch: 5 [44416/50000]\tLoss: 2.3663\tLR: 0.100000\n",
      "Training Epoch: 5 [44544/50000]\tLoss: 2.1473\tLR: 0.100000\n",
      "Training Epoch: 5 [44672/50000]\tLoss: 2.3931\tLR: 0.100000\n",
      "Training Epoch: 5 [44800/50000]\tLoss: 2.7180\tLR: 0.100000\n",
      "Training Epoch: 5 [44928/50000]\tLoss: 2.5787\tLR: 0.100000\n",
      "Training Epoch: 5 [45056/50000]\tLoss: 2.3889\tLR: 0.100000\n",
      "Training Epoch: 5 [45184/50000]\tLoss: 2.3835\tLR: 0.100000\n",
      "Training Epoch: 5 [45312/50000]\tLoss: 2.5304\tLR: 0.100000\n",
      "Training Epoch: 5 [45440/50000]\tLoss: 2.4581\tLR: 0.100000\n",
      "Training Epoch: 5 [45568/50000]\tLoss: 2.3589\tLR: 0.100000\n",
      "Training Epoch: 5 [45696/50000]\tLoss: 2.4498\tLR: 0.100000\n",
      "Training Epoch: 5 [45824/50000]\tLoss: 2.2512\tLR: 0.100000\n",
      "Training Epoch: 5 [45952/50000]\tLoss: 2.2538\tLR: 0.100000\n",
      "Training Epoch: 5 [46080/50000]\tLoss: 2.3144\tLR: 0.100000\n",
      "Training Epoch: 5 [46208/50000]\tLoss: 2.2635\tLR: 0.100000\n",
      "Training Epoch: 5 [46336/50000]\tLoss: 2.4133\tLR: 0.100000\n",
      "Training Epoch: 5 [46464/50000]\tLoss: 2.2713\tLR: 0.100000\n",
      "Training Epoch: 5 [46592/50000]\tLoss: 2.3647\tLR: 0.100000\n",
      "Training Epoch: 5 [46720/50000]\tLoss: 2.3749\tLR: 0.100000\n",
      "Training Epoch: 5 [46848/50000]\tLoss: 2.3974\tLR: 0.100000\n",
      "Training Epoch: 5 [46976/50000]\tLoss: 2.3110\tLR: 0.100000\n",
      "Training Epoch: 5 [47104/50000]\tLoss: 2.5891\tLR: 0.100000\n",
      "Training Epoch: 5 [47232/50000]\tLoss: 2.4824\tLR: 0.100000\n",
      "Training Epoch: 5 [47360/50000]\tLoss: 2.5318\tLR: 0.100000\n",
      "Training Epoch: 5 [47488/50000]\tLoss: 2.2118\tLR: 0.100000\n",
      "Training Epoch: 5 [47616/50000]\tLoss: 2.4266\tLR: 0.100000\n",
      "Training Epoch: 5 [47744/50000]\tLoss: 2.3613\tLR: 0.100000\n",
      "Training Epoch: 5 [47872/50000]\tLoss: 2.3439\tLR: 0.100000\n",
      "Training Epoch: 5 [48000/50000]\tLoss: 2.2127\tLR: 0.100000\n",
      "Training Epoch: 5 [48128/50000]\tLoss: 2.6480\tLR: 0.100000\n",
      "Training Epoch: 5 [48256/50000]\tLoss: 2.4830\tLR: 0.100000\n",
      "Training Epoch: 5 [48384/50000]\tLoss: 2.5903\tLR: 0.100000\n",
      "Training Epoch: 5 [48512/50000]\tLoss: 2.5563\tLR: 0.100000\n",
      "Training Epoch: 5 [48640/50000]\tLoss: 2.5047\tLR: 0.100000\n",
      "Training Epoch: 5 [48768/50000]\tLoss: 2.3918\tLR: 0.100000\n",
      "Training Epoch: 5 [48896/50000]\tLoss: 2.5853\tLR: 0.100000\n",
      "Training Epoch: 5 [49024/50000]\tLoss: 2.1443\tLR: 0.100000\n",
      "Training Epoch: 5 [49152/50000]\tLoss: 2.4394\tLR: 0.100000\n",
      "Training Epoch: 5 [49280/50000]\tLoss: 2.4990\tLR: 0.100000\n",
      "Training Epoch: 5 [49408/50000]\tLoss: 2.6047\tLR: 0.100000\n",
      "Training Epoch: 5 [49536/50000]\tLoss: 2.5603\tLR: 0.100000\n",
      "Training Epoch: 5 [49664/50000]\tLoss: 2.3943\tLR: 0.100000\n",
      "Training Epoch: 5 [49792/50000]\tLoss: 2.5468\tLR: 0.100000\n",
      "Training Epoch: 5 [49920/50000]\tLoss: 2.5932\tLR: 0.100000\n",
      "Training Epoch: 5 [50000/50000]\tLoss: 2.3947\tLR: 0.100000\n",
      "Test set: Average loss: 0.0249, Accuracy: 0.2516\n",
      "\n",
      "Training Epoch: 6 [128/50000]\tLoss: 2.3536\tLR: 0.015000\n",
      "Training Epoch: 6 [256/50000]\tLoss: 2.3083\tLR: 0.015000\n",
      "Training Epoch: 6 [384/50000]\tLoss: 2.2845\tLR: 0.015000\n",
      "Training Epoch: 6 [512/50000]\tLoss: 2.3312\tLR: 0.015000\n",
      "Training Epoch: 6 [640/50000]\tLoss: 2.1765\tLR: 0.015000\n",
      "Training Epoch: 6 [768/50000]\tLoss: 2.0966\tLR: 0.015000\n",
      "Training Epoch: 6 [896/50000]\tLoss: 2.2602\tLR: 0.015000\n",
      "Training Epoch: 6 [1024/50000]\tLoss: 2.2287\tLR: 0.015000\n",
      "Training Epoch: 6 [1152/50000]\tLoss: 2.2668\tLR: 0.015000\n",
      "Training Epoch: 6 [1280/50000]\tLoss: 2.5582\tLR: 0.015000\n",
      "Training Epoch: 6 [1408/50000]\tLoss: 2.2016\tLR: 0.015000\n",
      "Training Epoch: 6 [1536/50000]\tLoss: 2.6115\tLR: 0.015000\n",
      "Training Epoch: 6 [1664/50000]\tLoss: 2.1885\tLR: 0.015000\n",
      "Training Epoch: 6 [1792/50000]\tLoss: 1.9033\tLR: 0.015000\n",
      "Training Epoch: 6 [1920/50000]\tLoss: 2.4357\tLR: 0.015000\n",
      "Training Epoch: 6 [2048/50000]\tLoss: 2.0327\tLR: 0.015000\n",
      "Training Epoch: 6 [2176/50000]\tLoss: 2.3487\tLR: 0.015000\n",
      "Training Epoch: 6 [2304/50000]\tLoss: 2.2640\tLR: 0.015000\n",
      "Training Epoch: 6 [2432/50000]\tLoss: 2.1776\tLR: 0.015000\n",
      "Training Epoch: 6 [2560/50000]\tLoss: 2.1668\tLR: 0.015000\n",
      "Training Epoch: 6 [2688/50000]\tLoss: 2.2064\tLR: 0.015000\n",
      "Training Epoch: 6 [2816/50000]\tLoss: 2.1131\tLR: 0.015000\n",
      "Training Epoch: 6 [2944/50000]\tLoss: 2.1309\tLR: 0.015000\n",
      "Training Epoch: 6 [3072/50000]\tLoss: 2.0971\tLR: 0.015000\n",
      "Training Epoch: 6 [3200/50000]\tLoss: 1.9405\tLR: 0.015000\n",
      "Training Epoch: 6 [3328/50000]\tLoss: 1.9649\tLR: 0.015000\n",
      "Training Epoch: 6 [3456/50000]\tLoss: 2.2168\tLR: 0.015000\n",
      "Training Epoch: 6 [3584/50000]\tLoss: 2.0192\tLR: 0.015000\n",
      "Training Epoch: 6 [3712/50000]\tLoss: 1.9917\tLR: 0.015000\n",
      "Training Epoch: 6 [3840/50000]\tLoss: 2.0759\tLR: 0.015000\n",
      "Training Epoch: 6 [3968/50000]\tLoss: 1.9585\tLR: 0.015000\n",
      "Training Epoch: 6 [4096/50000]\tLoss: 2.0382\tLR: 0.015000\n",
      "Training Epoch: 6 [4224/50000]\tLoss: 2.0988\tLR: 0.015000\n",
      "Training Epoch: 6 [4352/50000]\tLoss: 2.3883\tLR: 0.015000\n",
      "Training Epoch: 6 [4480/50000]\tLoss: 2.1306\tLR: 0.015000\n",
      "Training Epoch: 6 [4608/50000]\tLoss: 1.9256\tLR: 0.015000\n",
      "Training Epoch: 6 [4736/50000]\tLoss: 1.9894\tLR: 0.015000\n",
      "Training Epoch: 6 [4864/50000]\tLoss: 1.8259\tLR: 0.015000\n",
      "Training Epoch: 6 [4992/50000]\tLoss: 2.2657\tLR: 0.015000\n",
      "Training Epoch: 6 [5120/50000]\tLoss: 2.1523\tLR: 0.015000\n",
      "Training Epoch: 6 [5248/50000]\tLoss: 2.1288\tLR: 0.015000\n",
      "Training Epoch: 6 [5376/50000]\tLoss: 2.0892\tLR: 0.015000\n",
      "Training Epoch: 6 [5504/50000]\tLoss: 2.0798\tLR: 0.015000\n",
      "Training Epoch: 6 [5632/50000]\tLoss: 2.0773\tLR: 0.015000\n",
      "Training Epoch: 6 [5760/50000]\tLoss: 1.9805\tLR: 0.015000\n",
      "Training Epoch: 6 [5888/50000]\tLoss: 2.2389\tLR: 0.015000\n",
      "Training Epoch: 6 [6016/50000]\tLoss: 1.9006\tLR: 0.015000\n",
      "Training Epoch: 6 [6144/50000]\tLoss: 2.3665\tLR: 0.015000\n",
      "Training Epoch: 6 [6272/50000]\tLoss: 2.1300\tLR: 0.015000\n",
      "Training Epoch: 6 [6400/50000]\tLoss: 1.9864\tLR: 0.015000\n",
      "Training Epoch: 6 [6528/50000]\tLoss: 1.9616\tLR: 0.015000\n",
      "Training Epoch: 6 [6656/50000]\tLoss: 1.9752\tLR: 0.015000\n",
      "Training Epoch: 6 [6784/50000]\tLoss: 1.8332\tLR: 0.015000\n",
      "Training Epoch: 6 [6912/50000]\tLoss: 1.8176\tLR: 0.015000\n",
      "Training Epoch: 6 [7040/50000]\tLoss: 1.7992\tLR: 0.015000\n",
      "Training Epoch: 6 [7168/50000]\tLoss: 2.0228\tLR: 0.015000\n",
      "Training Epoch: 6 [7296/50000]\tLoss: 1.9428\tLR: 0.015000\n",
      "Training Epoch: 6 [7424/50000]\tLoss: 2.2981\tLR: 0.015000\n",
      "Training Epoch: 6 [7552/50000]\tLoss: 2.1074\tLR: 0.015000\n",
      "Training Epoch: 6 [7680/50000]\tLoss: 2.1091\tLR: 0.015000\n",
      "Training Epoch: 6 [7808/50000]\tLoss: 2.1387\tLR: 0.015000\n",
      "Training Epoch: 6 [7936/50000]\tLoss: 2.1292\tLR: 0.015000\n",
      "Training Epoch: 6 [8064/50000]\tLoss: 2.0928\tLR: 0.015000\n",
      "Training Epoch: 6 [8192/50000]\tLoss: 2.0221\tLR: 0.015000\n",
      "Training Epoch: 6 [8320/50000]\tLoss: 2.2594\tLR: 0.015000\n",
      "Training Epoch: 6 [8448/50000]\tLoss: 2.1626\tLR: 0.015000\n",
      "Training Epoch: 6 [8576/50000]\tLoss: 2.1316\tLR: 0.015000\n",
      "Training Epoch: 6 [8704/50000]\tLoss: 1.8927\tLR: 0.015000\n",
      "Training Epoch: 6 [8832/50000]\tLoss: 2.1790\tLR: 0.015000\n",
      "Training Epoch: 6 [8960/50000]\tLoss: 2.0587\tLR: 0.015000\n",
      "Training Epoch: 6 [9088/50000]\tLoss: 1.7622\tLR: 0.015000\n",
      "Training Epoch: 6 [9216/50000]\tLoss: 1.8009\tLR: 0.015000\n",
      "Training Epoch: 6 [9344/50000]\tLoss: 1.9955\tLR: 0.015000\n",
      "Training Epoch: 6 [9472/50000]\tLoss: 1.8630\tLR: 0.015000\n",
      "Training Epoch: 6 [9600/50000]\tLoss: 1.9553\tLR: 0.015000\n",
      "Training Epoch: 6 [9728/50000]\tLoss: 2.1834\tLR: 0.015000\n",
      "Training Epoch: 6 [9856/50000]\tLoss: 2.0100\tLR: 0.015000\n",
      "Training Epoch: 6 [9984/50000]\tLoss: 1.9428\tLR: 0.015000\n",
      "Training Epoch: 6 [10112/50000]\tLoss: 2.0577\tLR: 0.015000\n",
      "Training Epoch: 6 [10240/50000]\tLoss: 1.9196\tLR: 0.015000\n",
      "Training Epoch: 6 [10368/50000]\tLoss: 1.8763\tLR: 0.015000\n",
      "Training Epoch: 6 [10496/50000]\tLoss: 2.0934\tLR: 0.015000\n",
      "Training Epoch: 6 [10624/50000]\tLoss: 2.1091\tLR: 0.015000\n",
      "Training Epoch: 6 [10752/50000]\tLoss: 2.1071\tLR: 0.015000\n",
      "Training Epoch: 6 [10880/50000]\tLoss: 2.0641\tLR: 0.015000\n",
      "Training Epoch: 6 [11008/50000]\tLoss: 2.2332\tLR: 0.015000\n",
      "Training Epoch: 6 [11136/50000]\tLoss: 2.1481\tLR: 0.015000\n",
      "Training Epoch: 6 [11264/50000]\tLoss: 1.9041\tLR: 0.015000\n",
      "Training Epoch: 6 [11392/50000]\tLoss: 1.8069\tLR: 0.015000\n",
      "Training Epoch: 6 [11520/50000]\tLoss: 1.9115\tLR: 0.015000\n",
      "Training Epoch: 6 [11648/50000]\tLoss: 2.1311\tLR: 0.015000\n",
      "Training Epoch: 6 [11776/50000]\tLoss: 1.9604\tLR: 0.015000\n",
      "Training Epoch: 6 [11904/50000]\tLoss: 1.9963\tLR: 0.015000\n",
      "Training Epoch: 6 [12032/50000]\tLoss: 1.9792\tLR: 0.015000\n",
      "Training Epoch: 6 [12160/50000]\tLoss: 1.9929\tLR: 0.015000\n",
      "Training Epoch: 6 [12288/50000]\tLoss: 2.2439\tLR: 0.015000\n",
      "Training Epoch: 6 [12416/50000]\tLoss: 2.0132\tLR: 0.015000\n",
      "Training Epoch: 6 [12544/50000]\tLoss: 2.0462\tLR: 0.015000\n",
      "Training Epoch: 6 [12672/50000]\tLoss: 1.6931\tLR: 0.015000\n",
      "Training Epoch: 6 [12800/50000]\tLoss: 2.0022\tLR: 0.015000\n",
      "Training Epoch: 6 [12928/50000]\tLoss: 2.0379\tLR: 0.015000\n",
      "Training Epoch: 6 [13056/50000]\tLoss: 2.2109\tLR: 0.015000\n",
      "Training Epoch: 6 [13184/50000]\tLoss: 2.1871\tLR: 0.015000\n",
      "Training Epoch: 6 [13312/50000]\tLoss: 1.9082\tLR: 0.015000\n",
      "Training Epoch: 6 [13440/50000]\tLoss: 2.0361\tLR: 0.015000\n",
      "Training Epoch: 6 [13568/50000]\tLoss: 1.8968\tLR: 0.015000\n",
      "Training Epoch: 6 [13696/50000]\tLoss: 2.3011\tLR: 0.015000\n",
      "Training Epoch: 6 [13824/50000]\tLoss: 2.1113\tLR: 0.015000\n",
      "Training Epoch: 6 [13952/50000]\tLoss: 1.8598\tLR: 0.015000\n",
      "Training Epoch: 6 [14080/50000]\tLoss: 2.0581\tLR: 0.015000\n",
      "Training Epoch: 6 [14208/50000]\tLoss: 1.9793\tLR: 0.015000\n",
      "Training Epoch: 6 [14336/50000]\tLoss: 1.9724\tLR: 0.015000\n",
      "Training Epoch: 6 [14464/50000]\tLoss: 2.1454\tLR: 0.015000\n",
      "Training Epoch: 6 [14592/50000]\tLoss: 2.1541\tLR: 0.015000\n",
      "Training Epoch: 6 [14720/50000]\tLoss: 1.8914\tLR: 0.015000\n",
      "Training Epoch: 6 [14848/50000]\tLoss: 2.1439\tLR: 0.015000\n",
      "Training Epoch: 6 [14976/50000]\tLoss: 1.8711\tLR: 0.015000\n",
      "Training Epoch: 6 [15104/50000]\tLoss: 2.1699\tLR: 0.015000\n",
      "Training Epoch: 6 [15232/50000]\tLoss: 1.9431\tLR: 0.015000\n",
      "Training Epoch: 6 [15360/50000]\tLoss: 1.8436\tLR: 0.015000\n",
      "Training Epoch: 6 [15488/50000]\tLoss: 1.8182\tLR: 0.015000\n",
      "Training Epoch: 6 [15616/50000]\tLoss: 2.0673\tLR: 0.015000\n",
      "Training Epoch: 6 [15744/50000]\tLoss: 1.9665\tLR: 0.015000\n",
      "Training Epoch: 6 [15872/50000]\tLoss: 1.8415\tLR: 0.015000\n",
      "Training Epoch: 6 [16000/50000]\tLoss: 1.9948\tLR: 0.015000\n",
      "Training Epoch: 6 [16128/50000]\tLoss: 2.0021\tLR: 0.015000\n",
      "Training Epoch: 6 [16256/50000]\tLoss: 1.8519\tLR: 0.015000\n",
      "Training Epoch: 6 [16384/50000]\tLoss: 2.2766\tLR: 0.015000\n",
      "Training Epoch: 6 [16512/50000]\tLoss: 2.1301\tLR: 0.015000\n",
      "Training Epoch: 6 [16640/50000]\tLoss: 2.2146\tLR: 0.015000\n",
      "Training Epoch: 6 [16768/50000]\tLoss: 1.6895\tLR: 0.015000\n",
      "Training Epoch: 6 [16896/50000]\tLoss: 2.1138\tLR: 0.015000\n",
      "Training Epoch: 6 [17024/50000]\tLoss: 2.1249\tLR: 0.015000\n",
      "Training Epoch: 6 [17152/50000]\tLoss: 1.9383\tLR: 0.015000\n",
      "Training Epoch: 6 [17280/50000]\tLoss: 2.1783\tLR: 0.015000\n",
      "Training Epoch: 6 [17408/50000]\tLoss: 2.0288\tLR: 0.015000\n",
      "Training Epoch: 6 [17536/50000]\tLoss: 2.0780\tLR: 0.015000\n",
      "Training Epoch: 6 [17664/50000]\tLoss: 2.0191\tLR: 0.015000\n",
      "Training Epoch: 6 [17792/50000]\tLoss: 2.2076\tLR: 0.015000\n",
      "Training Epoch: 6 [17920/50000]\tLoss: 1.9580\tLR: 0.015000\n",
      "Training Epoch: 6 [18048/50000]\tLoss: 2.3569\tLR: 0.015000\n",
      "Training Epoch: 6 [18176/50000]\tLoss: 2.0617\tLR: 0.015000\n",
      "Training Epoch: 6 [18304/50000]\tLoss: 1.8171\tLR: 0.015000\n",
      "Training Epoch: 6 [18432/50000]\tLoss: 2.1436\tLR: 0.015000\n",
      "Training Epoch: 6 [18560/50000]\tLoss: 2.0246\tLR: 0.015000\n",
      "Training Epoch: 6 [18688/50000]\tLoss: 1.9349\tLR: 0.015000\n",
      "Training Epoch: 6 [18816/50000]\tLoss: 2.1282\tLR: 0.015000\n",
      "Training Epoch: 6 [18944/50000]\tLoss: 1.8352\tLR: 0.015000\n",
      "Training Epoch: 6 [19072/50000]\tLoss: 2.0343\tLR: 0.015000\n",
      "Training Epoch: 6 [19200/50000]\tLoss: 1.7517\tLR: 0.015000\n",
      "Training Epoch: 6 [19328/50000]\tLoss: 2.0037\tLR: 0.015000\n",
      "Training Epoch: 6 [19456/50000]\tLoss: 2.1577\tLR: 0.015000\n",
      "Training Epoch: 6 [19584/50000]\tLoss: 2.1420\tLR: 0.015000\n",
      "Training Epoch: 6 [19712/50000]\tLoss: 1.8130\tLR: 0.015000\n",
      "Training Epoch: 6 [19840/50000]\tLoss: 1.9227\tLR: 0.015000\n",
      "Training Epoch: 6 [19968/50000]\tLoss: 2.1027\tLR: 0.015000\n",
      "Training Epoch: 6 [20096/50000]\tLoss: 2.0231\tLR: 0.015000\n",
      "Training Epoch: 6 [20224/50000]\tLoss: 1.7774\tLR: 0.015000\n",
      "Training Epoch: 6 [20352/50000]\tLoss: 2.0361\tLR: 0.015000\n",
      "Training Epoch: 6 [20480/50000]\tLoss: 1.8928\tLR: 0.015000\n",
      "Training Epoch: 6 [20608/50000]\tLoss: 2.1462\tLR: 0.015000\n",
      "Training Epoch: 6 [20736/50000]\tLoss: 2.1058\tLR: 0.015000\n",
      "Training Epoch: 6 [20864/50000]\tLoss: 1.6922\tLR: 0.015000\n",
      "Training Epoch: 6 [20992/50000]\tLoss: 2.0121\tLR: 0.015000\n",
      "Training Epoch: 6 [21120/50000]\tLoss: 1.9101\tLR: 0.015000\n",
      "Training Epoch: 6 [21248/50000]\tLoss: 1.6984\tLR: 0.015000\n",
      "Training Epoch: 6 [21376/50000]\tLoss: 1.8941\tLR: 0.015000\n",
      "Training Epoch: 6 [21504/50000]\tLoss: 1.9852\tLR: 0.015000\n",
      "Training Epoch: 6 [21632/50000]\tLoss: 2.0226\tLR: 0.015000\n",
      "Training Epoch: 6 [21760/50000]\tLoss: 1.9092\tLR: 0.015000\n",
      "Training Epoch: 6 [21888/50000]\tLoss: 1.8429\tLR: 0.015000\n",
      "Training Epoch: 6 [22016/50000]\tLoss: 2.0197\tLR: 0.015000\n",
      "Training Epoch: 6 [22144/50000]\tLoss: 2.0463\tLR: 0.015000\n",
      "Training Epoch: 6 [22272/50000]\tLoss: 1.9028\tLR: 0.015000\n",
      "Training Epoch: 6 [22400/50000]\tLoss: 2.0052\tLR: 0.015000\n",
      "Training Epoch: 6 [22528/50000]\tLoss: 2.0202\tLR: 0.015000\n",
      "Training Epoch: 6 [22656/50000]\tLoss: 1.7566\tLR: 0.015000\n",
      "Training Epoch: 6 [22784/50000]\tLoss: 1.8965\tLR: 0.015000\n",
      "Training Epoch: 6 [22912/50000]\tLoss: 1.8739\tLR: 0.015000\n",
      "Training Epoch: 6 [23040/50000]\tLoss: 1.7639\tLR: 0.015000\n",
      "Training Epoch: 6 [23168/50000]\tLoss: 1.8970\tLR: 0.015000\n",
      "Training Epoch: 6 [23296/50000]\tLoss: 1.8400\tLR: 0.015000\n",
      "Training Epoch: 6 [23424/50000]\tLoss: 1.8564\tLR: 0.015000\n",
      "Training Epoch: 6 [23552/50000]\tLoss: 1.9438\tLR: 0.015000\n",
      "Training Epoch: 6 [23680/50000]\tLoss: 1.8890\tLR: 0.015000\n",
      "Training Epoch: 6 [23808/50000]\tLoss: 1.8370\tLR: 0.015000\n",
      "Training Epoch: 6 [23936/50000]\tLoss: 1.9800\tLR: 0.015000\n",
      "Training Epoch: 6 [24064/50000]\tLoss: 1.9388\tLR: 0.015000\n",
      "Training Epoch: 6 [24192/50000]\tLoss: 1.8747\tLR: 0.015000\n",
      "Training Epoch: 6 [24320/50000]\tLoss: 1.9771\tLR: 0.015000\n",
      "Training Epoch: 6 [24448/50000]\tLoss: 1.8654\tLR: 0.015000\n",
      "Training Epoch: 6 [24576/50000]\tLoss: 1.8370\tLR: 0.015000\n",
      "Training Epoch: 6 [24704/50000]\tLoss: 1.9767\tLR: 0.015000\n",
      "Training Epoch: 6 [24832/50000]\tLoss: 2.1459\tLR: 0.015000\n",
      "Training Epoch: 6 [24960/50000]\tLoss: 1.8350\tLR: 0.015000\n",
      "Training Epoch: 6 [25088/50000]\tLoss: 2.1639\tLR: 0.015000\n",
      "Training Epoch: 6 [25216/50000]\tLoss: 1.8307\tLR: 0.015000\n",
      "Training Epoch: 6 [25344/50000]\tLoss: 2.0684\tLR: 0.015000\n",
      "Training Epoch: 6 [25472/50000]\tLoss: 1.8993\tLR: 0.015000\n",
      "Training Epoch: 6 [25600/50000]\tLoss: 1.9036\tLR: 0.015000\n",
      "Training Epoch: 6 [25728/50000]\tLoss: 1.9703\tLR: 0.015000\n",
      "Training Epoch: 6 [25856/50000]\tLoss: 1.8330\tLR: 0.015000\n",
      "Training Epoch: 6 [25984/50000]\tLoss: 1.7925\tLR: 0.015000\n",
      "Training Epoch: 6 [26112/50000]\tLoss: 1.8684\tLR: 0.015000\n",
      "Training Epoch: 6 [26240/50000]\tLoss: 1.8758\tLR: 0.015000\n",
      "Training Epoch: 6 [26368/50000]\tLoss: 2.1374\tLR: 0.015000\n",
      "Training Epoch: 6 [26496/50000]\tLoss: 2.3832\tLR: 0.015000\n",
      "Training Epoch: 6 [26624/50000]\tLoss: 1.8871\tLR: 0.015000\n",
      "Training Epoch: 6 [26752/50000]\tLoss: 1.9162\tLR: 0.015000\n",
      "Training Epoch: 6 [26880/50000]\tLoss: 1.9777\tLR: 0.015000\n",
      "Training Epoch: 6 [27008/50000]\tLoss: 1.7853\tLR: 0.015000\n",
      "Training Epoch: 6 [27136/50000]\tLoss: 1.8706\tLR: 0.015000\n",
      "Training Epoch: 6 [27264/50000]\tLoss: 2.2090\tLR: 0.015000\n",
      "Training Epoch: 6 [27392/50000]\tLoss: 2.1908\tLR: 0.015000\n",
      "Training Epoch: 6 [27520/50000]\tLoss: 1.8680\tLR: 0.015000\n",
      "Training Epoch: 6 [27648/50000]\tLoss: 1.9250\tLR: 0.015000\n",
      "Training Epoch: 6 [27776/50000]\tLoss: 1.8842\tLR: 0.015000\n",
      "Training Epoch: 6 [27904/50000]\tLoss: 1.9540\tLR: 0.015000\n",
      "Training Epoch: 6 [28032/50000]\tLoss: 1.8328\tLR: 0.015000\n",
      "Training Epoch: 6 [28160/50000]\tLoss: 1.9201\tLR: 0.015000\n",
      "Training Epoch: 6 [28288/50000]\tLoss: 1.7485\tLR: 0.015000\n",
      "Training Epoch: 6 [28416/50000]\tLoss: 1.8474\tLR: 0.015000\n",
      "Training Epoch: 6 [28544/50000]\tLoss: 1.8792\tLR: 0.015000\n",
      "Training Epoch: 6 [28672/50000]\tLoss: 2.0153\tLR: 0.015000\n",
      "Training Epoch: 6 [28800/50000]\tLoss: 1.7398\tLR: 0.015000\n",
      "Training Epoch: 6 [28928/50000]\tLoss: 2.0184\tLR: 0.015000\n",
      "Training Epoch: 6 [29056/50000]\tLoss: 1.8839\tLR: 0.015000\n",
      "Training Epoch: 6 [29184/50000]\tLoss: 2.0342\tLR: 0.015000\n",
      "Training Epoch: 6 [29312/50000]\tLoss: 2.0719\tLR: 0.015000\n",
      "Training Epoch: 6 [29440/50000]\tLoss: 1.9988\tLR: 0.015000\n",
      "Training Epoch: 6 [29568/50000]\tLoss: 1.9319\tLR: 0.015000\n",
      "Training Epoch: 6 [29696/50000]\tLoss: 1.8254\tLR: 0.015000\n",
      "Training Epoch: 6 [29824/50000]\tLoss: 2.1203\tLR: 0.015000\n",
      "Training Epoch: 6 [29952/50000]\tLoss: 1.8743\tLR: 0.015000\n",
      "Training Epoch: 6 [30080/50000]\tLoss: 1.7653\tLR: 0.015000\n",
      "Training Epoch: 6 [30208/50000]\tLoss: 1.9031\tLR: 0.015000\n",
      "Training Epoch: 6 [30336/50000]\tLoss: 1.9969\tLR: 0.015000\n",
      "Training Epoch: 6 [30464/50000]\tLoss: 1.5578\tLR: 0.015000\n",
      "Training Epoch: 6 [30592/50000]\tLoss: 2.1107\tLR: 0.015000\n",
      "Training Epoch: 6 [30720/50000]\tLoss: 1.8065\tLR: 0.015000\n",
      "Training Epoch: 6 [30848/50000]\tLoss: 2.0128\tLR: 0.015000\n",
      "Training Epoch: 6 [30976/50000]\tLoss: 2.0741\tLR: 0.015000\n",
      "Training Epoch: 6 [31104/50000]\tLoss: 1.8424\tLR: 0.015000\n",
      "Training Epoch: 6 [31232/50000]\tLoss: 1.7646\tLR: 0.015000\n",
      "Training Epoch: 6 [31360/50000]\tLoss: 1.9945\tLR: 0.015000\n",
      "Training Epoch: 6 [31488/50000]\tLoss: 2.0146\tLR: 0.015000\n",
      "Training Epoch: 6 [31616/50000]\tLoss: 1.8284\tLR: 0.015000\n",
      "Training Epoch: 6 [31744/50000]\tLoss: 1.7796\tLR: 0.015000\n",
      "Training Epoch: 6 [31872/50000]\tLoss: 2.0659\tLR: 0.015000\n",
      "Training Epoch: 6 [32000/50000]\tLoss: 1.8212\tLR: 0.015000\n",
      "Training Epoch: 6 [32128/50000]\tLoss: 2.1107\tLR: 0.015000\n",
      "Training Epoch: 6 [32256/50000]\tLoss: 1.8903\tLR: 0.015000\n",
      "Training Epoch: 6 [32384/50000]\tLoss: 1.7416\tLR: 0.015000\n",
      "Training Epoch: 6 [32512/50000]\tLoss: 1.8977\tLR: 0.015000\n",
      "Training Epoch: 6 [32640/50000]\tLoss: 1.9170\tLR: 0.015000\n",
      "Training Epoch: 6 [32768/50000]\tLoss: 1.7835\tLR: 0.015000\n",
      "Training Epoch: 6 [32896/50000]\tLoss: 1.7582\tLR: 0.015000\n",
      "Training Epoch: 6 [33024/50000]\tLoss: 1.8904\tLR: 0.015000\n",
      "Training Epoch: 6 [33152/50000]\tLoss: 2.0069\tLR: 0.015000\n",
      "Training Epoch: 6 [33280/50000]\tLoss: 1.8544\tLR: 0.015000\n",
      "Training Epoch: 6 [33408/50000]\tLoss: 2.3383\tLR: 0.015000\n",
      "Training Epoch: 6 [33536/50000]\tLoss: 1.9677\tLR: 0.015000\n",
      "Training Epoch: 6 [33664/50000]\tLoss: 2.0657\tLR: 0.015000\n",
      "Training Epoch: 6 [33792/50000]\tLoss: 2.1809\tLR: 0.015000\n",
      "Training Epoch: 6 [33920/50000]\tLoss: 1.9252\tLR: 0.015000\n",
      "Training Epoch: 6 [34048/50000]\tLoss: 1.8529\tLR: 0.015000\n",
      "Training Epoch: 6 [34176/50000]\tLoss: 1.7803\tLR: 0.015000\n",
      "Training Epoch: 6 [34304/50000]\tLoss: 1.8424\tLR: 0.015000\n",
      "Training Epoch: 6 [34432/50000]\tLoss: 1.6754\tLR: 0.015000\n",
      "Training Epoch: 6 [34560/50000]\tLoss: 1.7371\tLR: 0.015000\n",
      "Training Epoch: 6 [34688/50000]\tLoss: 1.8692\tLR: 0.015000\n",
      "Training Epoch: 6 [34816/50000]\tLoss: 1.7779\tLR: 0.015000\n",
      "Training Epoch: 6 [34944/50000]\tLoss: 1.9620\tLR: 0.015000\n",
      "Training Epoch: 6 [35072/50000]\tLoss: 2.1982\tLR: 0.015000\n",
      "Training Epoch: 6 [35200/50000]\tLoss: 1.9576\tLR: 0.015000\n",
      "Training Epoch: 6 [35328/50000]\tLoss: 1.9982\tLR: 0.015000\n",
      "Training Epoch: 6 [35456/50000]\tLoss: 2.0298\tLR: 0.015000\n",
      "Training Epoch: 6 [35584/50000]\tLoss: 1.8801\tLR: 0.015000\n",
      "Training Epoch: 6 [35712/50000]\tLoss: 1.7928\tLR: 0.015000\n",
      "Training Epoch: 6 [35840/50000]\tLoss: 1.7542\tLR: 0.015000\n",
      "Training Epoch: 6 [35968/50000]\tLoss: 1.9119\tLR: 0.015000\n",
      "Training Epoch: 6 [36096/50000]\tLoss: 1.7484\tLR: 0.015000\n",
      "Training Epoch: 6 [36224/50000]\tLoss: 2.0241\tLR: 0.015000\n",
      "Training Epoch: 6 [36352/50000]\tLoss: 2.0776\tLR: 0.015000\n",
      "Training Epoch: 6 [36480/50000]\tLoss: 2.0110\tLR: 0.015000\n",
      "Training Epoch: 6 [36608/50000]\tLoss: 1.9579\tLR: 0.015000\n",
      "Training Epoch: 6 [36736/50000]\tLoss: 1.8127\tLR: 0.015000\n",
      "Training Epoch: 6 [36864/50000]\tLoss: 1.9536\tLR: 0.015000\n",
      "Training Epoch: 6 [36992/50000]\tLoss: 1.8776\tLR: 0.015000\n",
      "Training Epoch: 6 [37120/50000]\tLoss: 2.0187\tLR: 0.015000\n",
      "Training Epoch: 6 [37248/50000]\tLoss: 1.9726\tLR: 0.015000\n",
      "Training Epoch: 6 [37376/50000]\tLoss: 1.7253\tLR: 0.015000\n",
      "Training Epoch: 6 [37504/50000]\tLoss: 2.2016\tLR: 0.015000\n",
      "Training Epoch: 6 [37632/50000]\tLoss: 1.9981\tLR: 0.015000\n",
      "Training Epoch: 6 [37760/50000]\tLoss: 1.6740\tLR: 0.015000\n",
      "Training Epoch: 6 [37888/50000]\tLoss: 1.8955\tLR: 0.015000\n",
      "Training Epoch: 6 [38016/50000]\tLoss: 1.8918\tLR: 0.015000\n",
      "Training Epoch: 6 [38144/50000]\tLoss: 1.9939\tLR: 0.015000\n",
      "Training Epoch: 6 [38272/50000]\tLoss: 1.9190\tLR: 0.015000\n",
      "Training Epoch: 6 [38400/50000]\tLoss: 2.0220\tLR: 0.015000\n",
      "Training Epoch: 6 [38528/50000]\tLoss: 2.0545\tLR: 0.015000\n",
      "Training Epoch: 6 [38656/50000]\tLoss: 1.8791\tLR: 0.015000\n",
      "Training Epoch: 6 [38784/50000]\tLoss: 1.7849\tLR: 0.015000\n",
      "Training Epoch: 6 [38912/50000]\tLoss: 2.0573\tLR: 0.015000\n",
      "Training Epoch: 6 [39040/50000]\tLoss: 1.8421\tLR: 0.015000\n",
      "Training Epoch: 6 [39168/50000]\tLoss: 1.8069\tLR: 0.015000\n",
      "Training Epoch: 6 [39296/50000]\tLoss: 2.0541\tLR: 0.015000\n",
      "Training Epoch: 6 [39424/50000]\tLoss: 1.7627\tLR: 0.015000\n",
      "Training Epoch: 6 [39552/50000]\tLoss: 2.0424\tLR: 0.015000\n",
      "Training Epoch: 6 [39680/50000]\tLoss: 1.9354\tLR: 0.015000\n",
      "Training Epoch: 6 [39808/50000]\tLoss: 1.9893\tLR: 0.015000\n",
      "Training Epoch: 6 [39936/50000]\tLoss: 2.1776\tLR: 0.015000\n",
      "Training Epoch: 6 [40064/50000]\tLoss: 1.9266\tLR: 0.015000\n",
      "Training Epoch: 6 [40192/50000]\tLoss: 1.7325\tLR: 0.015000\n",
      "Training Epoch: 6 [40320/50000]\tLoss: 1.7382\tLR: 0.015000\n",
      "Training Epoch: 6 [40448/50000]\tLoss: 2.0618\tLR: 0.015000\n",
      "Training Epoch: 6 [40576/50000]\tLoss: 1.9899\tLR: 0.015000\n",
      "Training Epoch: 6 [40704/50000]\tLoss: 2.3188\tLR: 0.015000\n",
      "Training Epoch: 6 [40832/50000]\tLoss: 2.0237\tLR: 0.015000\n",
      "Training Epoch: 6 [40960/50000]\tLoss: 1.9957\tLR: 0.015000\n",
      "Training Epoch: 6 [41088/50000]\tLoss: 2.0821\tLR: 0.015000\n",
      "Training Epoch: 6 [41216/50000]\tLoss: 1.9402\tLR: 0.015000\n",
      "Training Epoch: 6 [41344/50000]\tLoss: 1.7169\tLR: 0.015000\n",
      "Training Epoch: 6 [41472/50000]\tLoss: 1.8590\tLR: 0.015000\n",
      "Training Epoch: 6 [41600/50000]\tLoss: 1.7738\tLR: 0.015000\n",
      "Training Epoch: 6 [41728/50000]\tLoss: 1.9237\tLR: 0.015000\n",
      "Training Epoch: 6 [41856/50000]\tLoss: 1.8234\tLR: 0.015000\n",
      "Training Epoch: 6 [41984/50000]\tLoss: 1.6707\tLR: 0.015000\n",
      "Training Epoch: 6 [42112/50000]\tLoss: 1.7458\tLR: 0.015000\n",
      "Training Epoch: 6 [42240/50000]\tLoss: 1.9740\tLR: 0.015000\n",
      "Training Epoch: 6 [42368/50000]\tLoss: 1.9087\tLR: 0.015000\n",
      "Training Epoch: 6 [42496/50000]\tLoss: 1.8233\tLR: 0.015000\n",
      "Training Epoch: 6 [42624/50000]\tLoss: 2.0089\tLR: 0.015000\n",
      "Training Epoch: 6 [42752/50000]\tLoss: 2.0759\tLR: 0.015000\n",
      "Training Epoch: 6 [42880/50000]\tLoss: 1.9597\tLR: 0.015000\n",
      "Training Epoch: 6 [43008/50000]\tLoss: 1.6231\tLR: 0.015000\n",
      "Training Epoch: 6 [43136/50000]\tLoss: 1.8142\tLR: 0.015000\n",
      "Training Epoch: 6 [43264/50000]\tLoss: 2.0516\tLR: 0.015000\n",
      "Training Epoch: 6 [43392/50000]\tLoss: 1.8958\tLR: 0.015000\n",
      "Training Epoch: 6 [43520/50000]\tLoss: 1.7947\tLR: 0.015000\n",
      "Training Epoch: 6 [43648/50000]\tLoss: 1.8290\tLR: 0.015000\n",
      "Training Epoch: 6 [43776/50000]\tLoss: 1.8542\tLR: 0.015000\n",
      "Training Epoch: 6 [43904/50000]\tLoss: 1.8022\tLR: 0.015000\n",
      "Training Epoch: 6 [44032/50000]\tLoss: 1.6975\tLR: 0.015000\n",
      "Training Epoch: 6 [44160/50000]\tLoss: 1.8052\tLR: 0.015000\n",
      "Training Epoch: 6 [44288/50000]\tLoss: 2.1422\tLR: 0.015000\n",
      "Training Epoch: 6 [44416/50000]\tLoss: 1.8956\tLR: 0.015000\n",
      "Training Epoch: 6 [44544/50000]\tLoss: 1.9057\tLR: 0.015000\n",
      "Training Epoch: 6 [44672/50000]\tLoss: 1.8015\tLR: 0.015000\n",
      "Training Epoch: 6 [44800/50000]\tLoss: 1.7364\tLR: 0.015000\n",
      "Training Epoch: 6 [44928/50000]\tLoss: 1.6889\tLR: 0.015000\n",
      "Training Epoch: 6 [45056/50000]\tLoss: 1.8305\tLR: 0.015000\n",
      "Training Epoch: 6 [45184/50000]\tLoss: 1.7673\tLR: 0.015000\n",
      "Training Epoch: 6 [45312/50000]\tLoss: 1.9341\tLR: 0.015000\n",
      "Training Epoch: 6 [45440/50000]\tLoss: 2.0030\tLR: 0.015000\n",
      "Training Epoch: 6 [45568/50000]\tLoss: 1.8246\tLR: 0.015000\n",
      "Training Epoch: 6 [45696/50000]\tLoss: 1.6469\tLR: 0.015000\n",
      "Training Epoch: 6 [45824/50000]\tLoss: 1.9299\tLR: 0.015000\n",
      "Training Epoch: 6 [45952/50000]\tLoss: 2.1378\tLR: 0.015000\n",
      "Training Epoch: 6 [46080/50000]\tLoss: 1.6945\tLR: 0.015000\n",
      "Training Epoch: 6 [46208/50000]\tLoss: 1.8074\tLR: 0.015000\n",
      "Training Epoch: 6 [46336/50000]\tLoss: 1.9762\tLR: 0.015000\n",
      "Training Epoch: 6 [46464/50000]\tLoss: 1.7852\tLR: 0.015000\n",
      "Training Epoch: 6 [46592/50000]\tLoss: 1.8624\tLR: 0.015000\n",
      "Training Epoch: 6 [46720/50000]\tLoss: 1.9044\tLR: 0.015000\n",
      "Training Epoch: 6 [46848/50000]\tLoss: 2.1793\tLR: 0.015000\n",
      "Training Epoch: 6 [46976/50000]\tLoss: 1.7415\tLR: 0.015000\n",
      "Training Epoch: 6 [47104/50000]\tLoss: 1.8035\tLR: 0.015000\n",
      "Training Epoch: 6 [47232/50000]\tLoss: 1.8479\tLR: 0.015000\n",
      "Training Epoch: 6 [47360/50000]\tLoss: 1.7961\tLR: 0.015000\n",
      "Training Epoch: 6 [47488/50000]\tLoss: 1.7655\tLR: 0.015000\n",
      "Training Epoch: 6 [47616/50000]\tLoss: 1.8716\tLR: 0.015000\n",
      "Training Epoch: 6 [47744/50000]\tLoss: 1.9197\tLR: 0.015000\n",
      "Training Epoch: 6 [47872/50000]\tLoss: 1.8137\tLR: 0.015000\n",
      "Training Epoch: 6 [48000/50000]\tLoss: 2.0466\tLR: 0.015000\n",
      "Training Epoch: 6 [48128/50000]\tLoss: 1.9087\tLR: 0.015000\n",
      "Training Epoch: 6 [48256/50000]\tLoss: 1.8074\tLR: 0.015000\n",
      "Training Epoch: 6 [48384/50000]\tLoss: 1.8728\tLR: 0.015000\n",
      "Training Epoch: 6 [48512/50000]\tLoss: 2.0098\tLR: 0.015000\n",
      "Training Epoch: 6 [48640/50000]\tLoss: 1.7243\tLR: 0.015000\n",
      "Training Epoch: 6 [48768/50000]\tLoss: 1.6452\tLR: 0.015000\n",
      "Training Epoch: 6 [48896/50000]\tLoss: 1.9725\tLR: 0.015000\n",
      "Training Epoch: 6 [49024/50000]\tLoss: 1.8467\tLR: 0.015000\n",
      "Training Epoch: 6 [49152/50000]\tLoss: 1.8527\tLR: 0.015000\n",
      "Training Epoch: 6 [49280/50000]\tLoss: 1.9104\tLR: 0.015000\n",
      "Training Epoch: 6 [49408/50000]\tLoss: 1.9547\tLR: 0.015000\n",
      "Training Epoch: 6 [49536/50000]\tLoss: 1.9503\tLR: 0.015000\n",
      "Training Epoch: 6 [49664/50000]\tLoss: 1.7719\tLR: 0.015000\n",
      "Training Epoch: 6 [49792/50000]\tLoss: 2.2138\tLR: 0.015000\n",
      "Training Epoch: 6 [49920/50000]\tLoss: 1.9649\tLR: 0.015000\n",
      "Training Epoch: 6 [50000/50000]\tLoss: 2.0320\tLR: 0.015000\n",
      "Test set: Average loss: 0.0144, Accuracy: 0.4978\n",
      "\n",
      "Training Epoch: 7 [128/50000]\tLoss: 1.8718\tLR: 0.015000\n",
      "Training Epoch: 7 [256/50000]\tLoss: 2.1156\tLR: 0.015000\n",
      "Training Epoch: 7 [384/50000]\tLoss: 1.7750\tLR: 0.015000\n",
      "Training Epoch: 7 [512/50000]\tLoss: 1.7460\tLR: 0.015000\n",
      "Training Epoch: 7 [640/50000]\tLoss: 1.7160\tLR: 0.015000\n",
      "Training Epoch: 7 [768/50000]\tLoss: 1.8088\tLR: 0.015000\n",
      "Training Epoch: 7 [896/50000]\tLoss: 1.9982\tLR: 0.015000\n",
      "Training Epoch: 7 [1024/50000]\tLoss: 1.8866\tLR: 0.015000\n",
      "Training Epoch: 7 [1152/50000]\tLoss: 1.6368\tLR: 0.015000\n",
      "Training Epoch: 7 [1280/50000]\tLoss: 2.1426\tLR: 0.015000\n",
      "Training Epoch: 7 [1408/50000]\tLoss: 1.7838\tLR: 0.015000\n",
      "Training Epoch: 7 [1536/50000]\tLoss: 1.8378\tLR: 0.015000\n",
      "Training Epoch: 7 [1664/50000]\tLoss: 1.6976\tLR: 0.015000\n",
      "Training Epoch: 7 [1792/50000]\tLoss: 1.8383\tLR: 0.015000\n",
      "Training Epoch: 7 [1920/50000]\tLoss: 1.7600\tLR: 0.015000\n",
      "Training Epoch: 7 [2048/50000]\tLoss: 1.5913\tLR: 0.015000\n",
      "Training Epoch: 7 [2176/50000]\tLoss: 1.7917\tLR: 0.015000\n",
      "Training Epoch: 7 [2304/50000]\tLoss: 1.5462\tLR: 0.015000\n",
      "Training Epoch: 7 [2432/50000]\tLoss: 1.7315\tLR: 0.015000\n",
      "Training Epoch: 7 [2560/50000]\tLoss: 1.8770\tLR: 0.015000\n",
      "Training Epoch: 7 [2688/50000]\tLoss: 1.9571\tLR: 0.015000\n",
      "Training Epoch: 7 [2816/50000]\tLoss: 1.8487\tLR: 0.015000\n",
      "Training Epoch: 7 [2944/50000]\tLoss: 1.9036\tLR: 0.015000\n",
      "Training Epoch: 7 [3072/50000]\tLoss: 1.9403\tLR: 0.015000\n",
      "Training Epoch: 7 [3200/50000]\tLoss: 1.7848\tLR: 0.015000\n",
      "Training Epoch: 7 [3328/50000]\tLoss: 1.6785\tLR: 0.015000\n",
      "Training Epoch: 7 [3456/50000]\tLoss: 1.9432\tLR: 0.015000\n",
      "Training Epoch: 7 [3584/50000]\tLoss: 1.7601\tLR: 0.015000\n",
      "Training Epoch: 7 [3712/50000]\tLoss: 1.8682\tLR: 0.015000\n",
      "Training Epoch: 7 [3840/50000]\tLoss: 1.9415\tLR: 0.015000\n",
      "Training Epoch: 7 [3968/50000]\tLoss: 1.8660\tLR: 0.015000\n",
      "Training Epoch: 7 [4096/50000]\tLoss: 1.9165\tLR: 0.015000\n",
      "Training Epoch: 7 [4224/50000]\tLoss: 1.7213\tLR: 0.015000\n",
      "Training Epoch: 7 [4352/50000]\tLoss: 2.1016\tLR: 0.015000\n",
      "Training Epoch: 7 [4480/50000]\tLoss: 1.7456\tLR: 0.015000\n",
      "Training Epoch: 7 [4608/50000]\tLoss: 1.9439\tLR: 0.015000\n",
      "Training Epoch: 7 [4736/50000]\tLoss: 1.7571\tLR: 0.015000\n",
      "Training Epoch: 7 [4864/50000]\tLoss: 1.6873\tLR: 0.015000\n",
      "Training Epoch: 7 [4992/50000]\tLoss: 1.7600\tLR: 0.015000\n",
      "Training Epoch: 7 [5120/50000]\tLoss: 1.7422\tLR: 0.015000\n",
      "Training Epoch: 7 [5248/50000]\tLoss: 1.8260\tLR: 0.015000\n",
      "Training Epoch: 7 [5376/50000]\tLoss: 1.8194\tLR: 0.015000\n",
      "Training Epoch: 7 [5504/50000]\tLoss: 1.6185\tLR: 0.015000\n",
      "Training Epoch: 7 [5632/50000]\tLoss: 1.8429\tLR: 0.015000\n",
      "Training Epoch: 7 [5760/50000]\tLoss: 2.0859\tLR: 0.015000\n",
      "Training Epoch: 7 [5888/50000]\tLoss: 2.0198\tLR: 0.015000\n",
      "Training Epoch: 7 [6016/50000]\tLoss: 1.8689\tLR: 0.015000\n",
      "Training Epoch: 7 [6144/50000]\tLoss: 1.7355\tLR: 0.015000\n",
      "Training Epoch: 7 [6272/50000]\tLoss: 1.7264\tLR: 0.015000\n",
      "Training Epoch: 7 [6400/50000]\tLoss: 1.7364\tLR: 0.015000\n",
      "Training Epoch: 7 [6528/50000]\tLoss: 1.9515\tLR: 0.015000\n",
      "Training Epoch: 7 [6656/50000]\tLoss: 1.4673\tLR: 0.015000\n",
      "Training Epoch: 7 [6784/50000]\tLoss: 1.7595\tLR: 0.015000\n",
      "Training Epoch: 7 [6912/50000]\tLoss: 1.8356\tLR: 0.015000\n",
      "Training Epoch: 7 [7040/50000]\tLoss: 1.8310\tLR: 0.015000\n",
      "Training Epoch: 7 [7168/50000]\tLoss: 1.8585\tLR: 0.015000\n",
      "Training Epoch: 7 [7296/50000]\tLoss: 1.7760\tLR: 0.015000\n",
      "Training Epoch: 7 [7424/50000]\tLoss: 1.9023\tLR: 0.015000\n",
      "Training Epoch: 7 [7552/50000]\tLoss: 1.8992\tLR: 0.015000\n",
      "Training Epoch: 7 [7680/50000]\tLoss: 1.8146\tLR: 0.015000\n",
      "Training Epoch: 7 [7808/50000]\tLoss: 1.6718\tLR: 0.015000\n",
      "Training Epoch: 7 [7936/50000]\tLoss: 1.9376\tLR: 0.015000\n",
      "Training Epoch: 7 [8064/50000]\tLoss: 1.7596\tLR: 0.015000\n",
      "Training Epoch: 7 [8192/50000]\tLoss: 1.9497\tLR: 0.015000\n",
      "Training Epoch: 7 [8320/50000]\tLoss: 1.8486\tLR: 0.015000\n",
      "Training Epoch: 7 [8448/50000]\tLoss: 1.8270\tLR: 0.015000\n",
      "Training Epoch: 7 [8576/50000]\tLoss: 1.7371\tLR: 0.015000\n",
      "Training Epoch: 7 [8704/50000]\tLoss: 1.5610\tLR: 0.015000\n",
      "Training Epoch: 7 [8832/50000]\tLoss: 2.1432\tLR: 0.015000\n",
      "Training Epoch: 7 [8960/50000]\tLoss: 1.7830\tLR: 0.015000\n",
      "Training Epoch: 7 [9088/50000]\tLoss: 1.8554\tLR: 0.015000\n",
      "Training Epoch: 7 [9216/50000]\tLoss: 1.8518\tLR: 0.015000\n",
      "Training Epoch: 7 [9344/50000]\tLoss: 1.7432\tLR: 0.015000\n",
      "Training Epoch: 7 [9472/50000]\tLoss: 2.1383\tLR: 0.015000\n",
      "Training Epoch: 7 [9600/50000]\tLoss: 1.6720\tLR: 0.015000\n",
      "Training Epoch: 7 [9728/50000]\tLoss: 1.8906\tLR: 0.015000\n",
      "Training Epoch: 7 [9856/50000]\tLoss: 1.7537\tLR: 0.015000\n",
      "Training Epoch: 7 [9984/50000]\tLoss: 1.7923\tLR: 0.015000\n",
      "Training Epoch: 7 [10112/50000]\tLoss: 2.0046\tLR: 0.015000\n",
      "Training Epoch: 7 [10240/50000]\tLoss: 1.7626\tLR: 0.015000\n",
      "Training Epoch: 7 [10368/50000]\tLoss: 1.5232\tLR: 0.015000\n",
      "Training Epoch: 7 [10496/50000]\tLoss: 1.6710\tLR: 0.015000\n",
      "Training Epoch: 7 [10624/50000]\tLoss: 1.9595\tLR: 0.015000\n",
      "Training Epoch: 7 [10752/50000]\tLoss: 1.4947\tLR: 0.015000\n",
      "Training Epoch: 7 [10880/50000]\tLoss: 1.8972\tLR: 0.015000\n",
      "Training Epoch: 7 [11008/50000]\tLoss: 1.6618\tLR: 0.015000\n",
      "Training Epoch: 7 [11136/50000]\tLoss: 1.7068\tLR: 0.015000\n",
      "Training Epoch: 7 [11264/50000]\tLoss: 1.9064\tLR: 0.015000\n",
      "Training Epoch: 7 [11392/50000]\tLoss: 1.6667\tLR: 0.015000\n",
      "Training Epoch: 7 [11520/50000]\tLoss: 2.1210\tLR: 0.015000\n",
      "Training Epoch: 7 [11648/50000]\tLoss: 1.9345\tLR: 0.015000\n",
      "Training Epoch: 7 [11776/50000]\tLoss: 1.8407\tLR: 0.015000\n",
      "Training Epoch: 7 [11904/50000]\tLoss: 1.7953\tLR: 0.015000\n",
      "Training Epoch: 7 [12032/50000]\tLoss: 1.7795\tLR: 0.015000\n",
      "Training Epoch: 7 [12160/50000]\tLoss: 1.9001\tLR: 0.015000\n",
      "Training Epoch: 7 [12288/50000]\tLoss: 1.6573\tLR: 0.015000\n",
      "Training Epoch: 7 [12416/50000]\tLoss: 1.7762\tLR: 0.015000\n",
      "Training Epoch: 7 [12544/50000]\tLoss: 1.8160\tLR: 0.015000\n",
      "Training Epoch: 7 [12672/50000]\tLoss: 1.9943\tLR: 0.015000\n",
      "Training Epoch: 7 [12800/50000]\tLoss: 1.8631\tLR: 0.015000\n",
      "Training Epoch: 7 [12928/50000]\tLoss: 1.7533\tLR: 0.015000\n",
      "Training Epoch: 7 [13056/50000]\tLoss: 1.7064\tLR: 0.015000\n",
      "Training Epoch: 7 [13184/50000]\tLoss: 1.9703\tLR: 0.015000\n",
      "Training Epoch: 7 [13312/50000]\tLoss: 1.6784\tLR: 0.015000\n",
      "Training Epoch: 7 [13440/50000]\tLoss: 1.7888\tLR: 0.015000\n",
      "Training Epoch: 7 [13568/50000]\tLoss: 1.6644\tLR: 0.015000\n",
      "Training Epoch: 7 [13696/50000]\tLoss: 1.7036\tLR: 0.015000\n",
      "Training Epoch: 7 [13824/50000]\tLoss: 1.8345\tLR: 0.015000\n",
      "Training Epoch: 7 [13952/50000]\tLoss: 1.6671\tLR: 0.015000\n",
      "Training Epoch: 7 [14080/50000]\tLoss: 2.1084\tLR: 0.015000\n",
      "Training Epoch: 7 [14208/50000]\tLoss: 1.6731\tLR: 0.015000\n",
      "Training Epoch: 7 [14336/50000]\tLoss: 1.6756\tLR: 0.015000\n",
      "Training Epoch: 7 [14464/50000]\tLoss: 1.8209\tLR: 0.015000\n",
      "Training Epoch: 7 [14592/50000]\tLoss: 1.9648\tLR: 0.015000\n",
      "Training Epoch: 7 [14720/50000]\tLoss: 2.0336\tLR: 0.015000\n",
      "Training Epoch: 7 [14848/50000]\tLoss: 1.8734\tLR: 0.015000\n",
      "Training Epoch: 7 [14976/50000]\tLoss: 1.6843\tLR: 0.015000\n",
      "Training Epoch: 7 [15104/50000]\tLoss: 1.5917\tLR: 0.015000\n",
      "Training Epoch: 7 [15232/50000]\tLoss: 1.8449\tLR: 0.015000\n",
      "Training Epoch: 7 [15360/50000]\tLoss: 1.6988\tLR: 0.015000\n",
      "Training Epoch: 7 [15488/50000]\tLoss: 1.8906\tLR: 0.015000\n",
      "Training Epoch: 7 [15616/50000]\tLoss: 1.7207\tLR: 0.015000\n",
      "Training Epoch: 7 [15744/50000]\tLoss: 1.8902\tLR: 0.015000\n",
      "Training Epoch: 7 [15872/50000]\tLoss: 1.8373\tLR: 0.015000\n",
      "Training Epoch: 7 [16000/50000]\tLoss: 1.6300\tLR: 0.015000\n",
      "Training Epoch: 7 [16128/50000]\tLoss: 1.6187\tLR: 0.015000\n",
      "Training Epoch: 7 [16256/50000]\tLoss: 2.0165\tLR: 0.015000\n",
      "Training Epoch: 7 [16384/50000]\tLoss: 1.7988\tLR: 0.015000\n",
      "Training Epoch: 7 [16512/50000]\tLoss: 1.6024\tLR: 0.015000\n",
      "Training Epoch: 7 [16640/50000]\tLoss: 1.9730\tLR: 0.015000\n",
      "Training Epoch: 7 [16768/50000]\tLoss: 1.8690\tLR: 0.015000\n",
      "Training Epoch: 7 [16896/50000]\tLoss: 2.1018\tLR: 0.015000\n",
      "Training Epoch: 7 [17024/50000]\tLoss: 1.7743\tLR: 0.015000\n",
      "Training Epoch: 7 [17152/50000]\tLoss: 1.9607\tLR: 0.015000\n",
      "Training Epoch: 7 [17280/50000]\tLoss: 1.6359\tLR: 0.015000\n",
      "Training Epoch: 7 [17408/50000]\tLoss: 1.9824\tLR: 0.015000\n",
      "Training Epoch: 7 [17536/50000]\tLoss: 1.7018\tLR: 0.015000\n",
      "Training Epoch: 7 [17664/50000]\tLoss: 2.1711\tLR: 0.015000\n",
      "Training Epoch: 7 [17792/50000]\tLoss: 1.7210\tLR: 0.015000\n",
      "Training Epoch: 7 [17920/50000]\tLoss: 1.6816\tLR: 0.015000\n",
      "Training Epoch: 7 [18048/50000]\tLoss: 1.9450\tLR: 0.015000\n",
      "Training Epoch: 7 [18176/50000]\tLoss: 1.8230\tLR: 0.015000\n",
      "Training Epoch: 7 [18304/50000]\tLoss: 1.9124\tLR: 0.015000\n",
      "Training Epoch: 7 [18432/50000]\tLoss: 1.6489\tLR: 0.015000\n",
      "Training Epoch: 7 [18560/50000]\tLoss: 1.6340\tLR: 0.015000\n",
      "Training Epoch: 7 [18688/50000]\tLoss: 2.2085\tLR: 0.015000\n",
      "Training Epoch: 7 [18816/50000]\tLoss: 1.7334\tLR: 0.015000\n",
      "Training Epoch: 7 [18944/50000]\tLoss: 2.1058\tLR: 0.015000\n",
      "Training Epoch: 7 [19072/50000]\tLoss: 1.7543\tLR: 0.015000\n",
      "Training Epoch: 7 [19200/50000]\tLoss: 1.9870\tLR: 0.015000\n",
      "Training Epoch: 7 [19328/50000]\tLoss: 1.7384\tLR: 0.015000\n",
      "Training Epoch: 7 [19456/50000]\tLoss: 1.8227\tLR: 0.015000\n",
      "Training Epoch: 7 [19584/50000]\tLoss: 1.7873\tLR: 0.015000\n",
      "Training Epoch: 7 [19712/50000]\tLoss: 1.8060\tLR: 0.015000\n",
      "Training Epoch: 7 [19840/50000]\tLoss: 1.9565\tLR: 0.015000\n",
      "Training Epoch: 7 [19968/50000]\tLoss: 1.8641\tLR: 0.015000\n",
      "Training Epoch: 7 [20096/50000]\tLoss: 1.7921\tLR: 0.015000\n",
      "Training Epoch: 7 [20224/50000]\tLoss: 1.7704\tLR: 0.015000\n",
      "Training Epoch: 7 [20352/50000]\tLoss: 1.6973\tLR: 0.015000\n",
      "Training Epoch: 7 [20480/50000]\tLoss: 1.8061\tLR: 0.015000\n",
      "Training Epoch: 7 [20608/50000]\tLoss: 1.8922\tLR: 0.015000\n",
      "Training Epoch: 7 [20736/50000]\tLoss: 1.8341\tLR: 0.015000\n",
      "Training Epoch: 7 [20864/50000]\tLoss: 1.7488\tLR: 0.015000\n",
      "Training Epoch: 7 [20992/50000]\tLoss: 1.8106\tLR: 0.015000\n",
      "Training Epoch: 7 [21120/50000]\tLoss: 1.5381\tLR: 0.015000\n",
      "Training Epoch: 7 [21248/50000]\tLoss: 1.5895\tLR: 0.015000\n",
      "Training Epoch: 7 [21376/50000]\tLoss: 1.8134\tLR: 0.015000\n",
      "Training Epoch: 7 [21504/50000]\tLoss: 1.6942\tLR: 0.015000\n",
      "Training Epoch: 7 [21632/50000]\tLoss: 1.5581\tLR: 0.015000\n",
      "Training Epoch: 7 [21760/50000]\tLoss: 2.0647\tLR: 0.015000\n",
      "Training Epoch: 7 [21888/50000]\tLoss: 1.6481\tLR: 0.015000\n",
      "Training Epoch: 7 [22016/50000]\tLoss: 2.1695\tLR: 0.015000\n",
      "Training Epoch: 7 [22144/50000]\tLoss: 1.7342\tLR: 0.015000\n",
      "Training Epoch: 7 [22272/50000]\tLoss: 1.6384\tLR: 0.015000\n",
      "Training Epoch: 7 [22400/50000]\tLoss: 1.8553\tLR: 0.015000\n",
      "Training Epoch: 7 [22528/50000]\tLoss: 1.6668\tLR: 0.015000\n",
      "Training Epoch: 7 [22656/50000]\tLoss: 1.9423\tLR: 0.015000\n",
      "Training Epoch: 7 [22784/50000]\tLoss: 1.5608\tLR: 0.015000\n",
      "Training Epoch: 7 [22912/50000]\tLoss: 1.8615\tLR: 0.015000\n",
      "Training Epoch: 7 [23040/50000]\tLoss: 1.7777\tLR: 0.015000\n",
      "Training Epoch: 7 [23168/50000]\tLoss: 1.6760\tLR: 0.015000\n",
      "Training Epoch: 7 [23296/50000]\tLoss: 1.5987\tLR: 0.015000\n",
      "Training Epoch: 7 [23424/50000]\tLoss: 1.8902\tLR: 0.015000\n",
      "Training Epoch: 7 [23552/50000]\tLoss: 1.9124\tLR: 0.015000\n",
      "Training Epoch: 7 [23680/50000]\tLoss: 1.7905\tLR: 0.015000\n",
      "Training Epoch: 7 [23808/50000]\tLoss: 1.8153\tLR: 0.015000\n",
      "Training Epoch: 7 [23936/50000]\tLoss: 1.6231\tLR: 0.015000\n",
      "Training Epoch: 7 [24064/50000]\tLoss: 1.7124\tLR: 0.015000\n",
      "Training Epoch: 7 [24192/50000]\tLoss: 1.8115\tLR: 0.015000\n",
      "Training Epoch: 7 [24320/50000]\tLoss: 1.8629\tLR: 0.015000\n",
      "Training Epoch: 7 [24448/50000]\tLoss: 1.8977\tLR: 0.015000\n",
      "Training Epoch: 7 [24576/50000]\tLoss: 1.9057\tLR: 0.015000\n",
      "Training Epoch: 7 [24704/50000]\tLoss: 1.8011\tLR: 0.015000\n",
      "Training Epoch: 7 [24832/50000]\tLoss: 1.7834\tLR: 0.015000\n",
      "Training Epoch: 7 [24960/50000]\tLoss: 1.7260\tLR: 0.015000\n",
      "Training Epoch: 7 [25088/50000]\tLoss: 1.9489\tLR: 0.015000\n",
      "Training Epoch: 7 [25216/50000]\tLoss: 1.7169\tLR: 0.015000\n",
      "Training Epoch: 7 [25344/50000]\tLoss: 1.6773\tLR: 0.015000\n",
      "Training Epoch: 7 [25472/50000]\tLoss: 1.7641\tLR: 0.015000\n",
      "Training Epoch: 7 [25600/50000]\tLoss: 1.7781\tLR: 0.015000\n",
      "Training Epoch: 7 [25728/50000]\tLoss: 1.9657\tLR: 0.015000\n",
      "Training Epoch: 7 [25856/50000]\tLoss: 1.8262\tLR: 0.015000\n",
      "Training Epoch: 7 [25984/50000]\tLoss: 1.8625\tLR: 0.015000\n",
      "Training Epoch: 7 [26112/50000]\tLoss: 1.8046\tLR: 0.015000\n",
      "Training Epoch: 7 [26240/50000]\tLoss: 1.7790\tLR: 0.015000\n",
      "Training Epoch: 7 [26368/50000]\tLoss: 1.6000\tLR: 0.015000\n",
      "Training Epoch: 7 [26496/50000]\tLoss: 1.9358\tLR: 0.015000\n",
      "Training Epoch: 7 [26624/50000]\tLoss: 1.8531\tLR: 0.015000\n",
      "Training Epoch: 7 [26752/50000]\tLoss: 1.8763\tLR: 0.015000\n",
      "Training Epoch: 7 [26880/50000]\tLoss: 1.6884\tLR: 0.015000\n",
      "Training Epoch: 7 [27008/50000]\tLoss: 1.6865\tLR: 0.015000\n",
      "Training Epoch: 7 [27136/50000]\tLoss: 1.6970\tLR: 0.015000\n",
      "Training Epoch: 7 [27264/50000]\tLoss: 1.8860\tLR: 0.015000\n",
      "Training Epoch: 7 [27392/50000]\tLoss: 1.9124\tLR: 0.015000\n",
      "Training Epoch: 7 [27520/50000]\tLoss: 1.6826\tLR: 0.015000\n",
      "Training Epoch: 7 [27648/50000]\tLoss: 1.8536\tLR: 0.015000\n",
      "Training Epoch: 7 [27776/50000]\tLoss: 1.8730\tLR: 0.015000\n",
      "Training Epoch: 7 [27904/50000]\tLoss: 1.7657\tLR: 0.015000\n",
      "Training Epoch: 7 [28032/50000]\tLoss: 1.8442\tLR: 0.015000\n",
      "Training Epoch: 7 [28160/50000]\tLoss: 1.7997\tLR: 0.015000\n",
      "Training Epoch: 7 [28288/50000]\tLoss: 1.9339\tLR: 0.015000\n",
      "Training Epoch: 7 [28416/50000]\tLoss: 1.8015\tLR: 0.015000\n",
      "Training Epoch: 7 [28544/50000]\tLoss: 1.8772\tLR: 0.015000\n",
      "Training Epoch: 7 [28672/50000]\tLoss: 1.6781\tLR: 0.015000\n",
      "Training Epoch: 7 [28800/50000]\tLoss: 1.9830\tLR: 0.015000\n",
      "Training Epoch: 7 [28928/50000]\tLoss: 1.7013\tLR: 0.015000\n",
      "Training Epoch: 7 [29056/50000]\tLoss: 1.8194\tLR: 0.015000\n",
      "Training Epoch: 7 [29184/50000]\tLoss: 1.7903\tLR: 0.015000\n",
      "Training Epoch: 7 [29312/50000]\tLoss: 1.7418\tLR: 0.015000\n",
      "Training Epoch: 7 [29440/50000]\tLoss: 1.9508\tLR: 0.015000\n",
      "Training Epoch: 7 [29568/50000]\tLoss: 1.9221\tLR: 0.015000\n",
      "Training Epoch: 7 [29696/50000]\tLoss: 1.9050\tLR: 0.015000\n",
      "Training Epoch: 7 [29824/50000]\tLoss: 1.9524\tLR: 0.015000\n",
      "Training Epoch: 7 [29952/50000]\tLoss: 1.8350\tLR: 0.015000\n",
      "Training Epoch: 7 [30080/50000]\tLoss: 1.9934\tLR: 0.015000\n",
      "Training Epoch: 7 [30208/50000]\tLoss: 1.8030\tLR: 0.015000\n",
      "Training Epoch: 7 [30336/50000]\tLoss: 1.7599\tLR: 0.015000\n",
      "Training Epoch: 7 [30464/50000]\tLoss: 1.6705\tLR: 0.015000\n",
      "Training Epoch: 7 [30592/50000]\tLoss: 1.6843\tLR: 0.015000\n",
      "Training Epoch: 7 [30720/50000]\tLoss: 1.9489\tLR: 0.015000\n",
      "Training Epoch: 7 [30848/50000]\tLoss: 1.9155\tLR: 0.015000\n",
      "Training Epoch: 7 [30976/50000]\tLoss: 1.8951\tLR: 0.015000\n",
      "Training Epoch: 7 [31104/50000]\tLoss: 1.7302\tLR: 0.015000\n",
      "Training Epoch: 7 [31232/50000]\tLoss: 1.9054\tLR: 0.015000\n",
      "Training Epoch: 7 [31360/50000]\tLoss: 1.5431\tLR: 0.015000\n",
      "Training Epoch: 7 [31488/50000]\tLoss: 1.8649\tLR: 0.015000\n",
      "Training Epoch: 7 [31616/50000]\tLoss: 1.7009\tLR: 0.015000\n",
      "Training Epoch: 7 [31744/50000]\tLoss: 1.8079\tLR: 0.015000\n",
      "Training Epoch: 7 [31872/50000]\tLoss: 1.9076\tLR: 0.015000\n",
      "Training Epoch: 7 [32000/50000]\tLoss: 1.7792\tLR: 0.015000\n",
      "Training Epoch: 7 [32128/50000]\tLoss: 1.9736\tLR: 0.015000\n",
      "Training Epoch: 7 [32256/50000]\tLoss: 1.9038\tLR: 0.015000\n",
      "Training Epoch: 7 [32384/50000]\tLoss: 1.8973\tLR: 0.015000\n",
      "Training Epoch: 7 [32512/50000]\tLoss: 1.7659\tLR: 0.015000\n",
      "Training Epoch: 7 [32640/50000]\tLoss: 1.6625\tLR: 0.015000\n",
      "Training Epoch: 7 [32768/50000]\tLoss: 1.8798\tLR: 0.015000\n",
      "Training Epoch: 7 [32896/50000]\tLoss: 1.6875\tLR: 0.015000\n",
      "Training Epoch: 7 [33024/50000]\tLoss: 1.6494\tLR: 0.015000\n",
      "Training Epoch: 7 [33152/50000]\tLoss: 1.5698\tLR: 0.015000\n",
      "Training Epoch: 7 [33280/50000]\tLoss: 1.9625\tLR: 0.015000\n",
      "Training Epoch: 7 [33408/50000]\tLoss: 1.6504\tLR: 0.015000\n",
      "Training Epoch: 7 [33536/50000]\tLoss: 1.6976\tLR: 0.015000\n",
      "Training Epoch: 7 [33664/50000]\tLoss: 2.0374\tLR: 0.015000\n",
      "Training Epoch: 7 [33792/50000]\tLoss: 1.7776\tLR: 0.015000\n",
      "Training Epoch: 7 [33920/50000]\tLoss: 1.7569\tLR: 0.015000\n",
      "Training Epoch: 7 [34048/50000]\tLoss: 1.6202\tLR: 0.015000\n",
      "Training Epoch: 7 [34176/50000]\tLoss: 1.8274\tLR: 0.015000\n",
      "Training Epoch: 7 [34304/50000]\tLoss: 1.6439\tLR: 0.015000\n",
      "Training Epoch: 7 [34432/50000]\tLoss: 1.6707\tLR: 0.015000\n",
      "Training Epoch: 7 [34560/50000]\tLoss: 1.7937\tLR: 0.015000\n",
      "Training Epoch: 7 [34688/50000]\tLoss: 1.7907\tLR: 0.015000\n",
      "Training Epoch: 7 [34816/50000]\tLoss: 1.7861\tLR: 0.015000\n",
      "Training Epoch: 7 [34944/50000]\tLoss: 2.0530\tLR: 0.015000\n",
      "Training Epoch: 7 [35072/50000]\tLoss: 1.7597\tLR: 0.015000\n",
      "Training Epoch: 7 [35200/50000]\tLoss: 1.8961\tLR: 0.015000\n",
      "Training Epoch: 7 [35328/50000]\tLoss: 1.7467\tLR: 0.015000\n",
      "Training Epoch: 7 [35456/50000]\tLoss: 1.4137\tLR: 0.015000\n",
      "Training Epoch: 7 [35584/50000]\tLoss: 1.8212\tLR: 0.015000\n",
      "Training Epoch: 7 [35712/50000]\tLoss: 1.8265\tLR: 0.015000\n",
      "Training Epoch: 7 [35840/50000]\tLoss: 1.7382\tLR: 0.015000\n",
      "Training Epoch: 7 [35968/50000]\tLoss: 1.8921\tLR: 0.015000\n",
      "Training Epoch: 7 [36096/50000]\tLoss: 1.6692\tLR: 0.015000\n",
      "Training Epoch: 7 [36224/50000]\tLoss: 1.6967\tLR: 0.015000\n",
      "Training Epoch: 7 [36352/50000]\tLoss: 1.8902\tLR: 0.015000\n",
      "Training Epoch: 7 [36480/50000]\tLoss: 1.9177\tLR: 0.015000\n",
      "Training Epoch: 7 [36608/50000]\tLoss: 1.6097\tLR: 0.015000\n",
      "Training Epoch: 7 [36736/50000]\tLoss: 1.6098\tLR: 0.015000\n",
      "Training Epoch: 7 [36864/50000]\tLoss: 1.8234\tLR: 0.015000\n",
      "Training Epoch: 7 [36992/50000]\tLoss: 1.7349\tLR: 0.015000\n",
      "Training Epoch: 7 [37120/50000]\tLoss: 1.7305\tLR: 0.015000\n",
      "Training Epoch: 7 [37248/50000]\tLoss: 1.8848\tLR: 0.015000\n",
      "Training Epoch: 7 [37376/50000]\tLoss: 1.8558\tLR: 0.015000\n",
      "Training Epoch: 7 [37504/50000]\tLoss: 1.8351\tLR: 0.015000\n",
      "Training Epoch: 7 [37632/50000]\tLoss: 1.6954\tLR: 0.015000\n",
      "Training Epoch: 7 [37760/50000]\tLoss: 1.8114\tLR: 0.015000\n",
      "Training Epoch: 7 [37888/50000]\tLoss: 1.7472\tLR: 0.015000\n",
      "Training Epoch: 7 [38016/50000]\tLoss: 1.8837\tLR: 0.015000\n",
      "Training Epoch: 7 [38144/50000]\tLoss: 1.5767\tLR: 0.015000\n",
      "Training Epoch: 7 [38272/50000]\tLoss: 1.7728\tLR: 0.015000\n",
      "Training Epoch: 7 [38400/50000]\tLoss: 1.9979\tLR: 0.015000\n",
      "Training Epoch: 7 [38528/50000]\tLoss: 2.0406\tLR: 0.015000\n",
      "Training Epoch: 7 [38656/50000]\tLoss: 1.5990\tLR: 0.015000\n",
      "Training Epoch: 7 [38784/50000]\tLoss: 1.5571\tLR: 0.015000\n",
      "Training Epoch: 7 [38912/50000]\tLoss: 1.9988\tLR: 0.015000\n",
      "Training Epoch: 7 [39040/50000]\tLoss: 1.9879\tLR: 0.015000\n",
      "Training Epoch: 7 [39168/50000]\tLoss: 1.6454\tLR: 0.015000\n",
      "Training Epoch: 7 [39296/50000]\tLoss: 1.7341\tLR: 0.015000\n",
      "Training Epoch: 7 [39424/50000]\tLoss: 1.7709\tLR: 0.015000\n",
      "Training Epoch: 7 [39552/50000]\tLoss: 1.5613\tLR: 0.015000\n",
      "Training Epoch: 7 [39680/50000]\tLoss: 1.8017\tLR: 0.015000\n",
      "Training Epoch: 7 [39808/50000]\tLoss: 1.8716\tLR: 0.015000\n",
      "Training Epoch: 7 [39936/50000]\tLoss: 1.5429\tLR: 0.015000\n",
      "Training Epoch: 7 [40064/50000]\tLoss: 2.1110\tLR: 0.015000\n",
      "Training Epoch: 7 [40192/50000]\tLoss: 2.0481\tLR: 0.015000\n",
      "Training Epoch: 7 [40320/50000]\tLoss: 1.8298\tLR: 0.015000\n",
      "Training Epoch: 7 [40448/50000]\tLoss: 1.8037\tLR: 0.015000\n",
      "Training Epoch: 7 [40576/50000]\tLoss: 1.7196\tLR: 0.015000\n",
      "Training Epoch: 7 [40704/50000]\tLoss: 1.5963\tLR: 0.015000\n",
      "Training Epoch: 7 [40832/50000]\tLoss: 1.7495\tLR: 0.015000\n",
      "Training Epoch: 7 [40960/50000]\tLoss: 1.8110\tLR: 0.015000\n",
      "Training Epoch: 7 [41088/50000]\tLoss: 1.6681\tLR: 0.015000\n",
      "Training Epoch: 7 [41216/50000]\tLoss: 1.8914\tLR: 0.015000\n",
      "Training Epoch: 7 [41344/50000]\tLoss: 1.8786\tLR: 0.015000\n",
      "Training Epoch: 7 [41472/50000]\tLoss: 1.5460\tLR: 0.015000\n",
      "Training Epoch: 7 [41600/50000]\tLoss: 1.7198\tLR: 0.015000\n",
      "Training Epoch: 7 [41728/50000]\tLoss: 1.7900\tLR: 0.015000\n",
      "Training Epoch: 7 [41856/50000]\tLoss: 1.8464\tLR: 0.015000\n",
      "Training Epoch: 7 [41984/50000]\tLoss: 1.8031\tLR: 0.015000\n",
      "Training Epoch: 7 [42112/50000]\tLoss: 1.7200\tLR: 0.015000\n",
      "Training Epoch: 7 [42240/50000]\tLoss: 1.5667\tLR: 0.015000\n",
      "Training Epoch: 7 [42368/50000]\tLoss: 1.6963\tLR: 0.015000\n",
      "Training Epoch: 7 [42496/50000]\tLoss: 1.6371\tLR: 0.015000\n",
      "Training Epoch: 7 [42624/50000]\tLoss: 1.7817\tLR: 0.015000\n",
      "Training Epoch: 7 [42752/50000]\tLoss: 1.8308\tLR: 0.015000\n",
      "Training Epoch: 7 [42880/50000]\tLoss: 1.6888\tLR: 0.015000\n",
      "Training Epoch: 7 [43008/50000]\tLoss: 1.7132\tLR: 0.015000\n",
      "Training Epoch: 7 [43136/50000]\tLoss: 1.8389\tLR: 0.015000\n",
      "Training Epoch: 7 [43264/50000]\tLoss: 1.3794\tLR: 0.015000\n",
      "Training Epoch: 7 [43392/50000]\tLoss: 1.5999\tLR: 0.015000\n",
      "Training Epoch: 7 [43520/50000]\tLoss: 1.6497\tLR: 0.015000\n",
      "Training Epoch: 7 [43648/50000]\tLoss: 1.5715\tLR: 0.015000\n",
      "Training Epoch: 7 [43776/50000]\tLoss: 1.7962\tLR: 0.015000\n",
      "Training Epoch: 7 [43904/50000]\tLoss: 1.8387\tLR: 0.015000\n",
      "Training Epoch: 7 [44032/50000]\tLoss: 1.9251\tLR: 0.015000\n",
      "Training Epoch: 7 [44160/50000]\tLoss: 1.7934\tLR: 0.015000\n",
      "Training Epoch: 7 [44288/50000]\tLoss: 1.6241\tLR: 0.015000\n",
      "Training Epoch: 7 [44416/50000]\tLoss: 1.8639\tLR: 0.015000\n",
      "Training Epoch: 7 [44544/50000]\tLoss: 1.7241\tLR: 0.015000\n",
      "Training Epoch: 7 [44672/50000]\tLoss: 1.7660\tLR: 0.015000\n",
      "Training Epoch: 7 [44800/50000]\tLoss: 1.6816\tLR: 0.015000\n",
      "Training Epoch: 7 [44928/50000]\tLoss: 1.7694\tLR: 0.015000\n",
      "Training Epoch: 7 [45056/50000]\tLoss: 1.6871\tLR: 0.015000\n",
      "Training Epoch: 7 [45184/50000]\tLoss: 1.6422\tLR: 0.015000\n",
      "Training Epoch: 7 [45312/50000]\tLoss: 1.6744\tLR: 0.015000\n",
      "Training Epoch: 7 [45440/50000]\tLoss: 1.8192\tLR: 0.015000\n",
      "Training Epoch: 7 [45568/50000]\tLoss: 1.7659\tLR: 0.015000\n",
      "Training Epoch: 7 [45696/50000]\tLoss: 1.4960\tLR: 0.015000\n",
      "Training Epoch: 7 [45824/50000]\tLoss: 1.7798\tLR: 0.015000\n",
      "Training Epoch: 7 [45952/50000]\tLoss: 1.6019\tLR: 0.015000\n",
      "Training Epoch: 7 [46080/50000]\tLoss: 1.8456\tLR: 0.015000\n",
      "Training Epoch: 7 [46208/50000]\tLoss: 1.5602\tLR: 0.015000\n",
      "Training Epoch: 7 [46336/50000]\tLoss: 1.5862\tLR: 0.015000\n",
      "Training Epoch: 7 [46464/50000]\tLoss: 1.4512\tLR: 0.015000\n",
      "Training Epoch: 7 [46592/50000]\tLoss: 1.5857\tLR: 0.015000\n",
      "Training Epoch: 7 [46720/50000]\tLoss: 1.6523\tLR: 0.015000\n",
      "Training Epoch: 7 [46848/50000]\tLoss: 1.6856\tLR: 0.015000\n",
      "Training Epoch: 7 [46976/50000]\tLoss: 1.4890\tLR: 0.015000\n",
      "Training Epoch: 7 [47104/50000]\tLoss: 1.7888\tLR: 0.015000\n",
      "Training Epoch: 7 [47232/50000]\tLoss: 1.7828\tLR: 0.015000\n",
      "Training Epoch: 7 [47360/50000]\tLoss: 1.7559\tLR: 0.015000\n",
      "Training Epoch: 7 [47488/50000]\tLoss: 1.8542\tLR: 0.015000\n",
      "Training Epoch: 7 [47616/50000]\tLoss: 1.8312\tLR: 0.015000\n",
      "Training Epoch: 7 [47744/50000]\tLoss: 2.0182\tLR: 0.015000\n",
      "Training Epoch: 7 [47872/50000]\tLoss: 1.7157\tLR: 0.015000\n",
      "Training Epoch: 7 [48000/50000]\tLoss: 1.7963\tLR: 0.015000\n",
      "Training Epoch: 7 [48128/50000]\tLoss: 1.5240\tLR: 0.015000\n",
      "Training Epoch: 7 [48256/50000]\tLoss: 1.9081\tLR: 0.015000\n",
      "Training Epoch: 7 [48384/50000]\tLoss: 1.7797\tLR: 0.015000\n",
      "Training Epoch: 7 [48512/50000]\tLoss: 1.7328\tLR: 0.015000\n",
      "Training Epoch: 7 [48640/50000]\tLoss: 1.6179\tLR: 0.015000\n",
      "Training Epoch: 7 [48768/50000]\tLoss: 1.8068\tLR: 0.015000\n",
      "Training Epoch: 7 [48896/50000]\tLoss: 1.8594\tLR: 0.015000\n",
      "Training Epoch: 7 [49024/50000]\tLoss: 1.6281\tLR: 0.015000\n",
      "Training Epoch: 7 [49152/50000]\tLoss: 1.5953\tLR: 0.015000\n",
      "Training Epoch: 7 [49280/50000]\tLoss: 1.9310\tLR: 0.015000\n",
      "Training Epoch: 7 [49408/50000]\tLoss: 1.7358\tLR: 0.015000\n",
      "Training Epoch: 7 [49536/50000]\tLoss: 1.7569\tLR: 0.015000\n",
      "Training Epoch: 7 [49664/50000]\tLoss: 1.7738\tLR: 0.015000\n",
      "Training Epoch: 7 [49792/50000]\tLoss: 1.6552\tLR: 0.015000\n",
      "Training Epoch: 7 [49920/50000]\tLoss: 1.8080\tLR: 0.015000\n",
      "Training Epoch: 7 [50000/50000]\tLoss: 2.1338\tLR: 0.015000\n",
      "Test set: Average loss: 0.0136, Accuracy: 0.5147\n",
      "\n",
      "Training Epoch: 8 [128/50000]\tLoss: 1.7376\tLR: 0.015000\n",
      "Training Epoch: 8 [256/50000]\tLoss: 1.3916\tLR: 0.015000\n",
      "Training Epoch: 8 [384/50000]\tLoss: 1.7249\tLR: 0.015000\n",
      "Training Epoch: 8 [512/50000]\tLoss: 1.5789\tLR: 0.015000\n",
      "Training Epoch: 8 [640/50000]\tLoss: 1.6862\tLR: 0.015000\n",
      "Training Epoch: 8 [768/50000]\tLoss: 1.5519\tLR: 0.015000\n",
      "Training Epoch: 8 [896/50000]\tLoss: 1.5133\tLR: 0.015000\n",
      "Training Epoch: 8 [1024/50000]\tLoss: 1.9156\tLR: 0.015000\n",
      "Training Epoch: 8 [1152/50000]\tLoss: 1.7983\tLR: 0.015000\n",
      "Training Epoch: 8 [1280/50000]\tLoss: 1.6277\tLR: 0.015000\n",
      "Training Epoch: 8 [1408/50000]\tLoss: 1.5664\tLR: 0.015000\n",
      "Training Epoch: 8 [1536/50000]\tLoss: 1.8947\tLR: 0.015000\n",
      "Training Epoch: 8 [1664/50000]\tLoss: 1.6903\tLR: 0.015000\n",
      "Training Epoch: 8 [1792/50000]\tLoss: 1.7480\tLR: 0.015000\n",
      "Training Epoch: 8 [1920/50000]\tLoss: 1.7360\tLR: 0.015000\n",
      "Training Epoch: 8 [2048/50000]\tLoss: 1.7901\tLR: 0.015000\n",
      "Training Epoch: 8 [2176/50000]\tLoss: 1.8103\tLR: 0.015000\n",
      "Training Epoch: 8 [2304/50000]\tLoss: 1.6923\tLR: 0.015000\n",
      "Training Epoch: 8 [2432/50000]\tLoss: 1.5702\tLR: 0.015000\n",
      "Training Epoch: 8 [2560/50000]\tLoss: 1.5867\tLR: 0.015000\n",
      "Training Epoch: 8 [2688/50000]\tLoss: 1.6783\tLR: 0.015000\n",
      "Training Epoch: 8 [2816/50000]\tLoss: 1.5997\tLR: 0.015000\n",
      "Training Epoch: 8 [2944/50000]\tLoss: 1.7041\tLR: 0.015000\n",
      "Training Epoch: 8 [3072/50000]\tLoss: 1.5163\tLR: 0.015000\n",
      "Training Epoch: 8 [3200/50000]\tLoss: 1.7689\tLR: 0.015000\n",
      "Training Epoch: 8 [3328/50000]\tLoss: 1.7057\tLR: 0.015000\n",
      "Training Epoch: 8 [3456/50000]\tLoss: 1.7786\tLR: 0.015000\n",
      "Training Epoch: 8 [3584/50000]\tLoss: 1.6065\tLR: 0.015000\n",
      "Training Epoch: 8 [3712/50000]\tLoss: 1.8641\tLR: 0.015000\n",
      "Training Epoch: 8 [3840/50000]\tLoss: 1.6684\tLR: 0.015000\n",
      "Training Epoch: 8 [3968/50000]\tLoss: 1.5480\tLR: 0.015000\n",
      "Training Epoch: 8 [4096/50000]\tLoss: 1.7736\tLR: 0.015000\n",
      "Training Epoch: 8 [4224/50000]\tLoss: 1.7593\tLR: 0.015000\n",
      "Training Epoch: 8 [4352/50000]\tLoss: 1.9619\tLR: 0.015000\n",
      "Training Epoch: 8 [4480/50000]\tLoss: 1.7088\tLR: 0.015000\n",
      "Training Epoch: 8 [4608/50000]\tLoss: 1.7637\tLR: 0.015000\n",
      "Training Epoch: 8 [4736/50000]\tLoss: 1.9734\tLR: 0.015000\n",
      "Training Epoch: 8 [4864/50000]\tLoss: 1.4880\tLR: 0.015000\n",
      "Training Epoch: 8 [4992/50000]\tLoss: 1.6732\tLR: 0.015000\n",
      "Training Epoch: 8 [5120/50000]\tLoss: 1.6985\tLR: 0.015000\n",
      "Training Epoch: 8 [5248/50000]\tLoss: 1.8500\tLR: 0.015000\n",
      "Training Epoch: 8 [5376/50000]\tLoss: 1.7116\tLR: 0.015000\n",
      "Training Epoch: 8 [5504/50000]\tLoss: 1.4710\tLR: 0.015000\n",
      "Training Epoch: 8 [5632/50000]\tLoss: 1.7245\tLR: 0.015000\n",
      "Training Epoch: 8 [5760/50000]\tLoss: 1.3144\tLR: 0.015000\n",
      "Training Epoch: 8 [5888/50000]\tLoss: 1.8773\tLR: 0.015000\n",
      "Training Epoch: 8 [6016/50000]\tLoss: 1.7771\tLR: 0.015000\n",
      "Training Epoch: 8 [6144/50000]\tLoss: 1.9076\tLR: 0.015000\n",
      "Training Epoch: 8 [6272/50000]\tLoss: 1.6747\tLR: 0.015000\n",
      "Training Epoch: 8 [6400/50000]\tLoss: 1.4702\tLR: 0.015000\n",
      "Training Epoch: 8 [6528/50000]\tLoss: 1.7966\tLR: 0.015000\n",
      "Training Epoch: 8 [6656/50000]\tLoss: 1.5046\tLR: 0.015000\n",
      "Training Epoch: 8 [6784/50000]\tLoss: 1.6369\tLR: 0.015000\n",
      "Training Epoch: 8 [6912/50000]\tLoss: 1.3301\tLR: 0.015000\n",
      "Training Epoch: 8 [7040/50000]\tLoss: 1.5352\tLR: 0.015000\n",
      "Training Epoch: 8 [7168/50000]\tLoss: 1.7582\tLR: 0.015000\n",
      "Training Epoch: 8 [7296/50000]\tLoss: 1.8939\tLR: 0.015000\n",
      "Training Epoch: 8 [7424/50000]\tLoss: 1.6140\tLR: 0.015000\n",
      "Training Epoch: 8 [7552/50000]\tLoss: 1.5997\tLR: 0.015000\n",
      "Training Epoch: 8 [7680/50000]\tLoss: 2.0156\tLR: 0.015000\n",
      "Training Epoch: 8 [7808/50000]\tLoss: 1.6604\tLR: 0.015000\n",
      "Training Epoch: 8 [7936/50000]\tLoss: 1.7660\tLR: 0.015000\n",
      "Training Epoch: 8 [8064/50000]\tLoss: 1.6178\tLR: 0.015000\n",
      "Training Epoch: 8 [8192/50000]\tLoss: 1.5253\tLR: 0.015000\n",
      "Training Epoch: 8 [8320/50000]\tLoss: 1.4714\tLR: 0.015000\n",
      "Training Epoch: 8 [8448/50000]\tLoss: 1.7068\tLR: 0.015000\n",
      "Training Epoch: 8 [8576/50000]\tLoss: 1.6121\tLR: 0.015000\n",
      "Training Epoch: 8 [8704/50000]\tLoss: 1.5805\tLR: 0.015000\n",
      "Training Epoch: 8 [8832/50000]\tLoss: 1.4555\tLR: 0.015000\n",
      "Training Epoch: 8 [8960/50000]\tLoss: 1.4123\tLR: 0.015000\n",
      "Training Epoch: 8 [9088/50000]\tLoss: 1.7865\tLR: 0.015000\n",
      "Training Epoch: 8 [9216/50000]\tLoss: 1.7991\tLR: 0.015000\n",
      "Training Epoch: 8 [9344/50000]\tLoss: 1.7679\tLR: 0.015000\n",
      "Training Epoch: 8 [9472/50000]\tLoss: 1.8310\tLR: 0.015000\n",
      "Training Epoch: 8 [9600/50000]\tLoss: 1.7177\tLR: 0.015000\n",
      "Training Epoch: 8 [9728/50000]\tLoss: 1.5815\tLR: 0.015000\n",
      "Training Epoch: 8 [9856/50000]\tLoss: 1.6167\tLR: 0.015000\n",
      "Training Epoch: 8 [9984/50000]\tLoss: 1.8976\tLR: 0.015000\n",
      "Training Epoch: 8 [10112/50000]\tLoss: 1.7428\tLR: 0.015000\n",
      "Training Epoch: 8 [10240/50000]\tLoss: 1.6576\tLR: 0.015000\n",
      "Training Epoch: 8 [10368/50000]\tLoss: 1.6777\tLR: 0.015000\n",
      "Training Epoch: 8 [10496/50000]\tLoss: 1.7331\tLR: 0.015000\n",
      "Training Epoch: 8 [10624/50000]\tLoss: 1.5704\tLR: 0.015000\n",
      "Training Epoch: 8 [10752/50000]\tLoss: 1.8439\tLR: 0.015000\n",
      "Training Epoch: 8 [10880/50000]\tLoss: 1.7154\tLR: 0.015000\n",
      "Training Epoch: 8 [11008/50000]\tLoss: 1.8079\tLR: 0.015000\n",
      "Training Epoch: 8 [11136/50000]\tLoss: 1.6033\tLR: 0.015000\n",
      "Training Epoch: 8 [11264/50000]\tLoss: 1.6536\tLR: 0.015000\n",
      "Training Epoch: 8 [11392/50000]\tLoss: 1.6012\tLR: 0.015000\n",
      "Training Epoch: 8 [11520/50000]\tLoss: 1.8416\tLR: 0.015000\n",
      "Training Epoch: 8 [11648/50000]\tLoss: 1.9304\tLR: 0.015000\n",
      "Training Epoch: 8 [11776/50000]\tLoss: 1.8059\tLR: 0.015000\n",
      "Training Epoch: 8 [11904/50000]\tLoss: 1.7443\tLR: 0.015000\n",
      "Training Epoch: 8 [12032/50000]\tLoss: 1.7598\tLR: 0.015000\n",
      "Training Epoch: 8 [12160/50000]\tLoss: 1.7532\tLR: 0.015000\n",
      "Training Epoch: 8 [12288/50000]\tLoss: 1.8636\tLR: 0.015000\n",
      "Training Epoch: 8 [12416/50000]\tLoss: 1.5887\tLR: 0.015000\n",
      "Training Epoch: 8 [12544/50000]\tLoss: 1.7228\tLR: 0.015000\n",
      "Training Epoch: 8 [12672/50000]\tLoss: 1.6648\tLR: 0.015000\n",
      "Training Epoch: 8 [12800/50000]\tLoss: 1.4853\tLR: 0.015000\n",
      "Training Epoch: 8 [12928/50000]\tLoss: 1.6562\tLR: 0.015000\n",
      "Training Epoch: 8 [13056/50000]\tLoss: 1.8369\tLR: 0.015000\n",
      "Training Epoch: 8 [13184/50000]\tLoss: 1.6413\tLR: 0.015000\n",
      "Training Epoch: 8 [13312/50000]\tLoss: 1.7179\tLR: 0.015000\n",
      "Training Epoch: 8 [13440/50000]\tLoss: 1.6123\tLR: 0.015000\n",
      "Training Epoch: 8 [13568/50000]\tLoss: 1.5869\tLR: 0.015000\n",
      "Training Epoch: 8 [13696/50000]\tLoss: 1.5144\tLR: 0.015000\n",
      "Training Epoch: 8 [13824/50000]\tLoss: 1.6548\tLR: 0.015000\n",
      "Training Epoch: 8 [13952/50000]\tLoss: 1.6068\tLR: 0.015000\n",
      "Training Epoch: 8 [14080/50000]\tLoss: 1.5778\tLR: 0.015000\n",
      "Training Epoch: 8 [14208/50000]\tLoss: 1.7553\tLR: 0.015000\n",
      "Training Epoch: 8 [14336/50000]\tLoss: 1.7429\tLR: 0.015000\n",
      "Training Epoch: 8 [14464/50000]\tLoss: 1.4546\tLR: 0.015000\n",
      "Training Epoch: 8 [14592/50000]\tLoss: 1.7779\tLR: 0.015000\n",
      "Training Epoch: 8 [14720/50000]\tLoss: 1.8081\tLR: 0.015000\n",
      "Training Epoch: 8 [14848/50000]\tLoss: 1.8630\tLR: 0.015000\n",
      "Training Epoch: 8 [14976/50000]\tLoss: 1.5781\tLR: 0.015000\n",
      "Training Epoch: 8 [15104/50000]\tLoss: 1.7349\tLR: 0.015000\n",
      "Training Epoch: 8 [15232/50000]\tLoss: 1.6069\tLR: 0.015000\n",
      "Training Epoch: 8 [15360/50000]\tLoss: 1.6650\tLR: 0.015000\n",
      "Training Epoch: 8 [15488/50000]\tLoss: 1.7390\tLR: 0.015000\n",
      "Training Epoch: 8 [15616/50000]\tLoss: 1.6505\tLR: 0.015000\n",
      "Training Epoch: 8 [15744/50000]\tLoss: 1.8956\tLR: 0.015000\n",
      "Training Epoch: 8 [15872/50000]\tLoss: 1.5187\tLR: 0.015000\n",
      "Training Epoch: 8 [16000/50000]\tLoss: 1.5318\tLR: 0.015000\n",
      "Training Epoch: 8 [16128/50000]\tLoss: 1.5176\tLR: 0.015000\n",
      "Training Epoch: 8 [16256/50000]\tLoss: 1.6395\tLR: 0.015000\n",
      "Training Epoch: 8 [16384/50000]\tLoss: 1.7469\tLR: 0.015000\n",
      "Training Epoch: 8 [16512/50000]\tLoss: 1.7623\tLR: 0.015000\n",
      "Training Epoch: 8 [16640/50000]\tLoss: 1.6613\tLR: 0.015000\n",
      "Training Epoch: 8 [16768/50000]\tLoss: 1.7180\tLR: 0.015000\n",
      "Training Epoch: 8 [16896/50000]\tLoss: 1.6816\tLR: 0.015000\n",
      "Training Epoch: 8 [17024/50000]\tLoss: 1.6804\tLR: 0.015000\n",
      "Training Epoch: 8 [17152/50000]\tLoss: 1.8161\tLR: 0.015000\n",
      "Training Epoch: 8 [17280/50000]\tLoss: 1.4980\tLR: 0.015000\n",
      "Training Epoch: 8 [17408/50000]\tLoss: 1.7804\tLR: 0.015000\n",
      "Training Epoch: 8 [17536/50000]\tLoss: 1.7862\tLR: 0.015000\n",
      "Training Epoch: 8 [17664/50000]\tLoss: 1.6624\tLR: 0.015000\n",
      "Training Epoch: 8 [17792/50000]\tLoss: 1.5039\tLR: 0.015000\n",
      "Training Epoch: 8 [17920/50000]\tLoss: 1.6340\tLR: 0.015000\n",
      "Training Epoch: 8 [18048/50000]\tLoss: 1.6048\tLR: 0.015000\n",
      "Training Epoch: 8 [18176/50000]\tLoss: 2.0050\tLR: 0.015000\n",
      "Training Epoch: 8 [18304/50000]\tLoss: 1.7073\tLR: 0.015000\n",
      "Training Epoch: 8 [18432/50000]\tLoss: 1.8290\tLR: 0.015000\n",
      "Training Epoch: 8 [18560/50000]\tLoss: 1.6247\tLR: 0.015000\n",
      "Training Epoch: 8 [18688/50000]\tLoss: 1.6066\tLR: 0.015000\n",
      "Training Epoch: 8 [18816/50000]\tLoss: 1.7543\tLR: 0.015000\n",
      "Training Epoch: 8 [18944/50000]\tLoss: 1.5011\tLR: 0.015000\n",
      "Training Epoch: 8 [19072/50000]\tLoss: 1.6376\tLR: 0.015000\n",
      "Training Epoch: 8 [19200/50000]\tLoss: 1.6598\tLR: 0.015000\n",
      "Training Epoch: 8 [19328/50000]\tLoss: 1.7427\tLR: 0.015000\n",
      "Training Epoch: 8 [19456/50000]\tLoss: 1.6800\tLR: 0.015000\n",
      "Training Epoch: 8 [19584/50000]\tLoss: 1.5344\tLR: 0.015000\n",
      "Training Epoch: 8 [19712/50000]\tLoss: 1.7560\tLR: 0.015000\n",
      "Training Epoch: 8 [19840/50000]\tLoss: 1.8489\tLR: 0.015000\n",
      "Training Epoch: 8 [19968/50000]\tLoss: 1.8708\tLR: 0.015000\n",
      "Training Epoch: 8 [20096/50000]\tLoss: 1.5171\tLR: 0.015000\n",
      "Training Epoch: 8 [20224/50000]\tLoss: 1.6982\tLR: 0.015000\n",
      "Training Epoch: 8 [20352/50000]\tLoss: 1.6899\tLR: 0.015000\n",
      "Training Epoch: 8 [20480/50000]\tLoss: 1.7962\tLR: 0.015000\n",
      "Training Epoch: 8 [20608/50000]\tLoss: 1.6444\tLR: 0.015000\n",
      "Training Epoch: 8 [20736/50000]\tLoss: 1.9609\tLR: 0.015000\n",
      "Training Epoch: 8 [20864/50000]\tLoss: 1.6095\tLR: 0.015000\n",
      "Training Epoch: 8 [20992/50000]\tLoss: 1.6193\tLR: 0.015000\n",
      "Training Epoch: 8 [21120/50000]\tLoss: 1.6909\tLR: 0.015000\n",
      "Training Epoch: 8 [21248/50000]\tLoss: 1.6346\tLR: 0.015000\n",
      "Training Epoch: 8 [21376/50000]\tLoss: 1.6956\tLR: 0.015000\n",
      "Training Epoch: 8 [21504/50000]\tLoss: 1.6061\tLR: 0.015000\n",
      "Training Epoch: 8 [21632/50000]\tLoss: 1.6385\tLR: 0.015000\n",
      "Training Epoch: 8 [21760/50000]\tLoss: 1.5820\tLR: 0.015000\n",
      "Training Epoch: 8 [21888/50000]\tLoss: 1.7069\tLR: 0.015000\n",
      "Training Epoch: 8 [22016/50000]\tLoss: 1.8090\tLR: 0.015000\n",
      "Training Epoch: 8 [22144/50000]\tLoss: 1.6504\tLR: 0.015000\n",
      "Training Epoch: 8 [22272/50000]\tLoss: 1.6316\tLR: 0.015000\n",
      "Training Epoch: 8 [22400/50000]\tLoss: 1.7103\tLR: 0.015000\n",
      "Training Epoch: 8 [22528/50000]\tLoss: 1.9083\tLR: 0.015000\n",
      "Training Epoch: 8 [22656/50000]\tLoss: 1.8351\tLR: 0.015000\n",
      "Training Epoch: 8 [22784/50000]\tLoss: 1.5901\tLR: 0.015000\n",
      "Training Epoch: 8 [22912/50000]\tLoss: 1.6099\tLR: 0.015000\n",
      "Training Epoch: 8 [23040/50000]\tLoss: 1.7467\tLR: 0.015000\n",
      "Training Epoch: 8 [23168/50000]\tLoss: 1.7359\tLR: 0.015000\n",
      "Training Epoch: 8 [23296/50000]\tLoss: 1.6368\tLR: 0.015000\n",
      "Training Epoch: 8 [23424/50000]\tLoss: 1.9418\tLR: 0.015000\n",
      "Training Epoch: 8 [23552/50000]\tLoss: 1.9134\tLR: 0.015000\n",
      "Training Epoch: 8 [23680/50000]\tLoss: 1.6502\tLR: 0.015000\n",
      "Training Epoch: 8 [23808/50000]\tLoss: 1.6791\tLR: 0.015000\n",
      "Training Epoch: 8 [23936/50000]\tLoss: 1.6555\tLR: 0.015000\n",
      "Training Epoch: 8 [24064/50000]\tLoss: 1.8066\tLR: 0.015000\n",
      "Training Epoch: 8 [24192/50000]\tLoss: 1.6279\tLR: 0.015000\n",
      "Training Epoch: 8 [24320/50000]\tLoss: 1.8868\tLR: 0.015000\n",
      "Training Epoch: 8 [24448/50000]\tLoss: 1.5684\tLR: 0.015000\n",
      "Training Epoch: 8 [24576/50000]\tLoss: 1.9561\tLR: 0.015000\n",
      "Training Epoch: 8 [24704/50000]\tLoss: 1.5153\tLR: 0.015000\n",
      "Training Epoch: 8 [24832/50000]\tLoss: 1.5973\tLR: 0.015000\n",
      "Training Epoch: 8 [24960/50000]\tLoss: 1.7341\tLR: 0.015000\n",
      "Training Epoch: 8 [25088/50000]\tLoss: 1.4915\tLR: 0.015000\n",
      "Training Epoch: 8 [25216/50000]\tLoss: 1.7167\tLR: 0.015000\n",
      "Training Epoch: 8 [25344/50000]\tLoss: 1.5811\tLR: 0.015000\n",
      "Training Epoch: 8 [25472/50000]\tLoss: 1.5045\tLR: 0.015000\n",
      "Training Epoch: 8 [25600/50000]\tLoss: 1.5402\tLR: 0.015000\n",
      "Training Epoch: 8 [25728/50000]\tLoss: 1.6959\tLR: 0.015000\n",
      "Training Epoch: 8 [25856/50000]\tLoss: 1.6085\tLR: 0.015000\n",
      "Training Epoch: 8 [25984/50000]\tLoss: 1.8048\tLR: 0.015000\n",
      "Training Epoch: 8 [26112/50000]\tLoss: 1.7009\tLR: 0.015000\n",
      "Training Epoch: 8 [26240/50000]\tLoss: 1.7021\tLR: 0.015000\n",
      "Training Epoch: 8 [26368/50000]\tLoss: 1.5376\tLR: 0.015000\n",
      "Training Epoch: 8 [26496/50000]\tLoss: 2.0053\tLR: 0.015000\n",
      "Training Epoch: 8 [26624/50000]\tLoss: 1.5900\tLR: 0.015000\n",
      "Training Epoch: 8 [26752/50000]\tLoss: 1.6764\tLR: 0.015000\n",
      "Training Epoch: 8 [26880/50000]\tLoss: 1.6978\tLR: 0.015000\n",
      "Training Epoch: 8 [27008/50000]\tLoss: 1.7148\tLR: 0.015000\n",
      "Training Epoch: 8 [27136/50000]\tLoss: 1.9519\tLR: 0.015000\n",
      "Training Epoch: 8 [27264/50000]\tLoss: 1.6514\tLR: 0.015000\n",
      "Training Epoch: 8 [27392/50000]\tLoss: 1.5758\tLR: 0.015000\n",
      "Training Epoch: 8 [27520/50000]\tLoss: 1.8609\tLR: 0.015000\n",
      "Training Epoch: 8 [27648/50000]\tLoss: 1.6121\tLR: 0.015000\n",
      "Training Epoch: 8 [27776/50000]\tLoss: 1.5588\tLR: 0.015000\n",
      "Training Epoch: 8 [27904/50000]\tLoss: 2.0636\tLR: 0.015000\n",
      "Training Epoch: 8 [28032/50000]\tLoss: 1.8884\tLR: 0.015000\n",
      "Training Epoch: 8 [28160/50000]\tLoss: 1.5953\tLR: 0.015000\n",
      "Training Epoch: 8 [28288/50000]\tLoss: 1.6780\tLR: 0.015000\n",
      "Training Epoch: 8 [28416/50000]\tLoss: 1.8184\tLR: 0.015000\n",
      "Training Epoch: 8 [28544/50000]\tLoss: 1.7928\tLR: 0.015000\n",
      "Training Epoch: 8 [28672/50000]\tLoss: 1.8116\tLR: 0.015000\n",
      "Training Epoch: 8 [28800/50000]\tLoss: 1.9268\tLR: 0.015000\n",
      "Training Epoch: 8 [28928/50000]\tLoss: 1.8934\tLR: 0.015000\n",
      "Training Epoch: 8 [29056/50000]\tLoss: 1.8482\tLR: 0.015000\n",
      "Training Epoch: 8 [29184/50000]\tLoss: 1.5088\tLR: 0.015000\n",
      "Training Epoch: 8 [29312/50000]\tLoss: 1.7056\tLR: 0.015000\n",
      "Training Epoch: 8 [29440/50000]\tLoss: 1.7358\tLR: 0.015000\n",
      "Training Epoch: 8 [29568/50000]\tLoss: 1.8281\tLR: 0.015000\n",
      "Training Epoch: 8 [29696/50000]\tLoss: 1.7153\tLR: 0.015000\n",
      "Training Epoch: 8 [29824/50000]\tLoss: 1.5105\tLR: 0.015000\n",
      "Training Epoch: 8 [29952/50000]\tLoss: 1.6843\tLR: 0.015000\n",
      "Training Epoch: 8 [30080/50000]\tLoss: 1.4230\tLR: 0.015000\n",
      "Training Epoch: 8 [30208/50000]\tLoss: 1.5257\tLR: 0.015000\n",
      "Training Epoch: 8 [30336/50000]\tLoss: 1.6098\tLR: 0.015000\n",
      "Training Epoch: 8 [30464/50000]\tLoss: 1.8783\tLR: 0.015000\n",
      "Training Epoch: 8 [30592/50000]\tLoss: 1.9988\tLR: 0.015000\n",
      "Training Epoch: 8 [30720/50000]\tLoss: 1.5801\tLR: 0.015000\n",
      "Training Epoch: 8 [30848/50000]\tLoss: 1.8168\tLR: 0.015000\n",
      "Training Epoch: 8 [30976/50000]\tLoss: 1.9019\tLR: 0.015000\n",
      "Training Epoch: 8 [31104/50000]\tLoss: 1.3754\tLR: 0.015000\n",
      "Training Epoch: 8 [31232/50000]\tLoss: 1.6421\tLR: 0.015000\n",
      "Training Epoch: 8 [31360/50000]\tLoss: 1.6439\tLR: 0.015000\n",
      "Training Epoch: 8 [31488/50000]\tLoss: 1.6296\tLR: 0.015000\n",
      "Training Epoch: 8 [31616/50000]\tLoss: 1.7101\tLR: 0.015000\n",
      "Training Epoch: 8 [31744/50000]\tLoss: 1.8608\tLR: 0.015000\n",
      "Training Epoch: 8 [31872/50000]\tLoss: 1.8871\tLR: 0.015000\n",
      "Training Epoch: 8 [32000/50000]\tLoss: 1.7979\tLR: 0.015000\n",
      "Training Epoch: 8 [32128/50000]\tLoss: 1.7938\tLR: 0.015000\n",
      "Training Epoch: 8 [32256/50000]\tLoss: 1.7826\tLR: 0.015000\n",
      "Training Epoch: 8 [32384/50000]\tLoss: 1.7166\tLR: 0.015000\n",
      "Training Epoch: 8 [32512/50000]\tLoss: 1.6031\tLR: 0.015000\n",
      "Training Epoch: 8 [32640/50000]\tLoss: 1.5163\tLR: 0.015000\n",
      "Training Epoch: 8 [32768/50000]\tLoss: 1.9257\tLR: 0.015000\n",
      "Training Epoch: 8 [32896/50000]\tLoss: 1.7469\tLR: 0.015000\n",
      "Training Epoch: 8 [33024/50000]\tLoss: 1.7967\tLR: 0.015000\n",
      "Training Epoch: 8 [33152/50000]\tLoss: 1.8098\tLR: 0.015000\n",
      "Training Epoch: 8 [33280/50000]\tLoss: 1.7365\tLR: 0.015000\n",
      "Training Epoch: 8 [33408/50000]\tLoss: 1.6220\tLR: 0.015000\n",
      "Training Epoch: 8 [33536/50000]\tLoss: 1.6056\tLR: 0.015000\n",
      "Training Epoch: 8 [33664/50000]\tLoss: 1.5407\tLR: 0.015000\n",
      "Training Epoch: 8 [33792/50000]\tLoss: 1.5749\tLR: 0.015000\n",
      "Training Epoch: 8 [33920/50000]\tLoss: 1.5462\tLR: 0.015000\n",
      "Training Epoch: 8 [34048/50000]\tLoss: 1.8160\tLR: 0.015000\n",
      "Training Epoch: 8 [34176/50000]\tLoss: 1.8893\tLR: 0.015000\n",
      "Training Epoch: 8 [34304/50000]\tLoss: 1.7053\tLR: 0.015000\n",
      "Training Epoch: 8 [34432/50000]\tLoss: 1.6253\tLR: 0.015000\n",
      "Training Epoch: 8 [34560/50000]\tLoss: 1.5420\tLR: 0.015000\n",
      "Training Epoch: 8 [34688/50000]\tLoss: 1.7450\tLR: 0.015000\n",
      "Training Epoch: 8 [34816/50000]\tLoss: 1.8363\tLR: 0.015000\n",
      "Training Epoch: 8 [34944/50000]\tLoss: 1.8042\tLR: 0.015000\n",
      "Training Epoch: 8 [35072/50000]\tLoss: 1.6899\tLR: 0.015000\n",
      "Training Epoch: 8 [35200/50000]\tLoss: 1.7254\tLR: 0.015000\n",
      "Training Epoch: 8 [35328/50000]\tLoss: 1.9231\tLR: 0.015000\n",
      "Training Epoch: 8 [35456/50000]\tLoss: 1.8530\tLR: 0.015000\n",
      "Training Epoch: 8 [35584/50000]\tLoss: 1.5615\tLR: 0.015000\n",
      "Training Epoch: 8 [35712/50000]\tLoss: 1.6013\tLR: 0.015000\n",
      "Training Epoch: 8 [35840/50000]\tLoss: 1.7602\tLR: 0.015000\n",
      "Training Epoch: 8 [35968/50000]\tLoss: 1.5347\tLR: 0.015000\n",
      "Training Epoch: 8 [36096/50000]\tLoss: 1.8337\tLR: 0.015000\n",
      "Training Epoch: 8 [36224/50000]\tLoss: 1.5617\tLR: 0.015000\n",
      "Training Epoch: 8 [36352/50000]\tLoss: 1.7224\tLR: 0.015000\n",
      "Training Epoch: 8 [36480/50000]\tLoss: 1.8380\tLR: 0.015000\n",
      "Training Epoch: 8 [36608/50000]\tLoss: 1.7994\tLR: 0.015000\n",
      "Training Epoch: 8 [36736/50000]\tLoss: 1.3995\tLR: 0.015000\n",
      "Training Epoch: 8 [36864/50000]\tLoss: 1.8733\tLR: 0.015000\n",
      "Training Epoch: 8 [36992/50000]\tLoss: 2.1666\tLR: 0.015000\n",
      "Training Epoch: 8 [37120/50000]\tLoss: 1.5844\tLR: 0.015000\n",
      "Training Epoch: 8 [37248/50000]\tLoss: 1.6814\tLR: 0.015000\n",
      "Training Epoch: 8 [37376/50000]\tLoss: 1.8173\tLR: 0.015000\n",
      "Training Epoch: 8 [37504/50000]\tLoss: 1.6948\tLR: 0.015000\n",
      "Training Epoch: 8 [37632/50000]\tLoss: 1.6582\tLR: 0.015000\n",
      "Training Epoch: 8 [37760/50000]\tLoss: 1.6115\tLR: 0.015000\n",
      "Training Epoch: 8 [37888/50000]\tLoss: 1.6580\tLR: 0.015000\n",
      "Training Epoch: 8 [38016/50000]\tLoss: 1.7776\tLR: 0.015000\n",
      "Training Epoch: 8 [38144/50000]\tLoss: 1.7201\tLR: 0.015000\n",
      "Training Epoch: 8 [38272/50000]\tLoss: 1.6041\tLR: 0.015000\n",
      "Training Epoch: 8 [38400/50000]\tLoss: 1.7874\tLR: 0.015000\n",
      "Training Epoch: 8 [38528/50000]\tLoss: 1.7057\tLR: 0.015000\n",
      "Training Epoch: 8 [38656/50000]\tLoss: 1.6436\tLR: 0.015000\n",
      "Training Epoch: 8 [38784/50000]\tLoss: 1.5465\tLR: 0.015000\n",
      "Training Epoch: 8 [38912/50000]\tLoss: 1.6748\tLR: 0.015000\n",
      "Training Epoch: 8 [39040/50000]\tLoss: 1.6035\tLR: 0.015000\n",
      "Training Epoch: 8 [39168/50000]\tLoss: 1.7466\tLR: 0.015000\n",
      "Training Epoch: 8 [39296/50000]\tLoss: 1.9127\tLR: 0.015000\n",
      "Training Epoch: 8 [39424/50000]\tLoss: 1.6790\tLR: 0.015000\n",
      "Training Epoch: 8 [39552/50000]\tLoss: 1.6134\tLR: 0.015000\n",
      "Training Epoch: 8 [39680/50000]\tLoss: 1.4273\tLR: 0.015000\n",
      "Training Epoch: 8 [39808/50000]\tLoss: 1.4797\tLR: 0.015000\n",
      "Training Epoch: 8 [39936/50000]\tLoss: 1.5787\tLR: 0.015000\n",
      "Training Epoch: 8 [40064/50000]\tLoss: 1.8802\tLR: 0.015000\n",
      "Training Epoch: 8 [40192/50000]\tLoss: 1.5568\tLR: 0.015000\n",
      "Training Epoch: 8 [40320/50000]\tLoss: 1.7054\tLR: 0.015000\n",
      "Training Epoch: 8 [40448/50000]\tLoss: 1.4460\tLR: 0.015000\n",
      "Training Epoch: 8 [40576/50000]\tLoss: 1.5431\tLR: 0.015000\n",
      "Training Epoch: 8 [40704/50000]\tLoss: 1.8217\tLR: 0.015000\n",
      "Training Epoch: 8 [40832/50000]\tLoss: 1.5952\tLR: 0.015000\n",
      "Training Epoch: 8 [40960/50000]\tLoss: 1.6463\tLR: 0.015000\n",
      "Training Epoch: 8 [41088/50000]\tLoss: 1.6528\tLR: 0.015000\n",
      "Training Epoch: 8 [41216/50000]\tLoss: 1.6270\tLR: 0.015000\n",
      "Training Epoch: 8 [41344/50000]\tLoss: 1.5846\tLR: 0.015000\n",
      "Training Epoch: 8 [41472/50000]\tLoss: 1.7033\tLR: 0.015000\n",
      "Training Epoch: 8 [41600/50000]\tLoss: 1.6850\tLR: 0.015000\n",
      "Training Epoch: 8 [41728/50000]\tLoss: 1.5966\tLR: 0.015000\n",
      "Training Epoch: 8 [41856/50000]\tLoss: 1.6607\tLR: 0.015000\n",
      "Training Epoch: 8 [41984/50000]\tLoss: 1.6807\tLR: 0.015000\n",
      "Training Epoch: 8 [42112/50000]\tLoss: 1.8594\tLR: 0.015000\n",
      "Training Epoch: 8 [42240/50000]\tLoss: 1.6231\tLR: 0.015000\n",
      "Training Epoch: 8 [42368/50000]\tLoss: 1.7292\tLR: 0.015000\n",
      "Training Epoch: 8 [42496/50000]\tLoss: 1.5310\tLR: 0.015000\n",
      "Training Epoch: 8 [42624/50000]\tLoss: 2.0153\tLR: 0.015000\n",
      "Training Epoch: 8 [42752/50000]\tLoss: 1.5419\tLR: 0.015000\n",
      "Training Epoch: 8 [42880/50000]\tLoss: 1.6985\tLR: 0.015000\n",
      "Training Epoch: 8 [43008/50000]\tLoss: 1.6550\tLR: 0.015000\n",
      "Training Epoch: 8 [43136/50000]\tLoss: 1.8145\tLR: 0.015000\n",
      "Training Epoch: 8 [43264/50000]\tLoss: 1.8239\tLR: 0.015000\n",
      "Training Epoch: 8 [43392/50000]\tLoss: 1.8166\tLR: 0.015000\n",
      "Training Epoch: 8 [43520/50000]\tLoss: 1.6265\tLR: 0.015000\n",
      "Training Epoch: 8 [43648/50000]\tLoss: 1.5859\tLR: 0.015000\n",
      "Training Epoch: 8 [43776/50000]\tLoss: 1.8724\tLR: 0.015000\n",
      "Training Epoch: 8 [43904/50000]\tLoss: 1.6634\tLR: 0.015000\n",
      "Training Epoch: 8 [44032/50000]\tLoss: 1.7381\tLR: 0.015000\n",
      "Training Epoch: 8 [44160/50000]\tLoss: 1.7319\tLR: 0.015000\n",
      "Training Epoch: 8 [44288/50000]\tLoss: 1.6515\tLR: 0.015000\n",
      "Training Epoch: 8 [44416/50000]\tLoss: 1.7242\tLR: 0.015000\n",
      "Training Epoch: 8 [44544/50000]\tLoss: 1.6514\tLR: 0.015000\n",
      "Training Epoch: 8 [44672/50000]\tLoss: 1.4405\tLR: 0.015000\n",
      "Training Epoch: 8 [44800/50000]\tLoss: 1.7626\tLR: 0.015000\n",
      "Training Epoch: 8 [44928/50000]\tLoss: 1.6947\tLR: 0.015000\n",
      "Training Epoch: 8 [45056/50000]\tLoss: 1.7592\tLR: 0.015000\n",
      "Training Epoch: 8 [45184/50000]\tLoss: 1.6230\tLR: 0.015000\n",
      "Training Epoch: 8 [45312/50000]\tLoss: 1.5678\tLR: 0.015000\n",
      "Training Epoch: 8 [45440/50000]\tLoss: 2.0101\tLR: 0.015000\n",
      "Training Epoch: 8 [45568/50000]\tLoss: 1.6312\tLR: 0.015000\n",
      "Training Epoch: 8 [45696/50000]\tLoss: 1.5427\tLR: 0.015000\n",
      "Training Epoch: 8 [45824/50000]\tLoss: 1.7385\tLR: 0.015000\n",
      "Training Epoch: 8 [45952/50000]\tLoss: 1.8216\tLR: 0.015000\n",
      "Training Epoch: 8 [46080/50000]\tLoss: 1.6948\tLR: 0.015000\n",
      "Training Epoch: 8 [46208/50000]\tLoss: 1.7573\tLR: 0.015000\n",
      "Training Epoch: 8 [46336/50000]\tLoss: 1.5059\tLR: 0.015000\n",
      "Training Epoch: 8 [46464/50000]\tLoss: 1.6023\tLR: 0.015000\n",
      "Training Epoch: 8 [46592/50000]\tLoss: 1.6442\tLR: 0.015000\n",
      "Training Epoch: 8 [46720/50000]\tLoss: 1.5256\tLR: 0.015000\n",
      "Training Epoch: 8 [46848/50000]\tLoss: 1.5928\tLR: 0.015000\n",
      "Training Epoch: 8 [46976/50000]\tLoss: 1.5727\tLR: 0.015000\n",
      "Training Epoch: 8 [47104/50000]\tLoss: 1.5892\tLR: 0.015000\n",
      "Training Epoch: 8 [47232/50000]\tLoss: 1.7503\tLR: 0.015000\n",
      "Training Epoch: 8 [47360/50000]\tLoss: 1.4486\tLR: 0.015000\n",
      "Training Epoch: 8 [47488/50000]\tLoss: 1.7511\tLR: 0.015000\n",
      "Training Epoch: 8 [47616/50000]\tLoss: 1.7240\tLR: 0.015000\n",
      "Training Epoch: 8 [47744/50000]\tLoss: 1.5530\tLR: 0.015000\n",
      "Training Epoch: 8 [47872/50000]\tLoss: 1.4160\tLR: 0.015000\n",
      "Training Epoch: 8 [48000/50000]\tLoss: 1.6706\tLR: 0.015000\n",
      "Training Epoch: 8 [48128/50000]\tLoss: 1.6027\tLR: 0.015000\n",
      "Training Epoch: 8 [48256/50000]\tLoss: 1.5145\tLR: 0.015000\n",
      "Training Epoch: 8 [48384/50000]\tLoss: 1.5600\tLR: 0.015000\n",
      "Training Epoch: 8 [48512/50000]\tLoss: 1.7076\tLR: 0.015000\n",
      "Training Epoch: 8 [48640/50000]\tLoss: 1.6660\tLR: 0.015000\n",
      "Training Epoch: 8 [48768/50000]\tLoss: 1.7829\tLR: 0.015000\n",
      "Training Epoch: 8 [48896/50000]\tLoss: 1.6077\tLR: 0.015000\n",
      "Training Epoch: 8 [49024/50000]\tLoss: 1.7436\tLR: 0.015000\n",
      "Training Epoch: 8 [49152/50000]\tLoss: 1.4402\tLR: 0.015000\n",
      "Training Epoch: 8 [49280/50000]\tLoss: 1.6639\tLR: 0.015000\n",
      "Training Epoch: 8 [49408/50000]\tLoss: 1.7183\tLR: 0.015000\n",
      "Training Epoch: 8 [49536/50000]\tLoss: 1.6457\tLR: 0.015000\n",
      "Training Epoch: 8 [49664/50000]\tLoss: 1.5354\tLR: 0.015000\n",
      "Training Epoch: 8 [49792/50000]\tLoss: 1.6168\tLR: 0.015000\n",
      "Training Epoch: 8 [49920/50000]\tLoss: 1.6408\tLR: 0.015000\n",
      "Training Epoch: 8 [50000/50000]\tLoss: 1.7047\tLR: 0.015000\n",
      "Test set: Average loss: 0.0132, Accuracy: 0.5318\n",
      "\n",
      "Training Epoch: 9 [128/50000]\tLoss: 1.5715\tLR: 0.015000\n",
      "Training Epoch: 9 [256/50000]\tLoss: 1.5779\tLR: 0.015000\n",
      "Training Epoch: 9 [384/50000]\tLoss: 1.7395\tLR: 0.015000\n",
      "Training Epoch: 9 [512/50000]\tLoss: 1.4811\tLR: 0.015000\n",
      "Training Epoch: 9 [640/50000]\tLoss: 1.6583\tLR: 0.015000\n",
      "Training Epoch: 9 [768/50000]\tLoss: 1.5877\tLR: 0.015000\n",
      "Training Epoch: 9 [896/50000]\tLoss: 1.8138\tLR: 0.015000\n",
      "Training Epoch: 9 [1024/50000]\tLoss: 1.6814\tLR: 0.015000\n",
      "Training Epoch: 9 [1152/50000]\tLoss: 1.5607\tLR: 0.015000\n",
      "Training Epoch: 9 [1280/50000]\tLoss: 1.8327\tLR: 0.015000\n",
      "Training Epoch: 9 [1408/50000]\tLoss: 1.6329\tLR: 0.015000\n",
      "Training Epoch: 9 [1536/50000]\tLoss: 1.5611\tLR: 0.015000\n",
      "Training Epoch: 9 [1664/50000]\tLoss: 1.7041\tLR: 0.015000\n",
      "Training Epoch: 9 [1792/50000]\tLoss: 1.7007\tLR: 0.015000\n",
      "Training Epoch: 9 [1920/50000]\tLoss: 1.5596\tLR: 0.015000\n",
      "Training Epoch: 9 [2048/50000]\tLoss: 1.6415\tLR: 0.015000\n",
      "Training Epoch: 9 [2176/50000]\tLoss: 1.5868\tLR: 0.015000\n",
      "Training Epoch: 9 [2304/50000]\tLoss: 1.8329\tLR: 0.015000\n",
      "Training Epoch: 9 [2432/50000]\tLoss: 1.5945\tLR: 0.015000\n",
      "Training Epoch: 9 [2560/50000]\tLoss: 1.5967\tLR: 0.015000\n",
      "Training Epoch: 9 [2688/50000]\tLoss: 1.5312\tLR: 0.015000\n",
      "Training Epoch: 9 [2816/50000]\tLoss: 1.5568\tLR: 0.015000\n",
      "Training Epoch: 9 [2944/50000]\tLoss: 1.7911\tLR: 0.015000\n",
      "Training Epoch: 9 [3072/50000]\tLoss: 1.5150\tLR: 0.015000\n",
      "Training Epoch: 9 [3200/50000]\tLoss: 1.5937\tLR: 0.015000\n",
      "Training Epoch: 9 [3328/50000]\tLoss: 1.6370\tLR: 0.015000\n",
      "Training Epoch: 9 [3456/50000]\tLoss: 1.5236\tLR: 0.015000\n",
      "Training Epoch: 9 [3584/50000]\tLoss: 1.6309\tLR: 0.015000\n",
      "Training Epoch: 9 [3712/50000]\tLoss: 1.5744\tLR: 0.015000\n",
      "Training Epoch: 9 [3840/50000]\tLoss: 1.7337\tLR: 0.015000\n",
      "Training Epoch: 9 [3968/50000]\tLoss: 1.8179\tLR: 0.015000\n",
      "Training Epoch: 9 [4096/50000]\tLoss: 1.6894\tLR: 0.015000\n",
      "Training Epoch: 9 [4224/50000]\tLoss: 1.5426\tLR: 0.015000\n",
      "Training Epoch: 9 [4352/50000]\tLoss: 1.5904\tLR: 0.015000\n",
      "Training Epoch: 9 [4480/50000]\tLoss: 1.7005\tLR: 0.015000\n",
      "Training Epoch: 9 [4608/50000]\tLoss: 1.6663\tLR: 0.015000\n",
      "Training Epoch: 9 [4736/50000]\tLoss: 1.6848\tLR: 0.015000\n",
      "Training Epoch: 9 [4864/50000]\tLoss: 1.4104\tLR: 0.015000\n",
      "Training Epoch: 9 [4992/50000]\tLoss: 1.9215\tLR: 0.015000\n",
      "Training Epoch: 9 [5120/50000]\tLoss: 1.7075\tLR: 0.015000\n",
      "Training Epoch: 9 [5248/50000]\tLoss: 1.7336\tLR: 0.015000\n",
      "Training Epoch: 9 [5376/50000]\tLoss: 1.6507\tLR: 0.015000\n",
      "Training Epoch: 9 [5504/50000]\tLoss: 1.7915\tLR: 0.015000\n",
      "Training Epoch: 9 [5632/50000]\tLoss: 1.5073\tLR: 0.015000\n",
      "Training Epoch: 9 [5760/50000]\tLoss: 1.3828\tLR: 0.015000\n",
      "Training Epoch: 9 [5888/50000]\tLoss: 1.5121\tLR: 0.015000\n",
      "Training Epoch: 9 [6016/50000]\tLoss: 1.6768\tLR: 0.015000\n",
      "Training Epoch: 9 [6144/50000]\tLoss: 1.5866\tLR: 0.015000\n",
      "Training Epoch: 9 [6272/50000]\tLoss: 1.7842\tLR: 0.015000\n",
      "Training Epoch: 9 [6400/50000]\tLoss: 1.7239\tLR: 0.015000\n",
      "Training Epoch: 9 [6528/50000]\tLoss: 1.4949\tLR: 0.015000\n",
      "Training Epoch: 9 [6656/50000]\tLoss: 1.6913\tLR: 0.015000\n",
      "Training Epoch: 9 [6784/50000]\tLoss: 1.7798\tLR: 0.015000\n",
      "Training Epoch: 9 [6912/50000]\tLoss: 1.5286\tLR: 0.015000\n",
      "Training Epoch: 9 [7040/50000]\tLoss: 1.4726\tLR: 0.015000\n",
      "Training Epoch: 9 [7168/50000]\tLoss: 1.7316\tLR: 0.015000\n",
      "Training Epoch: 9 [7296/50000]\tLoss: 1.4675\tLR: 0.015000\n",
      "Training Epoch: 9 [7424/50000]\tLoss: 1.5345\tLR: 0.015000\n",
      "Training Epoch: 9 [7552/50000]\tLoss: 1.4030\tLR: 0.015000\n",
      "Training Epoch: 9 [7680/50000]\tLoss: 1.4695\tLR: 0.015000\n",
      "Training Epoch: 9 [7808/50000]\tLoss: 1.6995\tLR: 0.015000\n",
      "Training Epoch: 9 [7936/50000]\tLoss: 1.7117\tLR: 0.015000\n",
      "Training Epoch: 9 [8064/50000]\tLoss: 1.5070\tLR: 0.015000\n",
      "Training Epoch: 9 [8192/50000]\tLoss: 1.4862\tLR: 0.015000\n",
      "Training Epoch: 9 [8320/50000]\tLoss: 1.5286\tLR: 0.015000\n",
      "Training Epoch: 9 [8448/50000]\tLoss: 1.6002\tLR: 0.015000\n",
      "Training Epoch: 9 [8576/50000]\tLoss: 1.6021\tLR: 0.015000\n",
      "Training Epoch: 9 [8704/50000]\tLoss: 1.5326\tLR: 0.015000\n",
      "Training Epoch: 9 [8832/50000]\tLoss: 1.5793\tLR: 0.015000\n",
      "Training Epoch: 9 [8960/50000]\tLoss: 1.4203\tLR: 0.015000\n",
      "Training Epoch: 9 [9088/50000]\tLoss: 1.6531\tLR: 0.015000\n",
      "Training Epoch: 9 [9216/50000]\tLoss: 1.5378\tLR: 0.015000\n",
      "Training Epoch: 9 [9344/50000]\tLoss: 1.5084\tLR: 0.015000\n",
      "Training Epoch: 9 [9472/50000]\tLoss: 1.5705\tLR: 0.015000\n",
      "Training Epoch: 9 [9600/50000]\tLoss: 1.5415\tLR: 0.015000\n",
      "Training Epoch: 9 [9728/50000]\tLoss: 1.6137\tLR: 0.015000\n",
      "Training Epoch: 9 [9856/50000]\tLoss: 1.5829\tLR: 0.015000\n",
      "Training Epoch: 9 [9984/50000]\tLoss: 1.4135\tLR: 0.015000\n",
      "Training Epoch: 9 [10112/50000]\tLoss: 1.4848\tLR: 0.015000\n",
      "Training Epoch: 9 [10240/50000]\tLoss: 1.7045\tLR: 0.015000\n",
      "Training Epoch: 9 [10368/50000]\tLoss: 1.6136\tLR: 0.015000\n",
      "Training Epoch: 9 [10496/50000]\tLoss: 1.7687\tLR: 0.015000\n",
      "Training Epoch: 9 [10624/50000]\tLoss: 1.6512\tLR: 0.015000\n",
      "Training Epoch: 9 [10752/50000]\tLoss: 1.5478\tLR: 0.015000\n",
      "Training Epoch: 9 [10880/50000]\tLoss: 1.5092\tLR: 0.015000\n",
      "Training Epoch: 9 [11008/50000]\tLoss: 1.4778\tLR: 0.015000\n",
      "Training Epoch: 9 [11136/50000]\tLoss: 1.4207\tLR: 0.015000\n",
      "Training Epoch: 9 [11264/50000]\tLoss: 1.5292\tLR: 0.015000\n",
      "Training Epoch: 9 [11392/50000]\tLoss: 1.6850\tLR: 0.015000\n",
      "Training Epoch: 9 [11520/50000]\tLoss: 1.4935\tLR: 0.015000\n",
      "Training Epoch: 9 [11648/50000]\tLoss: 1.5892\tLR: 0.015000\n",
      "Training Epoch: 9 [11776/50000]\tLoss: 1.8152\tLR: 0.015000\n",
      "Training Epoch: 9 [11904/50000]\tLoss: 1.7609\tLR: 0.015000\n",
      "Training Epoch: 9 [12032/50000]\tLoss: 1.4564\tLR: 0.015000\n",
      "Training Epoch: 9 [12160/50000]\tLoss: 1.7317\tLR: 0.015000\n",
      "Training Epoch: 9 [12288/50000]\tLoss: 1.8888\tLR: 0.015000\n",
      "Training Epoch: 9 [12416/50000]\tLoss: 1.6827\tLR: 0.015000\n",
      "Training Epoch: 9 [12544/50000]\tLoss: 1.6146\tLR: 0.015000\n",
      "Training Epoch: 9 [12672/50000]\tLoss: 1.5728\tLR: 0.015000\n",
      "Training Epoch: 9 [12800/50000]\tLoss: 1.5102\tLR: 0.015000\n",
      "Training Epoch: 9 [12928/50000]\tLoss: 1.6835\tLR: 0.015000\n",
      "Training Epoch: 9 [13056/50000]\tLoss: 1.4483\tLR: 0.015000\n",
      "Training Epoch: 9 [13184/50000]\tLoss: 1.6985\tLR: 0.015000\n",
      "Training Epoch: 9 [13312/50000]\tLoss: 1.3520\tLR: 0.015000\n",
      "Training Epoch: 9 [13440/50000]\tLoss: 1.6747\tLR: 0.015000\n",
      "Training Epoch: 9 [13568/50000]\tLoss: 1.4026\tLR: 0.015000\n",
      "Training Epoch: 9 [13696/50000]\tLoss: 1.6073\tLR: 0.015000\n",
      "Training Epoch: 9 [13824/50000]\tLoss: 1.5396\tLR: 0.015000\n",
      "Training Epoch: 9 [13952/50000]\tLoss: 1.5029\tLR: 0.015000\n",
      "Training Epoch: 9 [14080/50000]\tLoss: 1.7428\tLR: 0.015000\n",
      "Training Epoch: 9 [14208/50000]\tLoss: 1.5805\tLR: 0.015000\n",
      "Training Epoch: 9 [14336/50000]\tLoss: 1.4014\tLR: 0.015000\n",
      "Training Epoch: 9 [14464/50000]\tLoss: 1.6314\tLR: 0.015000\n",
      "Training Epoch: 9 [14592/50000]\tLoss: 1.8429\tLR: 0.015000\n",
      "Training Epoch: 9 [14720/50000]\tLoss: 1.9471\tLR: 0.015000\n",
      "Training Epoch: 9 [14848/50000]\tLoss: 1.6232\tLR: 0.015000\n",
      "Training Epoch: 9 [14976/50000]\tLoss: 1.7865\tLR: 0.015000\n",
      "Training Epoch: 9 [15104/50000]\tLoss: 1.5391\tLR: 0.015000\n",
      "Training Epoch: 9 [15232/50000]\tLoss: 1.3825\tLR: 0.015000\n",
      "Training Epoch: 9 [15360/50000]\tLoss: 1.6529\tLR: 0.015000\n",
      "Training Epoch: 9 [15488/50000]\tLoss: 1.9546\tLR: 0.015000\n",
      "Training Epoch: 9 [15616/50000]\tLoss: 1.6828\tLR: 0.015000\n",
      "Training Epoch: 9 [15744/50000]\tLoss: 1.5675\tLR: 0.015000\n",
      "Training Epoch: 9 [15872/50000]\tLoss: 1.6760\tLR: 0.015000\n",
      "Training Epoch: 9 [16000/50000]\tLoss: 1.3974\tLR: 0.015000\n",
      "Training Epoch: 9 [16128/50000]\tLoss: 1.7098\tLR: 0.015000\n",
      "Training Epoch: 9 [16256/50000]\tLoss: 1.7129\tLR: 0.015000\n",
      "Training Epoch: 9 [16384/50000]\tLoss: 1.5477\tLR: 0.015000\n",
      "Training Epoch: 9 [16512/50000]\tLoss: 1.5040\tLR: 0.015000\n",
      "Training Epoch: 9 [16640/50000]\tLoss: 1.5634\tLR: 0.015000\n",
      "Training Epoch: 9 [16768/50000]\tLoss: 1.5182\tLR: 0.015000\n",
      "Training Epoch: 9 [16896/50000]\tLoss: 1.7635\tLR: 0.015000\n",
      "Training Epoch: 9 [17024/50000]\tLoss: 1.7021\tLR: 0.015000\n",
      "Training Epoch: 9 [17152/50000]\tLoss: 1.6718\tLR: 0.015000\n",
      "Training Epoch: 9 [17280/50000]\tLoss: 1.4750\tLR: 0.015000\n",
      "Training Epoch: 9 [17408/50000]\tLoss: 1.5463\tLR: 0.015000\n",
      "Training Epoch: 9 [17536/50000]\tLoss: 1.7499\tLR: 0.015000\n",
      "Training Epoch: 9 [17664/50000]\tLoss: 1.5568\tLR: 0.015000\n",
      "Training Epoch: 9 [17792/50000]\tLoss: 1.6841\tLR: 0.015000\n",
      "Training Epoch: 9 [17920/50000]\tLoss: 1.6593\tLR: 0.015000\n",
      "Training Epoch: 9 [18048/50000]\tLoss: 1.5874\tLR: 0.015000\n",
      "Training Epoch: 9 [18176/50000]\tLoss: 1.4777\tLR: 0.015000\n",
      "Training Epoch: 9 [18304/50000]\tLoss: 1.4673\tLR: 0.015000\n",
      "Training Epoch: 9 [18432/50000]\tLoss: 1.4951\tLR: 0.015000\n",
      "Training Epoch: 9 [18560/50000]\tLoss: 1.4100\tLR: 0.015000\n",
      "Training Epoch: 9 [18688/50000]\tLoss: 1.4772\tLR: 0.015000\n",
      "Training Epoch: 9 [18816/50000]\tLoss: 1.5274\tLR: 0.015000\n",
      "Training Epoch: 9 [18944/50000]\tLoss: 1.4711\tLR: 0.015000\n",
      "Training Epoch: 9 [19072/50000]\tLoss: 1.6583\tLR: 0.015000\n",
      "Training Epoch: 9 [19200/50000]\tLoss: 1.3093\tLR: 0.015000\n",
      "Training Epoch: 9 [19328/50000]\tLoss: 1.5152\tLR: 0.015000\n",
      "Training Epoch: 9 [19456/50000]\tLoss: 1.8028\tLR: 0.015000\n",
      "Training Epoch: 9 [19584/50000]\tLoss: 1.5172\tLR: 0.015000\n",
      "Training Epoch: 9 [19712/50000]\tLoss: 1.8289\tLR: 0.015000\n",
      "Training Epoch: 9 [19840/50000]\tLoss: 1.4568\tLR: 0.015000\n",
      "Training Epoch: 9 [19968/50000]\tLoss: 1.2243\tLR: 0.015000\n",
      "Training Epoch: 9 [20096/50000]\tLoss: 1.5344\tLR: 0.015000\n",
      "Training Epoch: 9 [20224/50000]\tLoss: 1.8018\tLR: 0.015000\n",
      "Training Epoch: 9 [20352/50000]\tLoss: 1.6442\tLR: 0.015000\n",
      "Training Epoch: 9 [20480/50000]\tLoss: 1.6720\tLR: 0.015000\n",
      "Training Epoch: 9 [20608/50000]\tLoss: 1.5290\tLR: 0.015000\n",
      "Training Epoch: 9 [20736/50000]\tLoss: 1.8736\tLR: 0.015000\n",
      "Training Epoch: 9 [20864/50000]\tLoss: 1.5616\tLR: 0.015000\n",
      "Training Epoch: 9 [20992/50000]\tLoss: 1.7314\tLR: 0.015000\n",
      "Training Epoch: 9 [21120/50000]\tLoss: 1.5239\tLR: 0.015000\n",
      "Training Epoch: 9 [21248/50000]\tLoss: 1.4963\tLR: 0.015000\n",
      "Training Epoch: 9 [21376/50000]\tLoss: 1.2614\tLR: 0.015000\n",
      "Training Epoch: 9 [21504/50000]\tLoss: 1.5451\tLR: 0.015000\n",
      "Training Epoch: 9 [21632/50000]\tLoss: 1.5964\tLR: 0.015000\n",
      "Training Epoch: 9 [21760/50000]\tLoss: 1.5699\tLR: 0.015000\n",
      "Training Epoch: 9 [21888/50000]\tLoss: 1.4712\tLR: 0.015000\n",
      "Training Epoch: 9 [22016/50000]\tLoss: 1.5546\tLR: 0.015000\n",
      "Training Epoch: 9 [22144/50000]\tLoss: 1.7844\tLR: 0.015000\n",
      "Training Epoch: 9 [22272/50000]\tLoss: 1.6194\tLR: 0.015000\n",
      "Training Epoch: 9 [22400/50000]\tLoss: 1.5993\tLR: 0.015000\n",
      "Training Epoch: 9 [22528/50000]\tLoss: 1.8062\tLR: 0.015000\n",
      "Training Epoch: 9 [22656/50000]\tLoss: 1.6150\tLR: 0.015000\n",
      "Training Epoch: 9 [22784/50000]\tLoss: 1.5210\tLR: 0.015000\n",
      "Training Epoch: 9 [22912/50000]\tLoss: 1.5073\tLR: 0.015000\n",
      "Training Epoch: 9 [23040/50000]\tLoss: 1.4467\tLR: 0.015000\n",
      "Training Epoch: 9 [23168/50000]\tLoss: 1.7502\tLR: 0.015000\n",
      "Training Epoch: 9 [23296/50000]\tLoss: 1.3819\tLR: 0.015000\n",
      "Training Epoch: 9 [23424/50000]\tLoss: 1.6166\tLR: 0.015000\n",
      "Training Epoch: 9 [23552/50000]\tLoss: 1.8318\tLR: 0.015000\n",
      "Training Epoch: 9 [23680/50000]\tLoss: 1.6260\tLR: 0.015000\n",
      "Training Epoch: 9 [23808/50000]\tLoss: 1.2828\tLR: 0.015000\n",
      "Training Epoch: 9 [23936/50000]\tLoss: 1.7402\tLR: 0.015000\n",
      "Training Epoch: 9 [24064/50000]\tLoss: 1.6534\tLR: 0.015000\n",
      "Training Epoch: 9 [24192/50000]\tLoss: 1.4830\tLR: 0.015000\n",
      "Training Epoch: 9 [24320/50000]\tLoss: 1.7104\tLR: 0.015000\n",
      "Training Epoch: 9 [24448/50000]\tLoss: 1.4188\tLR: 0.015000\n",
      "Training Epoch: 9 [24576/50000]\tLoss: 1.4944\tLR: 0.015000\n",
      "Training Epoch: 9 [24704/50000]\tLoss: 1.6540\tLR: 0.015000\n",
      "Training Epoch: 9 [24832/50000]\tLoss: 1.5349\tLR: 0.015000\n",
      "Training Epoch: 9 [24960/50000]\tLoss: 1.5035\tLR: 0.015000\n",
      "Training Epoch: 9 [25088/50000]\tLoss: 1.8806\tLR: 0.015000\n",
      "Training Epoch: 9 [25216/50000]\tLoss: 1.6797\tLR: 0.015000\n",
      "Training Epoch: 9 [25344/50000]\tLoss: 1.8451\tLR: 0.015000\n",
      "Training Epoch: 9 [25472/50000]\tLoss: 1.6930\tLR: 0.015000\n",
      "Training Epoch: 9 [25600/50000]\tLoss: 1.5851\tLR: 0.015000\n",
      "Training Epoch: 9 [25728/50000]\tLoss: 1.7671\tLR: 0.015000\n",
      "Training Epoch: 9 [25856/50000]\tLoss: 1.7587\tLR: 0.015000\n",
      "Training Epoch: 9 [25984/50000]\tLoss: 1.3494\tLR: 0.015000\n",
      "Training Epoch: 9 [26112/50000]\tLoss: 1.5921\tLR: 0.015000\n",
      "Training Epoch: 9 [26240/50000]\tLoss: 1.7752\tLR: 0.015000\n",
      "Training Epoch: 9 [26368/50000]\tLoss: 1.8999\tLR: 0.015000\n",
      "Training Epoch: 9 [26496/50000]\tLoss: 1.6043\tLR: 0.015000\n",
      "Training Epoch: 9 [26624/50000]\tLoss: 1.6717\tLR: 0.015000\n",
      "Training Epoch: 9 [26752/50000]\tLoss: 1.6538\tLR: 0.015000\n",
      "Training Epoch: 9 [26880/50000]\tLoss: 1.4300\tLR: 0.015000\n",
      "Training Epoch: 9 [27008/50000]\tLoss: 1.7299\tLR: 0.015000\n",
      "Training Epoch: 9 [27136/50000]\tLoss: 1.6180\tLR: 0.015000\n",
      "Training Epoch: 9 [27264/50000]\tLoss: 1.7081\tLR: 0.015000\n",
      "Training Epoch: 9 [27392/50000]\tLoss: 1.8431\tLR: 0.015000\n",
      "Training Epoch: 9 [27520/50000]\tLoss: 1.6791\tLR: 0.015000\n",
      "Training Epoch: 9 [27648/50000]\tLoss: 1.6236\tLR: 0.015000\n",
      "Training Epoch: 9 [27776/50000]\tLoss: 1.6404\tLR: 0.015000\n",
      "Training Epoch: 9 [27904/50000]\tLoss: 1.8482\tLR: 0.015000\n",
      "Training Epoch: 9 [28032/50000]\tLoss: 1.7135\tLR: 0.015000\n",
      "Training Epoch: 9 [28160/50000]\tLoss: 1.6512\tLR: 0.015000\n",
      "Training Epoch: 9 [28288/50000]\tLoss: 1.8314\tLR: 0.015000\n",
      "Training Epoch: 9 [28416/50000]\tLoss: 1.7620\tLR: 0.015000\n",
      "Training Epoch: 9 [28544/50000]\tLoss: 1.5647\tLR: 0.015000\n",
      "Training Epoch: 9 [28672/50000]\tLoss: 1.4731\tLR: 0.015000\n",
      "Training Epoch: 9 [28800/50000]\tLoss: 1.5122\tLR: 0.015000\n",
      "Training Epoch: 9 [28928/50000]\tLoss: 1.3836\tLR: 0.015000\n",
      "Training Epoch: 9 [29056/50000]\tLoss: 1.4351\tLR: 0.015000\n",
      "Training Epoch: 9 [29184/50000]\tLoss: 1.6748\tLR: 0.015000\n",
      "Training Epoch: 9 [29312/50000]\tLoss: 1.3492\tLR: 0.015000\n",
      "Training Epoch: 9 [29440/50000]\tLoss: 1.5293\tLR: 0.015000\n",
      "Training Epoch: 9 [29568/50000]\tLoss: 1.7628\tLR: 0.015000\n",
      "Training Epoch: 9 [29696/50000]\tLoss: 1.6797\tLR: 0.015000\n",
      "Training Epoch: 9 [29824/50000]\tLoss: 1.6836\tLR: 0.015000\n",
      "Training Epoch: 9 [29952/50000]\tLoss: 1.1862\tLR: 0.015000\n",
      "Training Epoch: 9 [30080/50000]\tLoss: 1.4647\tLR: 0.015000\n",
      "Training Epoch: 9 [30208/50000]\tLoss: 1.6723\tLR: 0.015000\n",
      "Training Epoch: 9 [30336/50000]\tLoss: 1.7724\tLR: 0.015000\n",
      "Training Epoch: 9 [30464/50000]\tLoss: 1.5107\tLR: 0.015000\n",
      "Training Epoch: 9 [30592/50000]\tLoss: 1.5399\tLR: 0.015000\n",
      "Training Epoch: 9 [30720/50000]\tLoss: 1.5870\tLR: 0.015000\n",
      "Training Epoch: 9 [30848/50000]\tLoss: 1.3480\tLR: 0.015000\n",
      "Training Epoch: 9 [30976/50000]\tLoss: 1.7196\tLR: 0.015000\n",
      "Training Epoch: 9 [31104/50000]\tLoss: 1.6106\tLR: 0.015000\n",
      "Training Epoch: 9 [31232/50000]\tLoss: 1.6186\tLR: 0.015000\n",
      "Training Epoch: 9 [31360/50000]\tLoss: 1.7082\tLR: 0.015000\n",
      "Training Epoch: 9 [31488/50000]\tLoss: 1.6760\tLR: 0.015000\n",
      "Training Epoch: 9 [31616/50000]\tLoss: 1.6595\tLR: 0.015000\n",
      "Training Epoch: 9 [31744/50000]\tLoss: 1.5578\tLR: 0.015000\n",
      "Training Epoch: 9 [31872/50000]\tLoss: 1.7882\tLR: 0.015000\n",
      "Training Epoch: 9 [32000/50000]\tLoss: 1.7888\tLR: 0.015000\n",
      "Training Epoch: 9 [32128/50000]\tLoss: 1.4232\tLR: 0.015000\n",
      "Training Epoch: 9 [32256/50000]\tLoss: 1.5412\tLR: 0.015000\n",
      "Training Epoch: 9 [32384/50000]\tLoss: 1.6359\tLR: 0.015000\n",
      "Training Epoch: 9 [32512/50000]\tLoss: 1.5517\tLR: 0.015000\n",
      "Training Epoch: 9 [32640/50000]\tLoss: 1.7592\tLR: 0.015000\n",
      "Training Epoch: 9 [32768/50000]\tLoss: 1.5242\tLR: 0.015000\n",
      "Training Epoch: 9 [32896/50000]\tLoss: 1.5732\tLR: 0.015000\n",
      "Training Epoch: 9 [33024/50000]\tLoss: 1.6303\tLR: 0.015000\n",
      "Training Epoch: 9 [33152/50000]\tLoss: 1.5043\tLR: 0.015000\n",
      "Training Epoch: 9 [33280/50000]\tLoss: 1.6363\tLR: 0.015000\n",
      "Training Epoch: 9 [33408/50000]\tLoss: 1.5061\tLR: 0.015000\n",
      "Training Epoch: 9 [33536/50000]\tLoss: 1.5037\tLR: 0.015000\n",
      "Training Epoch: 9 [33664/50000]\tLoss: 1.4467\tLR: 0.015000\n",
      "Training Epoch: 9 [33792/50000]\tLoss: 1.7207\tLR: 0.015000\n",
      "Training Epoch: 9 [33920/50000]\tLoss: 1.5988\tLR: 0.015000\n",
      "Training Epoch: 9 [34048/50000]\tLoss: 1.7015\tLR: 0.015000\n",
      "Training Epoch: 9 [34176/50000]\tLoss: 1.6573\tLR: 0.015000\n",
      "Training Epoch: 9 [34304/50000]\tLoss: 1.5729\tLR: 0.015000\n",
      "Training Epoch: 9 [34432/50000]\tLoss: 1.4897\tLR: 0.015000\n",
      "Training Epoch: 9 [34560/50000]\tLoss: 1.7193\tLR: 0.015000\n",
      "Training Epoch: 9 [34688/50000]\tLoss: 1.6477\tLR: 0.015000\n",
      "Training Epoch: 9 [34816/50000]\tLoss: 1.7478\tLR: 0.015000\n",
      "Training Epoch: 9 [34944/50000]\tLoss: 1.7975\tLR: 0.015000\n",
      "Training Epoch: 9 [35072/50000]\tLoss: 1.5105\tLR: 0.015000\n",
      "Training Epoch: 9 [35200/50000]\tLoss: 1.4964\tLR: 0.015000\n",
      "Training Epoch: 9 [35328/50000]\tLoss: 1.5018\tLR: 0.015000\n",
      "Training Epoch: 9 [35456/50000]\tLoss: 1.8250\tLR: 0.015000\n",
      "Training Epoch: 9 [35584/50000]\tLoss: 1.5361\tLR: 0.015000\n",
      "Training Epoch: 9 [35712/50000]\tLoss: 1.4949\tLR: 0.015000\n",
      "Training Epoch: 9 [35840/50000]\tLoss: 1.7421\tLR: 0.015000\n",
      "Training Epoch: 9 [35968/50000]\tLoss: 1.4599\tLR: 0.015000\n",
      "Training Epoch: 9 [36096/50000]\tLoss: 1.6884\tLR: 0.015000\n",
      "Training Epoch: 9 [36224/50000]\tLoss: 1.6240\tLR: 0.015000\n",
      "Training Epoch: 9 [36352/50000]\tLoss: 1.5999\tLR: 0.015000\n",
      "Training Epoch: 9 [36480/50000]\tLoss: 1.7661\tLR: 0.015000\n",
      "Training Epoch: 9 [36608/50000]\tLoss: 1.4641\tLR: 0.015000\n",
      "Training Epoch: 9 [36736/50000]\tLoss: 1.6338\tLR: 0.015000\n",
      "Training Epoch: 9 [36864/50000]\tLoss: 1.5931\tLR: 0.015000\n",
      "Training Epoch: 9 [36992/50000]\tLoss: 1.4127\tLR: 0.015000\n",
      "Training Epoch: 9 [37120/50000]\tLoss: 1.4119\tLR: 0.015000\n",
      "Training Epoch: 9 [37248/50000]\tLoss: 1.6733\tLR: 0.015000\n",
      "Training Epoch: 9 [37376/50000]\tLoss: 1.6625\tLR: 0.015000\n",
      "Training Epoch: 9 [37504/50000]\tLoss: 1.7911\tLR: 0.015000\n",
      "Training Epoch: 9 [37632/50000]\tLoss: 1.8137\tLR: 0.015000\n",
      "Training Epoch: 9 [37760/50000]\tLoss: 1.5059\tLR: 0.015000\n",
      "Training Epoch: 9 [37888/50000]\tLoss: 1.4986\tLR: 0.015000\n",
      "Training Epoch: 9 [38016/50000]\tLoss: 1.7249\tLR: 0.015000\n",
      "Training Epoch: 9 [38144/50000]\tLoss: 1.5464\tLR: 0.015000\n",
      "Training Epoch: 9 [38272/50000]\tLoss: 1.7837\tLR: 0.015000\n",
      "Training Epoch: 9 [38400/50000]\tLoss: 1.5401\tLR: 0.015000\n",
      "Training Epoch: 9 [38528/50000]\tLoss: 1.4301\tLR: 0.015000\n",
      "Training Epoch: 9 [38656/50000]\tLoss: 1.8271\tLR: 0.015000\n",
      "Training Epoch: 9 [38784/50000]\tLoss: 1.3527\tLR: 0.015000\n",
      "Training Epoch: 9 [38912/50000]\tLoss: 1.5524\tLR: 0.015000\n",
      "Training Epoch: 9 [39040/50000]\tLoss: 1.5389\tLR: 0.015000\n",
      "Training Epoch: 9 [39168/50000]\tLoss: 1.4485\tLR: 0.015000\n",
      "Training Epoch: 9 [39296/50000]\tLoss: 1.6632\tLR: 0.015000\n",
      "Training Epoch: 9 [39424/50000]\tLoss: 1.4841\tLR: 0.015000\n",
      "Training Epoch: 9 [39552/50000]\tLoss: 1.6693\tLR: 0.015000\n",
      "Training Epoch: 9 [39680/50000]\tLoss: 1.7895\tLR: 0.015000\n",
      "Training Epoch: 9 [39808/50000]\tLoss: 1.6810\tLR: 0.015000\n",
      "Training Epoch: 9 [39936/50000]\tLoss: 1.6409\tLR: 0.015000\n",
      "Training Epoch: 9 [40064/50000]\tLoss: 1.4048\tLR: 0.015000\n",
      "Training Epoch: 9 [40192/50000]\tLoss: 1.5940\tLR: 0.015000\n",
      "Training Epoch: 9 [40320/50000]\tLoss: 1.7920\tLR: 0.015000\n",
      "Training Epoch: 9 [40448/50000]\tLoss: 1.3837\tLR: 0.015000\n",
      "Training Epoch: 9 [40576/50000]\tLoss: 1.4047\tLR: 0.015000\n",
      "Training Epoch: 9 [40704/50000]\tLoss: 1.4050\tLR: 0.015000\n",
      "Training Epoch: 9 [40832/50000]\tLoss: 1.4928\tLR: 0.015000\n",
      "Training Epoch: 9 [40960/50000]\tLoss: 1.5697\tLR: 0.015000\n",
      "Training Epoch: 9 [41088/50000]\tLoss: 1.7375\tLR: 0.015000\n",
      "Training Epoch: 9 [41216/50000]\tLoss: 1.5735\tLR: 0.015000\n",
      "Training Epoch: 9 [41344/50000]\tLoss: 1.5484\tLR: 0.015000\n",
      "Training Epoch: 9 [41472/50000]\tLoss: 1.4069\tLR: 0.015000\n",
      "Training Epoch: 9 [41600/50000]\tLoss: 1.6746\tLR: 0.015000\n",
      "Training Epoch: 9 [41728/50000]\tLoss: 1.6405\tLR: 0.015000\n",
      "Training Epoch: 9 [41856/50000]\tLoss: 1.5009\tLR: 0.015000\n",
      "Training Epoch: 9 [41984/50000]\tLoss: 1.5387\tLR: 0.015000\n",
      "Training Epoch: 9 [42112/50000]\tLoss: 1.7042\tLR: 0.015000\n",
      "Training Epoch: 9 [42240/50000]\tLoss: 1.3400\tLR: 0.015000\n",
      "Training Epoch: 9 [42368/50000]\tLoss: 1.6141\tLR: 0.015000\n",
      "Training Epoch: 9 [42496/50000]\tLoss: 1.8408\tLR: 0.015000\n",
      "Training Epoch: 9 [42624/50000]\tLoss: 1.7550\tLR: 0.015000\n",
      "Training Epoch: 9 [42752/50000]\tLoss: 1.6006\tLR: 0.015000\n",
      "Training Epoch: 9 [42880/50000]\tLoss: 1.5910\tLR: 0.015000\n",
      "Training Epoch: 9 [43008/50000]\tLoss: 1.6106\tLR: 0.015000\n",
      "Training Epoch: 9 [43136/50000]\tLoss: 1.5837\tLR: 0.015000\n",
      "Training Epoch: 9 [43264/50000]\tLoss: 1.5257\tLR: 0.015000\n",
      "Training Epoch: 9 [43392/50000]\tLoss: 1.2004\tLR: 0.015000\n",
      "Training Epoch: 9 [43520/50000]\tLoss: 1.3654\tLR: 0.015000\n",
      "Training Epoch: 9 [43648/50000]\tLoss: 1.2703\tLR: 0.015000\n",
      "Training Epoch: 9 [43776/50000]\tLoss: 1.9844\tLR: 0.015000\n",
      "Training Epoch: 9 [43904/50000]\tLoss: 1.5510\tLR: 0.015000\n",
      "Training Epoch: 9 [44032/50000]\tLoss: 1.4090\tLR: 0.015000\n",
      "Training Epoch: 9 [44160/50000]\tLoss: 1.7407\tLR: 0.015000\n",
      "Training Epoch: 9 [44288/50000]\tLoss: 1.5147\tLR: 0.015000\n",
      "Training Epoch: 9 [44416/50000]\tLoss: 1.5261\tLR: 0.015000\n",
      "Training Epoch: 9 [44544/50000]\tLoss: 1.6445\tLR: 0.015000\n",
      "Training Epoch: 9 [44672/50000]\tLoss: 1.5888\tLR: 0.015000\n",
      "Training Epoch: 9 [44800/50000]\tLoss: 1.6656\tLR: 0.015000\n",
      "Training Epoch: 9 [44928/50000]\tLoss: 1.7045\tLR: 0.015000\n",
      "Training Epoch: 9 [45056/50000]\tLoss: 1.5815\tLR: 0.015000\n",
      "Training Epoch: 9 [45184/50000]\tLoss: 1.6122\tLR: 0.015000\n",
      "Training Epoch: 9 [45312/50000]\tLoss: 1.6289\tLR: 0.015000\n",
      "Training Epoch: 9 [45440/50000]\tLoss: 1.8293\tLR: 0.015000\n",
      "Training Epoch: 9 [45568/50000]\tLoss: 1.6033\tLR: 0.015000\n",
      "Training Epoch: 9 [45696/50000]\tLoss: 1.5824\tLR: 0.015000\n",
      "Training Epoch: 9 [45824/50000]\tLoss: 1.6944\tLR: 0.015000\n",
      "Training Epoch: 9 [45952/50000]\tLoss: 1.5144\tLR: 0.015000\n",
      "Training Epoch: 9 [46080/50000]\tLoss: 1.8150\tLR: 0.015000\n",
      "Training Epoch: 9 [46208/50000]\tLoss: 1.4499\tLR: 0.015000\n",
      "Training Epoch: 9 [46336/50000]\tLoss: 1.6985\tLR: 0.015000\n",
      "Training Epoch: 9 [46464/50000]\tLoss: 1.5748\tLR: 0.015000\n",
      "Training Epoch: 9 [46592/50000]\tLoss: 1.6529\tLR: 0.015000\n",
      "Training Epoch: 9 [46720/50000]\tLoss: 1.5700\tLR: 0.015000\n",
      "Training Epoch: 9 [46848/50000]\tLoss: 1.4306\tLR: 0.015000\n",
      "Training Epoch: 9 [46976/50000]\tLoss: 1.5193\tLR: 0.015000\n",
      "Training Epoch: 9 [47104/50000]\tLoss: 1.5742\tLR: 0.015000\n",
      "Training Epoch: 9 [47232/50000]\tLoss: 1.8672\tLR: 0.015000\n",
      "Training Epoch: 9 [47360/50000]\tLoss: 1.8264\tLR: 0.015000\n",
      "Training Epoch: 9 [47488/50000]\tLoss: 1.6656\tLR: 0.015000\n",
      "Training Epoch: 9 [47616/50000]\tLoss: 1.3634\tLR: 0.015000\n",
      "Training Epoch: 9 [47744/50000]\tLoss: 1.5477\tLR: 0.015000\n",
      "Training Epoch: 9 [47872/50000]\tLoss: 1.7812\tLR: 0.015000\n",
      "Training Epoch: 9 [48000/50000]\tLoss: 1.4499\tLR: 0.015000\n",
      "Training Epoch: 9 [48128/50000]\tLoss: 1.4799\tLR: 0.015000\n",
      "Training Epoch: 9 [48256/50000]\tLoss: 1.5072\tLR: 0.015000\n",
      "Training Epoch: 9 [48384/50000]\tLoss: 1.6607\tLR: 0.015000\n",
      "Training Epoch: 9 [48512/50000]\tLoss: 1.7014\tLR: 0.015000\n",
      "Training Epoch: 9 [48640/50000]\tLoss: 1.6547\tLR: 0.015000\n",
      "Training Epoch: 9 [48768/50000]\tLoss: 1.5631\tLR: 0.015000\n",
      "Training Epoch: 9 [48896/50000]\tLoss: 1.4841\tLR: 0.015000\n",
      "Training Epoch: 9 [49024/50000]\tLoss: 1.6149\tLR: 0.015000\n",
      "Training Epoch: 9 [49152/50000]\tLoss: 1.4028\tLR: 0.015000\n",
      "Training Epoch: 9 [49280/50000]\tLoss: 1.3477\tLR: 0.015000\n",
      "Training Epoch: 9 [49408/50000]\tLoss: 1.9784\tLR: 0.015000\n",
      "Training Epoch: 9 [49536/50000]\tLoss: 1.4621\tLR: 0.015000\n",
      "Training Epoch: 9 [49664/50000]\tLoss: 1.5660\tLR: 0.015000\n",
      "Training Epoch: 9 [49792/50000]\tLoss: 1.6216\tLR: 0.015000\n",
      "Training Epoch: 9 [49920/50000]\tLoss: 1.5423\tLR: 0.015000\n",
      "Training Epoch: 9 [50000/50000]\tLoss: 1.5107\tLR: 0.015000\n",
      "Test set: Average loss: 0.0128, Accuracy: 0.5469\n",
      "\n",
      "Training Epoch: 10 [128/50000]\tLoss: 1.4971\tLR: 0.015000\n",
      "Training Epoch: 10 [256/50000]\tLoss: 1.5848\tLR: 0.015000\n",
      "Training Epoch: 10 [384/50000]\tLoss: 1.5574\tLR: 0.015000\n",
      "Training Epoch: 10 [512/50000]\tLoss: 1.5895\tLR: 0.015000\n",
      "Training Epoch: 10 [640/50000]\tLoss: 1.4866\tLR: 0.015000\n",
      "Training Epoch: 10 [768/50000]\tLoss: 1.4471\tLR: 0.015000\n",
      "Training Epoch: 10 [896/50000]\tLoss: 1.6429\tLR: 0.015000\n",
      "Training Epoch: 10 [1024/50000]\tLoss: 1.4570\tLR: 0.015000\n",
      "Training Epoch: 10 [1152/50000]\tLoss: 1.7640\tLR: 0.015000\n",
      "Training Epoch: 10 [1280/50000]\tLoss: 1.2587\tLR: 0.015000\n",
      "Training Epoch: 10 [1408/50000]\tLoss: 1.4899\tLR: 0.015000\n",
      "Training Epoch: 10 [1536/50000]\tLoss: 1.3736\tLR: 0.015000\n",
      "Training Epoch: 10 [1664/50000]\tLoss: 1.4672\tLR: 0.015000\n",
      "Training Epoch: 10 [1792/50000]\tLoss: 1.6746\tLR: 0.015000\n",
      "Training Epoch: 10 [1920/50000]\tLoss: 1.4160\tLR: 0.015000\n",
      "Training Epoch: 10 [2048/50000]\tLoss: 1.6010\tLR: 0.015000\n",
      "Training Epoch: 10 [2176/50000]\tLoss: 1.5465\tLR: 0.015000\n",
      "Training Epoch: 10 [2304/50000]\tLoss: 1.4043\tLR: 0.015000\n",
      "Training Epoch: 10 [2432/50000]\tLoss: 1.4302\tLR: 0.015000\n",
      "Training Epoch: 10 [2560/50000]\tLoss: 1.4758\tLR: 0.015000\n",
      "Training Epoch: 10 [2688/50000]\tLoss: 1.6525\tLR: 0.015000\n",
      "Training Epoch: 10 [2816/50000]\tLoss: 1.7195\tLR: 0.015000\n",
      "Training Epoch: 10 [2944/50000]\tLoss: 1.4266\tLR: 0.015000\n",
      "Training Epoch: 10 [3072/50000]\tLoss: 1.4605\tLR: 0.015000\n",
      "Training Epoch: 10 [3200/50000]\tLoss: 1.4576\tLR: 0.015000\n",
      "Training Epoch: 10 [3328/50000]\tLoss: 1.4764\tLR: 0.015000\n",
      "Training Epoch: 10 [3456/50000]\tLoss: 1.7075\tLR: 0.015000\n",
      "Training Epoch: 10 [3584/50000]\tLoss: 1.5262\tLR: 0.015000\n",
      "Training Epoch: 10 [3712/50000]\tLoss: 1.7215\tLR: 0.015000\n",
      "Training Epoch: 10 [3840/50000]\tLoss: 1.3533\tLR: 0.015000\n",
      "Training Epoch: 10 [3968/50000]\tLoss: 1.3387\tLR: 0.015000\n",
      "Training Epoch: 10 [4096/50000]\tLoss: 1.6564\tLR: 0.015000\n",
      "Training Epoch: 10 [4224/50000]\tLoss: 1.3328\tLR: 0.015000\n",
      "Training Epoch: 10 [4352/50000]\tLoss: 1.4010\tLR: 0.015000\n",
      "Training Epoch: 10 [4480/50000]\tLoss: 1.4021\tLR: 0.015000\n",
      "Training Epoch: 10 [4608/50000]\tLoss: 1.4147\tLR: 0.015000\n",
      "Training Epoch: 10 [4736/50000]\tLoss: 1.5800\tLR: 0.015000\n",
      "Training Epoch: 10 [4864/50000]\tLoss: 1.5727\tLR: 0.015000\n",
      "Training Epoch: 10 [4992/50000]\tLoss: 1.5914\tLR: 0.015000\n",
      "Training Epoch: 10 [5120/50000]\tLoss: 1.3947\tLR: 0.015000\n",
      "Training Epoch: 10 [5248/50000]\tLoss: 1.4695\tLR: 0.015000\n",
      "Training Epoch: 10 [5376/50000]\tLoss: 1.5494\tLR: 0.015000\n",
      "Training Epoch: 10 [5504/50000]\tLoss: 1.4684\tLR: 0.015000\n",
      "Training Epoch: 10 [5632/50000]\tLoss: 1.6480\tLR: 0.015000\n",
      "Training Epoch: 10 [5760/50000]\tLoss: 1.3574\tLR: 0.015000\n",
      "Training Epoch: 10 [5888/50000]\tLoss: 1.4770\tLR: 0.015000\n",
      "Training Epoch: 10 [6016/50000]\tLoss: 1.6782\tLR: 0.015000\n",
      "Training Epoch: 10 [6144/50000]\tLoss: 1.7381\tLR: 0.015000\n",
      "Training Epoch: 10 [6272/50000]\tLoss: 1.5352\tLR: 0.015000\n",
      "Training Epoch: 10 [6400/50000]\tLoss: 1.4950\tLR: 0.015000\n",
      "Training Epoch: 10 [6528/50000]\tLoss: 1.3823\tLR: 0.015000\n",
      "Training Epoch: 10 [6656/50000]\tLoss: 1.3428\tLR: 0.015000\n",
      "Training Epoch: 10 [6784/50000]\tLoss: 1.3664\tLR: 0.015000\n",
      "Training Epoch: 10 [6912/50000]\tLoss: 1.6049\tLR: 0.015000\n",
      "Training Epoch: 10 [7040/50000]\tLoss: 1.3430\tLR: 0.015000\n",
      "Training Epoch: 10 [7168/50000]\tLoss: 1.5607\tLR: 0.015000\n",
      "Training Epoch: 10 [7296/50000]\tLoss: 1.6155\tLR: 0.015000\n",
      "Training Epoch: 10 [7424/50000]\tLoss: 1.4817\tLR: 0.015000\n",
      "Training Epoch: 10 [7552/50000]\tLoss: 1.2309\tLR: 0.015000\n",
      "Training Epoch: 10 [7680/50000]\tLoss: 1.6057\tLR: 0.015000\n",
      "Training Epoch: 10 [7808/50000]\tLoss: 1.4145\tLR: 0.015000\n",
      "Training Epoch: 10 [7936/50000]\tLoss: 1.6129\tLR: 0.015000\n",
      "Training Epoch: 10 [8064/50000]\tLoss: 1.4551\tLR: 0.015000\n",
      "Training Epoch: 10 [8192/50000]\tLoss: 1.7147\tLR: 0.015000\n",
      "Training Epoch: 10 [8320/50000]\tLoss: 1.5406\tLR: 0.015000\n",
      "Training Epoch: 10 [8448/50000]\tLoss: 1.6866\tLR: 0.015000\n",
      "Training Epoch: 10 [8576/50000]\tLoss: 1.6305\tLR: 0.015000\n",
      "Training Epoch: 10 [8704/50000]\tLoss: 1.5678\tLR: 0.015000\n",
      "Training Epoch: 10 [8832/50000]\tLoss: 1.4497\tLR: 0.015000\n",
      "Training Epoch: 10 [8960/50000]\tLoss: 1.4623\tLR: 0.015000\n",
      "Training Epoch: 10 [9088/50000]\tLoss: 1.4892\tLR: 0.015000\n",
      "Training Epoch: 10 [9216/50000]\tLoss: 1.4928\tLR: 0.015000\n",
      "Training Epoch: 10 [9344/50000]\tLoss: 1.5682\tLR: 0.015000\n",
      "Training Epoch: 10 [9472/50000]\tLoss: 1.6838\tLR: 0.015000\n",
      "Training Epoch: 10 [9600/50000]\tLoss: 1.4441\tLR: 0.015000\n",
      "Training Epoch: 10 [9728/50000]\tLoss: 1.2414\tLR: 0.015000\n",
      "Training Epoch: 10 [9856/50000]\tLoss: 1.3481\tLR: 0.015000\n",
      "Training Epoch: 10 [9984/50000]\tLoss: 1.4119\tLR: 0.015000\n",
      "Training Epoch: 10 [10112/50000]\tLoss: 1.6682\tLR: 0.015000\n",
      "Training Epoch: 10 [10240/50000]\tLoss: 1.4840\tLR: 0.015000\n",
      "Training Epoch: 10 [10368/50000]\tLoss: 1.3282\tLR: 0.015000\n",
      "Training Epoch: 10 [10496/50000]\tLoss: 1.7756\tLR: 0.015000\n",
      "Training Epoch: 10 [10624/50000]\tLoss: 1.4764\tLR: 0.015000\n",
      "Training Epoch: 10 [10752/50000]\tLoss: 1.5185\tLR: 0.015000\n",
      "Training Epoch: 10 [10880/50000]\tLoss: 1.5392\tLR: 0.015000\n",
      "Training Epoch: 10 [11008/50000]\tLoss: 1.4801\tLR: 0.015000\n",
      "Training Epoch: 10 [11136/50000]\tLoss: 1.7445\tLR: 0.015000\n",
      "Training Epoch: 10 [11264/50000]\tLoss: 1.6428\tLR: 0.015000\n",
      "Training Epoch: 10 [11392/50000]\tLoss: 1.4497\tLR: 0.015000\n",
      "Training Epoch: 10 [11520/50000]\tLoss: 1.5777\tLR: 0.015000\n",
      "Training Epoch: 10 [11648/50000]\tLoss: 1.5034\tLR: 0.015000\n",
      "Training Epoch: 10 [11776/50000]\tLoss: 1.4630\tLR: 0.015000\n",
      "Training Epoch: 10 [11904/50000]\tLoss: 1.7484\tLR: 0.015000\n",
      "Training Epoch: 10 [12032/50000]\tLoss: 1.4812\tLR: 0.015000\n",
      "Training Epoch: 10 [12160/50000]\tLoss: 1.5228\tLR: 0.015000\n",
      "Training Epoch: 10 [12288/50000]\tLoss: 1.6130\tLR: 0.015000\n",
      "Training Epoch: 10 [12416/50000]\tLoss: 1.7525\tLR: 0.015000\n",
      "Training Epoch: 10 [12544/50000]\tLoss: 1.5301\tLR: 0.015000\n",
      "Training Epoch: 10 [12672/50000]\tLoss: 1.5181\tLR: 0.015000\n",
      "Training Epoch: 10 [12800/50000]\tLoss: 1.9382\tLR: 0.015000\n",
      "Training Epoch: 10 [12928/50000]\tLoss: 1.6116\tLR: 0.015000\n",
      "Training Epoch: 10 [13056/50000]\tLoss: 1.5115\tLR: 0.015000\n",
      "Training Epoch: 10 [13184/50000]\tLoss: 1.2825\tLR: 0.015000\n",
      "Training Epoch: 10 [13312/50000]\tLoss: 1.6035\tLR: 0.015000\n",
      "Training Epoch: 10 [13440/50000]\tLoss: 1.4601\tLR: 0.015000\n",
      "Training Epoch: 10 [13568/50000]\tLoss: 1.6225\tLR: 0.015000\n",
      "Training Epoch: 10 [13696/50000]\tLoss: 1.5108\tLR: 0.015000\n",
      "Training Epoch: 10 [13824/50000]\tLoss: 1.5263\tLR: 0.015000\n",
      "Training Epoch: 10 [13952/50000]\tLoss: 1.5189\tLR: 0.015000\n",
      "Training Epoch: 10 [14080/50000]\tLoss: 1.2872\tLR: 0.015000\n",
      "Training Epoch: 10 [14208/50000]\tLoss: 1.4500\tLR: 0.015000\n",
      "Training Epoch: 10 [14336/50000]\tLoss: 1.5057\tLR: 0.015000\n",
      "Training Epoch: 10 [14464/50000]\tLoss: 1.3529\tLR: 0.015000\n",
      "Training Epoch: 10 [14592/50000]\tLoss: 1.4235\tLR: 0.015000\n",
      "Training Epoch: 10 [14720/50000]\tLoss: 1.3536\tLR: 0.015000\n",
      "Training Epoch: 10 [14848/50000]\tLoss: 1.3901\tLR: 0.015000\n",
      "Training Epoch: 10 [14976/50000]\tLoss: 1.3096\tLR: 0.015000\n",
      "Training Epoch: 10 [15104/50000]\tLoss: 1.5244\tLR: 0.015000\n",
      "Training Epoch: 10 [15232/50000]\tLoss: 1.4856\tLR: 0.015000\n",
      "Training Epoch: 10 [15360/50000]\tLoss: 1.4894\tLR: 0.015000\n",
      "Training Epoch: 10 [15488/50000]\tLoss: 1.7485\tLR: 0.015000\n",
      "Training Epoch: 10 [15616/50000]\tLoss: 1.4368\tLR: 0.015000\n",
      "Training Epoch: 10 [15744/50000]\tLoss: 1.5306\tLR: 0.015000\n",
      "Training Epoch: 10 [15872/50000]\tLoss: 1.3622\tLR: 0.015000\n",
      "Training Epoch: 10 [16000/50000]\tLoss: 1.4761\tLR: 0.015000\n",
      "Training Epoch: 10 [16128/50000]\tLoss: 1.2769\tLR: 0.015000\n",
      "Training Epoch: 10 [16256/50000]\tLoss: 1.3574\tLR: 0.015000\n",
      "Training Epoch: 10 [16384/50000]\tLoss: 1.3705\tLR: 0.015000\n",
      "Training Epoch: 10 [16512/50000]\tLoss: 1.5807\tLR: 0.015000\n",
      "Training Epoch: 10 [16640/50000]\tLoss: 1.3357\tLR: 0.015000\n",
      "Training Epoch: 10 [16768/50000]\tLoss: 1.5179\tLR: 0.015000\n",
      "Training Epoch: 10 [16896/50000]\tLoss: 1.7011\tLR: 0.015000\n",
      "Training Epoch: 10 [17024/50000]\tLoss: 1.3763\tLR: 0.015000\n",
      "Training Epoch: 10 [17152/50000]\tLoss: 1.6740\tLR: 0.015000\n",
      "Training Epoch: 10 [17280/50000]\tLoss: 1.5522\tLR: 0.015000\n",
      "Training Epoch: 10 [17408/50000]\tLoss: 1.3753\tLR: 0.015000\n",
      "Training Epoch: 10 [17536/50000]\tLoss: 1.7235\tLR: 0.015000\n",
      "Training Epoch: 10 [17664/50000]\tLoss: 1.6621\tLR: 0.015000\n",
      "Training Epoch: 10 [17792/50000]\tLoss: 1.6889\tLR: 0.015000\n",
      "Training Epoch: 10 [17920/50000]\tLoss: 1.5897\tLR: 0.015000\n",
      "Training Epoch: 10 [18048/50000]\tLoss: 1.4518\tLR: 0.015000\n",
      "Training Epoch: 10 [18176/50000]\tLoss: 1.4632\tLR: 0.015000\n",
      "Training Epoch: 10 [18304/50000]\tLoss: 1.3684\tLR: 0.015000\n",
      "Training Epoch: 10 [18432/50000]\tLoss: 1.6525\tLR: 0.015000\n",
      "Training Epoch: 10 [18560/50000]\tLoss: 1.5986\tLR: 0.015000\n",
      "Training Epoch: 10 [18688/50000]\tLoss: 1.7564\tLR: 0.015000\n",
      "Training Epoch: 10 [18816/50000]\tLoss: 1.5541\tLR: 0.015000\n",
      "Training Epoch: 10 [18944/50000]\tLoss: 1.6414\tLR: 0.015000\n",
      "Training Epoch: 10 [19072/50000]\tLoss: 1.6471\tLR: 0.015000\n",
      "Training Epoch: 10 [19200/50000]\tLoss: 1.3687\tLR: 0.015000\n",
      "Training Epoch: 10 [19328/50000]\tLoss: 1.5354\tLR: 0.015000\n",
      "Training Epoch: 10 [19456/50000]\tLoss: 1.4067\tLR: 0.015000\n",
      "Training Epoch: 10 [19584/50000]\tLoss: 1.5471\tLR: 0.015000\n",
      "Training Epoch: 10 [19712/50000]\tLoss: 1.4811\tLR: 0.015000\n",
      "Training Epoch: 10 [19840/50000]\tLoss: 1.4962\tLR: 0.015000\n",
      "Training Epoch: 10 [19968/50000]\tLoss: 1.5453\tLR: 0.015000\n",
      "Training Epoch: 10 [20096/50000]\tLoss: 1.6585\tLR: 0.015000\n",
      "Training Epoch: 10 [20224/50000]\tLoss: 1.2470\tLR: 0.015000\n",
      "Training Epoch: 10 [20352/50000]\tLoss: 1.6237\tLR: 0.015000\n",
      "Training Epoch: 10 [20480/50000]\tLoss: 1.5287\tLR: 0.015000\n",
      "Training Epoch: 10 [20608/50000]\tLoss: 1.6283\tLR: 0.015000\n",
      "Training Epoch: 10 [20736/50000]\tLoss: 1.7260\tLR: 0.015000\n",
      "Training Epoch: 10 [20864/50000]\tLoss: 1.9930\tLR: 0.015000\n",
      "Training Epoch: 10 [20992/50000]\tLoss: 1.5113\tLR: 0.015000\n",
      "Training Epoch: 10 [21120/50000]\tLoss: 1.5903\tLR: 0.015000\n",
      "Training Epoch: 10 [21248/50000]\tLoss: 1.5828\tLR: 0.015000\n",
      "Training Epoch: 10 [21376/50000]\tLoss: 1.5241\tLR: 0.015000\n",
      "Training Epoch: 10 [21504/50000]\tLoss: 1.7561\tLR: 0.015000\n",
      "Training Epoch: 10 [21632/50000]\tLoss: 1.5434\tLR: 0.015000\n",
      "Training Epoch: 10 [21760/50000]\tLoss: 1.4230\tLR: 0.015000\n",
      "Training Epoch: 10 [21888/50000]\tLoss: 1.5382\tLR: 0.015000\n",
      "Training Epoch: 10 [22016/50000]\tLoss: 1.7655\tLR: 0.015000\n",
      "Training Epoch: 10 [22144/50000]\tLoss: 1.6721\tLR: 0.015000\n",
      "Training Epoch: 10 [22272/50000]\tLoss: 1.3504\tLR: 0.015000\n",
      "Training Epoch: 10 [22400/50000]\tLoss: 1.5176\tLR: 0.015000\n",
      "Training Epoch: 10 [22528/50000]\tLoss: 1.5217\tLR: 0.015000\n",
      "Training Epoch: 10 [22656/50000]\tLoss: 1.5120\tLR: 0.015000\n",
      "Training Epoch: 10 [22784/50000]\tLoss: 1.6825\tLR: 0.015000\n",
      "Training Epoch: 10 [22912/50000]\tLoss: 1.6152\tLR: 0.015000\n",
      "Training Epoch: 10 [23040/50000]\tLoss: 1.6086\tLR: 0.015000\n",
      "Training Epoch: 10 [23168/50000]\tLoss: 1.5572\tLR: 0.015000\n",
      "Training Epoch: 10 [23296/50000]\tLoss: 1.4173\tLR: 0.015000\n",
      "Training Epoch: 10 [23424/50000]\tLoss: 1.6179\tLR: 0.015000\n",
      "Training Epoch: 10 [23552/50000]\tLoss: 1.5446\tLR: 0.015000\n",
      "Training Epoch: 10 [23680/50000]\tLoss: 1.5187\tLR: 0.015000\n",
      "Training Epoch: 10 [23808/50000]\tLoss: 1.3461\tLR: 0.015000\n",
      "Training Epoch: 10 [23936/50000]\tLoss: 1.4025\tLR: 0.015000\n",
      "Training Epoch: 10 [24064/50000]\tLoss: 1.6588\tLR: 0.015000\n",
      "Training Epoch: 10 [24192/50000]\tLoss: 1.5152\tLR: 0.015000\n",
      "Training Epoch: 10 [24320/50000]\tLoss: 1.4814\tLR: 0.015000\n",
      "Training Epoch: 10 [24448/50000]\tLoss: 1.4468\tLR: 0.015000\n",
      "Training Epoch: 10 [24576/50000]\tLoss: 1.4953\tLR: 0.015000\n",
      "Training Epoch: 10 [24704/50000]\tLoss: 1.7062\tLR: 0.015000\n",
      "Training Epoch: 10 [24832/50000]\tLoss: 1.5227\tLR: 0.015000\n",
      "Training Epoch: 10 [24960/50000]\tLoss: 1.4050\tLR: 0.015000\n",
      "Training Epoch: 10 [25088/50000]\tLoss: 1.5733\tLR: 0.015000\n",
      "Training Epoch: 10 [25216/50000]\tLoss: 1.6479\tLR: 0.015000\n",
      "Training Epoch: 10 [25344/50000]\tLoss: 1.7200\tLR: 0.015000\n",
      "Training Epoch: 10 [25472/50000]\tLoss: 1.5292\tLR: 0.015000\n",
      "Training Epoch: 10 [25600/50000]\tLoss: 1.5831\tLR: 0.015000\n",
      "Training Epoch: 10 [25728/50000]\tLoss: 1.4323\tLR: 0.015000\n",
      "Training Epoch: 10 [25856/50000]\tLoss: 1.6212\tLR: 0.015000\n",
      "Training Epoch: 10 [25984/50000]\tLoss: 1.5516\tLR: 0.015000\n",
      "Training Epoch: 10 [26112/50000]\tLoss: 1.5632\tLR: 0.015000\n",
      "Training Epoch: 10 [26240/50000]\tLoss: 1.3195\tLR: 0.015000\n",
      "Training Epoch: 10 [26368/50000]\tLoss: 1.6198\tLR: 0.015000\n",
      "Training Epoch: 10 [26496/50000]\tLoss: 1.5469\tLR: 0.015000\n",
      "Training Epoch: 10 [26624/50000]\tLoss: 1.6238\tLR: 0.015000\n",
      "Training Epoch: 10 [26752/50000]\tLoss: 1.5783\tLR: 0.015000\n",
      "Training Epoch: 10 [26880/50000]\tLoss: 1.5748\tLR: 0.015000\n",
      "Training Epoch: 10 [27008/50000]\tLoss: 1.6097\tLR: 0.015000\n",
      "Training Epoch: 10 [27136/50000]\tLoss: 1.6406\tLR: 0.015000\n",
      "Training Epoch: 10 [27264/50000]\tLoss: 1.6935\tLR: 0.015000\n",
      "Training Epoch: 10 [27392/50000]\tLoss: 1.3682\tLR: 0.015000\n",
      "Training Epoch: 10 [27520/50000]\tLoss: 1.5630\tLR: 0.015000\n",
      "Training Epoch: 10 [27648/50000]\tLoss: 1.4618\tLR: 0.015000\n",
      "Training Epoch: 10 [27776/50000]\tLoss: 1.5302\tLR: 0.015000\n",
      "Training Epoch: 10 [27904/50000]\tLoss: 1.7011\tLR: 0.015000\n",
      "Training Epoch: 10 [28032/50000]\tLoss: 1.7741\tLR: 0.015000\n",
      "Training Epoch: 10 [28160/50000]\tLoss: 1.4187\tLR: 0.015000\n",
      "Training Epoch: 10 [28288/50000]\tLoss: 1.5287\tLR: 0.015000\n",
      "Training Epoch: 10 [28416/50000]\tLoss: 1.5828\tLR: 0.015000\n",
      "Training Epoch: 10 [28544/50000]\tLoss: 1.7533\tLR: 0.015000\n",
      "Training Epoch: 10 [28672/50000]\tLoss: 1.3811\tLR: 0.015000\n",
      "Training Epoch: 10 [28800/50000]\tLoss: 1.6744\tLR: 0.015000\n",
      "Training Epoch: 10 [28928/50000]\tLoss: 1.5639\tLR: 0.015000\n",
      "Training Epoch: 10 [29056/50000]\tLoss: 1.5289\tLR: 0.015000\n",
      "Training Epoch: 10 [29184/50000]\tLoss: 1.6208\tLR: 0.015000\n",
      "Training Epoch: 10 [29312/50000]\tLoss: 1.4022\tLR: 0.015000\n",
      "Training Epoch: 10 [29440/50000]\tLoss: 1.6575\tLR: 0.015000\n",
      "Training Epoch: 10 [29568/50000]\tLoss: 1.6128\tLR: 0.015000\n",
      "Training Epoch: 10 [29696/50000]\tLoss: 1.3503\tLR: 0.015000\n",
      "Training Epoch: 10 [29824/50000]\tLoss: 1.5234\tLR: 0.015000\n",
      "Training Epoch: 10 [29952/50000]\tLoss: 1.4519\tLR: 0.015000\n",
      "Training Epoch: 10 [30080/50000]\tLoss: 1.3275\tLR: 0.015000\n",
      "Training Epoch: 10 [30208/50000]\tLoss: 1.6341\tLR: 0.015000\n",
      "Training Epoch: 10 [30336/50000]\tLoss: 1.4699\tLR: 0.015000\n",
      "Training Epoch: 10 [30464/50000]\tLoss: 1.3832\tLR: 0.015000\n",
      "Training Epoch: 10 [30592/50000]\tLoss: 1.5439\tLR: 0.015000\n",
      "Training Epoch: 10 [30720/50000]\tLoss: 1.4599\tLR: 0.015000\n",
      "Training Epoch: 10 [30848/50000]\tLoss: 1.2897\tLR: 0.015000\n",
      "Training Epoch: 10 [30976/50000]\tLoss: 1.8499\tLR: 0.015000\n",
      "Training Epoch: 10 [31104/50000]\tLoss: 1.4838\tLR: 0.015000\n",
      "Training Epoch: 10 [31232/50000]\tLoss: 1.4224\tLR: 0.015000\n",
      "Training Epoch: 10 [31360/50000]\tLoss: 1.4533\tLR: 0.015000\n",
      "Training Epoch: 10 [31488/50000]\tLoss: 1.4958\tLR: 0.015000\n",
      "Training Epoch: 10 [31616/50000]\tLoss: 1.6435\tLR: 0.015000\n",
      "Training Epoch: 10 [31744/50000]\tLoss: 1.6131\tLR: 0.015000\n",
      "Training Epoch: 10 [31872/50000]\tLoss: 1.4224\tLR: 0.015000\n",
      "Training Epoch: 10 [32000/50000]\tLoss: 1.5039\tLR: 0.015000\n",
      "Training Epoch: 10 [32128/50000]\tLoss: 1.5537\tLR: 0.015000\n",
      "Training Epoch: 10 [32256/50000]\tLoss: 1.4312\tLR: 0.015000\n",
      "Training Epoch: 10 [32384/50000]\tLoss: 1.3379\tLR: 0.015000\n",
      "Training Epoch: 10 [32512/50000]\tLoss: 1.4051\tLR: 0.015000\n",
      "Training Epoch: 10 [32640/50000]\tLoss: 1.4951\tLR: 0.015000\n",
      "Training Epoch: 10 [32768/50000]\tLoss: 1.6446\tLR: 0.015000\n",
      "Training Epoch: 10 [32896/50000]\tLoss: 1.5111\tLR: 0.015000\n",
      "Training Epoch: 10 [33024/50000]\tLoss: 1.6778\tLR: 0.015000\n",
      "Training Epoch: 10 [33152/50000]\tLoss: 1.6747\tLR: 0.015000\n",
      "Training Epoch: 10 [33280/50000]\tLoss: 1.4379\tLR: 0.015000\n",
      "Training Epoch: 10 [33408/50000]\tLoss: 1.5021\tLR: 0.015000\n",
      "Training Epoch: 10 [33536/50000]\tLoss: 1.5129\tLR: 0.015000\n",
      "Training Epoch: 10 [33664/50000]\tLoss: 1.6020\tLR: 0.015000\n",
      "Training Epoch: 10 [33792/50000]\tLoss: 1.5998\tLR: 0.015000\n",
      "Training Epoch: 10 [33920/50000]\tLoss: 1.6925\tLR: 0.015000\n",
      "Training Epoch: 10 [34048/50000]\tLoss: 1.5133\tLR: 0.015000\n",
      "Training Epoch: 10 [34176/50000]\tLoss: 1.4445\tLR: 0.015000\n",
      "Training Epoch: 10 [34304/50000]\tLoss: 1.6893\tLR: 0.015000\n",
      "Training Epoch: 10 [34432/50000]\tLoss: 1.1723\tLR: 0.015000\n",
      "Training Epoch: 10 [34560/50000]\tLoss: 1.6378\tLR: 0.015000\n",
      "Training Epoch: 10 [34688/50000]\tLoss: 1.1656\tLR: 0.015000\n",
      "Training Epoch: 10 [34816/50000]\tLoss: 1.7746\tLR: 0.015000\n",
      "Training Epoch: 10 [34944/50000]\tLoss: 1.5777\tLR: 0.015000\n",
      "Training Epoch: 10 [35072/50000]\tLoss: 1.2908\tLR: 0.015000\n",
      "Training Epoch: 10 [35200/50000]\tLoss: 1.3508\tLR: 0.015000\n",
      "Training Epoch: 10 [35328/50000]\tLoss: 1.4641\tLR: 0.015000\n",
      "Training Epoch: 10 [35456/50000]\tLoss: 1.7132\tLR: 0.015000\n",
      "Training Epoch: 10 [35584/50000]\tLoss: 1.6597\tLR: 0.015000\n",
      "Training Epoch: 10 [35712/50000]\tLoss: 1.5182\tLR: 0.015000\n",
      "Training Epoch: 10 [35840/50000]\tLoss: 1.4948\tLR: 0.015000\n",
      "Training Epoch: 10 [35968/50000]\tLoss: 1.5695\tLR: 0.015000\n",
      "Training Epoch: 10 [36096/50000]\tLoss: 1.7755\tLR: 0.015000\n",
      "Training Epoch: 10 [36224/50000]\tLoss: 1.6660\tLR: 0.015000\n",
      "Training Epoch: 10 [36352/50000]\tLoss: 1.4561\tLR: 0.015000\n",
      "Training Epoch: 10 [36480/50000]\tLoss: 1.4989\tLR: 0.015000\n",
      "Training Epoch: 10 [36608/50000]\tLoss: 1.5403\tLR: 0.015000\n",
      "Training Epoch: 10 [36736/50000]\tLoss: 1.6371\tLR: 0.015000\n",
      "Training Epoch: 10 [36864/50000]\tLoss: 1.4940\tLR: 0.015000\n",
      "Training Epoch: 10 [36992/50000]\tLoss: 1.5216\tLR: 0.015000\n",
      "Training Epoch: 10 [37120/50000]\tLoss: 1.3922\tLR: 0.015000\n",
      "Training Epoch: 10 [37248/50000]\tLoss: 1.5128\tLR: 0.015000\n",
      "Training Epoch: 10 [37376/50000]\tLoss: 1.4784\tLR: 0.015000\n",
      "Training Epoch: 10 [37504/50000]\tLoss: 1.6454\tLR: 0.015000\n",
      "Training Epoch: 10 [37632/50000]\tLoss: 1.6537\tLR: 0.015000\n",
      "Training Epoch: 10 [37760/50000]\tLoss: 1.5028\tLR: 0.015000\n",
      "Training Epoch: 10 [37888/50000]\tLoss: 1.5360\tLR: 0.015000\n",
      "Training Epoch: 10 [38016/50000]\tLoss: 1.6259\tLR: 0.015000\n",
      "Training Epoch: 10 [38144/50000]\tLoss: 1.5203\tLR: 0.015000\n",
      "Training Epoch: 10 [38272/50000]\tLoss: 1.5655\tLR: 0.015000\n",
      "Training Epoch: 10 [38400/50000]\tLoss: 1.5120\tLR: 0.015000\n",
      "Training Epoch: 10 [38528/50000]\tLoss: 1.5390\tLR: 0.015000\n",
      "Training Epoch: 10 [38656/50000]\tLoss: 1.6638\tLR: 0.015000\n",
      "Training Epoch: 10 [38784/50000]\tLoss: 1.3972\tLR: 0.015000\n",
      "Training Epoch: 10 [38912/50000]\tLoss: 1.6276\tLR: 0.015000\n",
      "Training Epoch: 10 [39040/50000]\tLoss: 1.6890\tLR: 0.015000\n",
      "Training Epoch: 10 [39168/50000]\tLoss: 1.4640\tLR: 0.015000\n",
      "Training Epoch: 10 [39296/50000]\tLoss: 1.6516\tLR: 0.015000\n",
      "Training Epoch: 10 [39424/50000]\tLoss: 1.6598\tLR: 0.015000\n",
      "Training Epoch: 10 [39552/50000]\tLoss: 1.5201\tLR: 0.015000\n",
      "Training Epoch: 10 [39680/50000]\tLoss: 1.5084\tLR: 0.015000\n",
      "Training Epoch: 10 [39808/50000]\tLoss: 1.6555\tLR: 0.015000\n",
      "Training Epoch: 10 [39936/50000]\tLoss: 1.2817\tLR: 0.015000\n",
      "Training Epoch: 10 [40064/50000]\tLoss: 1.4420\tLR: 0.015000\n",
      "Training Epoch: 10 [40192/50000]\tLoss: 1.7945\tLR: 0.015000\n",
      "Training Epoch: 10 [40320/50000]\tLoss: 1.5592\tLR: 0.015000\n",
      "Training Epoch: 10 [40448/50000]\tLoss: 1.6488\tLR: 0.015000\n",
      "Training Epoch: 10 [40576/50000]\tLoss: 1.4132\tLR: 0.015000\n",
      "Training Epoch: 10 [40704/50000]\tLoss: 1.5925\tLR: 0.015000\n",
      "Training Epoch: 10 [40832/50000]\tLoss: 1.6365\tLR: 0.015000\n",
      "Training Epoch: 10 [40960/50000]\tLoss: 1.7112\tLR: 0.015000\n",
      "Training Epoch: 10 [41088/50000]\tLoss: 1.5871\tLR: 0.015000\n",
      "Training Epoch: 10 [41216/50000]\tLoss: 1.4245\tLR: 0.015000\n",
      "Training Epoch: 10 [41344/50000]\tLoss: 1.4540\tLR: 0.015000\n",
      "Training Epoch: 10 [41472/50000]\tLoss: 1.5417\tLR: 0.015000\n",
      "Training Epoch: 10 [41600/50000]\tLoss: 1.6036\tLR: 0.015000\n",
      "Training Epoch: 10 [41728/50000]\tLoss: 1.4330\tLR: 0.015000\n",
      "Training Epoch: 10 [41856/50000]\tLoss: 1.4081\tLR: 0.015000\n",
      "Training Epoch: 10 [41984/50000]\tLoss: 1.4632\tLR: 0.015000\n",
      "Training Epoch: 10 [42112/50000]\tLoss: 1.5869\tLR: 0.015000\n",
      "Training Epoch: 10 [42240/50000]\tLoss: 1.3062\tLR: 0.015000\n",
      "Training Epoch: 10 [42368/50000]\tLoss: 1.5648\tLR: 0.015000\n",
      "Training Epoch: 10 [42496/50000]\tLoss: 1.3236\tLR: 0.015000\n",
      "Training Epoch: 10 [42624/50000]\tLoss: 1.6272\tLR: 0.015000\n",
      "Training Epoch: 10 [42752/50000]\tLoss: 1.5853\tLR: 0.015000\n",
      "Training Epoch: 10 [42880/50000]\tLoss: 1.6959\tLR: 0.015000\n",
      "Training Epoch: 10 [43008/50000]\tLoss: 1.4869\tLR: 0.015000\n",
      "Training Epoch: 10 [43136/50000]\tLoss: 1.6002\tLR: 0.015000\n",
      "Training Epoch: 10 [43264/50000]\tLoss: 1.6722\tLR: 0.015000\n",
      "Training Epoch: 10 [43392/50000]\tLoss: 1.8102\tLR: 0.015000\n",
      "Training Epoch: 10 [43520/50000]\tLoss: 1.4373\tLR: 0.015000\n",
      "Training Epoch: 10 [43648/50000]\tLoss: 1.4245\tLR: 0.015000\n",
      "Training Epoch: 10 [43776/50000]\tLoss: 1.1847\tLR: 0.015000\n",
      "Training Epoch: 10 [43904/50000]\tLoss: 1.5656\tLR: 0.015000\n",
      "Training Epoch: 10 [44032/50000]\tLoss: 1.6359\tLR: 0.015000\n",
      "Training Epoch: 10 [44160/50000]\tLoss: 1.4774\tLR: 0.015000\n",
      "Training Epoch: 10 [44288/50000]\tLoss: 1.5304\tLR: 0.015000\n",
      "Training Epoch: 10 [44416/50000]\tLoss: 1.4595\tLR: 0.015000\n",
      "Training Epoch: 10 [44544/50000]\tLoss: 1.3762\tLR: 0.015000\n",
      "Training Epoch: 10 [44672/50000]\tLoss: 1.5862\tLR: 0.015000\n",
      "Training Epoch: 10 [44800/50000]\tLoss: 1.4632\tLR: 0.015000\n",
      "Training Epoch: 10 [44928/50000]\tLoss: 1.5689\tLR: 0.015000\n",
      "Training Epoch: 10 [45056/50000]\tLoss: 1.4038\tLR: 0.015000\n",
      "Training Epoch: 10 [45184/50000]\tLoss: 1.6264\tLR: 0.015000\n",
      "Training Epoch: 10 [45312/50000]\tLoss: 1.4458\tLR: 0.015000\n",
      "Training Epoch: 10 [45440/50000]\tLoss: 1.3787\tLR: 0.015000\n",
      "Training Epoch: 10 [45568/50000]\tLoss: 1.3610\tLR: 0.015000\n",
      "Training Epoch: 10 [45696/50000]\tLoss: 1.6486\tLR: 0.015000\n",
      "Training Epoch: 10 [45824/50000]\tLoss: 1.4062\tLR: 0.015000\n",
      "Training Epoch: 10 [45952/50000]\tLoss: 1.4531\tLR: 0.015000\n",
      "Training Epoch: 10 [46080/50000]\tLoss: 1.5513\tLR: 0.015000\n",
      "Training Epoch: 10 [46208/50000]\tLoss: 1.4588\tLR: 0.015000\n",
      "Training Epoch: 10 [46336/50000]\tLoss: 1.6093\tLR: 0.015000\n",
      "Training Epoch: 10 [46464/50000]\tLoss: 1.5366\tLR: 0.015000\n",
      "Training Epoch: 10 [46592/50000]\tLoss: 1.4553\tLR: 0.015000\n",
      "Training Epoch: 10 [46720/50000]\tLoss: 1.5070\tLR: 0.015000\n",
      "Training Epoch: 10 [46848/50000]\tLoss: 1.6578\tLR: 0.015000\n",
      "Training Epoch: 10 [46976/50000]\tLoss: 1.5538\tLR: 0.015000\n",
      "Training Epoch: 10 [47104/50000]\tLoss: 1.6251\tLR: 0.015000\n",
      "Training Epoch: 10 [47232/50000]\tLoss: 1.4555\tLR: 0.015000\n",
      "Training Epoch: 10 [47360/50000]\tLoss: 1.2059\tLR: 0.015000\n",
      "Training Epoch: 10 [47488/50000]\tLoss: 1.6188\tLR: 0.015000\n",
      "Training Epoch: 10 [47616/50000]\tLoss: 1.3685\tLR: 0.015000\n",
      "Training Epoch: 10 [47744/50000]\tLoss: 1.6106\tLR: 0.015000\n",
      "Training Epoch: 10 [47872/50000]\tLoss: 1.4185\tLR: 0.015000\n",
      "Training Epoch: 10 [48000/50000]\tLoss: 1.6441\tLR: 0.015000\n",
      "Training Epoch: 10 [48128/50000]\tLoss: 1.5776\tLR: 0.015000\n",
      "Training Epoch: 10 [48256/50000]\tLoss: 1.6052\tLR: 0.015000\n",
      "Training Epoch: 10 [48384/50000]\tLoss: 1.6361\tLR: 0.015000\n",
      "Training Epoch: 10 [48512/50000]\tLoss: 1.5877\tLR: 0.015000\n",
      "Training Epoch: 10 [48640/50000]\tLoss: 1.6116\tLR: 0.015000\n",
      "Training Epoch: 10 [48768/50000]\tLoss: 1.5451\tLR: 0.015000\n",
      "Training Epoch: 10 [48896/50000]\tLoss: 1.5028\tLR: 0.015000\n",
      "Training Epoch: 10 [49024/50000]\tLoss: 1.5171\tLR: 0.015000\n",
      "Training Epoch: 10 [49152/50000]\tLoss: 1.7320\tLR: 0.015000\n",
      "Training Epoch: 10 [49280/50000]\tLoss: 1.6603\tLR: 0.015000\n",
      "Training Epoch: 10 [49408/50000]\tLoss: 1.3997\tLR: 0.015000\n",
      "Training Epoch: 10 [49536/50000]\tLoss: 1.6067\tLR: 0.015000\n",
      "Training Epoch: 10 [49664/50000]\tLoss: 1.5118\tLR: 0.015000\n",
      "Training Epoch: 10 [49792/50000]\tLoss: 1.7958\tLR: 0.015000\n",
      "Training Epoch: 10 [49920/50000]\tLoss: 1.5050\tLR: 0.015000\n",
      "Training Epoch: 10 [50000/50000]\tLoss: 1.4821\tLR: 0.015000\n",
      "Test set: Average loss: 0.0125, Accuracy: 0.5582\n",
      "\n",
      "Training Epoch: 11 [128/50000]\tLoss: 1.3200\tLR: 0.015000\n",
      "Training Epoch: 11 [256/50000]\tLoss: 1.4537\tLR: 0.015000\n",
      "Training Epoch: 11 [384/50000]\tLoss: 1.3237\tLR: 0.015000\n",
      "Training Epoch: 11 [512/50000]\tLoss: 1.4239\tLR: 0.015000\n",
      "Training Epoch: 11 [640/50000]\tLoss: 1.6524\tLR: 0.015000\n",
      "Training Epoch: 11 [768/50000]\tLoss: 1.2610\tLR: 0.015000\n",
      "Training Epoch: 11 [896/50000]\tLoss: 1.4466\tLR: 0.015000\n",
      "Training Epoch: 11 [1024/50000]\tLoss: 1.4171\tLR: 0.015000\n",
      "Training Epoch: 11 [1152/50000]\tLoss: 1.3959\tLR: 0.015000\n",
      "Training Epoch: 11 [1280/50000]\tLoss: 1.3067\tLR: 0.015000\n",
      "Training Epoch: 11 [1408/50000]\tLoss: 1.3341\tLR: 0.015000\n",
      "Training Epoch: 11 [1536/50000]\tLoss: 1.3548\tLR: 0.015000\n",
      "Training Epoch: 11 [1664/50000]\tLoss: 1.4071\tLR: 0.015000\n",
      "Training Epoch: 11 [1792/50000]\tLoss: 1.3301\tLR: 0.015000\n",
      "Training Epoch: 11 [1920/50000]\tLoss: 1.4989\tLR: 0.015000\n",
      "Training Epoch: 11 [2048/50000]\tLoss: 1.6301\tLR: 0.015000\n",
      "Training Epoch: 11 [2176/50000]\tLoss: 1.4939\tLR: 0.015000\n",
      "Training Epoch: 11 [2304/50000]\tLoss: 1.4257\tLR: 0.015000\n",
      "Training Epoch: 11 [2432/50000]\tLoss: 1.4220\tLR: 0.015000\n",
      "Training Epoch: 11 [2560/50000]\tLoss: 1.3782\tLR: 0.015000\n",
      "Training Epoch: 11 [2688/50000]\tLoss: 1.6216\tLR: 0.015000\n",
      "Training Epoch: 11 [2816/50000]\tLoss: 1.5938\tLR: 0.015000\n",
      "Training Epoch: 11 [2944/50000]\tLoss: 1.3256\tLR: 0.015000\n",
      "Training Epoch: 11 [3072/50000]\tLoss: 1.3628\tLR: 0.015000\n",
      "Training Epoch: 11 [3200/50000]\tLoss: 1.3083\tLR: 0.015000\n",
      "Training Epoch: 11 [3328/50000]\tLoss: 1.4594\tLR: 0.015000\n",
      "Training Epoch: 11 [3456/50000]\tLoss: 1.2393\tLR: 0.015000\n",
      "Training Epoch: 11 [3584/50000]\tLoss: 1.5328\tLR: 0.015000\n",
      "Training Epoch: 11 [3712/50000]\tLoss: 1.3799\tLR: 0.015000\n",
      "Training Epoch: 11 [3840/50000]\tLoss: 1.3356\tLR: 0.015000\n",
      "Training Epoch: 11 [3968/50000]\tLoss: 1.3204\tLR: 0.015000\n",
      "Training Epoch: 11 [4096/50000]\tLoss: 1.5966\tLR: 0.015000\n",
      "Training Epoch: 11 [4224/50000]\tLoss: 1.3748\tLR: 0.015000\n",
      "Training Epoch: 11 [4352/50000]\tLoss: 1.1925\tLR: 0.015000\n",
      "Training Epoch: 11 [4480/50000]\tLoss: 1.5526\tLR: 0.015000\n",
      "Training Epoch: 11 [4608/50000]\tLoss: 1.5867\tLR: 0.015000\n",
      "Training Epoch: 11 [4736/50000]\tLoss: 1.1986\tLR: 0.015000\n",
      "Training Epoch: 11 [4864/50000]\tLoss: 1.3089\tLR: 0.015000\n",
      "Training Epoch: 11 [4992/50000]\tLoss: 1.3597\tLR: 0.015000\n",
      "Training Epoch: 11 [5120/50000]\tLoss: 1.6845\tLR: 0.015000\n",
      "Training Epoch: 11 [5248/50000]\tLoss: 1.4104\tLR: 0.015000\n",
      "Training Epoch: 11 [5376/50000]\tLoss: 1.4915\tLR: 0.015000\n",
      "Training Epoch: 11 [5504/50000]\tLoss: 1.4525\tLR: 0.015000\n",
      "Training Epoch: 11 [5632/50000]\tLoss: 1.5420\tLR: 0.015000\n",
      "Training Epoch: 11 [5760/50000]\tLoss: 1.5389\tLR: 0.015000\n",
      "Training Epoch: 11 [5888/50000]\tLoss: 1.5916\tLR: 0.015000\n",
      "Training Epoch: 11 [6016/50000]\tLoss: 1.4107\tLR: 0.015000\n",
      "Training Epoch: 11 [6144/50000]\tLoss: 1.7144\tLR: 0.015000\n",
      "Training Epoch: 11 [6272/50000]\tLoss: 1.4015\tLR: 0.015000\n",
      "Training Epoch: 11 [6400/50000]\tLoss: 1.2769\tLR: 0.015000\n",
      "Training Epoch: 11 [6528/50000]\tLoss: 1.4490\tLR: 0.015000\n",
      "Training Epoch: 11 [6656/50000]\tLoss: 1.2764\tLR: 0.015000\n",
      "Training Epoch: 11 [6784/50000]\tLoss: 1.4867\tLR: 0.015000\n",
      "Training Epoch: 11 [6912/50000]\tLoss: 1.3824\tLR: 0.015000\n",
      "Training Epoch: 11 [7040/50000]\tLoss: 1.3625\tLR: 0.015000\n",
      "Training Epoch: 11 [7168/50000]\tLoss: 1.2987\tLR: 0.015000\n",
      "Training Epoch: 11 [7296/50000]\tLoss: 1.3985\tLR: 0.015000\n",
      "Training Epoch: 11 [7424/50000]\tLoss: 1.3146\tLR: 0.015000\n",
      "Training Epoch: 11 [7552/50000]\tLoss: 1.2349\tLR: 0.015000\n",
      "Training Epoch: 11 [7680/50000]\tLoss: 1.4952\tLR: 0.015000\n",
      "Training Epoch: 11 [7808/50000]\tLoss: 1.5241\tLR: 0.015000\n",
      "Training Epoch: 11 [7936/50000]\tLoss: 1.5947\tLR: 0.015000\n",
      "Training Epoch: 11 [8064/50000]\tLoss: 1.3111\tLR: 0.015000\n",
      "Training Epoch: 11 [8192/50000]\tLoss: 1.3549\tLR: 0.015000\n",
      "Training Epoch: 11 [8320/50000]\tLoss: 1.5606\tLR: 0.015000\n",
      "Training Epoch: 11 [8448/50000]\tLoss: 1.3670\tLR: 0.015000\n",
      "Training Epoch: 11 [8576/50000]\tLoss: 1.3666\tLR: 0.015000\n",
      "Training Epoch: 11 [8704/50000]\tLoss: 1.3264\tLR: 0.015000\n",
      "Training Epoch: 11 [8832/50000]\tLoss: 1.4520\tLR: 0.015000\n",
      "Training Epoch: 11 [8960/50000]\tLoss: 1.7341\tLR: 0.015000\n",
      "Training Epoch: 11 [9088/50000]\tLoss: 1.3387\tLR: 0.015000\n",
      "Training Epoch: 11 [9216/50000]\tLoss: 1.1317\tLR: 0.015000\n",
      "Training Epoch: 11 [9344/50000]\tLoss: 1.4264\tLR: 0.015000\n",
      "Training Epoch: 11 [9472/50000]\tLoss: 1.4703\tLR: 0.015000\n",
      "Training Epoch: 11 [9600/50000]\tLoss: 1.7262\tLR: 0.015000\n",
      "Training Epoch: 11 [9728/50000]\tLoss: 1.2902\tLR: 0.015000\n",
      "Training Epoch: 11 [9856/50000]\tLoss: 1.2269\tLR: 0.015000\n",
      "Training Epoch: 11 [9984/50000]\tLoss: 1.2009\tLR: 0.015000\n",
      "Training Epoch: 11 [10112/50000]\tLoss: 1.2370\tLR: 0.015000\n",
      "Training Epoch: 11 [10240/50000]\tLoss: 1.5027\tLR: 0.015000\n",
      "Training Epoch: 11 [10368/50000]\tLoss: 1.5573\tLR: 0.015000\n",
      "Training Epoch: 11 [10496/50000]\tLoss: 1.5860\tLR: 0.015000\n",
      "Training Epoch: 11 [10624/50000]\tLoss: 1.6149\tLR: 0.015000\n",
      "Training Epoch: 11 [10752/50000]\tLoss: 1.6737\tLR: 0.015000\n",
      "Training Epoch: 11 [10880/50000]\tLoss: 1.6822\tLR: 0.015000\n",
      "Training Epoch: 11 [11008/50000]\tLoss: 1.4601\tLR: 0.015000\n",
      "Training Epoch: 11 [11136/50000]\tLoss: 1.3236\tLR: 0.015000\n",
      "Training Epoch: 11 [11264/50000]\tLoss: 1.5429\tLR: 0.015000\n",
      "Training Epoch: 11 [11392/50000]\tLoss: 1.5073\tLR: 0.015000\n",
      "Training Epoch: 11 [11520/50000]\tLoss: 1.4043\tLR: 0.015000\n",
      "Training Epoch: 11 [11648/50000]\tLoss: 1.3441\tLR: 0.015000\n",
      "Training Epoch: 11 [11776/50000]\tLoss: 1.4968\tLR: 0.015000\n",
      "Training Epoch: 11 [11904/50000]\tLoss: 1.3392\tLR: 0.015000\n",
      "Training Epoch: 11 [12032/50000]\tLoss: 1.7255\tLR: 0.015000\n",
      "Training Epoch: 11 [12160/50000]\tLoss: 1.1893\tLR: 0.015000\n",
      "Training Epoch: 11 [12288/50000]\tLoss: 1.2840\tLR: 0.015000\n",
      "Training Epoch: 11 [12416/50000]\tLoss: 1.2806\tLR: 0.015000\n",
      "Training Epoch: 11 [12544/50000]\tLoss: 1.4649\tLR: 0.015000\n",
      "Training Epoch: 11 [12672/50000]\tLoss: 1.4362\tLR: 0.015000\n",
      "Training Epoch: 11 [12800/50000]\tLoss: 1.5688\tLR: 0.015000\n",
      "Training Epoch: 11 [12928/50000]\tLoss: 1.4075\tLR: 0.015000\n",
      "Training Epoch: 11 [13056/50000]\tLoss: 1.6493\tLR: 0.015000\n",
      "Training Epoch: 11 [13184/50000]\tLoss: 1.4477\tLR: 0.015000\n",
      "Training Epoch: 11 [13312/50000]\tLoss: 1.3731\tLR: 0.015000\n",
      "Training Epoch: 11 [13440/50000]\tLoss: 1.5057\tLR: 0.015000\n",
      "Training Epoch: 11 [13568/50000]\tLoss: 1.5078\tLR: 0.015000\n",
      "Training Epoch: 11 [13696/50000]\tLoss: 1.5066\tLR: 0.015000\n",
      "Training Epoch: 11 [13824/50000]\tLoss: 1.2585\tLR: 0.015000\n",
      "Training Epoch: 11 [13952/50000]\tLoss: 1.5293\tLR: 0.015000\n",
      "Training Epoch: 11 [14080/50000]\tLoss: 1.2923\tLR: 0.015000\n",
      "Training Epoch: 11 [14208/50000]\tLoss: 1.4359\tLR: 0.015000\n",
      "Training Epoch: 11 [14336/50000]\tLoss: 1.8034\tLR: 0.015000\n",
      "Training Epoch: 11 [14464/50000]\tLoss: 1.6050\tLR: 0.015000\n",
      "Training Epoch: 11 [14592/50000]\tLoss: 1.4054\tLR: 0.015000\n",
      "Training Epoch: 11 [14720/50000]\tLoss: 1.8095\tLR: 0.015000\n",
      "Training Epoch: 11 [14848/50000]\tLoss: 1.5093\tLR: 0.015000\n",
      "Training Epoch: 11 [14976/50000]\tLoss: 1.3735\tLR: 0.015000\n",
      "Training Epoch: 11 [15104/50000]\tLoss: 1.3504\tLR: 0.015000\n",
      "Training Epoch: 11 [15232/50000]\tLoss: 1.3122\tLR: 0.015000\n",
      "Training Epoch: 11 [15360/50000]\tLoss: 1.4141\tLR: 0.015000\n",
      "Training Epoch: 11 [15488/50000]\tLoss: 1.4119\tLR: 0.015000\n",
      "Training Epoch: 11 [15616/50000]\tLoss: 1.4918\tLR: 0.015000\n",
      "Training Epoch: 11 [15744/50000]\tLoss: 1.6797\tLR: 0.015000\n",
      "Training Epoch: 11 [15872/50000]\tLoss: 1.3726\tLR: 0.015000\n",
      "Training Epoch: 11 [16000/50000]\tLoss: 1.5777\tLR: 0.015000\n",
      "Training Epoch: 11 [16128/50000]\tLoss: 1.3971\tLR: 0.015000\n",
      "Training Epoch: 11 [16256/50000]\tLoss: 1.2993\tLR: 0.015000\n",
      "Training Epoch: 11 [16384/50000]\tLoss: 1.5239\tLR: 0.015000\n",
      "Training Epoch: 11 [16512/50000]\tLoss: 1.5746\tLR: 0.015000\n",
      "Training Epoch: 11 [16640/50000]\tLoss: 1.5024\tLR: 0.015000\n",
      "Training Epoch: 11 [16768/50000]\tLoss: 1.5051\tLR: 0.015000\n",
      "Training Epoch: 11 [16896/50000]\tLoss: 1.4086\tLR: 0.015000\n",
      "Training Epoch: 11 [17024/50000]\tLoss: 1.3953\tLR: 0.015000\n",
      "Training Epoch: 11 [17152/50000]\tLoss: 1.5461\tLR: 0.015000\n",
      "Training Epoch: 11 [17280/50000]\tLoss: 1.6953\tLR: 0.015000\n",
      "Training Epoch: 11 [17408/50000]\tLoss: 1.3139\tLR: 0.015000\n",
      "Training Epoch: 11 [17536/50000]\tLoss: 1.5419\tLR: 0.015000\n",
      "Training Epoch: 11 [17664/50000]\tLoss: 1.4252\tLR: 0.015000\n",
      "Training Epoch: 11 [17792/50000]\tLoss: 1.5019\tLR: 0.015000\n",
      "Training Epoch: 11 [17920/50000]\tLoss: 1.5310\tLR: 0.015000\n",
      "Training Epoch: 11 [18048/50000]\tLoss: 1.3841\tLR: 0.015000\n",
      "Training Epoch: 11 [18176/50000]\tLoss: 1.6421\tLR: 0.015000\n",
      "Training Epoch: 11 [18304/50000]\tLoss: 1.7196\tLR: 0.015000\n",
      "Training Epoch: 11 [18432/50000]\tLoss: 1.3131\tLR: 0.015000\n",
      "Training Epoch: 11 [18560/50000]\tLoss: 1.2897\tLR: 0.015000\n",
      "Training Epoch: 11 [18688/50000]\tLoss: 1.5024\tLR: 0.015000\n",
      "Training Epoch: 11 [18816/50000]\tLoss: 1.5016\tLR: 0.015000\n",
      "Training Epoch: 11 [18944/50000]\tLoss: 1.2867\tLR: 0.015000\n",
      "Training Epoch: 11 [19072/50000]\tLoss: 1.3880\tLR: 0.015000\n",
      "Training Epoch: 11 [19200/50000]\tLoss: 1.5168\tLR: 0.015000\n",
      "Training Epoch: 11 [19328/50000]\tLoss: 1.5030\tLR: 0.015000\n",
      "Training Epoch: 11 [19456/50000]\tLoss: 1.3856\tLR: 0.015000\n",
      "Training Epoch: 11 [19584/50000]\tLoss: 1.5419\tLR: 0.015000\n",
      "Training Epoch: 11 [19712/50000]\tLoss: 1.4818\tLR: 0.015000\n",
      "Training Epoch: 11 [19840/50000]\tLoss: 1.4970\tLR: 0.015000\n",
      "Training Epoch: 11 [19968/50000]\tLoss: 1.4437\tLR: 0.015000\n",
      "Training Epoch: 11 [20096/50000]\tLoss: 1.5125\tLR: 0.015000\n",
      "Training Epoch: 11 [20224/50000]\tLoss: 1.4726\tLR: 0.015000\n",
      "Training Epoch: 11 [20352/50000]\tLoss: 1.4278\tLR: 0.015000\n",
      "Training Epoch: 11 [20480/50000]\tLoss: 1.2882\tLR: 0.015000\n",
      "Training Epoch: 11 [20608/50000]\tLoss: 1.5131\tLR: 0.015000\n",
      "Training Epoch: 11 [20736/50000]\tLoss: 1.6165\tLR: 0.015000\n",
      "Training Epoch: 11 [20864/50000]\tLoss: 1.4603\tLR: 0.015000\n",
      "Training Epoch: 11 [20992/50000]\tLoss: 1.3373\tLR: 0.015000\n",
      "Training Epoch: 11 [21120/50000]\tLoss: 1.5720\tLR: 0.015000\n",
      "Training Epoch: 11 [21248/50000]\tLoss: 1.3672\tLR: 0.015000\n",
      "Training Epoch: 11 [21376/50000]\tLoss: 1.2676\tLR: 0.015000\n",
      "Training Epoch: 11 [21504/50000]\tLoss: 1.6077\tLR: 0.015000\n",
      "Training Epoch: 11 [21632/50000]\tLoss: 1.5486\tLR: 0.015000\n",
      "Training Epoch: 11 [21760/50000]\tLoss: 1.4135\tLR: 0.015000\n",
      "Training Epoch: 11 [21888/50000]\tLoss: 1.5505\tLR: 0.015000\n",
      "Training Epoch: 11 [22016/50000]\tLoss: 1.4821\tLR: 0.015000\n",
      "Training Epoch: 11 [22144/50000]\tLoss: 1.5370\tLR: 0.015000\n",
      "Training Epoch: 11 [22272/50000]\tLoss: 1.3412\tLR: 0.015000\n",
      "Training Epoch: 11 [22400/50000]\tLoss: 1.4319\tLR: 0.015000\n",
      "Training Epoch: 11 [22528/50000]\tLoss: 1.4757\tLR: 0.015000\n",
      "Training Epoch: 11 [22656/50000]\tLoss: 1.2625\tLR: 0.015000\n",
      "Training Epoch: 11 [22784/50000]\tLoss: 1.3231\tLR: 0.015000\n",
      "Training Epoch: 11 [22912/50000]\tLoss: 1.4673\tLR: 0.015000\n",
      "Training Epoch: 11 [23040/50000]\tLoss: 1.4895\tLR: 0.015000\n",
      "Training Epoch: 11 [23168/50000]\tLoss: 1.2132\tLR: 0.015000\n",
      "Training Epoch: 11 [23296/50000]\tLoss: 1.3043\tLR: 0.015000\n",
      "Training Epoch: 11 [23424/50000]\tLoss: 1.7817\tLR: 0.015000\n",
      "Training Epoch: 11 [23552/50000]\tLoss: 1.4488\tLR: 0.015000\n",
      "Training Epoch: 11 [23680/50000]\tLoss: 1.4413\tLR: 0.015000\n",
      "Training Epoch: 11 [23808/50000]\tLoss: 1.4461\tLR: 0.015000\n",
      "Training Epoch: 11 [23936/50000]\tLoss: 1.4831\tLR: 0.015000\n",
      "Training Epoch: 11 [24064/50000]\tLoss: 1.3793\tLR: 0.015000\n",
      "Training Epoch: 11 [24192/50000]\tLoss: 1.4005\tLR: 0.015000\n",
      "Training Epoch: 11 [24320/50000]\tLoss: 1.4082\tLR: 0.015000\n",
      "Training Epoch: 11 [24448/50000]\tLoss: 1.3887\tLR: 0.015000\n",
      "Training Epoch: 11 [24576/50000]\tLoss: 1.5400\tLR: 0.015000\n",
      "Training Epoch: 11 [24704/50000]\tLoss: 1.5885\tLR: 0.015000\n",
      "Training Epoch: 11 [24832/50000]\tLoss: 1.3500\tLR: 0.015000\n",
      "Training Epoch: 11 [24960/50000]\tLoss: 1.6792\tLR: 0.015000\n",
      "Training Epoch: 11 [25088/50000]\tLoss: 1.4873\tLR: 0.015000\n",
      "Training Epoch: 11 [25216/50000]\tLoss: 1.4903\tLR: 0.015000\n",
      "Training Epoch: 11 [25344/50000]\tLoss: 1.4376\tLR: 0.015000\n",
      "Training Epoch: 11 [25472/50000]\tLoss: 1.2971\tLR: 0.015000\n",
      "Training Epoch: 11 [25600/50000]\tLoss: 1.6227\tLR: 0.015000\n",
      "Training Epoch: 11 [25728/50000]\tLoss: 1.4960\tLR: 0.015000\n",
      "Training Epoch: 11 [25856/50000]\tLoss: 1.3957\tLR: 0.015000\n",
      "Training Epoch: 11 [25984/50000]\tLoss: 1.4031\tLR: 0.015000\n",
      "Training Epoch: 11 [26112/50000]\tLoss: 1.4787\tLR: 0.015000\n",
      "Training Epoch: 11 [26240/50000]\tLoss: 1.3851\tLR: 0.015000\n",
      "Training Epoch: 11 [26368/50000]\tLoss: 1.8661\tLR: 0.015000\n",
      "Training Epoch: 11 [26496/50000]\tLoss: 1.7519\tLR: 0.015000\n",
      "Training Epoch: 11 [26624/50000]\tLoss: 1.4951\tLR: 0.015000\n",
      "Training Epoch: 11 [26752/50000]\tLoss: 1.4961\tLR: 0.015000\n",
      "Training Epoch: 11 [26880/50000]\tLoss: 1.6089\tLR: 0.015000\n",
      "Training Epoch: 11 [27008/50000]\tLoss: 1.3218\tLR: 0.015000\n",
      "Training Epoch: 11 [27136/50000]\tLoss: 1.2846\tLR: 0.015000\n",
      "Training Epoch: 11 [27264/50000]\tLoss: 1.3837\tLR: 0.015000\n",
      "Training Epoch: 11 [27392/50000]\tLoss: 1.5633\tLR: 0.015000\n",
      "Training Epoch: 11 [27520/50000]\tLoss: 1.6738\tLR: 0.015000\n",
      "Training Epoch: 11 [27648/50000]\tLoss: 1.7104\tLR: 0.015000\n",
      "Training Epoch: 11 [27776/50000]\tLoss: 1.4666\tLR: 0.015000\n",
      "Training Epoch: 11 [27904/50000]\tLoss: 1.3652\tLR: 0.015000\n",
      "Training Epoch: 11 [28032/50000]\tLoss: 1.5752\tLR: 0.015000\n",
      "Training Epoch: 11 [28160/50000]\tLoss: 1.4775\tLR: 0.015000\n",
      "Training Epoch: 11 [28288/50000]\tLoss: 1.5088\tLR: 0.015000\n",
      "Training Epoch: 11 [28416/50000]\tLoss: 1.6255\tLR: 0.015000\n",
      "Training Epoch: 11 [28544/50000]\tLoss: 1.2795\tLR: 0.015000\n",
      "Training Epoch: 11 [28672/50000]\tLoss: 1.6406\tLR: 0.015000\n",
      "Training Epoch: 11 [28800/50000]\tLoss: 1.2313\tLR: 0.015000\n",
      "Training Epoch: 11 [28928/50000]\tLoss: 1.5374\tLR: 0.015000\n",
      "Training Epoch: 11 [29056/50000]\tLoss: 1.4619\tLR: 0.015000\n",
      "Training Epoch: 11 [29184/50000]\tLoss: 1.6115\tLR: 0.015000\n",
      "Training Epoch: 11 [29312/50000]\tLoss: 1.5805\tLR: 0.015000\n",
      "Training Epoch: 11 [29440/50000]\tLoss: 1.5285\tLR: 0.015000\n",
      "Training Epoch: 11 [29568/50000]\tLoss: 1.3534\tLR: 0.015000\n",
      "Training Epoch: 11 [29696/50000]\tLoss: 1.3642\tLR: 0.015000\n",
      "Training Epoch: 11 [29824/50000]\tLoss: 1.4379\tLR: 0.015000\n",
      "Training Epoch: 11 [29952/50000]\tLoss: 1.4179\tLR: 0.015000\n",
      "Training Epoch: 11 [30080/50000]\tLoss: 1.5141\tLR: 0.015000\n",
      "Training Epoch: 11 [30208/50000]\tLoss: 1.6280\tLR: 0.015000\n",
      "Training Epoch: 11 [30336/50000]\tLoss: 1.5321\tLR: 0.015000\n",
      "Training Epoch: 11 [30464/50000]\tLoss: 1.5802\tLR: 0.015000\n",
      "Training Epoch: 11 [30592/50000]\tLoss: 1.8149\tLR: 0.015000\n",
      "Training Epoch: 11 [30720/50000]\tLoss: 1.5029\tLR: 0.015000\n",
      "Training Epoch: 11 [30848/50000]\tLoss: 1.5952\tLR: 0.015000\n",
      "Training Epoch: 11 [30976/50000]\tLoss: 1.3226\tLR: 0.015000\n",
      "Training Epoch: 11 [31104/50000]\tLoss: 1.4844\tLR: 0.015000\n",
      "Training Epoch: 11 [31232/50000]\tLoss: 1.3712\tLR: 0.015000\n",
      "Training Epoch: 11 [31360/50000]\tLoss: 1.3771\tLR: 0.015000\n",
      "Training Epoch: 11 [31488/50000]\tLoss: 1.2501\tLR: 0.015000\n",
      "Training Epoch: 11 [31616/50000]\tLoss: 1.5863\tLR: 0.015000\n",
      "Training Epoch: 11 [31744/50000]\tLoss: 1.3414\tLR: 0.015000\n",
      "Training Epoch: 11 [31872/50000]\tLoss: 1.6826\tLR: 0.015000\n",
      "Training Epoch: 11 [32000/50000]\tLoss: 1.3616\tLR: 0.015000\n",
      "Training Epoch: 11 [32128/50000]\tLoss: 1.6803\tLR: 0.015000\n",
      "Training Epoch: 11 [32256/50000]\tLoss: 1.3858\tLR: 0.015000\n",
      "Training Epoch: 11 [32384/50000]\tLoss: 1.4926\tLR: 0.015000\n",
      "Training Epoch: 11 [32512/50000]\tLoss: 1.5361\tLR: 0.015000\n",
      "Training Epoch: 11 [32640/50000]\tLoss: 1.2172\tLR: 0.015000\n",
      "Training Epoch: 11 [32768/50000]\tLoss: 1.5339\tLR: 0.015000\n",
      "Training Epoch: 11 [32896/50000]\tLoss: 1.2384\tLR: 0.015000\n",
      "Training Epoch: 11 [33024/50000]\tLoss: 1.3106\tLR: 0.015000\n",
      "Training Epoch: 11 [33152/50000]\tLoss: 1.4466\tLR: 0.015000\n",
      "Training Epoch: 11 [33280/50000]\tLoss: 1.3316\tLR: 0.015000\n",
      "Training Epoch: 11 [33408/50000]\tLoss: 1.1980\tLR: 0.015000\n",
      "Training Epoch: 11 [33536/50000]\tLoss: 1.5042\tLR: 0.015000\n",
      "Training Epoch: 11 [33664/50000]\tLoss: 1.4292\tLR: 0.015000\n",
      "Training Epoch: 11 [33792/50000]\tLoss: 1.4054\tLR: 0.015000\n",
      "Training Epoch: 11 [33920/50000]\tLoss: 1.4841\tLR: 0.015000\n",
      "Training Epoch: 11 [34048/50000]\tLoss: 1.5587\tLR: 0.015000\n",
      "Training Epoch: 11 [34176/50000]\tLoss: 1.5356\tLR: 0.015000\n",
      "Training Epoch: 11 [34304/50000]\tLoss: 1.4433\tLR: 0.015000\n",
      "Training Epoch: 11 [34432/50000]\tLoss: 1.6559\tLR: 0.015000\n",
      "Training Epoch: 11 [34560/50000]\tLoss: 1.5667\tLR: 0.015000\n",
      "Training Epoch: 11 [34688/50000]\tLoss: 1.2384\tLR: 0.015000\n",
      "Training Epoch: 11 [34816/50000]\tLoss: 1.5378\tLR: 0.015000\n",
      "Training Epoch: 11 [34944/50000]\tLoss: 1.8031\tLR: 0.015000\n",
      "Training Epoch: 11 [35072/50000]\tLoss: 1.4047\tLR: 0.015000\n",
      "Training Epoch: 11 [35200/50000]\tLoss: 1.2898\tLR: 0.015000\n",
      "Training Epoch: 11 [35328/50000]\tLoss: 1.2966\tLR: 0.015000\n",
      "Training Epoch: 11 [35456/50000]\tLoss: 1.4416\tLR: 0.015000\n",
      "Training Epoch: 11 [35584/50000]\tLoss: 1.6485\tLR: 0.015000\n",
      "Training Epoch: 11 [35712/50000]\tLoss: 1.4950\tLR: 0.015000\n",
      "Training Epoch: 11 [35840/50000]\tLoss: 1.4801\tLR: 0.015000\n",
      "Training Epoch: 11 [35968/50000]\tLoss: 1.3471\tLR: 0.015000\n",
      "Training Epoch: 11 [36096/50000]\tLoss: 1.1083\tLR: 0.015000\n",
      "Training Epoch: 11 [36224/50000]\tLoss: 1.6210\tLR: 0.015000\n",
      "Training Epoch: 11 [36352/50000]\tLoss: 1.3028\tLR: 0.015000\n",
      "Training Epoch: 11 [36480/50000]\tLoss: 1.3672\tLR: 0.015000\n",
      "Training Epoch: 11 [36608/50000]\tLoss: 1.4930\tLR: 0.015000\n",
      "Training Epoch: 11 [36736/50000]\tLoss: 1.2585\tLR: 0.015000\n",
      "Training Epoch: 11 [36864/50000]\tLoss: 1.6743\tLR: 0.015000\n",
      "Training Epoch: 11 [36992/50000]\tLoss: 1.5557\tLR: 0.015000\n",
      "Training Epoch: 11 [37120/50000]\tLoss: 1.5785\tLR: 0.015000\n",
      "Training Epoch: 11 [37248/50000]\tLoss: 1.3709\tLR: 0.015000\n",
      "Training Epoch: 11 [37376/50000]\tLoss: 1.3553\tLR: 0.015000\n",
      "Training Epoch: 11 [37504/50000]\tLoss: 1.2993\tLR: 0.015000\n",
      "Training Epoch: 11 [37632/50000]\tLoss: 1.5494\tLR: 0.015000\n",
      "Training Epoch: 11 [37760/50000]\tLoss: 1.7132\tLR: 0.015000\n",
      "Training Epoch: 11 [37888/50000]\tLoss: 1.2446\tLR: 0.015000\n",
      "Training Epoch: 11 [38016/50000]\tLoss: 1.3207\tLR: 0.015000\n",
      "Training Epoch: 11 [38144/50000]\tLoss: 1.6953\tLR: 0.015000\n",
      "Training Epoch: 11 [38272/50000]\tLoss: 1.5119\tLR: 0.015000\n",
      "Training Epoch: 11 [38400/50000]\tLoss: 1.5852\tLR: 0.015000\n",
      "Training Epoch: 11 [38528/50000]\tLoss: 1.4197\tLR: 0.015000\n",
      "Training Epoch: 11 [38656/50000]\tLoss: 1.1331\tLR: 0.015000\n",
      "Training Epoch: 11 [38784/50000]\tLoss: 1.4167\tLR: 0.015000\n",
      "Training Epoch: 11 [38912/50000]\tLoss: 1.6017\tLR: 0.015000\n",
      "Training Epoch: 11 [39040/50000]\tLoss: 1.3869\tLR: 0.015000\n",
      "Training Epoch: 11 [39168/50000]\tLoss: 1.4384\tLR: 0.015000\n",
      "Training Epoch: 11 [39296/50000]\tLoss: 1.3197\tLR: 0.015000\n",
      "Training Epoch: 11 [39424/50000]\tLoss: 1.4135\tLR: 0.015000\n",
      "Training Epoch: 11 [39552/50000]\tLoss: 1.6393\tLR: 0.015000\n",
      "Training Epoch: 11 [39680/50000]\tLoss: 1.8305\tLR: 0.015000\n",
      "Training Epoch: 11 [39808/50000]\tLoss: 1.3999\tLR: 0.015000\n",
      "Training Epoch: 11 [39936/50000]\tLoss: 1.5111\tLR: 0.015000\n",
      "Training Epoch: 11 [40064/50000]\tLoss: 1.4082\tLR: 0.015000\n",
      "Training Epoch: 11 [40192/50000]\tLoss: 1.5092\tLR: 0.015000\n",
      "Training Epoch: 11 [40320/50000]\tLoss: 1.1469\tLR: 0.015000\n",
      "Training Epoch: 11 [40448/50000]\tLoss: 1.6023\tLR: 0.015000\n",
      "Training Epoch: 11 [40576/50000]\tLoss: 1.5166\tLR: 0.015000\n",
      "Training Epoch: 11 [40704/50000]\tLoss: 1.4006\tLR: 0.015000\n",
      "Training Epoch: 11 [40832/50000]\tLoss: 1.5265\tLR: 0.015000\n",
      "Training Epoch: 11 [40960/50000]\tLoss: 1.3844\tLR: 0.015000\n",
      "Training Epoch: 11 [41088/50000]\tLoss: 1.4494\tLR: 0.015000\n",
      "Training Epoch: 11 [41216/50000]\tLoss: 1.4165\tLR: 0.015000\n",
      "Training Epoch: 11 [41344/50000]\tLoss: 1.6310\tLR: 0.015000\n",
      "Training Epoch: 11 [41472/50000]\tLoss: 1.2843\tLR: 0.015000\n",
      "Training Epoch: 11 [41600/50000]\tLoss: 1.2082\tLR: 0.015000\n",
      "Training Epoch: 11 [41728/50000]\tLoss: 1.4275\tLR: 0.015000\n",
      "Training Epoch: 11 [41856/50000]\tLoss: 1.3876\tLR: 0.015000\n",
      "Training Epoch: 11 [41984/50000]\tLoss: 1.4518\tLR: 0.015000\n",
      "Training Epoch: 11 [42112/50000]\tLoss: 1.6979\tLR: 0.015000\n",
      "Training Epoch: 11 [42240/50000]\tLoss: 1.2653\tLR: 0.015000\n",
      "Training Epoch: 11 [42368/50000]\tLoss: 1.2842\tLR: 0.015000\n",
      "Training Epoch: 11 [42496/50000]\tLoss: 1.5137\tLR: 0.015000\n",
      "Training Epoch: 11 [42624/50000]\tLoss: 1.3638\tLR: 0.015000\n",
      "Training Epoch: 11 [42752/50000]\tLoss: 1.4190\tLR: 0.015000\n",
      "Training Epoch: 11 [42880/50000]\tLoss: 1.6845\tLR: 0.015000\n",
      "Training Epoch: 11 [43008/50000]\tLoss: 1.2448\tLR: 0.015000\n",
      "Training Epoch: 11 [43136/50000]\tLoss: 1.5026\tLR: 0.015000\n",
      "Training Epoch: 11 [43264/50000]\tLoss: 1.4346\tLR: 0.015000\n",
      "Training Epoch: 11 [43392/50000]\tLoss: 1.3514\tLR: 0.015000\n",
      "Training Epoch: 11 [43520/50000]\tLoss: 1.5697\tLR: 0.015000\n",
      "Training Epoch: 11 [43648/50000]\tLoss: 1.6297\tLR: 0.015000\n",
      "Training Epoch: 11 [43776/50000]\tLoss: 1.3090\tLR: 0.015000\n",
      "Training Epoch: 11 [43904/50000]\tLoss: 1.5534\tLR: 0.015000\n",
      "Training Epoch: 11 [44032/50000]\tLoss: 1.4796\tLR: 0.015000\n",
      "Training Epoch: 11 [44160/50000]\tLoss: 1.2171\tLR: 0.015000\n",
      "Training Epoch: 11 [44288/50000]\tLoss: 1.3900\tLR: 0.015000\n",
      "Training Epoch: 11 [44416/50000]\tLoss: 1.4086\tLR: 0.015000\n",
      "Training Epoch: 11 [44544/50000]\tLoss: 1.4499\tLR: 0.015000\n",
      "Training Epoch: 11 [44672/50000]\tLoss: 1.3285\tLR: 0.015000\n",
      "Training Epoch: 11 [44800/50000]\tLoss: 1.4932\tLR: 0.015000\n",
      "Training Epoch: 11 [44928/50000]\tLoss: 1.2419\tLR: 0.015000\n",
      "Training Epoch: 11 [45056/50000]\tLoss: 1.5774\tLR: 0.015000\n",
      "Training Epoch: 11 [45184/50000]\tLoss: 1.3808\tLR: 0.015000\n",
      "Training Epoch: 11 [45312/50000]\tLoss: 1.5142\tLR: 0.015000\n",
      "Training Epoch: 11 [45440/50000]\tLoss: 1.7157\tLR: 0.015000\n",
      "Training Epoch: 11 [45568/50000]\tLoss: 1.5534\tLR: 0.015000\n",
      "Training Epoch: 11 [45696/50000]\tLoss: 1.5591\tLR: 0.015000\n",
      "Training Epoch: 11 [45824/50000]\tLoss: 1.7832\tLR: 0.015000\n",
      "Training Epoch: 11 [45952/50000]\tLoss: 1.3307\tLR: 0.015000\n",
      "Training Epoch: 11 [46080/50000]\tLoss: 1.5121\tLR: 0.015000\n",
      "Training Epoch: 11 [46208/50000]\tLoss: 1.3576\tLR: 0.015000\n",
      "Training Epoch: 11 [46336/50000]\tLoss: 1.3984\tLR: 0.015000\n",
      "Training Epoch: 11 [46464/50000]\tLoss: 1.5810\tLR: 0.015000\n",
      "Training Epoch: 11 [46592/50000]\tLoss: 1.4047\tLR: 0.015000\n",
      "Training Epoch: 11 [46720/50000]\tLoss: 1.5258\tLR: 0.015000\n",
      "Training Epoch: 11 [46848/50000]\tLoss: 1.7418\tLR: 0.015000\n",
      "Training Epoch: 11 [46976/50000]\tLoss: 1.4982\tLR: 0.015000\n",
      "Training Epoch: 11 [47104/50000]\tLoss: 1.5811\tLR: 0.015000\n",
      "Training Epoch: 11 [47232/50000]\tLoss: 1.4490\tLR: 0.015000\n",
      "Training Epoch: 11 [47360/50000]\tLoss: 1.4094\tLR: 0.015000\n",
      "Training Epoch: 11 [47488/50000]\tLoss: 1.6046\tLR: 0.015000\n",
      "Training Epoch: 11 [47616/50000]\tLoss: 1.4446\tLR: 0.015000\n",
      "Training Epoch: 11 [47744/50000]\tLoss: 1.4491\tLR: 0.015000\n",
      "Training Epoch: 11 [47872/50000]\tLoss: 1.3995\tLR: 0.015000\n",
      "Training Epoch: 11 [48000/50000]\tLoss: 1.5291\tLR: 0.015000\n",
      "Training Epoch: 11 [48128/50000]\tLoss: 1.5254\tLR: 0.015000\n",
      "Training Epoch: 11 [48256/50000]\tLoss: 1.6619\tLR: 0.015000\n",
      "Training Epoch: 11 [48384/50000]\tLoss: 1.6732\tLR: 0.015000\n",
      "Training Epoch: 11 [48512/50000]\tLoss: 1.3637\tLR: 0.015000\n",
      "Training Epoch: 11 [48640/50000]\tLoss: 1.4472\tLR: 0.015000\n",
      "Training Epoch: 11 [48768/50000]\tLoss: 1.4455\tLR: 0.015000\n",
      "Training Epoch: 11 [48896/50000]\tLoss: 1.4326\tLR: 0.015000\n",
      "Training Epoch: 11 [49024/50000]\tLoss: 1.5992\tLR: 0.015000\n",
      "Training Epoch: 11 [49152/50000]\tLoss: 1.4193\tLR: 0.015000\n",
      "Training Epoch: 11 [49280/50000]\tLoss: 1.4492\tLR: 0.015000\n",
      "Training Epoch: 11 [49408/50000]\tLoss: 1.3772\tLR: 0.015000\n",
      "Training Epoch: 11 [49536/50000]\tLoss: 1.4745\tLR: 0.015000\n",
      "Training Epoch: 11 [49664/50000]\tLoss: 1.6222\tLR: 0.015000\n",
      "Training Epoch: 11 [49792/50000]\tLoss: 1.6111\tLR: 0.015000\n",
      "Training Epoch: 11 [49920/50000]\tLoss: 1.2453\tLR: 0.015000\n",
      "Training Epoch: 11 [50000/50000]\tLoss: 1.5342\tLR: 0.015000\n",
      "Test set: Average loss: 0.0126, Accuracy: 0.5568\n",
      "\n",
      "Training Epoch: 12 [128/50000]\tLoss: 1.3656\tLR: 0.002250\n",
      "Training Epoch: 12 [256/50000]\tLoss: 1.6072\tLR: 0.002250\n",
      "Training Epoch: 12 [384/50000]\tLoss: 1.3148\tLR: 0.002250\n",
      "Training Epoch: 12 [512/50000]\tLoss: 1.6088\tLR: 0.002250\n",
      "Training Epoch: 12 [640/50000]\tLoss: 1.4807\tLR: 0.002250\n",
      "Training Epoch: 12 [768/50000]\tLoss: 1.4471\tLR: 0.002250\n",
      "Training Epoch: 12 [896/50000]\tLoss: 1.6014\tLR: 0.002250\n",
      "Training Epoch: 12 [1024/50000]\tLoss: 1.2493\tLR: 0.002250\n",
      "Training Epoch: 12 [1152/50000]\tLoss: 1.5407\tLR: 0.002250\n",
      "Training Epoch: 12 [1280/50000]\tLoss: 1.3604\tLR: 0.002250\n",
      "Training Epoch: 12 [1408/50000]\tLoss: 1.2977\tLR: 0.002250\n",
      "Training Epoch: 12 [1536/50000]\tLoss: 1.4808\tLR: 0.002250\n",
      "Training Epoch: 12 [1664/50000]\tLoss: 1.2308\tLR: 0.002250\n",
      "Training Epoch: 12 [1792/50000]\tLoss: 1.4059\tLR: 0.002250\n",
      "Training Epoch: 12 [1920/50000]\tLoss: 1.3304\tLR: 0.002250\n",
      "Training Epoch: 12 [2048/50000]\tLoss: 1.2930\tLR: 0.002250\n",
      "Training Epoch: 12 [2176/50000]\tLoss: 1.2189\tLR: 0.002250\n",
      "Training Epoch: 12 [2304/50000]\tLoss: 1.2518\tLR: 0.002250\n",
      "Training Epoch: 12 [2432/50000]\tLoss: 1.5019\tLR: 0.002250\n",
      "Training Epoch: 12 [2560/50000]\tLoss: 1.4494\tLR: 0.002250\n",
      "Training Epoch: 12 [2688/50000]\tLoss: 1.5016\tLR: 0.002250\n",
      "Training Epoch: 12 [2816/50000]\tLoss: 1.1774\tLR: 0.002250\n",
      "Training Epoch: 12 [2944/50000]\tLoss: 1.3405\tLR: 0.002250\n",
      "Training Epoch: 12 [3072/50000]\tLoss: 1.3572\tLR: 0.002250\n",
      "Training Epoch: 12 [3200/50000]\tLoss: 1.2183\tLR: 0.002250\n",
      "Training Epoch: 12 [3328/50000]\tLoss: 1.1074\tLR: 0.002250\n",
      "Training Epoch: 12 [3456/50000]\tLoss: 1.1954\tLR: 0.002250\n",
      "Training Epoch: 12 [3584/50000]\tLoss: 1.1642\tLR: 0.002250\n",
      "Training Epoch: 12 [3712/50000]\tLoss: 1.6076\tLR: 0.002250\n",
      "Training Epoch: 12 [3840/50000]\tLoss: 1.4861\tLR: 0.002250\n",
      "Training Epoch: 12 [3968/50000]\tLoss: 1.3588\tLR: 0.002250\n",
      "Training Epoch: 12 [4096/50000]\tLoss: 1.3618\tLR: 0.002250\n",
      "Training Epoch: 12 [4224/50000]\tLoss: 1.2308\tLR: 0.002250\n",
      "Training Epoch: 12 [4352/50000]\tLoss: 1.2488\tLR: 0.002250\n",
      "Training Epoch: 12 [4480/50000]\tLoss: 1.3815\tLR: 0.002250\n",
      "Training Epoch: 12 [4608/50000]\tLoss: 1.4403\tLR: 0.002250\n",
      "Training Epoch: 12 [4736/50000]\tLoss: 1.2128\tLR: 0.002250\n",
      "Training Epoch: 12 [4864/50000]\tLoss: 1.4602\tLR: 0.002250\n",
      "Training Epoch: 12 [4992/50000]\tLoss: 1.0755\tLR: 0.002250\n",
      "Training Epoch: 12 [5120/50000]\tLoss: 1.3367\tLR: 0.002250\n",
      "Training Epoch: 12 [5248/50000]\tLoss: 1.2736\tLR: 0.002250\n",
      "Training Epoch: 12 [5376/50000]\tLoss: 1.1828\tLR: 0.002250\n",
      "Training Epoch: 12 [5504/50000]\tLoss: 1.2013\tLR: 0.002250\n",
      "Training Epoch: 12 [5632/50000]\tLoss: 1.1149\tLR: 0.002250\n",
      "Training Epoch: 12 [5760/50000]\tLoss: 1.4640\tLR: 0.002250\n",
      "Training Epoch: 12 [5888/50000]\tLoss: 1.2835\tLR: 0.002250\n",
      "Training Epoch: 12 [6016/50000]\tLoss: 1.1849\tLR: 0.002250\n",
      "Training Epoch: 12 [6144/50000]\tLoss: 1.2859\tLR: 0.002250\n",
      "Training Epoch: 12 [6272/50000]\tLoss: 1.4608\tLR: 0.002250\n",
      "Training Epoch: 12 [6400/50000]\tLoss: 1.4101\tLR: 0.002250\n",
      "Training Epoch: 12 [6528/50000]\tLoss: 1.3880\tLR: 0.002250\n",
      "Training Epoch: 12 [6656/50000]\tLoss: 1.3816\tLR: 0.002250\n",
      "Training Epoch: 12 [6784/50000]\tLoss: 1.3369\tLR: 0.002250\n",
      "Training Epoch: 12 [6912/50000]\tLoss: 1.2640\tLR: 0.002250\n",
      "Training Epoch: 12 [7040/50000]\tLoss: 1.4075\tLR: 0.002250\n",
      "Training Epoch: 12 [7168/50000]\tLoss: 1.3454\tLR: 0.002250\n",
      "Training Epoch: 12 [7296/50000]\tLoss: 1.3753\tLR: 0.002250\n",
      "Training Epoch: 12 [7424/50000]\tLoss: 1.3337\tLR: 0.002250\n",
      "Training Epoch: 12 [7552/50000]\tLoss: 1.2471\tLR: 0.002250\n",
      "Training Epoch: 12 [7680/50000]\tLoss: 1.3273\tLR: 0.002250\n",
      "Training Epoch: 12 [7808/50000]\tLoss: 1.4116\tLR: 0.002250\n",
      "Training Epoch: 12 [7936/50000]\tLoss: 1.3625\tLR: 0.002250\n",
      "Training Epoch: 12 [8064/50000]\tLoss: 1.3227\tLR: 0.002250\n",
      "Training Epoch: 12 [8192/50000]\tLoss: 1.3666\tLR: 0.002250\n",
      "Training Epoch: 12 [8320/50000]\tLoss: 1.1708\tLR: 0.002250\n",
      "Training Epoch: 12 [8448/50000]\tLoss: 1.0887\tLR: 0.002250\n",
      "Training Epoch: 12 [8576/50000]\tLoss: 1.3125\tLR: 0.002250\n",
      "Training Epoch: 12 [8704/50000]\tLoss: 1.2639\tLR: 0.002250\n",
      "Training Epoch: 12 [8832/50000]\tLoss: 1.4900\tLR: 0.002250\n",
      "Training Epoch: 12 [8960/50000]\tLoss: 1.2562\tLR: 0.002250\n",
      "Training Epoch: 12 [9088/50000]\tLoss: 1.3004\tLR: 0.002250\n",
      "Training Epoch: 12 [9216/50000]\tLoss: 1.3813\tLR: 0.002250\n",
      "Training Epoch: 12 [9344/50000]\tLoss: 1.0963\tLR: 0.002250\n",
      "Training Epoch: 12 [9472/50000]\tLoss: 1.1866\tLR: 0.002250\n",
      "Training Epoch: 12 [9600/50000]\tLoss: 1.3787\tLR: 0.002250\n",
      "Training Epoch: 12 [9728/50000]\tLoss: 1.5074\tLR: 0.002250\n",
      "Training Epoch: 12 [9856/50000]\tLoss: 1.2244\tLR: 0.002250\n",
      "Training Epoch: 12 [9984/50000]\tLoss: 0.8630\tLR: 0.002250\n",
      "Training Epoch: 12 [10112/50000]\tLoss: 1.1822\tLR: 0.002250\n",
      "Training Epoch: 12 [10240/50000]\tLoss: 1.3466\tLR: 0.002250\n",
      "Training Epoch: 12 [10368/50000]\tLoss: 1.2877\tLR: 0.002250\n",
      "Training Epoch: 12 [10496/50000]\tLoss: 1.2291\tLR: 0.002250\n",
      "Training Epoch: 12 [10624/50000]\tLoss: 1.1550\tLR: 0.002250\n",
      "Training Epoch: 12 [10752/50000]\tLoss: 1.3983\tLR: 0.002250\n",
      "Training Epoch: 12 [10880/50000]\tLoss: 1.3486\tLR: 0.002250\n",
      "Training Epoch: 12 [11008/50000]\tLoss: 1.1486\tLR: 0.002250\n",
      "Training Epoch: 12 [11136/50000]\tLoss: 1.5801\tLR: 0.002250\n",
      "Training Epoch: 12 [11264/50000]\tLoss: 1.5240\tLR: 0.002250\n",
      "Training Epoch: 12 [11392/50000]\tLoss: 1.3943\tLR: 0.002250\n",
      "Training Epoch: 12 [11520/50000]\tLoss: 1.2683\tLR: 0.002250\n",
      "Training Epoch: 12 [11648/50000]\tLoss: 0.9922\tLR: 0.002250\n",
      "Training Epoch: 12 [11776/50000]\tLoss: 1.2759\tLR: 0.002250\n",
      "Training Epoch: 12 [11904/50000]\tLoss: 1.2736\tLR: 0.002250\n",
      "Training Epoch: 12 [12032/50000]\tLoss: 1.0756\tLR: 0.002250\n",
      "Training Epoch: 12 [12160/50000]\tLoss: 1.2440\tLR: 0.002250\n",
      "Training Epoch: 12 [12288/50000]\tLoss: 1.2736\tLR: 0.002250\n",
      "Training Epoch: 12 [12416/50000]\tLoss: 1.4456\tLR: 0.002250\n",
      "Training Epoch: 12 [12544/50000]\tLoss: 1.4819\tLR: 0.002250\n",
      "Training Epoch: 12 [12672/50000]\tLoss: 1.2015\tLR: 0.002250\n",
      "Training Epoch: 12 [12800/50000]\tLoss: 1.2054\tLR: 0.002250\n",
      "Training Epoch: 12 [12928/50000]\tLoss: 1.2015\tLR: 0.002250\n",
      "Training Epoch: 12 [13056/50000]\tLoss: 1.2956\tLR: 0.002250\n",
      "Training Epoch: 12 [13184/50000]\tLoss: 1.2572\tLR: 0.002250\n",
      "Training Epoch: 12 [13312/50000]\tLoss: 1.2170\tLR: 0.002250\n",
      "Training Epoch: 12 [13440/50000]\tLoss: 1.1015\tLR: 0.002250\n",
      "Training Epoch: 12 [13568/50000]\tLoss: 1.4018\tLR: 0.002250\n",
      "Training Epoch: 12 [13696/50000]\tLoss: 1.2644\tLR: 0.002250\n",
      "Training Epoch: 12 [13824/50000]\tLoss: 1.4811\tLR: 0.002250\n",
      "Training Epoch: 12 [13952/50000]\tLoss: 1.2437\tLR: 0.002250\n",
      "Training Epoch: 12 [14080/50000]\tLoss: 1.3349\tLR: 0.002250\n",
      "Training Epoch: 12 [14208/50000]\tLoss: 1.0253\tLR: 0.002250\n",
      "Training Epoch: 12 [14336/50000]\tLoss: 1.1695\tLR: 0.002250\n",
      "Training Epoch: 12 [14464/50000]\tLoss: 1.4424\tLR: 0.002250\n",
      "Training Epoch: 12 [14592/50000]\tLoss: 1.3099\tLR: 0.002250\n",
      "Training Epoch: 12 [14720/50000]\tLoss: 1.3240\tLR: 0.002250\n",
      "Training Epoch: 12 [14848/50000]\tLoss: 1.3256\tLR: 0.002250\n",
      "Training Epoch: 12 [14976/50000]\tLoss: 1.4571\tLR: 0.002250\n",
      "Training Epoch: 12 [15104/50000]\tLoss: 1.2027\tLR: 0.002250\n",
      "Training Epoch: 12 [15232/50000]\tLoss: 1.1571\tLR: 0.002250\n",
      "Training Epoch: 12 [15360/50000]\tLoss: 1.3133\tLR: 0.002250\n",
      "Training Epoch: 12 [15488/50000]\tLoss: 1.0513\tLR: 0.002250\n",
      "Training Epoch: 12 [15616/50000]\tLoss: 1.2584\tLR: 0.002250\n",
      "Training Epoch: 12 [15744/50000]\tLoss: 1.4066\tLR: 0.002250\n",
      "Training Epoch: 12 [15872/50000]\tLoss: 1.2808\tLR: 0.002250\n",
      "Training Epoch: 12 [16000/50000]\tLoss: 1.5070\tLR: 0.002250\n",
      "Training Epoch: 12 [16128/50000]\tLoss: 1.2904\tLR: 0.002250\n",
      "Training Epoch: 12 [16256/50000]\tLoss: 1.4068\tLR: 0.002250\n",
      "Training Epoch: 12 [16384/50000]\tLoss: 1.1250\tLR: 0.002250\n",
      "Training Epoch: 12 [16512/50000]\tLoss: 1.3197\tLR: 0.002250\n",
      "Training Epoch: 12 [16640/50000]\tLoss: 1.3305\tLR: 0.002250\n",
      "Training Epoch: 12 [16768/50000]\tLoss: 1.1185\tLR: 0.002250\n",
      "Training Epoch: 12 [16896/50000]\tLoss: 1.2878\tLR: 0.002250\n",
      "Training Epoch: 12 [17024/50000]\tLoss: 1.4316\tLR: 0.002250\n",
      "Training Epoch: 12 [17152/50000]\tLoss: 1.2214\tLR: 0.002250\n",
      "Training Epoch: 12 [17280/50000]\tLoss: 1.3909\tLR: 0.002250\n",
      "Training Epoch: 12 [17408/50000]\tLoss: 1.2561\tLR: 0.002250\n",
      "Training Epoch: 12 [17536/50000]\tLoss: 1.3587\tLR: 0.002250\n",
      "Training Epoch: 12 [17664/50000]\tLoss: 1.2360\tLR: 0.002250\n",
      "Training Epoch: 12 [17792/50000]\tLoss: 1.1689\tLR: 0.002250\n",
      "Training Epoch: 12 [17920/50000]\tLoss: 1.1786\tLR: 0.002250\n",
      "Training Epoch: 12 [18048/50000]\tLoss: 1.2469\tLR: 0.002250\n",
      "Training Epoch: 12 [18176/50000]\tLoss: 1.2588\tLR: 0.002250\n",
      "Training Epoch: 12 [18304/50000]\tLoss: 1.3082\tLR: 0.002250\n",
      "Training Epoch: 12 [18432/50000]\tLoss: 1.2545\tLR: 0.002250\n",
      "Training Epoch: 12 [18560/50000]\tLoss: 1.2601\tLR: 0.002250\n",
      "Training Epoch: 12 [18688/50000]\tLoss: 1.2360\tLR: 0.002250\n",
      "Training Epoch: 12 [18816/50000]\tLoss: 1.3356\tLR: 0.002250\n",
      "Training Epoch: 12 [18944/50000]\tLoss: 1.1474\tLR: 0.002250\n",
      "Training Epoch: 12 [19072/50000]\tLoss: 1.2314\tLR: 0.002250\n",
      "Training Epoch: 12 [19200/50000]\tLoss: 1.3454\tLR: 0.002250\n",
      "Training Epoch: 12 [19328/50000]\tLoss: 1.3117\tLR: 0.002250\n",
      "Training Epoch: 12 [19456/50000]\tLoss: 1.4310\tLR: 0.002250\n",
      "Training Epoch: 12 [19584/50000]\tLoss: 1.3868\tLR: 0.002250\n",
      "Training Epoch: 12 [19712/50000]\tLoss: 1.2864\tLR: 0.002250\n",
      "Training Epoch: 12 [19840/50000]\tLoss: 1.2935\tLR: 0.002250\n",
      "Training Epoch: 12 [19968/50000]\tLoss: 1.1473\tLR: 0.002250\n",
      "Training Epoch: 12 [20096/50000]\tLoss: 1.3873\tLR: 0.002250\n",
      "Training Epoch: 12 [20224/50000]\tLoss: 1.2252\tLR: 0.002250\n",
      "Training Epoch: 12 [20352/50000]\tLoss: 1.1263\tLR: 0.002250\n",
      "Training Epoch: 12 [20480/50000]\tLoss: 1.2430\tLR: 0.002250\n",
      "Training Epoch: 12 [20608/50000]\tLoss: 1.1423\tLR: 0.002250\n",
      "Training Epoch: 12 [20736/50000]\tLoss: 1.0839\tLR: 0.002250\n",
      "Training Epoch: 12 [20864/50000]\tLoss: 1.1559\tLR: 0.002250\n",
      "Training Epoch: 12 [20992/50000]\tLoss: 1.3087\tLR: 0.002250\n",
      "Training Epoch: 12 [21120/50000]\tLoss: 1.3829\tLR: 0.002250\n",
      "Training Epoch: 12 [21248/50000]\tLoss: 1.0405\tLR: 0.002250\n",
      "Training Epoch: 12 [21376/50000]\tLoss: 1.3060\tLR: 0.002250\n",
      "Training Epoch: 12 [21504/50000]\tLoss: 1.4320\tLR: 0.002250\n",
      "Training Epoch: 12 [21632/50000]\tLoss: 1.1966\tLR: 0.002250\n",
      "Training Epoch: 12 [21760/50000]\tLoss: 1.1632\tLR: 0.002250\n",
      "Training Epoch: 12 [21888/50000]\tLoss: 1.3742\tLR: 0.002250\n",
      "Training Epoch: 12 [22016/50000]\tLoss: 1.1632\tLR: 0.002250\n",
      "Training Epoch: 12 [22144/50000]\tLoss: 1.0764\tLR: 0.002250\n",
      "Training Epoch: 12 [22272/50000]\tLoss: 1.3610\tLR: 0.002250\n",
      "Training Epoch: 12 [22400/50000]\tLoss: 1.2889\tLR: 0.002250\n",
      "Training Epoch: 12 [22528/50000]\tLoss: 1.3077\tLR: 0.002250\n",
      "Training Epoch: 12 [22656/50000]\tLoss: 1.1845\tLR: 0.002250\n",
      "Training Epoch: 12 [22784/50000]\tLoss: 1.2203\tLR: 0.002250\n",
      "Training Epoch: 12 [22912/50000]\tLoss: 1.3077\tLR: 0.002250\n",
      "Training Epoch: 12 [23040/50000]\tLoss: 1.5413\tLR: 0.002250\n",
      "Training Epoch: 12 [23168/50000]\tLoss: 1.3783\tLR: 0.002250\n",
      "Training Epoch: 12 [23296/50000]\tLoss: 1.2850\tLR: 0.002250\n",
      "Training Epoch: 12 [23424/50000]\tLoss: 1.2025\tLR: 0.002250\n",
      "Training Epoch: 12 [23552/50000]\tLoss: 1.2025\tLR: 0.002250\n",
      "Training Epoch: 12 [23680/50000]\tLoss: 1.3652\tLR: 0.002250\n",
      "Training Epoch: 12 [23808/50000]\tLoss: 1.4679\tLR: 0.002250\n",
      "Training Epoch: 12 [23936/50000]\tLoss: 1.0812\tLR: 0.002250\n",
      "Training Epoch: 12 [24064/50000]\tLoss: 1.2368\tLR: 0.002250\n",
      "Training Epoch: 12 [24192/50000]\tLoss: 1.2986\tLR: 0.002250\n",
      "Training Epoch: 12 [24320/50000]\tLoss: 1.3803\tLR: 0.002250\n",
      "Training Epoch: 12 [24448/50000]\tLoss: 1.1777\tLR: 0.002250\n",
      "Training Epoch: 12 [24576/50000]\tLoss: 1.2491\tLR: 0.002250\n",
      "Training Epoch: 12 [24704/50000]\tLoss: 1.4107\tLR: 0.002250\n",
      "Training Epoch: 12 [24832/50000]\tLoss: 1.0662\tLR: 0.002250\n",
      "Training Epoch: 12 [24960/50000]\tLoss: 1.0119\tLR: 0.002250\n",
      "Training Epoch: 12 [25088/50000]\tLoss: 1.3018\tLR: 0.002250\n",
      "Training Epoch: 12 [25216/50000]\tLoss: 1.1796\tLR: 0.002250\n",
      "Training Epoch: 12 [25344/50000]\tLoss: 1.2672\tLR: 0.002250\n",
      "Training Epoch: 12 [25472/50000]\tLoss: 1.1301\tLR: 0.002250\n",
      "Training Epoch: 12 [25600/50000]\tLoss: 1.2443\tLR: 0.002250\n",
      "Training Epoch: 12 [25728/50000]\tLoss: 1.3604\tLR: 0.002250\n",
      "Training Epoch: 12 [25856/50000]\tLoss: 1.2056\tLR: 0.002250\n",
      "Training Epoch: 12 [25984/50000]\tLoss: 1.2691\tLR: 0.002250\n",
      "Training Epoch: 12 [26112/50000]\tLoss: 1.1141\tLR: 0.002250\n",
      "Training Epoch: 12 [26240/50000]\tLoss: 1.2535\tLR: 0.002250\n",
      "Training Epoch: 12 [26368/50000]\tLoss: 1.2309\tLR: 0.002250\n",
      "Training Epoch: 12 [26496/50000]\tLoss: 1.5295\tLR: 0.002250\n",
      "Training Epoch: 12 [26624/50000]\tLoss: 1.1672\tLR: 0.002250\n",
      "Training Epoch: 12 [26752/50000]\tLoss: 1.1719\tLR: 0.002250\n",
      "Training Epoch: 12 [26880/50000]\tLoss: 1.1281\tLR: 0.002250\n",
      "Training Epoch: 12 [27008/50000]\tLoss: 1.1944\tLR: 0.002250\n",
      "Training Epoch: 12 [27136/50000]\tLoss: 1.1857\tLR: 0.002250\n",
      "Training Epoch: 12 [27264/50000]\tLoss: 1.1803\tLR: 0.002250\n",
      "Training Epoch: 12 [27392/50000]\tLoss: 1.2316\tLR: 0.002250\n",
      "Training Epoch: 12 [27520/50000]\tLoss: 1.1649\tLR: 0.002250\n",
      "Training Epoch: 12 [27648/50000]\tLoss: 1.2060\tLR: 0.002250\n",
      "Training Epoch: 12 [27776/50000]\tLoss: 1.3011\tLR: 0.002250\n",
      "Training Epoch: 12 [27904/50000]\tLoss: 1.3153\tLR: 0.002250\n",
      "Training Epoch: 12 [28032/50000]\tLoss: 1.1126\tLR: 0.002250\n",
      "Training Epoch: 12 [28160/50000]\tLoss: 1.3237\tLR: 0.002250\n",
      "Training Epoch: 12 [28288/50000]\tLoss: 1.3122\tLR: 0.002250\n",
      "Training Epoch: 12 [28416/50000]\tLoss: 1.3843\tLR: 0.002250\n",
      "Training Epoch: 12 [28544/50000]\tLoss: 1.2863\tLR: 0.002250\n",
      "Training Epoch: 12 [28672/50000]\tLoss: 1.3809\tLR: 0.002250\n",
      "Training Epoch: 12 [28800/50000]\tLoss: 1.5718\tLR: 0.002250\n",
      "Training Epoch: 12 [28928/50000]\tLoss: 1.1873\tLR: 0.002250\n",
      "Training Epoch: 12 [29056/50000]\tLoss: 1.2192\tLR: 0.002250\n",
      "Training Epoch: 12 [29184/50000]\tLoss: 1.3151\tLR: 0.002250\n",
      "Training Epoch: 12 [29312/50000]\tLoss: 1.5338\tLR: 0.002250\n",
      "Training Epoch: 12 [29440/50000]\tLoss: 1.4179\tLR: 0.002250\n",
      "Training Epoch: 12 [29568/50000]\tLoss: 1.0174\tLR: 0.002250\n",
      "Training Epoch: 12 [29696/50000]\tLoss: 1.2856\tLR: 0.002250\n",
      "Training Epoch: 12 [29824/50000]\tLoss: 1.3436\tLR: 0.002250\n",
      "Training Epoch: 12 [29952/50000]\tLoss: 1.0487\tLR: 0.002250\n",
      "Training Epoch: 12 [30080/50000]\tLoss: 1.2020\tLR: 0.002250\n",
      "Training Epoch: 12 [30208/50000]\tLoss: 1.4506\tLR: 0.002250\n",
      "Training Epoch: 12 [30336/50000]\tLoss: 1.1003\tLR: 0.002250\n",
      "Training Epoch: 12 [30464/50000]\tLoss: 1.4314\tLR: 0.002250\n",
      "Training Epoch: 12 [30592/50000]\tLoss: 1.0873\tLR: 0.002250\n",
      "Training Epoch: 12 [30720/50000]\tLoss: 1.2838\tLR: 0.002250\n",
      "Training Epoch: 12 [30848/50000]\tLoss: 1.2861\tLR: 0.002250\n",
      "Training Epoch: 12 [30976/50000]\tLoss: 1.1872\tLR: 0.002250\n",
      "Training Epoch: 12 [31104/50000]\tLoss: 1.2092\tLR: 0.002250\n",
      "Training Epoch: 12 [31232/50000]\tLoss: 1.4161\tLR: 0.002250\n",
      "Training Epoch: 12 [31360/50000]\tLoss: 1.2850\tLR: 0.002250\n",
      "Training Epoch: 12 [31488/50000]\tLoss: 1.1039\tLR: 0.002250\n",
      "Training Epoch: 12 [31616/50000]\tLoss: 1.3515\tLR: 0.002250\n",
      "Training Epoch: 12 [31744/50000]\tLoss: 1.1396\tLR: 0.002250\n",
      "Training Epoch: 12 [31872/50000]\tLoss: 1.1052\tLR: 0.002250\n",
      "Training Epoch: 12 [32000/50000]\tLoss: 1.2470\tLR: 0.002250\n",
      "Training Epoch: 12 [32128/50000]\tLoss: 1.3657\tLR: 0.002250\n",
      "Training Epoch: 12 [32256/50000]\tLoss: 1.3443\tLR: 0.002250\n",
      "Training Epoch: 12 [32384/50000]\tLoss: 1.3340\tLR: 0.002250\n",
      "Training Epoch: 12 [32512/50000]\tLoss: 1.3829\tLR: 0.002250\n",
      "Training Epoch: 12 [32640/50000]\tLoss: 1.3929\tLR: 0.002250\n",
      "Training Epoch: 12 [32768/50000]\tLoss: 1.5350\tLR: 0.002250\n",
      "Training Epoch: 12 [32896/50000]\tLoss: 1.0502\tLR: 0.002250\n",
      "Training Epoch: 12 [33024/50000]\tLoss: 1.2651\tLR: 0.002250\n",
      "Training Epoch: 12 [33152/50000]\tLoss: 1.2169\tLR: 0.002250\n",
      "Training Epoch: 12 [33280/50000]\tLoss: 1.3420\tLR: 0.002250\n",
      "Training Epoch: 12 [33408/50000]\tLoss: 1.2264\tLR: 0.002250\n",
      "Training Epoch: 12 [33536/50000]\tLoss: 1.4130\tLR: 0.002250\n",
      "Training Epoch: 12 [33664/50000]\tLoss: 1.2443\tLR: 0.002250\n",
      "Training Epoch: 12 [33792/50000]\tLoss: 1.3136\tLR: 0.002250\n",
      "Training Epoch: 12 [33920/50000]\tLoss: 1.3056\tLR: 0.002250\n",
      "Training Epoch: 12 [34048/50000]\tLoss: 1.2054\tLR: 0.002250\n",
      "Training Epoch: 12 [34176/50000]\tLoss: 1.2432\tLR: 0.002250\n",
      "Training Epoch: 12 [34304/50000]\tLoss: 1.3511\tLR: 0.002250\n",
      "Training Epoch: 12 [34432/50000]\tLoss: 1.3301\tLR: 0.002250\n",
      "Training Epoch: 12 [34560/50000]\tLoss: 1.3116\tLR: 0.002250\n",
      "Training Epoch: 12 [34688/50000]\tLoss: 1.2412\tLR: 0.002250\n",
      "Training Epoch: 12 [34816/50000]\tLoss: 1.1481\tLR: 0.002250\n",
      "Training Epoch: 12 [34944/50000]\tLoss: 1.3186\tLR: 0.002250\n",
      "Training Epoch: 12 [35072/50000]\tLoss: 1.3412\tLR: 0.002250\n",
      "Training Epoch: 12 [35200/50000]\tLoss: 1.2055\tLR: 0.002250\n",
      "Training Epoch: 12 [35328/50000]\tLoss: 1.3164\tLR: 0.002250\n",
      "Training Epoch: 12 [35456/50000]\tLoss: 1.1433\tLR: 0.002250\n",
      "Training Epoch: 12 [35584/50000]\tLoss: 1.3201\tLR: 0.002250\n",
      "Training Epoch: 12 [35712/50000]\tLoss: 1.3762\tLR: 0.002250\n",
      "Training Epoch: 12 [35840/50000]\tLoss: 1.1628\tLR: 0.002250\n",
      "Training Epoch: 12 [35968/50000]\tLoss: 1.0849\tLR: 0.002250\n",
      "Training Epoch: 12 [36096/50000]\tLoss: 1.2174\tLR: 0.002250\n",
      "Training Epoch: 12 [36224/50000]\tLoss: 1.4473\tLR: 0.002250\n",
      "Training Epoch: 12 [36352/50000]\tLoss: 1.1956\tLR: 0.002250\n",
      "Training Epoch: 12 [36480/50000]\tLoss: 1.3861\tLR: 0.002250\n",
      "Training Epoch: 12 [36608/50000]\tLoss: 1.3914\tLR: 0.002250\n",
      "Training Epoch: 12 [36736/50000]\tLoss: 1.2766\tLR: 0.002250\n",
      "Training Epoch: 12 [36864/50000]\tLoss: 1.3975\tLR: 0.002250\n",
      "Training Epoch: 12 [36992/50000]\tLoss: 1.1196\tLR: 0.002250\n",
      "Training Epoch: 12 [37120/50000]\tLoss: 1.1957\tLR: 0.002250\n",
      "Training Epoch: 12 [37248/50000]\tLoss: 1.0230\tLR: 0.002250\n",
      "Training Epoch: 12 [37376/50000]\tLoss: 1.2411\tLR: 0.002250\n",
      "Training Epoch: 12 [37504/50000]\tLoss: 1.4880\tLR: 0.002250\n",
      "Training Epoch: 12 [37632/50000]\tLoss: 1.3306\tLR: 0.002250\n",
      "Training Epoch: 12 [37760/50000]\tLoss: 1.1471\tLR: 0.002250\n",
      "Training Epoch: 12 [37888/50000]\tLoss: 1.3502\tLR: 0.002250\n",
      "Training Epoch: 12 [38016/50000]\tLoss: 1.1319\tLR: 0.002250\n",
      "Training Epoch: 12 [38144/50000]\tLoss: 1.3255\tLR: 0.002250\n",
      "Training Epoch: 12 [38272/50000]\tLoss: 1.4086\tLR: 0.002250\n",
      "Training Epoch: 12 [38400/50000]\tLoss: 1.2120\tLR: 0.002250\n",
      "Training Epoch: 12 [38528/50000]\tLoss: 1.3761\tLR: 0.002250\n",
      "Training Epoch: 12 [38656/50000]\tLoss: 1.0612\tLR: 0.002250\n",
      "Training Epoch: 12 [38784/50000]\tLoss: 1.3169\tLR: 0.002250\n",
      "Training Epoch: 12 [38912/50000]\tLoss: 1.2576\tLR: 0.002250\n",
      "Training Epoch: 12 [39040/50000]\tLoss: 1.5084\tLR: 0.002250\n",
      "Training Epoch: 12 [39168/50000]\tLoss: 1.2083\tLR: 0.002250\n",
      "Training Epoch: 12 [39296/50000]\tLoss: 1.1775\tLR: 0.002250\n",
      "Training Epoch: 12 [39424/50000]\tLoss: 1.3133\tLR: 0.002250\n",
      "Training Epoch: 12 [39552/50000]\tLoss: 1.3103\tLR: 0.002250\n",
      "Training Epoch: 12 [39680/50000]\tLoss: 1.3264\tLR: 0.002250\n",
      "Training Epoch: 12 [39808/50000]\tLoss: 1.1479\tLR: 0.002250\n",
      "Training Epoch: 12 [39936/50000]\tLoss: 1.3835\tLR: 0.002250\n",
      "Training Epoch: 12 [40064/50000]\tLoss: 1.1588\tLR: 0.002250\n",
      "Training Epoch: 12 [40192/50000]\tLoss: 1.2920\tLR: 0.002250\n",
      "Training Epoch: 12 [40320/50000]\tLoss: 1.0995\tLR: 0.002250\n",
      "Training Epoch: 12 [40448/50000]\tLoss: 1.1572\tLR: 0.002250\n",
      "Training Epoch: 12 [40576/50000]\tLoss: 1.2740\tLR: 0.002250\n",
      "Training Epoch: 12 [40704/50000]\tLoss: 1.0822\tLR: 0.002250\n",
      "Training Epoch: 12 [40832/50000]\tLoss: 1.2606\tLR: 0.002250\n",
      "Training Epoch: 12 [40960/50000]\tLoss: 1.1218\tLR: 0.002250\n",
      "Training Epoch: 12 [41088/50000]\tLoss: 1.2776\tLR: 0.002250\n",
      "Training Epoch: 12 [41216/50000]\tLoss: 1.1442\tLR: 0.002250\n",
      "Training Epoch: 12 [41344/50000]\tLoss: 1.1840\tLR: 0.002250\n",
      "Training Epoch: 12 [41472/50000]\tLoss: 1.2556\tLR: 0.002250\n",
      "Training Epoch: 12 [41600/50000]\tLoss: 1.1766\tLR: 0.002250\n",
      "Training Epoch: 12 [41728/50000]\tLoss: 1.1824\tLR: 0.002250\n",
      "Training Epoch: 12 [41856/50000]\tLoss: 1.3520\tLR: 0.002250\n",
      "Training Epoch: 12 [41984/50000]\tLoss: 1.2702\tLR: 0.002250\n",
      "Training Epoch: 12 [42112/50000]\tLoss: 1.2255\tLR: 0.002250\n",
      "Training Epoch: 12 [42240/50000]\tLoss: 1.3419\tLR: 0.002250\n",
      "Training Epoch: 12 [42368/50000]\tLoss: 1.1761\tLR: 0.002250\n",
      "Training Epoch: 12 [42496/50000]\tLoss: 1.1426\tLR: 0.002250\n",
      "Training Epoch: 12 [42624/50000]\tLoss: 1.1865\tLR: 0.002250\n",
      "Training Epoch: 12 [42752/50000]\tLoss: 1.1534\tLR: 0.002250\n",
      "Training Epoch: 12 [42880/50000]\tLoss: 1.5903\tLR: 0.002250\n",
      "Training Epoch: 12 [43008/50000]\tLoss: 1.3579\tLR: 0.002250\n",
      "Training Epoch: 12 [43136/50000]\tLoss: 1.3977\tLR: 0.002250\n",
      "Training Epoch: 12 [43264/50000]\tLoss: 1.5108\tLR: 0.002250\n",
      "Training Epoch: 12 [43392/50000]\tLoss: 1.0324\tLR: 0.002250\n",
      "Training Epoch: 12 [43520/50000]\tLoss: 1.1454\tLR: 0.002250\n",
      "Training Epoch: 12 [43648/50000]\tLoss: 1.2520\tLR: 0.002250\n",
      "Training Epoch: 12 [43776/50000]\tLoss: 1.1730\tLR: 0.002250\n",
      "Training Epoch: 12 [43904/50000]\tLoss: 1.1509\tLR: 0.002250\n",
      "Training Epoch: 12 [44032/50000]\tLoss: 1.1284\tLR: 0.002250\n",
      "Training Epoch: 12 [44160/50000]\tLoss: 1.1547\tLR: 0.002250\n",
      "Training Epoch: 12 [44288/50000]\tLoss: 1.3107\tLR: 0.002250\n",
      "Training Epoch: 12 [44416/50000]\tLoss: 1.1394\tLR: 0.002250\n",
      "Training Epoch: 12 [44544/50000]\tLoss: 1.3232\tLR: 0.002250\n",
      "Training Epoch: 12 [44672/50000]\tLoss: 1.2997\tLR: 0.002250\n",
      "Training Epoch: 12 [44800/50000]\tLoss: 1.1856\tLR: 0.002250\n",
      "Training Epoch: 12 [44928/50000]\tLoss: 1.2088\tLR: 0.002250\n",
      "Training Epoch: 12 [45056/50000]\tLoss: 1.3755\tLR: 0.002250\n",
      "Training Epoch: 12 [45184/50000]\tLoss: 1.2310\tLR: 0.002250\n",
      "Training Epoch: 12 [45312/50000]\tLoss: 1.0733\tLR: 0.002250\n",
      "Training Epoch: 12 [45440/50000]\tLoss: 1.4372\tLR: 0.002250\n",
      "Training Epoch: 12 [45568/50000]\tLoss: 1.3112\tLR: 0.002250\n",
      "Training Epoch: 12 [45696/50000]\tLoss: 1.3166\tLR: 0.002250\n",
      "Training Epoch: 12 [45824/50000]\tLoss: 1.3020\tLR: 0.002250\n",
      "Training Epoch: 12 [45952/50000]\tLoss: 1.2996\tLR: 0.002250\n",
      "Training Epoch: 12 [46080/50000]\tLoss: 1.1976\tLR: 0.002250\n",
      "Training Epoch: 12 [46208/50000]\tLoss: 1.3223\tLR: 0.002250\n",
      "Training Epoch: 12 [46336/50000]\tLoss: 1.1076\tLR: 0.002250\n",
      "Training Epoch: 12 [46464/50000]\tLoss: 1.0962\tLR: 0.002250\n",
      "Training Epoch: 12 [46592/50000]\tLoss: 1.3266\tLR: 0.002250\n",
      "Training Epoch: 12 [46720/50000]\tLoss: 1.3167\tLR: 0.002250\n",
      "Training Epoch: 12 [46848/50000]\tLoss: 1.3087\tLR: 0.002250\n",
      "Training Epoch: 12 [46976/50000]\tLoss: 1.1047\tLR: 0.002250\n",
      "Training Epoch: 12 [47104/50000]\tLoss: 1.0467\tLR: 0.002250\n",
      "Training Epoch: 12 [47232/50000]\tLoss: 1.2770\tLR: 0.002250\n",
      "Training Epoch: 12 [47360/50000]\tLoss: 1.2886\tLR: 0.002250\n",
      "Training Epoch: 12 [47488/50000]\tLoss: 1.3461\tLR: 0.002250\n",
      "Training Epoch: 12 [47616/50000]\tLoss: 1.2921\tLR: 0.002250\n",
      "Training Epoch: 12 [47744/50000]\tLoss: 1.1675\tLR: 0.002250\n",
      "Training Epoch: 12 [47872/50000]\tLoss: 1.3826\tLR: 0.002250\n",
      "Training Epoch: 12 [48000/50000]\tLoss: 1.2994\tLR: 0.002250\n",
      "Training Epoch: 12 [48128/50000]\tLoss: 1.1605\tLR: 0.002250\n",
      "Training Epoch: 12 [48256/50000]\tLoss: 1.2077\tLR: 0.002250\n",
      "Training Epoch: 12 [48384/50000]\tLoss: 1.4106\tLR: 0.002250\n",
      "Training Epoch: 12 [48512/50000]\tLoss: 1.2578\tLR: 0.002250\n",
      "Training Epoch: 12 [48640/50000]\tLoss: 1.6515\tLR: 0.002250\n",
      "Training Epoch: 12 [48768/50000]\tLoss: 1.1407\tLR: 0.002250\n",
      "Training Epoch: 12 [48896/50000]\tLoss: 1.3065\tLR: 0.002250\n",
      "Training Epoch: 12 [49024/50000]\tLoss: 1.0754\tLR: 0.002250\n",
      "Training Epoch: 12 [49152/50000]\tLoss: 1.0809\tLR: 0.002250\n",
      "Training Epoch: 12 [49280/50000]\tLoss: 1.3778\tLR: 0.002250\n",
      "Training Epoch: 12 [49408/50000]\tLoss: 1.3702\tLR: 0.002250\n",
      "Training Epoch: 12 [49536/50000]\tLoss: 1.2215\tLR: 0.002250\n",
      "Training Epoch: 12 [49664/50000]\tLoss: 1.2755\tLR: 0.002250\n",
      "Training Epoch: 12 [49792/50000]\tLoss: 1.2527\tLR: 0.002250\n",
      "Training Epoch: 12 [49920/50000]\tLoss: 1.2258\tLR: 0.002250\n",
      "Training Epoch: 12 [50000/50000]\tLoss: 1.0871\tLR: 0.002250\n",
      "Test set: Average loss: 0.0108, Accuracy: 0.6097\n",
      "\n",
      "Training Epoch: 13 [128/50000]\tLoss: 1.3529\tLR: 0.002250\n",
      "Training Epoch: 13 [256/50000]\tLoss: 1.2402\tLR: 0.002250\n",
      "Training Epoch: 13 [384/50000]\tLoss: 1.2265\tLR: 0.002250\n",
      "Training Epoch: 13 [512/50000]\tLoss: 1.3306\tLR: 0.002250\n",
      "Training Epoch: 13 [640/50000]\tLoss: 1.2671\tLR: 0.002250\n",
      "Training Epoch: 13 [768/50000]\tLoss: 1.1327\tLR: 0.002250\n",
      "Training Epoch: 13 [896/50000]\tLoss: 1.2261\tLR: 0.002250\n",
      "Training Epoch: 13 [1024/50000]\tLoss: 1.1321\tLR: 0.002250\n",
      "Training Epoch: 13 [1152/50000]\tLoss: 1.4613\tLR: 0.002250\n",
      "Training Epoch: 13 [1280/50000]\tLoss: 1.4027\tLR: 0.002250\n",
      "Training Epoch: 13 [1408/50000]\tLoss: 1.1792\tLR: 0.002250\n",
      "Training Epoch: 13 [1536/50000]\tLoss: 1.1336\tLR: 0.002250\n",
      "Training Epoch: 13 [1664/50000]\tLoss: 1.3417\tLR: 0.002250\n",
      "Training Epoch: 13 [1792/50000]\tLoss: 1.0816\tLR: 0.002250\n",
      "Training Epoch: 13 [1920/50000]\tLoss: 1.3148\tLR: 0.002250\n",
      "Training Epoch: 13 [2048/50000]\tLoss: 1.0821\tLR: 0.002250\n",
      "Training Epoch: 13 [2176/50000]\tLoss: 1.3134\tLR: 0.002250\n",
      "Training Epoch: 13 [2304/50000]\tLoss: 1.1780\tLR: 0.002250\n",
      "Training Epoch: 13 [2432/50000]\tLoss: 1.1646\tLR: 0.002250\n",
      "Training Epoch: 13 [2560/50000]\tLoss: 1.2058\tLR: 0.002250\n",
      "Training Epoch: 13 [2688/50000]\tLoss: 1.3962\tLR: 0.002250\n",
      "Training Epoch: 13 [2816/50000]\tLoss: 1.0580\tLR: 0.002250\n",
      "Training Epoch: 13 [2944/50000]\tLoss: 1.3144\tLR: 0.002250\n",
      "Training Epoch: 13 [3072/50000]\tLoss: 1.2437\tLR: 0.002250\n",
      "Training Epoch: 13 [3200/50000]\tLoss: 1.3751\tLR: 0.002250\n",
      "Training Epoch: 13 [3328/50000]\tLoss: 1.0785\tLR: 0.002250\n",
      "Training Epoch: 13 [3456/50000]\tLoss: 1.1626\tLR: 0.002250\n",
      "Training Epoch: 13 [3584/50000]\tLoss: 1.2777\tLR: 0.002250\n",
      "Training Epoch: 13 [3712/50000]\tLoss: 1.3122\tLR: 0.002250\n",
      "Training Epoch: 13 [3840/50000]\tLoss: 1.1164\tLR: 0.002250\n",
      "Training Epoch: 13 [3968/50000]\tLoss: 0.9754\tLR: 0.002250\n",
      "Training Epoch: 13 [4096/50000]\tLoss: 1.2746\tLR: 0.002250\n",
      "Training Epoch: 13 [4224/50000]\tLoss: 1.0991\tLR: 0.002250\n",
      "Training Epoch: 13 [4352/50000]\tLoss: 0.9746\tLR: 0.002250\n",
      "Training Epoch: 13 [4480/50000]\tLoss: 1.3145\tLR: 0.002250\n",
      "Training Epoch: 13 [4608/50000]\tLoss: 1.1390\tLR: 0.002250\n",
      "Training Epoch: 13 [4736/50000]\tLoss: 1.2047\tLR: 0.002250\n",
      "Training Epoch: 13 [4864/50000]\tLoss: 1.2564\tLR: 0.002250\n",
      "Training Epoch: 13 [4992/50000]\tLoss: 1.1027\tLR: 0.002250\n",
      "Training Epoch: 13 [5120/50000]\tLoss: 1.2207\tLR: 0.002250\n",
      "Training Epoch: 13 [5248/50000]\tLoss: 1.2216\tLR: 0.002250\n",
      "Training Epoch: 13 [5376/50000]\tLoss: 1.2658\tLR: 0.002250\n",
      "Training Epoch: 13 [5504/50000]\tLoss: 1.3167\tLR: 0.002250\n",
      "Training Epoch: 13 [5632/50000]\tLoss: 1.1112\tLR: 0.002250\n",
      "Training Epoch: 13 [5760/50000]\tLoss: 1.4173\tLR: 0.002250\n",
      "Training Epoch: 13 [5888/50000]\tLoss: 1.2956\tLR: 0.002250\n",
      "Training Epoch: 13 [6016/50000]\tLoss: 1.2354\tLR: 0.002250\n",
      "Training Epoch: 13 [6144/50000]\tLoss: 1.2751\tLR: 0.002250\n",
      "Training Epoch: 13 [6272/50000]\tLoss: 1.3759\tLR: 0.002250\n",
      "Training Epoch: 13 [6400/50000]\tLoss: 1.4221\tLR: 0.002250\n",
      "Training Epoch: 13 [6528/50000]\tLoss: 1.0895\tLR: 0.002250\n",
      "Training Epoch: 13 [6656/50000]\tLoss: 1.2358\tLR: 0.002250\n",
      "Training Epoch: 13 [6784/50000]\tLoss: 1.1221\tLR: 0.002250\n",
      "Training Epoch: 13 [6912/50000]\tLoss: 1.2064\tLR: 0.002250\n",
      "Training Epoch: 13 [7040/50000]\tLoss: 1.1687\tLR: 0.002250\n",
      "Training Epoch: 13 [7168/50000]\tLoss: 1.4476\tLR: 0.002250\n",
      "Training Epoch: 13 [7296/50000]\tLoss: 1.3646\tLR: 0.002250\n",
      "Training Epoch: 13 [7424/50000]\tLoss: 1.1176\tLR: 0.002250\n",
      "Training Epoch: 13 [7552/50000]\tLoss: 1.1821\tLR: 0.002250\n",
      "Training Epoch: 13 [7680/50000]\tLoss: 1.4738\tLR: 0.002250\n",
      "Training Epoch: 13 [7808/50000]\tLoss: 1.2742\tLR: 0.002250\n",
      "Training Epoch: 13 [7936/50000]\tLoss: 1.1437\tLR: 0.002250\n",
      "Training Epoch: 13 [8064/50000]\tLoss: 1.2059\tLR: 0.002250\n",
      "Training Epoch: 13 [8192/50000]\tLoss: 1.0673\tLR: 0.002250\n",
      "Training Epoch: 13 [8320/50000]\tLoss: 1.1782\tLR: 0.002250\n",
      "Training Epoch: 13 [8448/50000]\tLoss: 1.0666\tLR: 0.002250\n",
      "Training Epoch: 13 [8576/50000]\tLoss: 1.0693\tLR: 0.002250\n",
      "Training Epoch: 13 [8704/50000]\tLoss: 1.3320\tLR: 0.002250\n",
      "Training Epoch: 13 [8832/50000]\tLoss: 1.1533\tLR: 0.002250\n",
      "Training Epoch: 13 [8960/50000]\tLoss: 1.2136\tLR: 0.002250\n",
      "Training Epoch: 13 [9088/50000]\tLoss: 0.9755\tLR: 0.002250\n",
      "Training Epoch: 13 [9216/50000]\tLoss: 1.1006\tLR: 0.002250\n",
      "Training Epoch: 13 [9344/50000]\tLoss: 1.1794\tLR: 0.002250\n",
      "Training Epoch: 13 [9472/50000]\tLoss: 1.3678\tLR: 0.002250\n",
      "Training Epoch: 13 [9600/50000]\tLoss: 1.1582\tLR: 0.002250\n",
      "Training Epoch: 13 [9728/50000]\tLoss: 1.1811\tLR: 0.002250\n",
      "Training Epoch: 13 [9856/50000]\tLoss: 1.1195\tLR: 0.002250\n",
      "Training Epoch: 13 [9984/50000]\tLoss: 1.1041\tLR: 0.002250\n",
      "Training Epoch: 13 [10112/50000]\tLoss: 1.2464\tLR: 0.002250\n",
      "Training Epoch: 13 [10240/50000]\tLoss: 1.1488\tLR: 0.002250\n",
      "Training Epoch: 13 [10368/50000]\tLoss: 1.1876\tLR: 0.002250\n",
      "Training Epoch: 13 [10496/50000]\tLoss: 1.1660\tLR: 0.002250\n",
      "Training Epoch: 13 [10624/50000]\tLoss: 1.2899\tLR: 0.002250\n",
      "Training Epoch: 13 [10752/50000]\tLoss: 1.1359\tLR: 0.002250\n",
      "Training Epoch: 13 [10880/50000]\tLoss: 1.4981\tLR: 0.002250\n",
      "Training Epoch: 13 [11008/50000]\tLoss: 1.2185\tLR: 0.002250\n",
      "Training Epoch: 13 [11136/50000]\tLoss: 1.2759\tLR: 0.002250\n",
      "Training Epoch: 13 [11264/50000]\tLoss: 1.1092\tLR: 0.002250\n",
      "Training Epoch: 13 [11392/50000]\tLoss: 1.2650\tLR: 0.002250\n",
      "Training Epoch: 13 [11520/50000]\tLoss: 1.2276\tLR: 0.002250\n",
      "Training Epoch: 13 [11648/50000]\tLoss: 1.3276\tLR: 0.002250\n",
      "Training Epoch: 13 [11776/50000]\tLoss: 1.1498\tLR: 0.002250\n",
      "Training Epoch: 13 [11904/50000]\tLoss: 1.3690\tLR: 0.002250\n",
      "Training Epoch: 13 [12032/50000]\tLoss: 1.2650\tLR: 0.002250\n",
      "Training Epoch: 13 [12160/50000]\tLoss: 1.1890\tLR: 0.002250\n",
      "Training Epoch: 13 [12288/50000]\tLoss: 1.2488\tLR: 0.002250\n",
      "Training Epoch: 13 [12416/50000]\tLoss: 1.0683\tLR: 0.002250\n",
      "Training Epoch: 13 [12544/50000]\tLoss: 1.3863\tLR: 0.002250\n",
      "Training Epoch: 13 [12672/50000]\tLoss: 1.3355\tLR: 0.002250\n",
      "Training Epoch: 13 [12800/50000]\tLoss: 1.4456\tLR: 0.002250\n",
      "Training Epoch: 13 [12928/50000]\tLoss: 1.2616\tLR: 0.002250\n",
      "Training Epoch: 13 [13056/50000]\tLoss: 1.2552\tLR: 0.002250\n",
      "Training Epoch: 13 [13184/50000]\tLoss: 1.1537\tLR: 0.002250\n",
      "Training Epoch: 13 [13312/50000]\tLoss: 1.0832\tLR: 0.002250\n",
      "Training Epoch: 13 [13440/50000]\tLoss: 1.1481\tLR: 0.002250\n",
      "Training Epoch: 13 [13568/50000]\tLoss: 0.9808\tLR: 0.002250\n",
      "Training Epoch: 13 [13696/50000]\tLoss: 1.1997\tLR: 0.002250\n",
      "Training Epoch: 13 [13824/50000]\tLoss: 1.0207\tLR: 0.002250\n",
      "Training Epoch: 13 [13952/50000]\tLoss: 1.0158\tLR: 0.002250\n",
      "Training Epoch: 13 [14080/50000]\tLoss: 1.2464\tLR: 0.002250\n",
      "Training Epoch: 13 [14208/50000]\tLoss: 1.1568\tLR: 0.002250\n",
      "Training Epoch: 13 [14336/50000]\tLoss: 1.1217\tLR: 0.002250\n",
      "Training Epoch: 13 [14464/50000]\tLoss: 1.1251\tLR: 0.002250\n",
      "Training Epoch: 13 [14592/50000]\tLoss: 1.2170\tLR: 0.002250\n",
      "Training Epoch: 13 [14720/50000]\tLoss: 1.0756\tLR: 0.002250\n",
      "Training Epoch: 13 [14848/50000]\tLoss: 1.0670\tLR: 0.002250\n",
      "Training Epoch: 13 [14976/50000]\tLoss: 1.2495\tLR: 0.002250\n",
      "Training Epoch: 13 [15104/50000]\tLoss: 1.0949\tLR: 0.002250\n",
      "Training Epoch: 13 [15232/50000]\tLoss: 1.3069\tLR: 0.002250\n",
      "Training Epoch: 13 [15360/50000]\tLoss: 1.1553\tLR: 0.002250\n",
      "Training Epoch: 13 [15488/50000]\tLoss: 1.3028\tLR: 0.002250\n",
      "Training Epoch: 13 [15616/50000]\tLoss: 1.2488\tLR: 0.002250\n",
      "Training Epoch: 13 [15744/50000]\tLoss: 1.1399\tLR: 0.002250\n",
      "Training Epoch: 13 [15872/50000]\tLoss: 1.3850\tLR: 0.002250\n",
      "Training Epoch: 13 [16000/50000]\tLoss: 1.2014\tLR: 0.002250\n",
      "Training Epoch: 13 [16128/50000]\tLoss: 1.3058\tLR: 0.002250\n",
      "Training Epoch: 13 [16256/50000]\tLoss: 1.1647\tLR: 0.002250\n",
      "Training Epoch: 13 [16384/50000]\tLoss: 1.2055\tLR: 0.002250\n",
      "Training Epoch: 13 [16512/50000]\tLoss: 1.1470\tLR: 0.002250\n",
      "Training Epoch: 13 [16640/50000]\tLoss: 1.2630\tLR: 0.002250\n",
      "Training Epoch: 13 [16768/50000]\tLoss: 1.3507\tLR: 0.002250\n",
      "Training Epoch: 13 [16896/50000]\tLoss: 1.2316\tLR: 0.002250\n",
      "Training Epoch: 13 [17024/50000]\tLoss: 1.3518\tLR: 0.002250\n",
      "Training Epoch: 13 [17152/50000]\tLoss: 1.2875\tLR: 0.002250\n",
      "Training Epoch: 13 [17280/50000]\tLoss: 1.1217\tLR: 0.002250\n",
      "Training Epoch: 13 [17408/50000]\tLoss: 1.2467\tLR: 0.002250\n",
      "Training Epoch: 13 [17536/50000]\tLoss: 1.1760\tLR: 0.002250\n",
      "Training Epoch: 13 [17664/50000]\tLoss: 1.1581\tLR: 0.002250\n",
      "Training Epoch: 13 [17792/50000]\tLoss: 1.1631\tLR: 0.002250\n",
      "Training Epoch: 13 [17920/50000]\tLoss: 1.2560\tLR: 0.002250\n",
      "Training Epoch: 13 [18048/50000]\tLoss: 1.1136\tLR: 0.002250\n",
      "Training Epoch: 13 [18176/50000]\tLoss: 1.1696\tLR: 0.002250\n",
      "Training Epoch: 13 [18304/50000]\tLoss: 1.4203\tLR: 0.002250\n",
      "Training Epoch: 13 [18432/50000]\tLoss: 1.1677\tLR: 0.002250\n",
      "Training Epoch: 13 [18560/50000]\tLoss: 1.2226\tLR: 0.002250\n",
      "Training Epoch: 13 [18688/50000]\tLoss: 1.4369\tLR: 0.002250\n",
      "Training Epoch: 13 [18816/50000]\tLoss: 1.1602\tLR: 0.002250\n",
      "Training Epoch: 13 [18944/50000]\tLoss: 1.2874\tLR: 0.002250\n",
      "Training Epoch: 13 [19072/50000]\tLoss: 1.4666\tLR: 0.002250\n",
      "Training Epoch: 13 [19200/50000]\tLoss: 1.2329\tLR: 0.002250\n",
      "Training Epoch: 13 [19328/50000]\tLoss: 1.1060\tLR: 0.002250\n",
      "Training Epoch: 13 [19456/50000]\tLoss: 1.1842\tLR: 0.002250\n",
      "Training Epoch: 13 [19584/50000]\tLoss: 1.1684\tLR: 0.002250\n",
      "Training Epoch: 13 [19712/50000]\tLoss: 1.1308\tLR: 0.002250\n",
      "Training Epoch: 13 [19840/50000]\tLoss: 1.2905\tLR: 0.002250\n",
      "Training Epoch: 13 [19968/50000]\tLoss: 1.1845\tLR: 0.002250\n",
      "Training Epoch: 13 [20096/50000]\tLoss: 1.1606\tLR: 0.002250\n",
      "Training Epoch: 13 [20224/50000]\tLoss: 1.3049\tLR: 0.002250\n",
      "Training Epoch: 13 [20352/50000]\tLoss: 1.3285\tLR: 0.002250\n",
      "Training Epoch: 13 [20480/50000]\tLoss: 1.1317\tLR: 0.002250\n",
      "Training Epoch: 13 [20608/50000]\tLoss: 1.1406\tLR: 0.002250\n",
      "Training Epoch: 13 [20736/50000]\tLoss: 1.0373\tLR: 0.002250\n",
      "Training Epoch: 13 [20864/50000]\tLoss: 1.2937\tLR: 0.002250\n",
      "Training Epoch: 13 [20992/50000]\tLoss: 1.1652\tLR: 0.002250\n",
      "Training Epoch: 13 [21120/50000]\tLoss: 1.1253\tLR: 0.002250\n",
      "Training Epoch: 13 [21248/50000]\tLoss: 1.3552\tLR: 0.002250\n",
      "Training Epoch: 13 [21376/50000]\tLoss: 1.1633\tLR: 0.002250\n",
      "Training Epoch: 13 [21504/50000]\tLoss: 1.2470\tLR: 0.002250\n",
      "Training Epoch: 13 [21632/50000]\tLoss: 1.3243\tLR: 0.002250\n",
      "Training Epoch: 13 [21760/50000]\tLoss: 1.2268\tLR: 0.002250\n",
      "Training Epoch: 13 [21888/50000]\tLoss: 1.3911\tLR: 0.002250\n",
      "Training Epoch: 13 [22016/50000]\tLoss: 1.1748\tLR: 0.002250\n",
      "Training Epoch: 13 [22144/50000]\tLoss: 1.3336\tLR: 0.002250\n",
      "Training Epoch: 13 [22272/50000]\tLoss: 1.1586\tLR: 0.002250\n",
      "Training Epoch: 13 [22400/50000]\tLoss: 1.2832\tLR: 0.002250\n",
      "Training Epoch: 13 [22528/50000]\tLoss: 1.2884\tLR: 0.002250\n",
      "Training Epoch: 13 [22656/50000]\tLoss: 1.1485\tLR: 0.002250\n",
      "Training Epoch: 13 [22784/50000]\tLoss: 1.2808\tLR: 0.002250\n",
      "Training Epoch: 13 [22912/50000]\tLoss: 1.2767\tLR: 0.002250\n",
      "Training Epoch: 13 [23040/50000]\tLoss: 1.2871\tLR: 0.002250\n",
      "Training Epoch: 13 [23168/50000]\tLoss: 0.9979\tLR: 0.002250\n",
      "Training Epoch: 13 [23296/50000]\tLoss: 1.2879\tLR: 0.002250\n",
      "Training Epoch: 13 [23424/50000]\tLoss: 1.1202\tLR: 0.002250\n",
      "Training Epoch: 13 [23552/50000]\tLoss: 1.3004\tLR: 0.002250\n",
      "Training Epoch: 13 [23680/50000]\tLoss: 1.2140\tLR: 0.002250\n",
      "Training Epoch: 13 [23808/50000]\tLoss: 1.1616\tLR: 0.002250\n",
      "Training Epoch: 13 [23936/50000]\tLoss: 1.2009\tLR: 0.002250\n",
      "Training Epoch: 13 [24064/50000]\tLoss: 1.2039\tLR: 0.002250\n",
      "Training Epoch: 13 [24192/50000]\tLoss: 1.3646\tLR: 0.002250\n",
      "Training Epoch: 13 [24320/50000]\tLoss: 1.1996\tLR: 0.002250\n",
      "Training Epoch: 13 [24448/50000]\tLoss: 1.1926\tLR: 0.002250\n",
      "Training Epoch: 13 [24576/50000]\tLoss: 1.3914\tLR: 0.002250\n",
      "Training Epoch: 13 [24704/50000]\tLoss: 1.3924\tLR: 0.002250\n",
      "Training Epoch: 13 [24832/50000]\tLoss: 1.2195\tLR: 0.002250\n",
      "Training Epoch: 13 [24960/50000]\tLoss: 1.0064\tLR: 0.002250\n",
      "Training Epoch: 13 [25088/50000]\tLoss: 1.2127\tLR: 0.002250\n",
      "Training Epoch: 13 [25216/50000]\tLoss: 1.1307\tLR: 0.002250\n",
      "Training Epoch: 13 [25344/50000]\tLoss: 1.2143\tLR: 0.002250\n",
      "Training Epoch: 13 [25472/50000]\tLoss: 1.1253\tLR: 0.002250\n",
      "Training Epoch: 13 [25600/50000]\tLoss: 1.4937\tLR: 0.002250\n",
      "Training Epoch: 13 [25728/50000]\tLoss: 1.1584\tLR: 0.002250\n",
      "Training Epoch: 13 [25856/50000]\tLoss: 1.1215\tLR: 0.002250\n",
      "Training Epoch: 13 [25984/50000]\tLoss: 1.3331\tLR: 0.002250\n",
      "Training Epoch: 13 [26112/50000]\tLoss: 1.1708\tLR: 0.002250\n",
      "Training Epoch: 13 [26240/50000]\tLoss: 1.4021\tLR: 0.002250\n",
      "Training Epoch: 13 [26368/50000]\tLoss: 1.4220\tLR: 0.002250\n",
      "Training Epoch: 13 [26496/50000]\tLoss: 1.2739\tLR: 0.002250\n",
      "Training Epoch: 13 [26624/50000]\tLoss: 1.0720\tLR: 0.002250\n",
      "Training Epoch: 13 [26752/50000]\tLoss: 1.1664\tLR: 0.002250\n",
      "Training Epoch: 13 [26880/50000]\tLoss: 1.0607\tLR: 0.002250\n",
      "Training Epoch: 13 [27008/50000]\tLoss: 1.2649\tLR: 0.002250\n",
      "Training Epoch: 13 [27136/50000]\tLoss: 1.2706\tLR: 0.002250\n",
      "Training Epoch: 13 [27264/50000]\tLoss: 1.3783\tLR: 0.002250\n",
      "Training Epoch: 13 [27392/50000]\tLoss: 1.4501\tLR: 0.002250\n",
      "Training Epoch: 13 [27520/50000]\tLoss: 1.0557\tLR: 0.002250\n",
      "Training Epoch: 13 [27648/50000]\tLoss: 1.0235\tLR: 0.002250\n",
      "Training Epoch: 13 [27776/50000]\tLoss: 1.3317\tLR: 0.002250\n",
      "Training Epoch: 13 [27904/50000]\tLoss: 1.3011\tLR: 0.002250\n",
      "Training Epoch: 13 [28032/50000]\tLoss: 0.9837\tLR: 0.002250\n",
      "Training Epoch: 13 [28160/50000]\tLoss: 1.2498\tLR: 0.002250\n",
      "Training Epoch: 13 [28288/50000]\tLoss: 1.1156\tLR: 0.002250\n",
      "Training Epoch: 13 [28416/50000]\tLoss: 1.3222\tLR: 0.002250\n",
      "Training Epoch: 13 [28544/50000]\tLoss: 1.1972\tLR: 0.002250\n",
      "Training Epoch: 13 [28672/50000]\tLoss: 1.0225\tLR: 0.002250\n",
      "Training Epoch: 13 [28800/50000]\tLoss: 1.2407\tLR: 0.002250\n",
      "Training Epoch: 13 [28928/50000]\tLoss: 1.1418\tLR: 0.002250\n",
      "Training Epoch: 13 [29056/50000]\tLoss: 1.3747\tLR: 0.002250\n",
      "Training Epoch: 13 [29184/50000]\tLoss: 1.2713\tLR: 0.002250\n",
      "Training Epoch: 13 [29312/50000]\tLoss: 1.1879\tLR: 0.002250\n",
      "Training Epoch: 13 [29440/50000]\tLoss: 1.2250\tLR: 0.002250\n",
      "Training Epoch: 13 [29568/50000]\tLoss: 1.1607\tLR: 0.002250\n",
      "Training Epoch: 13 [29696/50000]\tLoss: 1.0901\tLR: 0.002250\n",
      "Training Epoch: 13 [29824/50000]\tLoss: 1.1433\tLR: 0.002250\n",
      "Training Epoch: 13 [29952/50000]\tLoss: 1.1894\tLR: 0.002250\n",
      "Training Epoch: 13 [30080/50000]\tLoss: 1.3015\tLR: 0.002250\n",
      "Training Epoch: 13 [30208/50000]\tLoss: 1.2695\tLR: 0.002250\n",
      "Training Epoch: 13 [30336/50000]\tLoss: 1.1803\tLR: 0.002250\n",
      "Training Epoch: 13 [30464/50000]\tLoss: 1.1048\tLR: 0.002250\n",
      "Training Epoch: 13 [30592/50000]\tLoss: 1.1213\tLR: 0.002250\n",
      "Training Epoch: 13 [30720/50000]\tLoss: 1.3022\tLR: 0.002250\n",
      "Training Epoch: 13 [30848/50000]\tLoss: 1.2466\tLR: 0.002250\n",
      "Training Epoch: 13 [30976/50000]\tLoss: 1.0893\tLR: 0.002250\n",
      "Training Epoch: 13 [31104/50000]\tLoss: 1.2956\tLR: 0.002250\n",
      "Training Epoch: 13 [31232/50000]\tLoss: 1.2326\tLR: 0.002250\n",
      "Training Epoch: 13 [31360/50000]\tLoss: 1.2650\tLR: 0.002250\n",
      "Training Epoch: 13 [31488/50000]\tLoss: 1.1254\tLR: 0.002250\n",
      "Training Epoch: 13 [31616/50000]\tLoss: 1.4579\tLR: 0.002250\n",
      "Training Epoch: 13 [31744/50000]\tLoss: 1.2547\tLR: 0.002250\n",
      "Training Epoch: 13 [31872/50000]\tLoss: 1.0593\tLR: 0.002250\n",
      "Training Epoch: 13 [32000/50000]\tLoss: 1.2020\tLR: 0.002250\n",
      "Training Epoch: 13 [32128/50000]\tLoss: 1.3246\tLR: 0.002250\n",
      "Training Epoch: 13 [32256/50000]\tLoss: 1.0377\tLR: 0.002250\n",
      "Training Epoch: 13 [32384/50000]\tLoss: 1.2510\tLR: 0.002250\n",
      "Training Epoch: 13 [32512/50000]\tLoss: 0.9559\tLR: 0.002250\n",
      "Training Epoch: 13 [32640/50000]\tLoss: 1.1414\tLR: 0.002250\n",
      "Training Epoch: 13 [32768/50000]\tLoss: 1.0085\tLR: 0.002250\n",
      "Training Epoch: 13 [32896/50000]\tLoss: 1.1598\tLR: 0.002250\n",
      "Training Epoch: 13 [33024/50000]\tLoss: 1.1939\tLR: 0.002250\n",
      "Training Epoch: 13 [33152/50000]\tLoss: 1.3787\tLR: 0.002250\n",
      "Training Epoch: 13 [33280/50000]\tLoss: 1.1962\tLR: 0.002250\n",
      "Training Epoch: 13 [33408/50000]\tLoss: 1.3292\tLR: 0.002250\n",
      "Training Epoch: 13 [33536/50000]\tLoss: 1.4706\tLR: 0.002250\n",
      "Training Epoch: 13 [33664/50000]\tLoss: 1.2812\tLR: 0.002250\n",
      "Training Epoch: 13 [33792/50000]\tLoss: 1.0494\tLR: 0.002250\n",
      "Training Epoch: 13 [33920/50000]\tLoss: 1.0905\tLR: 0.002250\n",
      "Training Epoch: 13 [34048/50000]\tLoss: 1.1364\tLR: 0.002250\n",
      "Training Epoch: 13 [34176/50000]\tLoss: 1.2305\tLR: 0.002250\n",
      "Training Epoch: 13 [34304/50000]\tLoss: 1.1982\tLR: 0.002250\n",
      "Training Epoch: 13 [34432/50000]\tLoss: 1.4634\tLR: 0.002250\n",
      "Training Epoch: 13 [34560/50000]\tLoss: 1.2719\tLR: 0.002250\n",
      "Training Epoch: 13 [34688/50000]\tLoss: 1.3972\tLR: 0.002250\n",
      "Training Epoch: 13 [34816/50000]\tLoss: 1.2910\tLR: 0.002250\n",
      "Training Epoch: 13 [34944/50000]\tLoss: 1.4030\tLR: 0.002250\n",
      "Training Epoch: 13 [35072/50000]\tLoss: 1.2223\tLR: 0.002250\n",
      "Training Epoch: 13 [35200/50000]\tLoss: 1.4328\tLR: 0.002250\n",
      "Training Epoch: 13 [35328/50000]\tLoss: 1.3118\tLR: 0.002250\n",
      "Training Epoch: 13 [35456/50000]\tLoss: 0.9884\tLR: 0.002250\n",
      "Training Epoch: 13 [35584/50000]\tLoss: 1.0251\tLR: 0.002250\n",
      "Training Epoch: 13 [35712/50000]\tLoss: 1.1023\tLR: 0.002250\n",
      "Training Epoch: 13 [35840/50000]\tLoss: 1.0576\tLR: 0.002250\n",
      "Training Epoch: 13 [35968/50000]\tLoss: 1.3620\tLR: 0.002250\n",
      "Training Epoch: 13 [36096/50000]\tLoss: 1.1666\tLR: 0.002250\n",
      "Training Epoch: 13 [36224/50000]\tLoss: 1.4384\tLR: 0.002250\n",
      "Training Epoch: 13 [36352/50000]\tLoss: 1.2272\tLR: 0.002250\n",
      "Training Epoch: 13 [36480/50000]\tLoss: 1.1249\tLR: 0.002250\n",
      "Training Epoch: 13 [36608/50000]\tLoss: 1.2723\tLR: 0.002250\n",
      "Training Epoch: 13 [36736/50000]\tLoss: 1.1261\tLR: 0.002250\n",
      "Training Epoch: 13 [36864/50000]\tLoss: 1.2447\tLR: 0.002250\n",
      "Training Epoch: 13 [36992/50000]\tLoss: 1.2454\tLR: 0.002250\n",
      "Training Epoch: 13 [37120/50000]\tLoss: 1.2248\tLR: 0.002250\n",
      "Training Epoch: 13 [37248/50000]\tLoss: 1.2127\tLR: 0.002250\n",
      "Training Epoch: 13 [37376/50000]\tLoss: 1.0030\tLR: 0.002250\n",
      "Training Epoch: 13 [37504/50000]\tLoss: 1.2523\tLR: 0.002250\n",
      "Training Epoch: 13 [37632/50000]\tLoss: 1.1168\tLR: 0.002250\n",
      "Training Epoch: 13 [37760/50000]\tLoss: 1.4827\tLR: 0.002250\n",
      "Training Epoch: 13 [37888/50000]\tLoss: 1.1127\tLR: 0.002250\n",
      "Training Epoch: 13 [38016/50000]\tLoss: 1.0166\tLR: 0.002250\n",
      "Training Epoch: 13 [38144/50000]\tLoss: 1.1908\tLR: 0.002250\n",
      "Training Epoch: 13 [38272/50000]\tLoss: 1.3043\tLR: 0.002250\n",
      "Training Epoch: 13 [38400/50000]\tLoss: 1.2769\tLR: 0.002250\n",
      "Training Epoch: 13 [38528/50000]\tLoss: 1.4686\tLR: 0.002250\n",
      "Training Epoch: 13 [38656/50000]\tLoss: 1.5162\tLR: 0.002250\n",
      "Training Epoch: 13 [38784/50000]\tLoss: 1.2155\tLR: 0.002250\n",
      "Training Epoch: 13 [38912/50000]\tLoss: 1.1266\tLR: 0.002250\n",
      "Training Epoch: 13 [39040/50000]\tLoss: 1.3862\tLR: 0.002250\n",
      "Training Epoch: 13 [39168/50000]\tLoss: 1.2432\tLR: 0.002250\n",
      "Training Epoch: 13 [39296/50000]\tLoss: 1.3432\tLR: 0.002250\n",
      "Training Epoch: 13 [39424/50000]\tLoss: 1.1303\tLR: 0.002250\n",
      "Training Epoch: 13 [39552/50000]\tLoss: 1.3083\tLR: 0.002250\n",
      "Training Epoch: 13 [39680/50000]\tLoss: 1.1262\tLR: 0.002250\n",
      "Training Epoch: 13 [39808/50000]\tLoss: 1.3478\tLR: 0.002250\n",
      "Training Epoch: 13 [39936/50000]\tLoss: 1.2674\tLR: 0.002250\n",
      "Training Epoch: 13 [40064/50000]\tLoss: 1.1862\tLR: 0.002250\n",
      "Training Epoch: 13 [40192/50000]\tLoss: 1.2429\tLR: 0.002250\n",
      "Training Epoch: 13 [40320/50000]\tLoss: 1.3270\tLR: 0.002250\n",
      "Training Epoch: 13 [40448/50000]\tLoss: 1.2118\tLR: 0.002250\n",
      "Training Epoch: 13 [40576/50000]\tLoss: 1.0657\tLR: 0.002250\n",
      "Training Epoch: 13 [40704/50000]\tLoss: 1.1849\tLR: 0.002250\n",
      "Training Epoch: 13 [40832/50000]\tLoss: 1.0696\tLR: 0.002250\n",
      "Training Epoch: 13 [40960/50000]\tLoss: 1.4816\tLR: 0.002250\n",
      "Training Epoch: 13 [41088/50000]\tLoss: 1.2101\tLR: 0.002250\n",
      "Training Epoch: 13 [41216/50000]\tLoss: 1.2969\tLR: 0.002250\n",
      "Training Epoch: 13 [41344/50000]\tLoss: 1.1795\tLR: 0.002250\n",
      "Training Epoch: 13 [41472/50000]\tLoss: 1.3032\tLR: 0.002250\n",
      "Training Epoch: 13 [41600/50000]\tLoss: 1.2786\tLR: 0.002250\n",
      "Training Epoch: 13 [41728/50000]\tLoss: 1.2556\tLR: 0.002250\n",
      "Training Epoch: 13 [41856/50000]\tLoss: 1.1676\tLR: 0.002250\n",
      "Training Epoch: 13 [41984/50000]\tLoss: 1.1816\tLR: 0.002250\n",
      "Training Epoch: 13 [42112/50000]\tLoss: 1.2356\tLR: 0.002250\n",
      "Training Epoch: 13 [42240/50000]\tLoss: 1.1737\tLR: 0.002250\n",
      "Training Epoch: 13 [42368/50000]\tLoss: 1.0888\tLR: 0.002250\n",
      "Training Epoch: 13 [42496/50000]\tLoss: 1.0097\tLR: 0.002250\n",
      "Training Epoch: 13 [42624/50000]\tLoss: 1.0766\tLR: 0.002250\n",
      "Training Epoch: 13 [42752/50000]\tLoss: 1.5179\tLR: 0.002250\n",
      "Training Epoch: 13 [42880/50000]\tLoss: 1.2935\tLR: 0.002250\n",
      "Training Epoch: 13 [43008/50000]\tLoss: 1.1775\tLR: 0.002250\n",
      "Training Epoch: 13 [43136/50000]\tLoss: 1.4332\tLR: 0.002250\n",
      "Training Epoch: 13 [43264/50000]\tLoss: 1.0750\tLR: 0.002250\n",
      "Training Epoch: 13 [43392/50000]\tLoss: 1.2035\tLR: 0.002250\n",
      "Training Epoch: 13 [43520/50000]\tLoss: 1.3629\tLR: 0.002250\n",
      "Training Epoch: 13 [43648/50000]\tLoss: 1.2455\tLR: 0.002250\n",
      "Training Epoch: 13 [43776/50000]\tLoss: 1.1974\tLR: 0.002250\n",
      "Training Epoch: 13 [43904/50000]\tLoss: 1.1299\tLR: 0.002250\n",
      "Training Epoch: 13 [44032/50000]\tLoss: 1.2174\tLR: 0.002250\n",
      "Training Epoch: 13 [44160/50000]\tLoss: 1.2287\tLR: 0.002250\n",
      "Training Epoch: 13 [44288/50000]\tLoss: 1.4803\tLR: 0.002250\n",
      "Training Epoch: 13 [44416/50000]\tLoss: 1.2968\tLR: 0.002250\n",
      "Training Epoch: 13 [44544/50000]\tLoss: 1.4722\tLR: 0.002250\n",
      "Training Epoch: 13 [44672/50000]\tLoss: 1.1357\tLR: 0.002250\n",
      "Training Epoch: 13 [44800/50000]\tLoss: 1.2976\tLR: 0.002250\n",
      "Training Epoch: 13 [44928/50000]\tLoss: 1.3515\tLR: 0.002250\n",
      "Training Epoch: 13 [45056/50000]\tLoss: 1.2458\tLR: 0.002250\n",
      "Training Epoch: 13 [45184/50000]\tLoss: 1.3427\tLR: 0.002250\n",
      "Training Epoch: 13 [45312/50000]\tLoss: 1.4442\tLR: 0.002250\n",
      "Training Epoch: 13 [45440/50000]\tLoss: 1.1870\tLR: 0.002250\n",
      "Training Epoch: 13 [45568/50000]\tLoss: 1.2565\tLR: 0.002250\n",
      "Training Epoch: 13 [45696/50000]\tLoss: 1.3588\tLR: 0.002250\n",
      "Training Epoch: 13 [45824/50000]\tLoss: 1.3243\tLR: 0.002250\n",
      "Training Epoch: 13 [45952/50000]\tLoss: 1.1679\tLR: 0.002250\n",
      "Training Epoch: 13 [46080/50000]\tLoss: 1.2483\tLR: 0.002250\n",
      "Training Epoch: 13 [46208/50000]\tLoss: 1.0663\tLR: 0.002250\n",
      "Training Epoch: 13 [46336/50000]\tLoss: 1.3155\tLR: 0.002250\n",
      "Training Epoch: 13 [46464/50000]\tLoss: 1.1704\tLR: 0.002250\n",
      "Training Epoch: 13 [46592/50000]\tLoss: 1.2225\tLR: 0.002250\n",
      "Training Epoch: 13 [46720/50000]\tLoss: 1.2174\tLR: 0.002250\n",
      "Training Epoch: 13 [46848/50000]\tLoss: 0.9936\tLR: 0.002250\n",
      "Training Epoch: 13 [46976/50000]\tLoss: 0.9535\tLR: 0.002250\n",
      "Training Epoch: 13 [47104/50000]\tLoss: 1.1062\tLR: 0.002250\n",
      "Training Epoch: 13 [47232/50000]\tLoss: 1.1072\tLR: 0.002250\n",
      "Training Epoch: 13 [47360/50000]\tLoss: 1.0912\tLR: 0.002250\n",
      "Training Epoch: 13 [47488/50000]\tLoss: 1.3068\tLR: 0.002250\n",
      "Training Epoch: 13 [47616/50000]\tLoss: 1.3304\tLR: 0.002250\n",
      "Training Epoch: 13 [47744/50000]\tLoss: 1.1292\tLR: 0.002250\n",
      "Training Epoch: 13 [47872/50000]\tLoss: 1.2440\tLR: 0.002250\n",
      "Training Epoch: 13 [48000/50000]\tLoss: 1.1444\tLR: 0.002250\n",
      "Training Epoch: 13 [48128/50000]\tLoss: 1.2571\tLR: 0.002250\n",
      "Training Epoch: 13 [48256/50000]\tLoss: 1.2378\tLR: 0.002250\n",
      "Training Epoch: 13 [48384/50000]\tLoss: 1.3532\tLR: 0.002250\n",
      "Training Epoch: 13 [48512/50000]\tLoss: 0.9793\tLR: 0.002250\n",
      "Training Epoch: 13 [48640/50000]\tLoss: 1.1154\tLR: 0.002250\n",
      "Training Epoch: 13 [48768/50000]\tLoss: 1.2408\tLR: 0.002250\n",
      "Training Epoch: 13 [48896/50000]\tLoss: 0.8751\tLR: 0.002250\n",
      "Training Epoch: 13 [49024/50000]\tLoss: 1.0656\tLR: 0.002250\n",
      "Training Epoch: 13 [49152/50000]\tLoss: 1.0622\tLR: 0.002250\n",
      "Training Epoch: 13 [49280/50000]\tLoss: 1.3108\tLR: 0.002250\n",
      "Training Epoch: 13 [49408/50000]\tLoss: 0.9921\tLR: 0.002250\n",
      "Training Epoch: 13 [49536/50000]\tLoss: 1.2663\tLR: 0.002250\n",
      "Training Epoch: 13 [49664/50000]\tLoss: 1.3793\tLR: 0.002250\n",
      "Training Epoch: 13 [49792/50000]\tLoss: 1.2557\tLR: 0.002250\n",
      "Training Epoch: 13 [49920/50000]\tLoss: 1.2754\tLR: 0.002250\n",
      "Training Epoch: 13 [50000/50000]\tLoss: 1.3323\tLR: 0.002250\n",
      "Test set: Average loss: 0.0106, Accuracy: 0.6146\n",
      "\n",
      "Training Epoch: 14 [128/50000]\tLoss: 1.2210\tLR: 0.002250\n",
      "Training Epoch: 14 [256/50000]\tLoss: 1.1708\tLR: 0.002250\n",
      "Training Epoch: 14 [384/50000]\tLoss: 1.0770\tLR: 0.002250\n",
      "Training Epoch: 14 [512/50000]\tLoss: 1.3046\tLR: 0.002250\n",
      "Training Epoch: 14 [640/50000]\tLoss: 1.1918\tLR: 0.002250\n",
      "Training Epoch: 14 [768/50000]\tLoss: 1.2137\tLR: 0.002250\n",
      "Training Epoch: 14 [896/50000]\tLoss: 1.3507\tLR: 0.002250\n",
      "Training Epoch: 14 [1024/50000]\tLoss: 1.0281\tLR: 0.002250\n",
      "Training Epoch: 14 [1152/50000]\tLoss: 1.2254\tLR: 0.002250\n",
      "Training Epoch: 14 [1280/50000]\tLoss: 0.9975\tLR: 0.002250\n",
      "Training Epoch: 14 [1408/50000]\tLoss: 1.2713\tLR: 0.002250\n",
      "Training Epoch: 14 [1536/50000]\tLoss: 1.4713\tLR: 0.002250\n",
      "Training Epoch: 14 [1664/50000]\tLoss: 1.2385\tLR: 0.002250\n",
      "Training Epoch: 14 [1792/50000]\tLoss: 1.1274\tLR: 0.002250\n",
      "Training Epoch: 14 [1920/50000]\tLoss: 1.3322\tLR: 0.002250\n",
      "Training Epoch: 14 [2048/50000]\tLoss: 1.1334\tLR: 0.002250\n",
      "Training Epoch: 14 [2176/50000]\tLoss: 1.1880\tLR: 0.002250\n",
      "Training Epoch: 14 [2304/50000]\tLoss: 1.1169\tLR: 0.002250\n",
      "Training Epoch: 14 [2432/50000]\tLoss: 1.0991\tLR: 0.002250\n",
      "Training Epoch: 14 [2560/50000]\tLoss: 1.1540\tLR: 0.002250\n",
      "Training Epoch: 14 [2688/50000]\tLoss: 1.2927\tLR: 0.002250\n",
      "Training Epoch: 14 [2816/50000]\tLoss: 1.1196\tLR: 0.002250\n",
      "Training Epoch: 14 [2944/50000]\tLoss: 1.1923\tLR: 0.002250\n",
      "Training Epoch: 14 [3072/50000]\tLoss: 1.1439\tLR: 0.002250\n",
      "Training Epoch: 14 [3200/50000]\tLoss: 1.2869\tLR: 0.002250\n",
      "Training Epoch: 14 [3328/50000]\tLoss: 1.2857\tLR: 0.002250\n",
      "Training Epoch: 14 [3456/50000]\tLoss: 1.2459\tLR: 0.002250\n",
      "Training Epoch: 14 [3584/50000]\tLoss: 1.1044\tLR: 0.002250\n",
      "Training Epoch: 14 [3712/50000]\tLoss: 1.0285\tLR: 0.002250\n",
      "Training Epoch: 14 [3840/50000]\tLoss: 1.2555\tLR: 0.002250\n",
      "Training Epoch: 14 [3968/50000]\tLoss: 1.2180\tLR: 0.002250\n",
      "Training Epoch: 14 [4096/50000]\tLoss: 1.0178\tLR: 0.002250\n",
      "Training Epoch: 14 [4224/50000]\tLoss: 1.3831\tLR: 0.002250\n",
      "Training Epoch: 14 [4352/50000]\tLoss: 1.1360\tLR: 0.002250\n",
      "Training Epoch: 14 [4480/50000]\tLoss: 1.1089\tLR: 0.002250\n",
      "Training Epoch: 14 [4608/50000]\tLoss: 1.1592\tLR: 0.002250\n",
      "Training Epoch: 14 [4736/50000]\tLoss: 1.1477\tLR: 0.002250\n",
      "Training Epoch: 14 [4864/50000]\tLoss: 1.4721\tLR: 0.002250\n",
      "Training Epoch: 14 [4992/50000]\tLoss: 1.1767\tLR: 0.002250\n",
      "Training Epoch: 14 [5120/50000]\tLoss: 1.1290\tLR: 0.002250\n",
      "Training Epoch: 14 [5248/50000]\tLoss: 1.5303\tLR: 0.002250\n",
      "Training Epoch: 14 [5376/50000]\tLoss: 1.1107\tLR: 0.002250\n",
      "Training Epoch: 14 [5504/50000]\tLoss: 1.2868\tLR: 0.002250\n",
      "Training Epoch: 14 [5632/50000]\tLoss: 1.1682\tLR: 0.002250\n",
      "Training Epoch: 14 [5760/50000]\tLoss: 1.0740\tLR: 0.002250\n",
      "Training Epoch: 14 [5888/50000]\tLoss: 1.0658\tLR: 0.002250\n",
      "Training Epoch: 14 [6016/50000]\tLoss: 1.1028\tLR: 0.002250\n",
      "Training Epoch: 14 [6144/50000]\tLoss: 1.2131\tLR: 0.002250\n",
      "Training Epoch: 14 [6272/50000]\tLoss: 1.3918\tLR: 0.002250\n",
      "Training Epoch: 14 [6400/50000]\tLoss: 1.1897\tLR: 0.002250\n",
      "Training Epoch: 14 [6528/50000]\tLoss: 1.0566\tLR: 0.002250\n",
      "Training Epoch: 14 [6656/50000]\tLoss: 1.1473\tLR: 0.002250\n",
      "Training Epoch: 14 [6784/50000]\tLoss: 1.0927\tLR: 0.002250\n",
      "Training Epoch: 14 [6912/50000]\tLoss: 1.1350\tLR: 0.002250\n",
      "Training Epoch: 14 [7040/50000]\tLoss: 0.9456\tLR: 0.002250\n",
      "Training Epoch: 14 [7168/50000]\tLoss: 1.2833\tLR: 0.002250\n",
      "Training Epoch: 14 [7296/50000]\tLoss: 1.1947\tLR: 0.002250\n",
      "Training Epoch: 14 [7424/50000]\tLoss: 1.2373\tLR: 0.002250\n",
      "Training Epoch: 14 [7552/50000]\tLoss: 1.2365\tLR: 0.002250\n",
      "Training Epoch: 14 [7680/50000]\tLoss: 1.1943\tLR: 0.002250\n",
      "Training Epoch: 14 [7808/50000]\tLoss: 1.0924\tLR: 0.002250\n",
      "Training Epoch: 14 [7936/50000]\tLoss: 1.0574\tLR: 0.002250\n",
      "Training Epoch: 14 [8064/50000]\tLoss: 1.2059\tLR: 0.002250\n",
      "Training Epoch: 14 [8192/50000]\tLoss: 1.2858\tLR: 0.002250\n",
      "Training Epoch: 14 [8320/50000]\tLoss: 1.0871\tLR: 0.002250\n",
      "Training Epoch: 14 [8448/50000]\tLoss: 1.0517\tLR: 0.002250\n",
      "Training Epoch: 14 [8576/50000]\tLoss: 1.0868\tLR: 0.002250\n",
      "Training Epoch: 14 [8704/50000]\tLoss: 1.1428\tLR: 0.002250\n",
      "Training Epoch: 14 [8832/50000]\tLoss: 0.8664\tLR: 0.002250\n",
      "Training Epoch: 14 [8960/50000]\tLoss: 1.1996\tLR: 0.002250\n",
      "Training Epoch: 14 [9088/50000]\tLoss: 1.1997\tLR: 0.002250\n",
      "Training Epoch: 14 [9216/50000]\tLoss: 1.1659\tLR: 0.002250\n",
      "Training Epoch: 14 [9344/50000]\tLoss: 1.0487\tLR: 0.002250\n",
      "Training Epoch: 14 [9472/50000]\tLoss: 1.0515\tLR: 0.002250\n",
      "Training Epoch: 14 [9600/50000]\tLoss: 1.1127\tLR: 0.002250\n",
      "Training Epoch: 14 [9728/50000]\tLoss: 1.1400\tLR: 0.002250\n",
      "Training Epoch: 14 [9856/50000]\tLoss: 0.9630\tLR: 0.002250\n",
      "Training Epoch: 14 [9984/50000]\tLoss: 1.3870\tLR: 0.002250\n",
      "Training Epoch: 14 [10112/50000]\tLoss: 0.8932\tLR: 0.002250\n",
      "Training Epoch: 14 [10240/50000]\tLoss: 1.3849\tLR: 0.002250\n",
      "Training Epoch: 14 [10368/50000]\tLoss: 1.0255\tLR: 0.002250\n",
      "Training Epoch: 14 [10496/50000]\tLoss: 1.2353\tLR: 0.002250\n",
      "Training Epoch: 14 [10624/50000]\tLoss: 1.2227\tLR: 0.002250\n",
      "Training Epoch: 14 [10752/50000]\tLoss: 1.1434\tLR: 0.002250\n",
      "Training Epoch: 14 [10880/50000]\tLoss: 1.2448\tLR: 0.002250\n",
      "Training Epoch: 14 [11008/50000]\tLoss: 0.9732\tLR: 0.002250\n",
      "Training Epoch: 14 [11136/50000]\tLoss: 1.2692\tLR: 0.002250\n",
      "Training Epoch: 14 [11264/50000]\tLoss: 1.2131\tLR: 0.002250\n",
      "Training Epoch: 14 [11392/50000]\tLoss: 1.1339\tLR: 0.002250\n",
      "Training Epoch: 14 [11520/50000]\tLoss: 0.9869\tLR: 0.002250\n",
      "Training Epoch: 14 [11648/50000]\tLoss: 1.1842\tLR: 0.002250\n",
      "Training Epoch: 14 [11776/50000]\tLoss: 1.2576\tLR: 0.002250\n",
      "Training Epoch: 14 [11904/50000]\tLoss: 1.0987\tLR: 0.002250\n",
      "Training Epoch: 14 [12032/50000]\tLoss: 1.0966\tLR: 0.002250\n",
      "Training Epoch: 14 [12160/50000]\tLoss: 1.1948\tLR: 0.002250\n",
      "Training Epoch: 14 [12288/50000]\tLoss: 1.1841\tLR: 0.002250\n",
      "Training Epoch: 14 [12416/50000]\tLoss: 1.0421\tLR: 0.002250\n",
      "Training Epoch: 14 [12544/50000]\tLoss: 1.0292\tLR: 0.002250\n",
      "Training Epoch: 14 [12672/50000]\tLoss: 1.3150\tLR: 0.002250\n",
      "Training Epoch: 14 [12800/50000]\tLoss: 1.0449\tLR: 0.002250\n",
      "Training Epoch: 14 [12928/50000]\tLoss: 1.3531\tLR: 0.002250\n",
      "Training Epoch: 14 [13056/50000]\tLoss: 1.3091\tLR: 0.002250\n",
      "Training Epoch: 14 [13184/50000]\tLoss: 1.1937\tLR: 0.002250\n",
      "Training Epoch: 14 [13312/50000]\tLoss: 1.4701\tLR: 0.002250\n",
      "Training Epoch: 14 [13440/50000]\tLoss: 1.1000\tLR: 0.002250\n",
      "Training Epoch: 14 [13568/50000]\tLoss: 1.1441\tLR: 0.002250\n",
      "Training Epoch: 14 [13696/50000]\tLoss: 1.1383\tLR: 0.002250\n",
      "Training Epoch: 14 [13824/50000]\tLoss: 1.1121\tLR: 0.002250\n",
      "Training Epoch: 14 [13952/50000]\tLoss: 1.1420\tLR: 0.002250\n",
      "Training Epoch: 14 [14080/50000]\tLoss: 1.0338\tLR: 0.002250\n",
      "Training Epoch: 14 [14208/50000]\tLoss: 1.1946\tLR: 0.002250\n",
      "Training Epoch: 14 [14336/50000]\tLoss: 1.1060\tLR: 0.002250\n",
      "Training Epoch: 14 [14464/50000]\tLoss: 1.2096\tLR: 0.002250\n",
      "Training Epoch: 14 [14592/50000]\tLoss: 1.3232\tLR: 0.002250\n",
      "Training Epoch: 14 [14720/50000]\tLoss: 1.2258\tLR: 0.002250\n",
      "Training Epoch: 14 [14848/50000]\tLoss: 1.2381\tLR: 0.002250\n",
      "Training Epoch: 14 [14976/50000]\tLoss: 1.2301\tLR: 0.002250\n",
      "Training Epoch: 14 [15104/50000]\tLoss: 1.1271\tLR: 0.002250\n",
      "Training Epoch: 14 [15232/50000]\tLoss: 1.2509\tLR: 0.002250\n",
      "Training Epoch: 14 [15360/50000]\tLoss: 1.1044\tLR: 0.002250\n",
      "Training Epoch: 14 [15488/50000]\tLoss: 1.1086\tLR: 0.002250\n",
      "Training Epoch: 14 [15616/50000]\tLoss: 1.3092\tLR: 0.002250\n",
      "Training Epoch: 14 [15744/50000]\tLoss: 1.3938\tLR: 0.002250\n",
      "Training Epoch: 14 [15872/50000]\tLoss: 1.0393\tLR: 0.002250\n",
      "Training Epoch: 14 [16000/50000]\tLoss: 1.0307\tLR: 0.002250\n",
      "Training Epoch: 14 [16128/50000]\tLoss: 1.1202\tLR: 0.002250\n",
      "Training Epoch: 14 [16256/50000]\tLoss: 1.3919\tLR: 0.002250\n",
      "Training Epoch: 14 [16384/50000]\tLoss: 1.1624\tLR: 0.002250\n",
      "Training Epoch: 14 [16512/50000]\tLoss: 1.2616\tLR: 0.002250\n",
      "Training Epoch: 14 [16640/50000]\tLoss: 1.1833\tLR: 0.002250\n",
      "Training Epoch: 14 [16768/50000]\tLoss: 1.2037\tLR: 0.002250\n",
      "Training Epoch: 14 [16896/50000]\tLoss: 0.9604\tLR: 0.002250\n",
      "Training Epoch: 14 [17024/50000]\tLoss: 1.3059\tLR: 0.002250\n",
      "Training Epoch: 14 [17152/50000]\tLoss: 1.1873\tLR: 0.002250\n",
      "Training Epoch: 14 [17280/50000]\tLoss: 1.1912\tLR: 0.002250\n",
      "Training Epoch: 14 [17408/50000]\tLoss: 1.4092\tLR: 0.002250\n",
      "Training Epoch: 14 [17536/50000]\tLoss: 1.1080\tLR: 0.002250\n",
      "Training Epoch: 14 [17664/50000]\tLoss: 1.3245\tLR: 0.002250\n",
      "Training Epoch: 14 [17792/50000]\tLoss: 1.3014\tLR: 0.002250\n",
      "Training Epoch: 14 [17920/50000]\tLoss: 1.0571\tLR: 0.002250\n",
      "Training Epoch: 14 [18048/50000]\tLoss: 1.2055\tLR: 0.002250\n",
      "Training Epoch: 14 [18176/50000]\tLoss: 1.1051\tLR: 0.002250\n",
      "Training Epoch: 14 [18304/50000]\tLoss: 1.3870\tLR: 0.002250\n",
      "Training Epoch: 14 [18432/50000]\tLoss: 1.0697\tLR: 0.002250\n",
      "Training Epoch: 14 [18560/50000]\tLoss: 1.1485\tLR: 0.002250\n",
      "Training Epoch: 14 [18688/50000]\tLoss: 1.1048\tLR: 0.002250\n",
      "Training Epoch: 14 [18816/50000]\tLoss: 1.3037\tLR: 0.002250\n",
      "Training Epoch: 14 [18944/50000]\tLoss: 1.2561\tLR: 0.002250\n",
      "Training Epoch: 14 [19072/50000]\tLoss: 1.4322\tLR: 0.002250\n",
      "Training Epoch: 14 [19200/50000]\tLoss: 1.3718\tLR: 0.002250\n",
      "Training Epoch: 14 [19328/50000]\tLoss: 1.2206\tLR: 0.002250\n",
      "Training Epoch: 14 [19456/50000]\tLoss: 1.0876\tLR: 0.002250\n",
      "Training Epoch: 14 [19584/50000]\tLoss: 1.0712\tLR: 0.002250\n",
      "Training Epoch: 14 [19712/50000]\tLoss: 1.3003\tLR: 0.002250\n",
      "Training Epoch: 14 [19840/50000]\tLoss: 1.2400\tLR: 0.002250\n",
      "Training Epoch: 14 [19968/50000]\tLoss: 1.1504\tLR: 0.002250\n",
      "Training Epoch: 14 [20096/50000]\tLoss: 1.1598\tLR: 0.002250\n",
      "Training Epoch: 14 [20224/50000]\tLoss: 1.1299\tLR: 0.002250\n",
      "Training Epoch: 14 [20352/50000]\tLoss: 1.1302\tLR: 0.002250\n",
      "Training Epoch: 14 [20480/50000]\tLoss: 1.1801\tLR: 0.002250\n",
      "Training Epoch: 14 [20608/50000]\tLoss: 1.0050\tLR: 0.002250\n",
      "Training Epoch: 14 [20736/50000]\tLoss: 1.2338\tLR: 0.002250\n",
      "Training Epoch: 14 [20864/50000]\tLoss: 1.1338\tLR: 0.002250\n",
      "Training Epoch: 14 [20992/50000]\tLoss: 1.1085\tLR: 0.002250\n",
      "Training Epoch: 14 [21120/50000]\tLoss: 1.1689\tLR: 0.002250\n",
      "Training Epoch: 14 [21248/50000]\tLoss: 1.1493\tLR: 0.002250\n",
      "Training Epoch: 14 [21376/50000]\tLoss: 1.0426\tLR: 0.002250\n",
      "Training Epoch: 14 [21504/50000]\tLoss: 1.2558\tLR: 0.002250\n",
      "Training Epoch: 14 [21632/50000]\tLoss: 1.0690\tLR: 0.002250\n",
      "Training Epoch: 14 [21760/50000]\tLoss: 1.2183\tLR: 0.002250\n",
      "Training Epoch: 14 [21888/50000]\tLoss: 1.1516\tLR: 0.002250\n",
      "Training Epoch: 14 [22016/50000]\tLoss: 1.3433\tLR: 0.002250\n",
      "Training Epoch: 14 [22144/50000]\tLoss: 1.1092\tLR: 0.002250\n",
      "Training Epoch: 14 [22272/50000]\tLoss: 1.1335\tLR: 0.002250\n",
      "Training Epoch: 14 [22400/50000]\tLoss: 1.1782\tLR: 0.002250\n",
      "Training Epoch: 14 [22528/50000]\tLoss: 1.6394\tLR: 0.002250\n",
      "Training Epoch: 14 [22656/50000]\tLoss: 1.1619\tLR: 0.002250\n",
      "Training Epoch: 14 [22784/50000]\tLoss: 1.1291\tLR: 0.002250\n",
      "Training Epoch: 14 [22912/50000]\tLoss: 1.4060\tLR: 0.002250\n",
      "Training Epoch: 14 [23040/50000]\tLoss: 1.2425\tLR: 0.002250\n",
      "Training Epoch: 14 [23168/50000]\tLoss: 1.1613\tLR: 0.002250\n",
      "Training Epoch: 14 [23296/50000]\tLoss: 1.2106\tLR: 0.002250\n",
      "Training Epoch: 14 [23424/50000]\tLoss: 1.1202\tLR: 0.002250\n",
      "Training Epoch: 14 [23552/50000]\tLoss: 1.0165\tLR: 0.002250\n",
      "Training Epoch: 14 [23680/50000]\tLoss: 1.0154\tLR: 0.002250\n",
      "Training Epoch: 14 [23808/50000]\tLoss: 1.2977\tLR: 0.002250\n",
      "Training Epoch: 14 [23936/50000]\tLoss: 1.3476\tLR: 0.002250\n",
      "Training Epoch: 14 [24064/50000]\tLoss: 1.4037\tLR: 0.002250\n",
      "Training Epoch: 14 [24192/50000]\tLoss: 1.0732\tLR: 0.002250\n",
      "Training Epoch: 14 [24320/50000]\tLoss: 1.3080\tLR: 0.002250\n",
      "Training Epoch: 14 [24448/50000]\tLoss: 1.3221\tLR: 0.002250\n",
      "Training Epoch: 14 [24576/50000]\tLoss: 1.3009\tLR: 0.002250\n",
      "Training Epoch: 14 [24704/50000]\tLoss: 1.2678\tLR: 0.002250\n",
      "Training Epoch: 14 [24832/50000]\tLoss: 1.3754\tLR: 0.002250\n",
      "Training Epoch: 14 [24960/50000]\tLoss: 1.1017\tLR: 0.002250\n",
      "Training Epoch: 14 [25088/50000]\tLoss: 1.3129\tLR: 0.002250\n",
      "Training Epoch: 14 [25216/50000]\tLoss: 1.0633\tLR: 0.002250\n",
      "Training Epoch: 14 [25344/50000]\tLoss: 1.1578\tLR: 0.002250\n",
      "Training Epoch: 14 [25472/50000]\tLoss: 1.4007\tLR: 0.002250\n",
      "Training Epoch: 14 [25600/50000]\tLoss: 1.3283\tLR: 0.002250\n",
      "Training Epoch: 14 [25728/50000]\tLoss: 1.2703\tLR: 0.002250\n",
      "Training Epoch: 14 [25856/50000]\tLoss: 1.1787\tLR: 0.002250\n",
      "Training Epoch: 14 [25984/50000]\tLoss: 1.2539\tLR: 0.002250\n",
      "Training Epoch: 14 [26112/50000]\tLoss: 1.3699\tLR: 0.002250\n",
      "Training Epoch: 14 [26240/50000]\tLoss: 1.3654\tLR: 0.002250\n",
      "Training Epoch: 14 [26368/50000]\tLoss: 1.1327\tLR: 0.002250\n",
      "Training Epoch: 14 [26496/50000]\tLoss: 1.2552\tLR: 0.002250\n",
      "Training Epoch: 14 [26624/50000]\tLoss: 1.4418\tLR: 0.002250\n",
      "Training Epoch: 14 [26752/50000]\tLoss: 1.2759\tLR: 0.002250\n",
      "Training Epoch: 14 [26880/50000]\tLoss: 1.1680\tLR: 0.002250\n",
      "Training Epoch: 14 [27008/50000]\tLoss: 1.0872\tLR: 0.002250\n",
      "Training Epoch: 14 [27136/50000]\tLoss: 1.2222\tLR: 0.002250\n",
      "Training Epoch: 14 [27264/50000]\tLoss: 1.3225\tLR: 0.002250\n",
      "Training Epoch: 14 [27392/50000]\tLoss: 1.2118\tLR: 0.002250\n",
      "Training Epoch: 14 [27520/50000]\tLoss: 0.8824\tLR: 0.002250\n",
      "Training Epoch: 14 [27648/50000]\tLoss: 1.1898\tLR: 0.002250\n",
      "Training Epoch: 14 [27776/50000]\tLoss: 1.1887\tLR: 0.002250\n",
      "Training Epoch: 14 [27904/50000]\tLoss: 1.1771\tLR: 0.002250\n",
      "Training Epoch: 14 [28032/50000]\tLoss: 1.1123\tLR: 0.002250\n",
      "Training Epoch: 14 [28160/50000]\tLoss: 1.3005\tLR: 0.002250\n",
      "Training Epoch: 14 [28288/50000]\tLoss: 1.1337\tLR: 0.002250\n",
      "Training Epoch: 14 [28416/50000]\tLoss: 1.1592\tLR: 0.002250\n",
      "Training Epoch: 14 [28544/50000]\tLoss: 1.4144\tLR: 0.002250\n",
      "Training Epoch: 14 [28672/50000]\tLoss: 1.5165\tLR: 0.002250\n",
      "Training Epoch: 14 [28800/50000]\tLoss: 1.2421\tLR: 0.002250\n",
      "Training Epoch: 14 [28928/50000]\tLoss: 1.0674\tLR: 0.002250\n",
      "Training Epoch: 14 [29056/50000]\tLoss: 1.1286\tLR: 0.002250\n",
      "Training Epoch: 14 [29184/50000]\tLoss: 0.9511\tLR: 0.002250\n",
      "Training Epoch: 14 [29312/50000]\tLoss: 1.1641\tLR: 0.002250\n",
      "Training Epoch: 14 [29440/50000]\tLoss: 1.0000\tLR: 0.002250\n",
      "Training Epoch: 14 [29568/50000]\tLoss: 1.2128\tLR: 0.002250\n",
      "Training Epoch: 14 [29696/50000]\tLoss: 1.1867\tLR: 0.002250\n",
      "Training Epoch: 14 [29824/50000]\tLoss: 1.2377\tLR: 0.002250\n",
      "Training Epoch: 14 [29952/50000]\tLoss: 1.1293\tLR: 0.002250\n",
      "Training Epoch: 14 [30080/50000]\tLoss: 1.5056\tLR: 0.002250\n",
      "Training Epoch: 14 [30208/50000]\tLoss: 1.2024\tLR: 0.002250\n",
      "Training Epoch: 14 [30336/50000]\tLoss: 1.4821\tLR: 0.002250\n",
      "Training Epoch: 14 [30464/50000]\tLoss: 1.1692\tLR: 0.002250\n",
      "Training Epoch: 14 [30592/50000]\tLoss: 1.2025\tLR: 0.002250\n",
      "Training Epoch: 14 [30720/50000]\tLoss: 1.3162\tLR: 0.002250\n",
      "Training Epoch: 14 [30848/50000]\tLoss: 1.0452\tLR: 0.002250\n",
      "Training Epoch: 14 [30976/50000]\tLoss: 1.2348\tLR: 0.002250\n",
      "Training Epoch: 14 [31104/50000]\tLoss: 1.2533\tLR: 0.002250\n",
      "Training Epoch: 14 [31232/50000]\tLoss: 1.1342\tLR: 0.002250\n",
      "Training Epoch: 14 [31360/50000]\tLoss: 1.0250\tLR: 0.002250\n",
      "Training Epoch: 14 [31488/50000]\tLoss: 1.3062\tLR: 0.002250\n",
      "Training Epoch: 14 [31616/50000]\tLoss: 1.3572\tLR: 0.002250\n",
      "Training Epoch: 14 [31744/50000]\tLoss: 1.2421\tLR: 0.002250\n",
      "Training Epoch: 14 [31872/50000]\tLoss: 1.3952\tLR: 0.002250\n",
      "Training Epoch: 14 [32000/50000]\tLoss: 1.4183\tLR: 0.002250\n",
      "Training Epoch: 14 [32128/50000]\tLoss: 1.2936\tLR: 0.002250\n",
      "Training Epoch: 14 [32256/50000]\tLoss: 1.1610\tLR: 0.002250\n",
      "Training Epoch: 14 [32384/50000]\tLoss: 1.2419\tLR: 0.002250\n",
      "Training Epoch: 14 [32512/50000]\tLoss: 1.2629\tLR: 0.002250\n",
      "Training Epoch: 14 [32640/50000]\tLoss: 1.2202\tLR: 0.002250\n",
      "Training Epoch: 14 [32768/50000]\tLoss: 1.1429\tLR: 0.002250\n",
      "Training Epoch: 14 [32896/50000]\tLoss: 1.1545\tLR: 0.002250\n",
      "Training Epoch: 14 [33024/50000]\tLoss: 0.9617\tLR: 0.002250\n",
      "Training Epoch: 14 [33152/50000]\tLoss: 1.2269\tLR: 0.002250\n",
      "Training Epoch: 14 [33280/50000]\tLoss: 1.1320\tLR: 0.002250\n",
      "Training Epoch: 14 [33408/50000]\tLoss: 1.2131\tLR: 0.002250\n",
      "Training Epoch: 14 [33536/50000]\tLoss: 1.4016\tLR: 0.002250\n",
      "Training Epoch: 14 [33664/50000]\tLoss: 1.0999\tLR: 0.002250\n",
      "Training Epoch: 14 [33792/50000]\tLoss: 1.0156\tLR: 0.002250\n",
      "Training Epoch: 14 [33920/50000]\tLoss: 1.1946\tLR: 0.002250\n",
      "Training Epoch: 14 [34048/50000]\tLoss: 1.2335\tLR: 0.002250\n",
      "Training Epoch: 14 [34176/50000]\tLoss: 1.2854\tLR: 0.002250\n",
      "Training Epoch: 14 [34304/50000]\tLoss: 1.2678\tLR: 0.002250\n",
      "Training Epoch: 14 [34432/50000]\tLoss: 1.3305\tLR: 0.002250\n",
      "Training Epoch: 14 [34560/50000]\tLoss: 1.2292\tLR: 0.002250\n",
      "Training Epoch: 14 [34688/50000]\tLoss: 1.3391\tLR: 0.002250\n",
      "Training Epoch: 14 [34816/50000]\tLoss: 1.1513\tLR: 0.002250\n",
      "Training Epoch: 14 [34944/50000]\tLoss: 1.1451\tLR: 0.002250\n",
      "Training Epoch: 14 [35072/50000]\tLoss: 1.2714\tLR: 0.002250\n",
      "Training Epoch: 14 [35200/50000]\tLoss: 0.9543\tLR: 0.002250\n",
      "Training Epoch: 14 [35328/50000]\tLoss: 1.2627\tLR: 0.002250\n",
      "Training Epoch: 14 [35456/50000]\tLoss: 1.2390\tLR: 0.002250\n",
      "Training Epoch: 14 [35584/50000]\tLoss: 1.2885\tLR: 0.002250\n",
      "Training Epoch: 14 [35712/50000]\tLoss: 1.2050\tLR: 0.002250\n",
      "Training Epoch: 14 [35840/50000]\tLoss: 1.2257\tLR: 0.002250\n",
      "Training Epoch: 14 [35968/50000]\tLoss: 1.3088\tLR: 0.002250\n",
      "Training Epoch: 14 [36096/50000]\tLoss: 1.1314\tLR: 0.002250\n",
      "Training Epoch: 14 [36224/50000]\tLoss: 1.5080\tLR: 0.002250\n",
      "Training Epoch: 14 [36352/50000]\tLoss: 1.2935\tLR: 0.002250\n",
      "Training Epoch: 14 [36480/50000]\tLoss: 1.2227\tLR: 0.002250\n",
      "Training Epoch: 14 [36608/50000]\tLoss: 1.0927\tLR: 0.002250\n",
      "Training Epoch: 14 [36736/50000]\tLoss: 1.1965\tLR: 0.002250\n",
      "Training Epoch: 14 [36864/50000]\tLoss: 1.3266\tLR: 0.002250\n",
      "Training Epoch: 14 [36992/50000]\tLoss: 1.2720\tLR: 0.002250\n",
      "Training Epoch: 14 [37120/50000]\tLoss: 1.2391\tLR: 0.002250\n",
      "Training Epoch: 14 [37248/50000]\tLoss: 1.3241\tLR: 0.002250\n",
      "Training Epoch: 14 [37376/50000]\tLoss: 1.2349\tLR: 0.002250\n",
      "Training Epoch: 14 [37504/50000]\tLoss: 1.3798\tLR: 0.002250\n",
      "Training Epoch: 14 [37632/50000]\tLoss: 1.2053\tLR: 0.002250\n",
      "Training Epoch: 14 [37760/50000]\tLoss: 1.1420\tLR: 0.002250\n",
      "Training Epoch: 14 [37888/50000]\tLoss: 1.2033\tLR: 0.002250\n",
      "Training Epoch: 14 [38016/50000]\tLoss: 0.9246\tLR: 0.002250\n",
      "Training Epoch: 14 [38144/50000]\tLoss: 1.3133\tLR: 0.002250\n",
      "Training Epoch: 14 [38272/50000]\tLoss: 1.1447\tLR: 0.002250\n",
      "Training Epoch: 14 [38400/50000]\tLoss: 1.2096\tLR: 0.002250\n",
      "Training Epoch: 14 [38528/50000]\tLoss: 1.2590\tLR: 0.002250\n",
      "Training Epoch: 14 [38656/50000]\tLoss: 1.3128\tLR: 0.002250\n",
      "Training Epoch: 14 [38784/50000]\tLoss: 1.2521\tLR: 0.002250\n",
      "Training Epoch: 14 [38912/50000]\tLoss: 1.2677\tLR: 0.002250\n",
      "Training Epoch: 14 [39040/50000]\tLoss: 1.0985\tLR: 0.002250\n",
      "Training Epoch: 14 [39168/50000]\tLoss: 1.2098\tLR: 0.002250\n",
      "Training Epoch: 14 [39296/50000]\tLoss: 1.2007\tLR: 0.002250\n",
      "Training Epoch: 14 [39424/50000]\tLoss: 1.1841\tLR: 0.002250\n",
      "Training Epoch: 14 [39552/50000]\tLoss: 1.1229\tLR: 0.002250\n",
      "Training Epoch: 14 [39680/50000]\tLoss: 1.2222\tLR: 0.002250\n",
      "Training Epoch: 14 [39808/50000]\tLoss: 1.1414\tLR: 0.002250\n",
      "Training Epoch: 14 [39936/50000]\tLoss: 1.2162\tLR: 0.002250\n",
      "Training Epoch: 14 [40064/50000]\tLoss: 1.1883\tLR: 0.002250\n",
      "Training Epoch: 14 [40192/50000]\tLoss: 1.0987\tLR: 0.002250\n",
      "Training Epoch: 14 [40320/50000]\tLoss: 1.1933\tLR: 0.002250\n",
      "Training Epoch: 14 [40448/50000]\tLoss: 1.0786\tLR: 0.002250\n",
      "Training Epoch: 14 [40576/50000]\tLoss: 1.1768\tLR: 0.002250\n",
      "Training Epoch: 14 [40704/50000]\tLoss: 1.0934\tLR: 0.002250\n",
      "Training Epoch: 14 [40832/50000]\tLoss: 1.1740\tLR: 0.002250\n",
      "Training Epoch: 14 [40960/50000]\tLoss: 1.1641\tLR: 0.002250\n",
      "Training Epoch: 14 [41088/50000]\tLoss: 1.0280\tLR: 0.002250\n",
      "Training Epoch: 14 [41216/50000]\tLoss: 1.1465\tLR: 0.002250\n",
      "Training Epoch: 14 [41344/50000]\tLoss: 1.3009\tLR: 0.002250\n",
      "Training Epoch: 14 [41472/50000]\tLoss: 1.0940\tLR: 0.002250\n",
      "Training Epoch: 14 [41600/50000]\tLoss: 1.0385\tLR: 0.002250\n",
      "Training Epoch: 14 [41728/50000]\tLoss: 1.0382\tLR: 0.002250\n",
      "Training Epoch: 14 [41856/50000]\tLoss: 1.1022\tLR: 0.002250\n",
      "Training Epoch: 14 [41984/50000]\tLoss: 1.1778\tLR: 0.002250\n",
      "Training Epoch: 14 [42112/50000]\tLoss: 1.2035\tLR: 0.002250\n",
      "Training Epoch: 14 [42240/50000]\tLoss: 1.2665\tLR: 0.002250\n",
      "Training Epoch: 14 [42368/50000]\tLoss: 1.4144\tLR: 0.002250\n",
      "Training Epoch: 14 [42496/50000]\tLoss: 1.3858\tLR: 0.002250\n",
      "Training Epoch: 14 [42624/50000]\tLoss: 1.1561\tLR: 0.002250\n",
      "Training Epoch: 14 [42752/50000]\tLoss: 1.2801\tLR: 0.002250\n",
      "Training Epoch: 14 [42880/50000]\tLoss: 1.4135\tLR: 0.002250\n",
      "Training Epoch: 14 [43008/50000]\tLoss: 1.2807\tLR: 0.002250\n",
      "Training Epoch: 14 [43136/50000]\tLoss: 1.1204\tLR: 0.002250\n",
      "Training Epoch: 14 [43264/50000]\tLoss: 1.2320\tLR: 0.002250\n",
      "Training Epoch: 14 [43392/50000]\tLoss: 1.1504\tLR: 0.002250\n",
      "Training Epoch: 14 [43520/50000]\tLoss: 1.0930\tLR: 0.002250\n",
      "Training Epoch: 14 [43648/50000]\tLoss: 1.2586\tLR: 0.002250\n",
      "Training Epoch: 14 [43776/50000]\tLoss: 1.1955\tLR: 0.002250\n",
      "Training Epoch: 14 [43904/50000]\tLoss: 1.2234\tLR: 0.002250\n",
      "Training Epoch: 14 [44032/50000]\tLoss: 1.2463\tLR: 0.002250\n",
      "Training Epoch: 14 [44160/50000]\tLoss: 1.2459\tLR: 0.002250\n",
      "Training Epoch: 14 [44288/50000]\tLoss: 1.0970\tLR: 0.002250\n",
      "Training Epoch: 14 [44416/50000]\tLoss: 1.2918\tLR: 0.002250\n",
      "Training Epoch: 14 [44544/50000]\tLoss: 1.1756\tLR: 0.002250\n",
      "Training Epoch: 14 [44672/50000]\tLoss: 1.3586\tLR: 0.002250\n",
      "Training Epoch: 14 [44800/50000]\tLoss: 0.9483\tLR: 0.002250\n",
      "Training Epoch: 14 [44928/50000]\tLoss: 1.0744\tLR: 0.002250\n",
      "Training Epoch: 14 [45056/50000]\tLoss: 0.9468\tLR: 0.002250\n",
      "Training Epoch: 14 [45184/50000]\tLoss: 0.9939\tLR: 0.002250\n",
      "Training Epoch: 14 [45312/50000]\tLoss: 1.3422\tLR: 0.002250\n",
      "Training Epoch: 14 [45440/50000]\tLoss: 1.4411\tLR: 0.002250\n",
      "Training Epoch: 14 [45568/50000]\tLoss: 1.3196\tLR: 0.002250\n",
      "Training Epoch: 14 [45696/50000]\tLoss: 1.2771\tLR: 0.002250\n",
      "Training Epoch: 14 [45824/50000]\tLoss: 1.2766\tLR: 0.002250\n",
      "Training Epoch: 14 [45952/50000]\tLoss: 1.3253\tLR: 0.002250\n",
      "Training Epoch: 14 [46080/50000]\tLoss: 1.2388\tLR: 0.002250\n",
      "Training Epoch: 14 [46208/50000]\tLoss: 1.0337\tLR: 0.002250\n",
      "Training Epoch: 14 [46336/50000]\tLoss: 1.3112\tLR: 0.002250\n",
      "Training Epoch: 14 [46464/50000]\tLoss: 1.1888\tLR: 0.002250\n",
      "Training Epoch: 14 [46592/50000]\tLoss: 0.9822\tLR: 0.002250\n",
      "Training Epoch: 14 [46720/50000]\tLoss: 1.2609\tLR: 0.002250\n",
      "Training Epoch: 14 [46848/50000]\tLoss: 1.2159\tLR: 0.002250\n",
      "Training Epoch: 14 [46976/50000]\tLoss: 1.0517\tLR: 0.002250\n",
      "Training Epoch: 14 [47104/50000]\tLoss: 1.1125\tLR: 0.002250\n",
      "Training Epoch: 14 [47232/50000]\tLoss: 1.5867\tLR: 0.002250\n",
      "Training Epoch: 14 [47360/50000]\tLoss: 1.2607\tLR: 0.002250\n",
      "Training Epoch: 14 [47488/50000]\tLoss: 0.9953\tLR: 0.002250\n",
      "Training Epoch: 14 [47616/50000]\tLoss: 1.1520\tLR: 0.002250\n",
      "Training Epoch: 14 [47744/50000]\tLoss: 1.2709\tLR: 0.002250\n",
      "Training Epoch: 14 [47872/50000]\tLoss: 1.1061\tLR: 0.002250\n",
      "Training Epoch: 14 [48000/50000]\tLoss: 0.9712\tLR: 0.002250\n",
      "Training Epoch: 14 [48128/50000]\tLoss: 1.1062\tLR: 0.002250\n",
      "Training Epoch: 14 [48256/50000]\tLoss: 1.1304\tLR: 0.002250\n",
      "Training Epoch: 14 [48384/50000]\tLoss: 1.0383\tLR: 0.002250\n",
      "Training Epoch: 14 [48512/50000]\tLoss: 1.3078\tLR: 0.002250\n",
      "Training Epoch: 14 [48640/50000]\tLoss: 1.0432\tLR: 0.002250\n",
      "Training Epoch: 14 [48768/50000]\tLoss: 0.8842\tLR: 0.002250\n",
      "Training Epoch: 14 [48896/50000]\tLoss: 1.0308\tLR: 0.002250\n",
      "Training Epoch: 14 [49024/50000]\tLoss: 1.0978\tLR: 0.002250\n",
      "Training Epoch: 14 [49152/50000]\tLoss: 1.1962\tLR: 0.002250\n",
      "Training Epoch: 14 [49280/50000]\tLoss: 1.3024\tLR: 0.002250\n",
      "Training Epoch: 14 [49408/50000]\tLoss: 1.3287\tLR: 0.002250\n",
      "Training Epoch: 14 [49536/50000]\tLoss: 1.3721\tLR: 0.002250\n",
      "Training Epoch: 14 [49664/50000]\tLoss: 1.1563\tLR: 0.002250\n",
      "Training Epoch: 14 [49792/50000]\tLoss: 1.3424\tLR: 0.002250\n",
      "Training Epoch: 14 [49920/50000]\tLoss: 1.1396\tLR: 0.002250\n",
      "Training Epoch: 14 [50000/50000]\tLoss: 1.2556\tLR: 0.002250\n",
      "Test set: Average loss: 0.0106, Accuracy: 0.6183\n",
      "\n",
      "Training Epoch: 15 [128/50000]\tLoss: 1.1303\tLR: 0.002250\n",
      "Training Epoch: 15 [256/50000]\tLoss: 1.0573\tLR: 0.002250\n",
      "Training Epoch: 15 [384/50000]\tLoss: 1.1646\tLR: 0.002250\n",
      "Training Epoch: 15 [512/50000]\tLoss: 1.0285\tLR: 0.002250\n",
      "Training Epoch: 15 [640/50000]\tLoss: 1.2490\tLR: 0.002250\n",
      "Training Epoch: 15 [768/50000]\tLoss: 1.2686\tLR: 0.002250\n",
      "Training Epoch: 15 [896/50000]\tLoss: 1.1115\tLR: 0.002250\n",
      "Training Epoch: 15 [1024/50000]\tLoss: 1.3260\tLR: 0.002250\n",
      "Training Epoch: 15 [1152/50000]\tLoss: 1.0606\tLR: 0.002250\n",
      "Training Epoch: 15 [1280/50000]\tLoss: 0.9314\tLR: 0.002250\n",
      "Training Epoch: 15 [1408/50000]\tLoss: 1.1274\tLR: 0.002250\n",
      "Training Epoch: 15 [1536/50000]\tLoss: 1.2435\tLR: 0.002250\n",
      "Training Epoch: 15 [1664/50000]\tLoss: 1.2861\tLR: 0.002250\n",
      "Training Epoch: 15 [1792/50000]\tLoss: 1.1272\tLR: 0.002250\n",
      "Training Epoch: 15 [1920/50000]\tLoss: 1.1845\tLR: 0.002250\n",
      "Training Epoch: 15 [2048/50000]\tLoss: 0.9768\tLR: 0.002250\n",
      "Training Epoch: 15 [2176/50000]\tLoss: 1.1965\tLR: 0.002250\n",
      "Training Epoch: 15 [2304/50000]\tLoss: 0.9961\tLR: 0.002250\n",
      "Training Epoch: 15 [2432/50000]\tLoss: 1.4071\tLR: 0.002250\n",
      "Training Epoch: 15 [2560/50000]\tLoss: 1.3179\tLR: 0.002250\n",
      "Training Epoch: 15 [2688/50000]\tLoss: 1.2404\tLR: 0.002250\n",
      "Training Epoch: 15 [2816/50000]\tLoss: 1.1149\tLR: 0.002250\n",
      "Training Epoch: 15 [2944/50000]\tLoss: 1.2555\tLR: 0.002250\n",
      "Training Epoch: 15 [3072/50000]\tLoss: 1.0579\tLR: 0.002250\n",
      "Training Epoch: 15 [3200/50000]\tLoss: 1.0644\tLR: 0.002250\n",
      "Training Epoch: 15 [3328/50000]\tLoss: 1.1563\tLR: 0.002250\n",
      "Training Epoch: 15 [3456/50000]\tLoss: 1.1220\tLR: 0.002250\n",
      "Training Epoch: 15 [3584/50000]\tLoss: 1.0007\tLR: 0.002250\n",
      "Training Epoch: 15 [3712/50000]\tLoss: 1.1932\tLR: 0.002250\n",
      "Training Epoch: 15 [3840/50000]\tLoss: 1.3810\tLR: 0.002250\n",
      "Training Epoch: 15 [3968/50000]\tLoss: 1.1195\tLR: 0.002250\n",
      "Training Epoch: 15 [4096/50000]\tLoss: 1.1364\tLR: 0.002250\n",
      "Training Epoch: 15 [4224/50000]\tLoss: 0.9392\tLR: 0.002250\n",
      "Training Epoch: 15 [4352/50000]\tLoss: 1.1168\tLR: 0.002250\n",
      "Training Epoch: 15 [4480/50000]\tLoss: 1.3440\tLR: 0.002250\n",
      "Training Epoch: 15 [4608/50000]\tLoss: 0.9321\tLR: 0.002250\n",
      "Training Epoch: 15 [4736/50000]\tLoss: 1.2883\tLR: 0.002250\n",
      "Training Epoch: 15 [4864/50000]\tLoss: 0.9692\tLR: 0.002250\n",
      "Training Epoch: 15 [4992/50000]\tLoss: 1.0898\tLR: 0.002250\n",
      "Training Epoch: 15 [5120/50000]\tLoss: 1.2879\tLR: 0.002250\n",
      "Training Epoch: 15 [5248/50000]\tLoss: 1.1684\tLR: 0.002250\n",
      "Training Epoch: 15 [5376/50000]\tLoss: 1.0743\tLR: 0.002250\n",
      "Training Epoch: 15 [5504/50000]\tLoss: 1.0752\tLR: 0.002250\n",
      "Training Epoch: 15 [5632/50000]\tLoss: 1.0156\tLR: 0.002250\n",
      "Training Epoch: 15 [5760/50000]\tLoss: 1.1930\tLR: 0.002250\n",
      "Training Epoch: 15 [5888/50000]\tLoss: 1.0794\tLR: 0.002250\n",
      "Training Epoch: 15 [6016/50000]\tLoss: 1.2853\tLR: 0.002250\n",
      "Training Epoch: 15 [6144/50000]\tLoss: 0.9451\tLR: 0.002250\n",
      "Training Epoch: 15 [6272/50000]\tLoss: 1.2883\tLR: 0.002250\n",
      "Training Epoch: 15 [6400/50000]\tLoss: 1.3327\tLR: 0.002250\n",
      "Training Epoch: 15 [6528/50000]\tLoss: 1.2449\tLR: 0.002250\n",
      "Training Epoch: 15 [6656/50000]\tLoss: 1.1721\tLR: 0.002250\n",
      "Training Epoch: 15 [6784/50000]\tLoss: 1.1200\tLR: 0.002250\n",
      "Training Epoch: 15 [6912/50000]\tLoss: 1.3438\tLR: 0.002250\n",
      "Training Epoch: 15 [7040/50000]\tLoss: 1.1278\tLR: 0.002250\n",
      "Training Epoch: 15 [7168/50000]\tLoss: 0.8042\tLR: 0.002250\n",
      "Training Epoch: 15 [7296/50000]\tLoss: 1.1531\tLR: 0.002250\n",
      "Training Epoch: 15 [7424/50000]\tLoss: 1.1656\tLR: 0.002250\n",
      "Training Epoch: 15 [7552/50000]\tLoss: 0.9765\tLR: 0.002250\n",
      "Training Epoch: 15 [7680/50000]\tLoss: 1.0441\tLR: 0.002250\n",
      "Training Epoch: 15 [7808/50000]\tLoss: 1.1985\tLR: 0.002250\n",
      "Training Epoch: 15 [7936/50000]\tLoss: 1.5016\tLR: 0.002250\n",
      "Training Epoch: 15 [8064/50000]\tLoss: 0.9677\tLR: 0.002250\n",
      "Training Epoch: 15 [8192/50000]\tLoss: 1.2338\tLR: 0.002250\n",
      "Training Epoch: 15 [8320/50000]\tLoss: 1.1895\tLR: 0.002250\n",
      "Training Epoch: 15 [8448/50000]\tLoss: 1.0946\tLR: 0.002250\n",
      "Training Epoch: 15 [8576/50000]\tLoss: 1.1677\tLR: 0.002250\n",
      "Training Epoch: 15 [8704/50000]\tLoss: 1.3729\tLR: 0.002250\n",
      "Training Epoch: 15 [8832/50000]\tLoss: 1.1774\tLR: 0.002250\n",
      "Training Epoch: 15 [8960/50000]\tLoss: 1.2510\tLR: 0.002250\n",
      "Training Epoch: 15 [9088/50000]\tLoss: 1.0029\tLR: 0.002250\n",
      "Training Epoch: 15 [9216/50000]\tLoss: 1.2009\tLR: 0.002250\n",
      "Training Epoch: 15 [9344/50000]\tLoss: 1.3436\tLR: 0.002250\n",
      "Training Epoch: 15 [9472/50000]\tLoss: 1.2613\tLR: 0.002250\n",
      "Training Epoch: 15 [9600/50000]\tLoss: 1.1290\tLR: 0.002250\n",
      "Training Epoch: 15 [9728/50000]\tLoss: 0.8894\tLR: 0.002250\n",
      "Training Epoch: 15 [9856/50000]\tLoss: 1.1636\tLR: 0.002250\n",
      "Training Epoch: 15 [9984/50000]\tLoss: 1.3428\tLR: 0.002250\n",
      "Training Epoch: 15 [10112/50000]\tLoss: 1.1196\tLR: 0.002250\n",
      "Training Epoch: 15 [10240/50000]\tLoss: 1.2070\tLR: 0.002250\n",
      "Training Epoch: 15 [10368/50000]\tLoss: 1.3117\tLR: 0.002250\n",
      "Training Epoch: 15 [10496/50000]\tLoss: 1.2455\tLR: 0.002250\n",
      "Training Epoch: 15 [10624/50000]\tLoss: 1.2034\tLR: 0.002250\n",
      "Training Epoch: 15 [10752/50000]\tLoss: 1.0777\tLR: 0.002250\n",
      "Training Epoch: 15 [10880/50000]\tLoss: 1.3094\tLR: 0.002250\n",
      "Training Epoch: 15 [11008/50000]\tLoss: 1.1512\tLR: 0.002250\n",
      "Training Epoch: 15 [11136/50000]\tLoss: 1.0254\tLR: 0.002250\n",
      "Training Epoch: 15 [11264/50000]\tLoss: 1.1146\tLR: 0.002250\n",
      "Training Epoch: 15 [11392/50000]\tLoss: 1.0306\tLR: 0.002250\n",
      "Training Epoch: 15 [11520/50000]\tLoss: 1.3841\tLR: 0.002250\n",
      "Training Epoch: 15 [11648/50000]\tLoss: 1.2992\tLR: 0.002250\n",
      "Training Epoch: 15 [11776/50000]\tLoss: 1.1948\tLR: 0.002250\n",
      "Training Epoch: 15 [11904/50000]\tLoss: 1.2486\tLR: 0.002250\n",
      "Training Epoch: 15 [12032/50000]\tLoss: 1.1395\tLR: 0.002250\n",
      "Training Epoch: 15 [12160/50000]\tLoss: 1.1252\tLR: 0.002250\n",
      "Training Epoch: 15 [12288/50000]\tLoss: 1.0710\tLR: 0.002250\n",
      "Training Epoch: 15 [12416/50000]\tLoss: 1.1502\tLR: 0.002250\n",
      "Training Epoch: 15 [12544/50000]\tLoss: 1.1168\tLR: 0.002250\n",
      "Training Epoch: 15 [12672/50000]\tLoss: 1.1932\tLR: 0.002250\n",
      "Training Epoch: 15 [12800/50000]\tLoss: 0.9737\tLR: 0.002250\n",
      "Training Epoch: 15 [12928/50000]\tLoss: 1.3528\tLR: 0.002250\n",
      "Training Epoch: 15 [13056/50000]\tLoss: 1.2906\tLR: 0.002250\n",
      "Training Epoch: 15 [13184/50000]\tLoss: 1.3659\tLR: 0.002250\n",
      "Training Epoch: 15 [13312/50000]\tLoss: 1.1744\tLR: 0.002250\n",
      "Training Epoch: 15 [13440/50000]\tLoss: 0.9968\tLR: 0.002250\n",
      "Training Epoch: 15 [13568/50000]\tLoss: 0.9856\tLR: 0.002250\n",
      "Training Epoch: 15 [13696/50000]\tLoss: 1.2859\tLR: 0.002250\n",
      "Training Epoch: 15 [13824/50000]\tLoss: 1.2953\tLR: 0.002250\n",
      "Training Epoch: 15 [13952/50000]\tLoss: 1.1315\tLR: 0.002250\n",
      "Training Epoch: 15 [14080/50000]\tLoss: 1.2635\tLR: 0.002250\n",
      "Training Epoch: 15 [14208/50000]\tLoss: 1.0398\tLR: 0.002250\n",
      "Training Epoch: 15 [14336/50000]\tLoss: 1.1210\tLR: 0.002250\n",
      "Training Epoch: 15 [14464/50000]\tLoss: 1.2020\tLR: 0.002250\n",
      "Training Epoch: 15 [14592/50000]\tLoss: 1.1042\tLR: 0.002250\n",
      "Training Epoch: 15 [14720/50000]\tLoss: 1.0307\tLR: 0.002250\n",
      "Training Epoch: 15 [14848/50000]\tLoss: 1.0860\tLR: 0.002250\n",
      "Training Epoch: 15 [14976/50000]\tLoss: 1.0356\tLR: 0.002250\n",
      "Training Epoch: 15 [15104/50000]\tLoss: 1.0964\tLR: 0.002250\n",
      "Training Epoch: 15 [15232/50000]\tLoss: 1.0669\tLR: 0.002250\n",
      "Training Epoch: 15 [15360/50000]\tLoss: 1.0892\tLR: 0.002250\n",
      "Training Epoch: 15 [15488/50000]\tLoss: 1.1727\tLR: 0.002250\n",
      "Training Epoch: 15 [15616/50000]\tLoss: 1.2660\tLR: 0.002250\n",
      "Training Epoch: 15 [15744/50000]\tLoss: 1.1030\tLR: 0.002250\n",
      "Training Epoch: 15 [15872/50000]\tLoss: 1.0635\tLR: 0.002250\n",
      "Training Epoch: 15 [16000/50000]\tLoss: 1.2865\tLR: 0.002250\n",
      "Training Epoch: 15 [16128/50000]\tLoss: 1.2826\tLR: 0.002250\n",
      "Training Epoch: 15 [16256/50000]\tLoss: 1.1211\tLR: 0.002250\n",
      "Training Epoch: 15 [16384/50000]\tLoss: 1.1691\tLR: 0.002250\n",
      "Training Epoch: 15 [16512/50000]\tLoss: 1.1524\tLR: 0.002250\n",
      "Training Epoch: 15 [16640/50000]\tLoss: 1.3677\tLR: 0.002250\n",
      "Training Epoch: 15 [16768/50000]\tLoss: 1.0820\tLR: 0.002250\n",
      "Training Epoch: 15 [16896/50000]\tLoss: 1.2810\tLR: 0.002250\n",
      "Training Epoch: 15 [17024/50000]\tLoss: 1.0860\tLR: 0.002250\n",
      "Training Epoch: 15 [17152/50000]\tLoss: 1.1444\tLR: 0.002250\n",
      "Training Epoch: 15 [17280/50000]\tLoss: 1.1387\tLR: 0.002250\n",
      "Training Epoch: 15 [17408/50000]\tLoss: 1.2931\tLR: 0.002250\n",
      "Training Epoch: 15 [17536/50000]\tLoss: 1.2080\tLR: 0.002250\n",
      "Training Epoch: 15 [17664/50000]\tLoss: 1.1289\tLR: 0.002250\n",
      "Training Epoch: 15 [17792/50000]\tLoss: 1.1031\tLR: 0.002250\n",
      "Training Epoch: 15 [17920/50000]\tLoss: 1.0919\tLR: 0.002250\n",
      "Training Epoch: 15 [18048/50000]\tLoss: 1.2620\tLR: 0.002250\n",
      "Training Epoch: 15 [18176/50000]\tLoss: 1.1046\tLR: 0.002250\n",
      "Training Epoch: 15 [18304/50000]\tLoss: 1.2332\tLR: 0.002250\n",
      "Training Epoch: 15 [18432/50000]\tLoss: 1.1285\tLR: 0.002250\n",
      "Training Epoch: 15 [18560/50000]\tLoss: 1.1440\tLR: 0.002250\n",
      "Training Epoch: 15 [18688/50000]\tLoss: 1.2993\tLR: 0.002250\n",
      "Training Epoch: 15 [18816/50000]\tLoss: 1.0055\tLR: 0.002250\n",
      "Training Epoch: 15 [18944/50000]\tLoss: 1.2004\tLR: 0.002250\n",
      "Training Epoch: 15 [19072/50000]\tLoss: 1.2249\tLR: 0.002250\n",
      "Training Epoch: 15 [19200/50000]\tLoss: 1.1768\tLR: 0.002250\n",
      "Training Epoch: 15 [19328/50000]\tLoss: 1.1303\tLR: 0.002250\n",
      "Training Epoch: 15 [19456/50000]\tLoss: 1.1700\tLR: 0.002250\n",
      "Training Epoch: 15 [19584/50000]\tLoss: 1.1775\tLR: 0.002250\n",
      "Training Epoch: 15 [19712/50000]\tLoss: 1.2132\tLR: 0.002250\n",
      "Training Epoch: 15 [19840/50000]\tLoss: 1.0238\tLR: 0.002250\n",
      "Training Epoch: 15 [19968/50000]\tLoss: 1.0749\tLR: 0.002250\n",
      "Training Epoch: 15 [20096/50000]\tLoss: 1.3285\tLR: 0.002250\n",
      "Training Epoch: 15 [20224/50000]\tLoss: 1.0814\tLR: 0.002250\n",
      "Training Epoch: 15 [20352/50000]\tLoss: 1.2576\tLR: 0.002250\n",
      "Training Epoch: 15 [20480/50000]\tLoss: 1.1976\tLR: 0.002250\n",
      "Training Epoch: 15 [20608/50000]\tLoss: 0.9995\tLR: 0.002250\n",
      "Training Epoch: 15 [20736/50000]\tLoss: 1.2744\tLR: 0.002250\n",
      "Training Epoch: 15 [20864/50000]\tLoss: 1.1567\tLR: 0.002250\n",
      "Training Epoch: 15 [20992/50000]\tLoss: 1.1848\tLR: 0.002250\n",
      "Training Epoch: 15 [21120/50000]\tLoss: 0.9603\tLR: 0.002250\n",
      "Training Epoch: 15 [21248/50000]\tLoss: 1.2112\tLR: 0.002250\n",
      "Training Epoch: 15 [21376/50000]\tLoss: 1.1364\tLR: 0.002250\n",
      "Training Epoch: 15 [21504/50000]\tLoss: 0.9769\tLR: 0.002250\n",
      "Training Epoch: 15 [21632/50000]\tLoss: 1.2596\tLR: 0.002250\n",
      "Training Epoch: 15 [21760/50000]\tLoss: 1.1000\tLR: 0.002250\n",
      "Training Epoch: 15 [21888/50000]\tLoss: 1.4475\tLR: 0.002250\n",
      "Training Epoch: 15 [22016/50000]\tLoss: 1.3498\tLR: 0.002250\n",
      "Training Epoch: 15 [22144/50000]\tLoss: 1.3102\tLR: 0.002250\n",
      "Training Epoch: 15 [22272/50000]\tLoss: 1.0755\tLR: 0.002250\n",
      "Training Epoch: 15 [22400/50000]\tLoss: 1.1132\tLR: 0.002250\n",
      "Training Epoch: 15 [22528/50000]\tLoss: 1.0548\tLR: 0.002250\n",
      "Training Epoch: 15 [22656/50000]\tLoss: 1.3118\tLR: 0.002250\n",
      "Training Epoch: 15 [22784/50000]\tLoss: 1.2288\tLR: 0.002250\n",
      "Training Epoch: 15 [22912/50000]\tLoss: 1.1580\tLR: 0.002250\n",
      "Training Epoch: 15 [23040/50000]\tLoss: 0.7209\tLR: 0.002250\n",
      "Training Epoch: 15 [23168/50000]\tLoss: 1.0634\tLR: 0.002250\n",
      "Training Epoch: 15 [23296/50000]\tLoss: 1.1172\tLR: 0.002250\n",
      "Training Epoch: 15 [23424/50000]\tLoss: 1.3295\tLR: 0.002250\n",
      "Training Epoch: 15 [23552/50000]\tLoss: 1.3042\tLR: 0.002250\n",
      "Training Epoch: 15 [23680/50000]\tLoss: 1.1748\tLR: 0.002250\n",
      "Training Epoch: 15 [23808/50000]\tLoss: 1.2184\tLR: 0.002250\n",
      "Training Epoch: 15 [23936/50000]\tLoss: 1.2359\tLR: 0.002250\n",
      "Training Epoch: 15 [24064/50000]\tLoss: 1.1752\tLR: 0.002250\n",
      "Training Epoch: 15 [24192/50000]\tLoss: 1.2983\tLR: 0.002250\n",
      "Training Epoch: 15 [24320/50000]\tLoss: 1.2411\tLR: 0.002250\n",
      "Training Epoch: 15 [24448/50000]\tLoss: 1.2545\tLR: 0.002250\n",
      "Training Epoch: 15 [24576/50000]\tLoss: 1.2014\tLR: 0.002250\n",
      "Training Epoch: 15 [24704/50000]\tLoss: 1.1350\tLR: 0.002250\n",
      "Training Epoch: 15 [24832/50000]\tLoss: 1.1581\tLR: 0.002250\n",
      "Training Epoch: 15 [24960/50000]\tLoss: 1.0592\tLR: 0.002250\n",
      "Training Epoch: 15 [25088/50000]\tLoss: 1.1786\tLR: 0.002250\n",
      "Training Epoch: 15 [25216/50000]\tLoss: 1.1707\tLR: 0.002250\n",
      "Training Epoch: 15 [25344/50000]\tLoss: 1.2109\tLR: 0.002250\n",
      "Training Epoch: 15 [25472/50000]\tLoss: 1.0765\tLR: 0.002250\n",
      "Training Epoch: 15 [25600/50000]\tLoss: 1.3538\tLR: 0.002250\n",
      "Training Epoch: 15 [25728/50000]\tLoss: 1.1033\tLR: 0.002250\n",
      "Training Epoch: 15 [25856/50000]\tLoss: 0.9834\tLR: 0.002250\n",
      "Training Epoch: 15 [25984/50000]\tLoss: 1.1453\tLR: 0.002250\n",
      "Training Epoch: 15 [26112/50000]\tLoss: 1.2079\tLR: 0.002250\n",
      "Training Epoch: 15 [26240/50000]\tLoss: 1.2505\tLR: 0.002250\n",
      "Training Epoch: 15 [26368/50000]\tLoss: 1.1148\tLR: 0.002250\n",
      "Training Epoch: 15 [26496/50000]\tLoss: 1.0669\tLR: 0.002250\n",
      "Training Epoch: 15 [26624/50000]\tLoss: 1.3356\tLR: 0.002250\n",
      "Training Epoch: 15 [26752/50000]\tLoss: 1.0020\tLR: 0.002250\n",
      "Training Epoch: 15 [26880/50000]\tLoss: 1.2147\tLR: 0.002250\n",
      "Training Epoch: 15 [27008/50000]\tLoss: 1.1404\tLR: 0.002250\n",
      "Training Epoch: 15 [27136/50000]\tLoss: 0.9098\tLR: 0.002250\n",
      "Training Epoch: 15 [27264/50000]\tLoss: 1.1703\tLR: 0.002250\n",
      "Training Epoch: 15 [27392/50000]\tLoss: 1.2373\tLR: 0.002250\n",
      "Training Epoch: 15 [27520/50000]\tLoss: 1.2104\tLR: 0.002250\n",
      "Training Epoch: 15 [27648/50000]\tLoss: 1.1115\tLR: 0.002250\n",
      "Training Epoch: 15 [27776/50000]\tLoss: 1.2074\tLR: 0.002250\n",
      "Training Epoch: 15 [27904/50000]\tLoss: 1.3815\tLR: 0.002250\n",
      "Training Epoch: 15 [28032/50000]\tLoss: 1.2635\tLR: 0.002250\n",
      "Training Epoch: 15 [28160/50000]\tLoss: 1.2378\tLR: 0.002250\n",
      "Training Epoch: 15 [28288/50000]\tLoss: 1.1640\tLR: 0.002250\n",
      "Training Epoch: 15 [28416/50000]\tLoss: 1.0449\tLR: 0.002250\n",
      "Training Epoch: 15 [28544/50000]\tLoss: 1.0789\tLR: 0.002250\n",
      "Training Epoch: 15 [28672/50000]\tLoss: 1.0311\tLR: 0.002250\n",
      "Training Epoch: 15 [28800/50000]\tLoss: 1.1628\tLR: 0.002250\n",
      "Training Epoch: 15 [28928/50000]\tLoss: 1.2751\tLR: 0.002250\n",
      "Training Epoch: 15 [29056/50000]\tLoss: 1.0386\tLR: 0.002250\n",
      "Training Epoch: 15 [29184/50000]\tLoss: 1.0661\tLR: 0.002250\n",
      "Training Epoch: 15 [29312/50000]\tLoss: 0.9966\tLR: 0.002250\n",
      "Training Epoch: 15 [29440/50000]\tLoss: 1.2995\tLR: 0.002250\n",
      "Training Epoch: 15 [29568/50000]\tLoss: 1.3054\tLR: 0.002250\n",
      "Training Epoch: 15 [29696/50000]\tLoss: 1.0797\tLR: 0.002250\n",
      "Training Epoch: 15 [29824/50000]\tLoss: 1.1829\tLR: 0.002250\n",
      "Training Epoch: 15 [29952/50000]\tLoss: 1.1814\tLR: 0.002250\n",
      "Training Epoch: 15 [30080/50000]\tLoss: 1.1702\tLR: 0.002250\n",
      "Training Epoch: 15 [30208/50000]\tLoss: 1.1350\tLR: 0.002250\n",
      "Training Epoch: 15 [30336/50000]\tLoss: 1.2464\tLR: 0.002250\n",
      "Training Epoch: 15 [30464/50000]\tLoss: 1.2709\tLR: 0.002250\n",
      "Training Epoch: 15 [30592/50000]\tLoss: 1.1295\tLR: 0.002250\n",
      "Training Epoch: 15 [30720/50000]\tLoss: 1.1801\tLR: 0.002250\n",
      "Training Epoch: 15 [30848/50000]\tLoss: 1.1195\tLR: 0.002250\n",
      "Training Epoch: 15 [30976/50000]\tLoss: 1.1424\tLR: 0.002250\n",
      "Training Epoch: 15 [31104/50000]\tLoss: 1.0640\tLR: 0.002250\n",
      "Training Epoch: 15 [31232/50000]\tLoss: 1.0036\tLR: 0.002250\n",
      "Training Epoch: 15 [31360/50000]\tLoss: 1.1297\tLR: 0.002250\n",
      "Training Epoch: 15 [31488/50000]\tLoss: 1.3409\tLR: 0.002250\n",
      "Training Epoch: 15 [31616/50000]\tLoss: 1.1232\tLR: 0.002250\n",
      "Training Epoch: 15 [31744/50000]\tLoss: 1.2598\tLR: 0.002250\n",
      "Training Epoch: 15 [31872/50000]\tLoss: 1.1760\tLR: 0.002250\n",
      "Training Epoch: 15 [32000/50000]\tLoss: 1.0447\tLR: 0.002250\n",
      "Training Epoch: 15 [32128/50000]\tLoss: 1.1551\tLR: 0.002250\n",
      "Training Epoch: 15 [32256/50000]\tLoss: 1.0221\tLR: 0.002250\n",
      "Training Epoch: 15 [32384/50000]\tLoss: 1.2824\tLR: 0.002250\n",
      "Training Epoch: 15 [32512/50000]\tLoss: 1.0153\tLR: 0.002250\n",
      "Training Epoch: 15 [32640/50000]\tLoss: 1.1348\tLR: 0.002250\n",
      "Training Epoch: 15 [32768/50000]\tLoss: 1.2187\tLR: 0.002250\n",
      "Training Epoch: 15 [32896/50000]\tLoss: 1.0646\tLR: 0.002250\n",
      "Training Epoch: 15 [33024/50000]\tLoss: 1.2165\tLR: 0.002250\n",
      "Training Epoch: 15 [33152/50000]\tLoss: 1.0826\tLR: 0.002250\n",
      "Training Epoch: 15 [33280/50000]\tLoss: 1.0528\tLR: 0.002250\n",
      "Training Epoch: 15 [33408/50000]\tLoss: 1.1968\tLR: 0.002250\n",
      "Training Epoch: 15 [33536/50000]\tLoss: 1.1229\tLR: 0.002250\n",
      "Training Epoch: 15 [33664/50000]\tLoss: 0.9570\tLR: 0.002250\n",
      "Training Epoch: 15 [33792/50000]\tLoss: 1.0156\tLR: 0.002250\n",
      "Training Epoch: 15 [33920/50000]\tLoss: 1.0921\tLR: 0.002250\n",
      "Training Epoch: 15 [34048/50000]\tLoss: 1.0518\tLR: 0.002250\n",
      "Training Epoch: 15 [34176/50000]\tLoss: 1.2543\tLR: 0.002250\n",
      "Training Epoch: 15 [34304/50000]\tLoss: 1.0160\tLR: 0.002250\n",
      "Training Epoch: 15 [34432/50000]\tLoss: 1.1818\tLR: 0.002250\n",
      "Training Epoch: 15 [34560/50000]\tLoss: 1.0812\tLR: 0.002250\n",
      "Training Epoch: 15 [34688/50000]\tLoss: 1.2894\tLR: 0.002250\n",
      "Training Epoch: 15 [34816/50000]\tLoss: 1.1780\tLR: 0.002250\n",
      "Training Epoch: 15 [34944/50000]\tLoss: 1.3807\tLR: 0.002250\n",
      "Training Epoch: 15 [35072/50000]\tLoss: 0.9760\tLR: 0.002250\n",
      "Training Epoch: 15 [35200/50000]\tLoss: 1.2884\tLR: 0.002250\n",
      "Training Epoch: 15 [35328/50000]\tLoss: 1.3343\tLR: 0.002250\n",
      "Training Epoch: 15 [35456/50000]\tLoss: 1.3612\tLR: 0.002250\n",
      "Training Epoch: 15 [35584/50000]\tLoss: 1.2211\tLR: 0.002250\n",
      "Training Epoch: 15 [35712/50000]\tLoss: 0.9516\tLR: 0.002250\n",
      "Training Epoch: 15 [35840/50000]\tLoss: 1.4032\tLR: 0.002250\n",
      "Training Epoch: 15 [35968/50000]\tLoss: 1.2503\tLR: 0.002250\n",
      "Training Epoch: 15 [36096/50000]\tLoss: 1.2058\tLR: 0.002250\n",
      "Training Epoch: 15 [36224/50000]\tLoss: 1.1286\tLR: 0.002250\n",
      "Training Epoch: 15 [36352/50000]\tLoss: 1.0678\tLR: 0.002250\n",
      "Training Epoch: 15 [36480/50000]\tLoss: 0.9978\tLR: 0.002250\n",
      "Training Epoch: 15 [36608/50000]\tLoss: 1.2797\tLR: 0.002250\n",
      "Training Epoch: 15 [36736/50000]\tLoss: 1.1052\tLR: 0.002250\n",
      "Training Epoch: 15 [36864/50000]\tLoss: 1.0923\tLR: 0.002250\n",
      "Training Epoch: 15 [36992/50000]\tLoss: 1.1788\tLR: 0.002250\n",
      "Training Epoch: 15 [37120/50000]\tLoss: 1.0965\tLR: 0.002250\n",
      "Training Epoch: 15 [37248/50000]\tLoss: 1.3805\tLR: 0.002250\n",
      "Training Epoch: 15 [37376/50000]\tLoss: 1.2798\tLR: 0.002250\n",
      "Training Epoch: 15 [37504/50000]\tLoss: 1.1301\tLR: 0.002250\n",
      "Training Epoch: 15 [37632/50000]\tLoss: 1.0044\tLR: 0.002250\n",
      "Training Epoch: 15 [37760/50000]\tLoss: 1.3328\tLR: 0.002250\n",
      "Training Epoch: 15 [37888/50000]\tLoss: 1.2494\tLR: 0.002250\n",
      "Training Epoch: 15 [38016/50000]\tLoss: 1.1479\tLR: 0.002250\n",
      "Training Epoch: 15 [38144/50000]\tLoss: 1.3314\tLR: 0.002250\n",
      "Training Epoch: 15 [38272/50000]\tLoss: 1.1301\tLR: 0.002250\n",
      "Training Epoch: 15 [38400/50000]\tLoss: 1.2226\tLR: 0.002250\n",
      "Training Epoch: 15 [38528/50000]\tLoss: 1.0838\tLR: 0.002250\n",
      "Training Epoch: 15 [38656/50000]\tLoss: 1.2884\tLR: 0.002250\n",
      "Training Epoch: 15 [38784/50000]\tLoss: 1.4107\tLR: 0.002250\n",
      "Training Epoch: 15 [38912/50000]\tLoss: 1.2910\tLR: 0.002250\n",
      "Training Epoch: 15 [39040/50000]\tLoss: 1.1706\tLR: 0.002250\n",
      "Training Epoch: 15 [39168/50000]\tLoss: 1.3737\tLR: 0.002250\n",
      "Training Epoch: 15 [39296/50000]\tLoss: 1.1623\tLR: 0.002250\n",
      "Training Epoch: 15 [39424/50000]\tLoss: 0.8308\tLR: 0.002250\n",
      "Training Epoch: 15 [39552/50000]\tLoss: 1.0667\tLR: 0.002250\n",
      "Training Epoch: 15 [39680/50000]\tLoss: 1.0488\tLR: 0.002250\n",
      "Training Epoch: 15 [39808/50000]\tLoss: 1.3894\tLR: 0.002250\n",
      "Training Epoch: 15 [39936/50000]\tLoss: 1.1410\tLR: 0.002250\n",
      "Training Epoch: 15 [40064/50000]\tLoss: 0.9994\tLR: 0.002250\n",
      "Training Epoch: 15 [40192/50000]\tLoss: 1.1972\tLR: 0.002250\n",
      "Training Epoch: 15 [40320/50000]\tLoss: 1.2313\tLR: 0.002250\n",
      "Training Epoch: 15 [40448/50000]\tLoss: 1.1992\tLR: 0.002250\n",
      "Training Epoch: 15 [40576/50000]\tLoss: 1.1552\tLR: 0.002250\n",
      "Training Epoch: 15 [40704/50000]\tLoss: 1.2466\tLR: 0.002250\n",
      "Training Epoch: 15 [40832/50000]\tLoss: 1.3276\tLR: 0.002250\n",
      "Training Epoch: 15 [40960/50000]\tLoss: 1.1599\tLR: 0.002250\n",
      "Training Epoch: 15 [41088/50000]\tLoss: 1.2512\tLR: 0.002250\n",
      "Training Epoch: 15 [41216/50000]\tLoss: 1.1624\tLR: 0.002250\n",
      "Training Epoch: 15 [41344/50000]\tLoss: 1.0005\tLR: 0.002250\n",
      "Training Epoch: 15 [41472/50000]\tLoss: 1.0909\tLR: 0.002250\n",
      "Training Epoch: 15 [41600/50000]\tLoss: 1.1300\tLR: 0.002250\n",
      "Training Epoch: 15 [41728/50000]\tLoss: 1.5018\tLR: 0.002250\n",
      "Training Epoch: 15 [41856/50000]\tLoss: 1.1185\tLR: 0.002250\n",
      "Training Epoch: 15 [41984/50000]\tLoss: 1.0038\tLR: 0.002250\n",
      "Training Epoch: 15 [42112/50000]\tLoss: 1.1429\tLR: 0.002250\n",
      "Training Epoch: 15 [42240/50000]\tLoss: 1.0750\tLR: 0.002250\n",
      "Training Epoch: 15 [42368/50000]\tLoss: 1.2349\tLR: 0.002250\n",
      "Training Epoch: 15 [42496/50000]\tLoss: 1.1048\tLR: 0.002250\n",
      "Training Epoch: 15 [42624/50000]\tLoss: 1.3139\tLR: 0.002250\n",
      "Training Epoch: 15 [42752/50000]\tLoss: 1.1546\tLR: 0.002250\n",
      "Training Epoch: 15 [42880/50000]\tLoss: 1.1895\tLR: 0.002250\n",
      "Training Epoch: 15 [43008/50000]\tLoss: 1.3686\tLR: 0.002250\n",
      "Training Epoch: 15 [43136/50000]\tLoss: 0.9905\tLR: 0.002250\n",
      "Training Epoch: 15 [43264/50000]\tLoss: 1.1960\tLR: 0.002250\n",
      "Training Epoch: 15 [43392/50000]\tLoss: 1.2931\tLR: 0.002250\n",
      "Training Epoch: 15 [43520/50000]\tLoss: 1.2229\tLR: 0.002250\n",
      "Training Epoch: 15 [43648/50000]\tLoss: 1.0759\tLR: 0.002250\n",
      "Training Epoch: 15 [43776/50000]\tLoss: 1.2302\tLR: 0.002250\n",
      "Training Epoch: 15 [43904/50000]\tLoss: 1.1578\tLR: 0.002250\n",
      "Training Epoch: 15 [44032/50000]\tLoss: 1.1198\tLR: 0.002250\n",
      "Training Epoch: 15 [44160/50000]\tLoss: 1.3207\tLR: 0.002250\n",
      "Training Epoch: 15 [44288/50000]\tLoss: 1.1513\tLR: 0.002250\n",
      "Training Epoch: 15 [44416/50000]\tLoss: 1.1657\tLR: 0.002250\n",
      "Training Epoch: 15 [44544/50000]\tLoss: 1.2817\tLR: 0.002250\n",
      "Training Epoch: 15 [44672/50000]\tLoss: 1.1550\tLR: 0.002250\n",
      "Training Epoch: 15 [44800/50000]\tLoss: 1.3339\tLR: 0.002250\n",
      "Training Epoch: 15 [44928/50000]\tLoss: 0.9438\tLR: 0.002250\n",
      "Training Epoch: 15 [45056/50000]\tLoss: 1.0014\tLR: 0.002250\n",
      "Training Epoch: 15 [45184/50000]\tLoss: 1.1799\tLR: 0.002250\n",
      "Training Epoch: 15 [45312/50000]\tLoss: 1.1876\tLR: 0.002250\n",
      "Training Epoch: 15 [45440/50000]\tLoss: 1.1708\tLR: 0.002250\n",
      "Training Epoch: 15 [45568/50000]\tLoss: 1.0625\tLR: 0.002250\n",
      "Training Epoch: 15 [45696/50000]\tLoss: 1.0379\tLR: 0.002250\n",
      "Training Epoch: 15 [45824/50000]\tLoss: 1.0863\tLR: 0.002250\n",
      "Training Epoch: 15 [45952/50000]\tLoss: 1.0352\tLR: 0.002250\n",
      "Training Epoch: 15 [46080/50000]\tLoss: 1.3256\tLR: 0.002250\n",
      "Training Epoch: 15 [46208/50000]\tLoss: 1.0568\tLR: 0.002250\n",
      "Training Epoch: 15 [46336/50000]\tLoss: 1.2106\tLR: 0.002250\n",
      "Training Epoch: 15 [46464/50000]\tLoss: 1.1140\tLR: 0.002250\n",
      "Training Epoch: 15 [46592/50000]\tLoss: 1.0809\tLR: 0.002250\n",
      "Training Epoch: 15 [46720/50000]\tLoss: 1.1466\tLR: 0.002250\n",
      "Training Epoch: 15 [46848/50000]\tLoss: 0.8928\tLR: 0.002250\n",
      "Training Epoch: 15 [46976/50000]\tLoss: 1.3451\tLR: 0.002250\n",
      "Training Epoch: 15 [47104/50000]\tLoss: 1.1786\tLR: 0.002250\n",
      "Training Epoch: 15 [47232/50000]\tLoss: 1.0960\tLR: 0.002250\n",
      "Training Epoch: 15 [47360/50000]\tLoss: 1.3973\tLR: 0.002250\n",
      "Training Epoch: 15 [47488/50000]\tLoss: 1.3601\tLR: 0.002250\n",
      "Training Epoch: 15 [47616/50000]\tLoss: 1.1484\tLR: 0.002250\n",
      "Training Epoch: 15 [47744/50000]\tLoss: 1.2081\tLR: 0.002250\n",
      "Training Epoch: 15 [47872/50000]\tLoss: 1.0600\tLR: 0.002250\n",
      "Training Epoch: 15 [48000/50000]\tLoss: 1.0986\tLR: 0.002250\n",
      "Training Epoch: 15 [48128/50000]\tLoss: 1.0968\tLR: 0.002250\n",
      "Training Epoch: 15 [48256/50000]\tLoss: 0.9975\tLR: 0.002250\n",
      "Training Epoch: 15 [48384/50000]\tLoss: 1.0634\tLR: 0.002250\n",
      "Training Epoch: 15 [48512/50000]\tLoss: 1.1391\tLR: 0.002250\n",
      "Training Epoch: 15 [48640/50000]\tLoss: 1.0592\tLR: 0.002250\n",
      "Training Epoch: 15 [48768/50000]\tLoss: 1.0521\tLR: 0.002250\n",
      "Training Epoch: 15 [48896/50000]\tLoss: 1.2088\tLR: 0.002250\n",
      "Training Epoch: 15 [49024/50000]\tLoss: 1.1211\tLR: 0.002250\n",
      "Training Epoch: 15 [49152/50000]\tLoss: 1.1042\tLR: 0.002250\n",
      "Training Epoch: 15 [49280/50000]\tLoss: 1.1688\tLR: 0.002250\n",
      "Training Epoch: 15 [49408/50000]\tLoss: 1.0159\tLR: 0.002250\n",
      "Training Epoch: 15 [49536/50000]\tLoss: 1.0209\tLR: 0.002250\n",
      "Training Epoch: 15 [49664/50000]\tLoss: 0.9947\tLR: 0.002250\n",
      "Training Epoch: 15 [49792/50000]\tLoss: 1.1982\tLR: 0.002250\n",
      "Training Epoch: 15 [49920/50000]\tLoss: 1.1174\tLR: 0.002250\n",
      "Training Epoch: 15 [50000/50000]\tLoss: 1.3537\tLR: 0.002250\n",
      "Test set: Average loss: 0.0105, Accuracy: 0.6212\n",
      "\n",
      "Training Epoch: 16 [128/50000]\tLoss: 1.0033\tLR: 0.000337\n",
      "Training Epoch: 16 [256/50000]\tLoss: 1.0327\tLR: 0.000337\n",
      "Training Epoch: 16 [384/50000]\tLoss: 1.1560\tLR: 0.000337\n",
      "Training Epoch: 16 [512/50000]\tLoss: 1.2742\tLR: 0.000337\n",
      "Training Epoch: 16 [640/50000]\tLoss: 0.9618\tLR: 0.000337\n",
      "Training Epoch: 16 [768/50000]\tLoss: 1.0224\tLR: 0.000337\n",
      "Training Epoch: 16 [896/50000]\tLoss: 1.2177\tLR: 0.000337\n",
      "Training Epoch: 16 [1024/50000]\tLoss: 1.1119\tLR: 0.000337\n",
      "Training Epoch: 16 [1152/50000]\tLoss: 1.2196\tLR: 0.000337\n",
      "Training Epoch: 16 [1280/50000]\tLoss: 1.3055\tLR: 0.000337\n",
      "Training Epoch: 16 [1408/50000]\tLoss: 1.1114\tLR: 0.000337\n",
      "Training Epoch: 16 [1536/50000]\tLoss: 1.2569\tLR: 0.000337\n",
      "Training Epoch: 16 [1664/50000]\tLoss: 1.0435\tLR: 0.000337\n",
      "Training Epoch: 16 [1792/50000]\tLoss: 1.1126\tLR: 0.000337\n",
      "Training Epoch: 16 [1920/50000]\tLoss: 1.2022\tLR: 0.000337\n",
      "Training Epoch: 16 [2048/50000]\tLoss: 1.1140\tLR: 0.000337\n",
      "Training Epoch: 16 [2176/50000]\tLoss: 1.1569\tLR: 0.000337\n",
      "Training Epoch: 16 [2304/50000]\tLoss: 0.8904\tLR: 0.000337\n",
      "Training Epoch: 16 [2432/50000]\tLoss: 1.2090\tLR: 0.000337\n",
      "Training Epoch: 16 [2560/50000]\tLoss: 1.2366\tLR: 0.000337\n",
      "Training Epoch: 16 [2688/50000]\tLoss: 1.3087\tLR: 0.000337\n",
      "Training Epoch: 16 [2816/50000]\tLoss: 1.0460\tLR: 0.000337\n",
      "Training Epoch: 16 [2944/50000]\tLoss: 1.2357\tLR: 0.000337\n",
      "Training Epoch: 16 [3072/50000]\tLoss: 0.9795\tLR: 0.000337\n",
      "Training Epoch: 16 [3200/50000]\tLoss: 1.0123\tLR: 0.000337\n",
      "Training Epoch: 16 [3328/50000]\tLoss: 1.1149\tLR: 0.000337\n",
      "Training Epoch: 16 [3456/50000]\tLoss: 1.2022\tLR: 0.000337\n",
      "Training Epoch: 16 [3584/50000]\tLoss: 1.2114\tLR: 0.000337\n",
      "Training Epoch: 16 [3712/50000]\tLoss: 1.0063\tLR: 0.000337\n",
      "Training Epoch: 16 [3840/50000]\tLoss: 0.8364\tLR: 0.000337\n",
      "Training Epoch: 16 [3968/50000]\tLoss: 1.1139\tLR: 0.000337\n",
      "Training Epoch: 16 [4096/50000]\tLoss: 1.1133\tLR: 0.000337\n",
      "Training Epoch: 16 [4224/50000]\tLoss: 1.0489\tLR: 0.000337\n",
      "Training Epoch: 16 [4352/50000]\tLoss: 1.1519\tLR: 0.000337\n",
      "Training Epoch: 16 [4480/50000]\tLoss: 1.2642\tLR: 0.000337\n",
      "Training Epoch: 16 [4608/50000]\tLoss: 1.1443\tLR: 0.000337\n",
      "Training Epoch: 16 [4736/50000]\tLoss: 1.1138\tLR: 0.000337\n",
      "Training Epoch: 16 [4864/50000]\tLoss: 1.0756\tLR: 0.000337\n",
      "Training Epoch: 16 [4992/50000]\tLoss: 0.9435\tLR: 0.000337\n",
      "Training Epoch: 16 [5120/50000]\tLoss: 1.3185\tLR: 0.000337\n",
      "Training Epoch: 16 [5248/50000]\tLoss: 0.8534\tLR: 0.000337\n",
      "Training Epoch: 16 [5376/50000]\tLoss: 1.0743\tLR: 0.000337\n",
      "Training Epoch: 16 [5504/50000]\tLoss: 1.1267\tLR: 0.000337\n",
      "Training Epoch: 16 [5632/50000]\tLoss: 1.0734\tLR: 0.000337\n",
      "Training Epoch: 16 [5760/50000]\tLoss: 1.2985\tLR: 0.000337\n",
      "Training Epoch: 16 [5888/50000]\tLoss: 1.0259\tLR: 0.000337\n",
      "Training Epoch: 16 [6016/50000]\tLoss: 0.9681\tLR: 0.000337\n",
      "Training Epoch: 16 [6144/50000]\tLoss: 1.2701\tLR: 0.000337\n",
      "Training Epoch: 16 [6272/50000]\tLoss: 0.9418\tLR: 0.000337\n",
      "Training Epoch: 16 [6400/50000]\tLoss: 0.9231\tLR: 0.000337\n",
      "Training Epoch: 16 [6528/50000]\tLoss: 1.1670\tLR: 0.000337\n",
      "Training Epoch: 16 [6656/50000]\tLoss: 1.0218\tLR: 0.000337\n",
      "Training Epoch: 16 [6784/50000]\tLoss: 1.0219\tLR: 0.000337\n",
      "Training Epoch: 16 [6912/50000]\tLoss: 1.2146\tLR: 0.000337\n",
      "Training Epoch: 16 [7040/50000]\tLoss: 1.1163\tLR: 0.000337\n",
      "Training Epoch: 16 [7168/50000]\tLoss: 1.1815\tLR: 0.000337\n",
      "Training Epoch: 16 [7296/50000]\tLoss: 1.1930\tLR: 0.000337\n",
      "Training Epoch: 16 [7424/50000]\tLoss: 1.1505\tLR: 0.000337\n",
      "Training Epoch: 16 [7552/50000]\tLoss: 1.3844\tLR: 0.000337\n",
      "Training Epoch: 16 [7680/50000]\tLoss: 0.9486\tLR: 0.000337\n",
      "Training Epoch: 16 [7808/50000]\tLoss: 1.1131\tLR: 0.000337\n",
      "Training Epoch: 16 [7936/50000]\tLoss: 1.0616\tLR: 0.000337\n",
      "Training Epoch: 16 [8064/50000]\tLoss: 1.1503\tLR: 0.000337\n",
      "Training Epoch: 16 [8192/50000]\tLoss: 0.8636\tLR: 0.000337\n",
      "Training Epoch: 16 [8320/50000]\tLoss: 1.0492\tLR: 0.000337\n",
      "Training Epoch: 16 [8448/50000]\tLoss: 1.2135\tLR: 0.000337\n",
      "Training Epoch: 16 [8576/50000]\tLoss: 0.9235\tLR: 0.000337\n",
      "Training Epoch: 16 [8704/50000]\tLoss: 1.1941\tLR: 0.000337\n",
      "Training Epoch: 16 [8832/50000]\tLoss: 0.9762\tLR: 0.000337\n",
      "Training Epoch: 16 [8960/50000]\tLoss: 1.1724\tLR: 0.000337\n",
      "Training Epoch: 16 [9088/50000]\tLoss: 1.1589\tLR: 0.000337\n",
      "Training Epoch: 16 [9216/50000]\tLoss: 0.9578\tLR: 0.000337\n",
      "Training Epoch: 16 [9344/50000]\tLoss: 1.2012\tLR: 0.000337\n",
      "Training Epoch: 16 [9472/50000]\tLoss: 1.1033\tLR: 0.000337\n",
      "Training Epoch: 16 [9600/50000]\tLoss: 1.1848\tLR: 0.000337\n",
      "Training Epoch: 16 [9728/50000]\tLoss: 1.2749\tLR: 0.000337\n",
      "Training Epoch: 16 [9856/50000]\tLoss: 1.1330\tLR: 0.000337\n",
      "Training Epoch: 16 [9984/50000]\tLoss: 1.2725\tLR: 0.000337\n",
      "Training Epoch: 16 [10112/50000]\tLoss: 1.1934\tLR: 0.000337\n",
      "Training Epoch: 16 [10240/50000]\tLoss: 1.0596\tLR: 0.000337\n",
      "Training Epoch: 16 [10368/50000]\tLoss: 1.3328\tLR: 0.000337\n",
      "Training Epoch: 16 [10496/50000]\tLoss: 1.0491\tLR: 0.000337\n",
      "Training Epoch: 16 [10624/50000]\tLoss: 1.4263\tLR: 0.000337\n",
      "Training Epoch: 16 [10752/50000]\tLoss: 1.2092\tLR: 0.000337\n",
      "Training Epoch: 16 [10880/50000]\tLoss: 0.8492\tLR: 0.000337\n",
      "Training Epoch: 16 [11008/50000]\tLoss: 1.0025\tLR: 0.000337\n",
      "Training Epoch: 16 [11136/50000]\tLoss: 1.2044\tLR: 0.000337\n",
      "Training Epoch: 16 [11264/50000]\tLoss: 1.3227\tLR: 0.000337\n",
      "Training Epoch: 16 [11392/50000]\tLoss: 1.1921\tLR: 0.000337\n",
      "Training Epoch: 16 [11520/50000]\tLoss: 1.0074\tLR: 0.000337\n",
      "Training Epoch: 16 [11648/50000]\tLoss: 1.1305\tLR: 0.000337\n",
      "Training Epoch: 16 [11776/50000]\tLoss: 1.0686\tLR: 0.000337\n",
      "Training Epoch: 16 [11904/50000]\tLoss: 0.9037\tLR: 0.000337\n",
      "Training Epoch: 16 [12032/50000]\tLoss: 1.4544\tLR: 0.000337\n",
      "Training Epoch: 16 [12160/50000]\tLoss: 0.9878\tLR: 0.000337\n",
      "Training Epoch: 16 [12288/50000]\tLoss: 1.2488\tLR: 0.000337\n",
      "Training Epoch: 16 [12416/50000]\tLoss: 1.0313\tLR: 0.000337\n",
      "Training Epoch: 16 [12544/50000]\tLoss: 0.9492\tLR: 0.000337\n",
      "Training Epoch: 16 [12672/50000]\tLoss: 1.1057\tLR: 0.000337\n",
      "Training Epoch: 16 [12800/50000]\tLoss: 1.1135\tLR: 0.000337\n",
      "Training Epoch: 16 [12928/50000]\tLoss: 1.3875\tLR: 0.000337\n",
      "Training Epoch: 16 [13056/50000]\tLoss: 1.3160\tLR: 0.000337\n",
      "Training Epoch: 16 [13184/50000]\tLoss: 1.2844\tLR: 0.000337\n",
      "Training Epoch: 16 [13312/50000]\tLoss: 1.1168\tLR: 0.000337\n",
      "Training Epoch: 16 [13440/50000]\tLoss: 1.1766\tLR: 0.000337\n",
      "Training Epoch: 16 [13568/50000]\tLoss: 0.9006\tLR: 0.000337\n",
      "Training Epoch: 16 [13696/50000]\tLoss: 1.0216\tLR: 0.000337\n",
      "Training Epoch: 16 [13824/50000]\tLoss: 1.1770\tLR: 0.000337\n",
      "Training Epoch: 16 [13952/50000]\tLoss: 1.1482\tLR: 0.000337\n",
      "Training Epoch: 16 [14080/50000]\tLoss: 1.0392\tLR: 0.000337\n",
      "Training Epoch: 16 [14208/50000]\tLoss: 1.2126\tLR: 0.000337\n",
      "Training Epoch: 16 [14336/50000]\tLoss: 1.1543\tLR: 0.000337\n",
      "Training Epoch: 16 [14464/50000]\tLoss: 1.0287\tLR: 0.000337\n",
      "Training Epoch: 16 [14592/50000]\tLoss: 1.1987\tLR: 0.000337\n",
      "Training Epoch: 16 [14720/50000]\tLoss: 1.1369\tLR: 0.000337\n",
      "Training Epoch: 16 [14848/50000]\tLoss: 1.0533\tLR: 0.000337\n",
      "Training Epoch: 16 [14976/50000]\tLoss: 1.1370\tLR: 0.000337\n",
      "Training Epoch: 16 [15104/50000]\tLoss: 1.1261\tLR: 0.000337\n",
      "Training Epoch: 16 [15232/50000]\tLoss: 1.0262\tLR: 0.000337\n",
      "Training Epoch: 16 [15360/50000]\tLoss: 0.9760\tLR: 0.000337\n",
      "Training Epoch: 16 [15488/50000]\tLoss: 1.0622\tLR: 0.000337\n",
      "Training Epoch: 16 [15616/50000]\tLoss: 0.8623\tLR: 0.000337\n",
      "Training Epoch: 16 [15744/50000]\tLoss: 1.2272\tLR: 0.000337\n",
      "Training Epoch: 16 [15872/50000]\tLoss: 1.0396\tLR: 0.000337\n",
      "Training Epoch: 16 [16000/50000]\tLoss: 1.0755\tLR: 0.000337\n",
      "Training Epoch: 16 [16128/50000]\tLoss: 1.1631\tLR: 0.000337\n",
      "Training Epoch: 16 [16256/50000]\tLoss: 1.1084\tLR: 0.000337\n",
      "Training Epoch: 16 [16384/50000]\tLoss: 0.9225\tLR: 0.000337\n",
      "Training Epoch: 16 [16512/50000]\tLoss: 1.1586\tLR: 0.000337\n",
      "Training Epoch: 16 [16640/50000]\tLoss: 1.3159\tLR: 0.000337\n",
      "Training Epoch: 16 [16768/50000]\tLoss: 1.0454\tLR: 0.000337\n",
      "Training Epoch: 16 [16896/50000]\tLoss: 1.0429\tLR: 0.000337\n",
      "Training Epoch: 16 [17024/50000]\tLoss: 1.0927\tLR: 0.000337\n",
      "Training Epoch: 16 [17152/50000]\tLoss: 0.8377\tLR: 0.000337\n",
      "Training Epoch: 16 [17280/50000]\tLoss: 1.2732\tLR: 0.000337\n",
      "Training Epoch: 16 [17408/50000]\tLoss: 1.1513\tLR: 0.000337\n",
      "Training Epoch: 16 [17536/50000]\tLoss: 1.1043\tLR: 0.000337\n",
      "Training Epoch: 16 [17664/50000]\tLoss: 1.4815\tLR: 0.000337\n",
      "Training Epoch: 16 [17792/50000]\tLoss: 1.2489\tLR: 0.000337\n",
      "Training Epoch: 16 [17920/50000]\tLoss: 1.0577\tLR: 0.000337\n",
      "Training Epoch: 16 [18048/50000]\tLoss: 1.2218\tLR: 0.000337\n",
      "Training Epoch: 16 [18176/50000]\tLoss: 1.1020\tLR: 0.000337\n",
      "Training Epoch: 16 [18304/50000]\tLoss: 1.0795\tLR: 0.000337\n",
      "Training Epoch: 16 [18432/50000]\tLoss: 1.1697\tLR: 0.000337\n",
      "Training Epoch: 16 [18560/50000]\tLoss: 0.9553\tLR: 0.000337\n",
      "Training Epoch: 16 [18688/50000]\tLoss: 1.2232\tLR: 0.000337\n",
      "Training Epoch: 16 [18816/50000]\tLoss: 1.0754\tLR: 0.000337\n",
      "Training Epoch: 16 [18944/50000]\tLoss: 0.9951\tLR: 0.000337\n",
      "Training Epoch: 16 [19072/50000]\tLoss: 1.0874\tLR: 0.000337\n",
      "Training Epoch: 16 [19200/50000]\tLoss: 1.2500\tLR: 0.000337\n",
      "Training Epoch: 16 [19328/50000]\tLoss: 1.0177\tLR: 0.000337\n",
      "Training Epoch: 16 [19456/50000]\tLoss: 1.0781\tLR: 0.000337\n",
      "Training Epoch: 16 [19584/50000]\tLoss: 1.1461\tLR: 0.000337\n",
      "Training Epoch: 16 [19712/50000]\tLoss: 0.9563\tLR: 0.000337\n",
      "Training Epoch: 16 [19840/50000]\tLoss: 1.0837\tLR: 0.000337\n",
      "Training Epoch: 16 [19968/50000]\tLoss: 1.1792\tLR: 0.000337\n",
      "Training Epoch: 16 [20096/50000]\tLoss: 1.0284\tLR: 0.000337\n",
      "Training Epoch: 16 [20224/50000]\tLoss: 1.1905\tLR: 0.000337\n",
      "Training Epoch: 16 [20352/50000]\tLoss: 1.0698\tLR: 0.000337\n",
      "Training Epoch: 16 [20480/50000]\tLoss: 0.9543\tLR: 0.000337\n",
      "Training Epoch: 16 [20608/50000]\tLoss: 0.9675\tLR: 0.000337\n",
      "Training Epoch: 16 [20736/50000]\tLoss: 1.1172\tLR: 0.000337\n",
      "Training Epoch: 16 [20864/50000]\tLoss: 1.1755\tLR: 0.000337\n",
      "Training Epoch: 16 [20992/50000]\tLoss: 1.0502\tLR: 0.000337\n",
      "Training Epoch: 16 [21120/50000]\tLoss: 1.1296\tLR: 0.000337\n",
      "Training Epoch: 16 [21248/50000]\tLoss: 0.9999\tLR: 0.000337\n",
      "Training Epoch: 16 [21376/50000]\tLoss: 1.1913\tLR: 0.000337\n",
      "Training Epoch: 16 [21504/50000]\tLoss: 0.9887\tLR: 0.000337\n",
      "Training Epoch: 16 [21632/50000]\tLoss: 1.1824\tLR: 0.000337\n",
      "Training Epoch: 16 [21760/50000]\tLoss: 1.1597\tLR: 0.000337\n",
      "Training Epoch: 16 [21888/50000]\tLoss: 1.0890\tLR: 0.000337\n",
      "Training Epoch: 16 [22016/50000]\tLoss: 1.1497\tLR: 0.000337\n",
      "Training Epoch: 16 [22144/50000]\tLoss: 1.2066\tLR: 0.000337\n",
      "Training Epoch: 16 [22272/50000]\tLoss: 1.1140\tLR: 0.000337\n",
      "Training Epoch: 16 [22400/50000]\tLoss: 1.2872\tLR: 0.000337\n",
      "Training Epoch: 16 [22528/50000]\tLoss: 1.1542\tLR: 0.000337\n",
      "Training Epoch: 16 [22656/50000]\tLoss: 1.2354\tLR: 0.000337\n",
      "Training Epoch: 16 [22784/50000]\tLoss: 1.1992\tLR: 0.000337\n",
      "Training Epoch: 16 [22912/50000]\tLoss: 1.2237\tLR: 0.000337\n",
      "Training Epoch: 16 [23040/50000]\tLoss: 1.3858\tLR: 0.000337\n",
      "Training Epoch: 16 [23168/50000]\tLoss: 1.2607\tLR: 0.000337\n",
      "Training Epoch: 16 [23296/50000]\tLoss: 1.0441\tLR: 0.000337\n",
      "Training Epoch: 16 [23424/50000]\tLoss: 1.1293\tLR: 0.000337\n",
      "Training Epoch: 16 [23552/50000]\tLoss: 1.3156\tLR: 0.000337\n",
      "Training Epoch: 16 [23680/50000]\tLoss: 0.9132\tLR: 0.000337\n",
      "Training Epoch: 16 [23808/50000]\tLoss: 1.1203\tLR: 0.000337\n",
      "Training Epoch: 16 [23936/50000]\tLoss: 1.2876\tLR: 0.000337\n",
      "Training Epoch: 16 [24064/50000]\tLoss: 1.2574\tLR: 0.000337\n",
      "Training Epoch: 16 [24192/50000]\tLoss: 1.0187\tLR: 0.000337\n",
      "Training Epoch: 16 [24320/50000]\tLoss: 1.2778\tLR: 0.000337\n",
      "Training Epoch: 16 [24448/50000]\tLoss: 1.1008\tLR: 0.000337\n",
      "Training Epoch: 16 [24576/50000]\tLoss: 1.0274\tLR: 0.000337\n",
      "Training Epoch: 16 [24704/50000]\tLoss: 1.1578\tLR: 0.000337\n",
      "Training Epoch: 16 [24832/50000]\tLoss: 1.0101\tLR: 0.000337\n",
      "Training Epoch: 16 [24960/50000]\tLoss: 1.3127\tLR: 0.000337\n",
      "Training Epoch: 16 [25088/50000]\tLoss: 1.3143\tLR: 0.000337\n",
      "Training Epoch: 16 [25216/50000]\tLoss: 1.0276\tLR: 0.000337\n",
      "Training Epoch: 16 [25344/50000]\tLoss: 1.0825\tLR: 0.000337\n",
      "Training Epoch: 16 [25472/50000]\tLoss: 1.2704\tLR: 0.000337\n",
      "Training Epoch: 16 [25600/50000]\tLoss: 1.2425\tLR: 0.000337\n",
      "Training Epoch: 16 [25728/50000]\tLoss: 1.0331\tLR: 0.000337\n",
      "Training Epoch: 16 [25856/50000]\tLoss: 1.0285\tLR: 0.000337\n",
      "Training Epoch: 16 [25984/50000]\tLoss: 1.0900\tLR: 0.000337\n",
      "Training Epoch: 16 [26112/50000]\tLoss: 1.2930\tLR: 0.000337\n",
      "Training Epoch: 16 [26240/50000]\tLoss: 1.2159\tLR: 0.000337\n",
      "Training Epoch: 16 [26368/50000]\tLoss: 1.2002\tLR: 0.000337\n",
      "Training Epoch: 16 [26496/50000]\tLoss: 1.2549\tLR: 0.000337\n",
      "Training Epoch: 16 [26624/50000]\tLoss: 0.9347\tLR: 0.000337\n",
      "Training Epoch: 16 [26752/50000]\tLoss: 1.2374\tLR: 0.000337\n",
      "Training Epoch: 16 [26880/50000]\tLoss: 1.2106\tLR: 0.000337\n",
      "Training Epoch: 16 [27008/50000]\tLoss: 1.1154\tLR: 0.000337\n",
      "Training Epoch: 16 [27136/50000]\tLoss: 1.2976\tLR: 0.000337\n",
      "Training Epoch: 16 [27264/50000]\tLoss: 1.0501\tLR: 0.000337\n",
      "Training Epoch: 16 [27392/50000]\tLoss: 1.1456\tLR: 0.000337\n",
      "Training Epoch: 16 [27520/50000]\tLoss: 1.2471\tLR: 0.000337\n",
      "Training Epoch: 16 [27648/50000]\tLoss: 1.0315\tLR: 0.000337\n",
      "Training Epoch: 16 [27776/50000]\tLoss: 1.1885\tLR: 0.000337\n",
      "Training Epoch: 16 [27904/50000]\tLoss: 1.3078\tLR: 0.000337\n",
      "Training Epoch: 16 [28032/50000]\tLoss: 1.0364\tLR: 0.000337\n",
      "Training Epoch: 16 [28160/50000]\tLoss: 1.1900\tLR: 0.000337\n",
      "Training Epoch: 16 [28288/50000]\tLoss: 1.2646\tLR: 0.000337\n",
      "Training Epoch: 16 [28416/50000]\tLoss: 1.2824\tLR: 0.000337\n",
      "Training Epoch: 16 [28544/50000]\tLoss: 1.2452\tLR: 0.000337\n",
      "Training Epoch: 16 [28672/50000]\tLoss: 1.2029\tLR: 0.000337\n",
      "Training Epoch: 16 [28800/50000]\tLoss: 1.2470\tLR: 0.000337\n",
      "Training Epoch: 16 [28928/50000]\tLoss: 1.3318\tLR: 0.000337\n",
      "Training Epoch: 16 [29056/50000]\tLoss: 1.3940\tLR: 0.000337\n",
      "Training Epoch: 16 [29184/50000]\tLoss: 1.4064\tLR: 0.000337\n",
      "Training Epoch: 16 [29312/50000]\tLoss: 0.9387\tLR: 0.000337\n",
      "Training Epoch: 16 [29440/50000]\tLoss: 1.0450\tLR: 0.000337\n",
      "Training Epoch: 16 [29568/50000]\tLoss: 1.2294\tLR: 0.000337\n",
      "Training Epoch: 16 [29696/50000]\tLoss: 1.1903\tLR: 0.000337\n",
      "Training Epoch: 16 [29824/50000]\tLoss: 1.0090\tLR: 0.000337\n",
      "Training Epoch: 16 [29952/50000]\tLoss: 1.0946\tLR: 0.000337\n",
      "Training Epoch: 16 [30080/50000]\tLoss: 1.1736\tLR: 0.000337\n",
      "Training Epoch: 16 [30208/50000]\tLoss: 1.1482\tLR: 0.000337\n",
      "Training Epoch: 16 [30336/50000]\tLoss: 1.1469\tLR: 0.000337\n",
      "Training Epoch: 16 [30464/50000]\tLoss: 1.0566\tLR: 0.000337\n",
      "Training Epoch: 16 [30592/50000]\tLoss: 1.2080\tLR: 0.000337\n",
      "Training Epoch: 16 [30720/50000]\tLoss: 1.1664\tLR: 0.000337\n",
      "Training Epoch: 16 [30848/50000]\tLoss: 1.0860\tLR: 0.000337\n",
      "Training Epoch: 16 [30976/50000]\tLoss: 1.0179\tLR: 0.000337\n",
      "Training Epoch: 16 [31104/50000]\tLoss: 0.9729\tLR: 0.000337\n",
      "Training Epoch: 16 [31232/50000]\tLoss: 0.9488\tLR: 0.000337\n",
      "Training Epoch: 16 [31360/50000]\tLoss: 1.2237\tLR: 0.000337\n",
      "Training Epoch: 16 [31488/50000]\tLoss: 1.1439\tLR: 0.000337\n",
      "Training Epoch: 16 [31616/50000]\tLoss: 1.0390\tLR: 0.000337\n",
      "Training Epoch: 16 [31744/50000]\tLoss: 1.0427\tLR: 0.000337\n",
      "Training Epoch: 16 [31872/50000]\tLoss: 1.3204\tLR: 0.000337\n",
      "Training Epoch: 16 [32000/50000]\tLoss: 0.9400\tLR: 0.000337\n",
      "Training Epoch: 16 [32128/50000]\tLoss: 0.9755\tLR: 0.000337\n",
      "Training Epoch: 16 [32256/50000]\tLoss: 1.2045\tLR: 0.000337\n",
      "Training Epoch: 16 [32384/50000]\tLoss: 1.0385\tLR: 0.000337\n",
      "Training Epoch: 16 [32512/50000]\tLoss: 1.0154\tLR: 0.000337\n",
      "Training Epoch: 16 [32640/50000]\tLoss: 1.1545\tLR: 0.000337\n",
      "Training Epoch: 16 [32768/50000]\tLoss: 0.9166\tLR: 0.000337\n",
      "Training Epoch: 16 [32896/50000]\tLoss: 1.0791\tLR: 0.000337\n",
      "Training Epoch: 16 [33024/50000]\tLoss: 1.1270\tLR: 0.000337\n",
      "Training Epoch: 16 [33152/50000]\tLoss: 1.2247\tLR: 0.000337\n",
      "Training Epoch: 16 [33280/50000]\tLoss: 1.2186\tLR: 0.000337\n",
      "Training Epoch: 16 [33408/50000]\tLoss: 0.9192\tLR: 0.000337\n",
      "Training Epoch: 16 [33536/50000]\tLoss: 1.2148\tLR: 0.000337\n",
      "Training Epoch: 16 [33664/50000]\tLoss: 1.2193\tLR: 0.000337\n",
      "Training Epoch: 16 [33792/50000]\tLoss: 1.0589\tLR: 0.000337\n",
      "Training Epoch: 16 [33920/50000]\tLoss: 1.1098\tLR: 0.000337\n",
      "Training Epoch: 16 [34048/50000]\tLoss: 0.9764\tLR: 0.000337\n",
      "Training Epoch: 16 [34176/50000]\tLoss: 1.1196\tLR: 0.000337\n",
      "Training Epoch: 16 [34304/50000]\tLoss: 1.3524\tLR: 0.000337\n",
      "Training Epoch: 16 [34432/50000]\tLoss: 1.0641\tLR: 0.000337\n",
      "Training Epoch: 16 [34560/50000]\tLoss: 1.1514\tLR: 0.000337\n",
      "Training Epoch: 16 [34688/50000]\tLoss: 1.1970\tLR: 0.000337\n",
      "Training Epoch: 16 [34816/50000]\tLoss: 1.3648\tLR: 0.000337\n",
      "Training Epoch: 16 [34944/50000]\tLoss: 1.0920\tLR: 0.000337\n",
      "Training Epoch: 16 [35072/50000]\tLoss: 1.2742\tLR: 0.000337\n",
      "Training Epoch: 16 [35200/50000]\tLoss: 0.9052\tLR: 0.000337\n",
      "Training Epoch: 16 [35328/50000]\tLoss: 1.3595\tLR: 0.000337\n",
      "Training Epoch: 16 [35456/50000]\tLoss: 1.1561\tLR: 0.000337\n",
      "Training Epoch: 16 [35584/50000]\tLoss: 1.1696\tLR: 0.000337\n",
      "Training Epoch: 16 [35712/50000]\tLoss: 1.0147\tLR: 0.000337\n",
      "Training Epoch: 16 [35840/50000]\tLoss: 1.0054\tLR: 0.000337\n",
      "Training Epoch: 16 [35968/50000]\tLoss: 1.0349\tLR: 0.000337\n",
      "Training Epoch: 16 [36096/50000]\tLoss: 1.1501\tLR: 0.000337\n",
      "Training Epoch: 16 [36224/50000]\tLoss: 1.1322\tLR: 0.000337\n",
      "Training Epoch: 16 [36352/50000]\tLoss: 0.9928\tLR: 0.000337\n",
      "Training Epoch: 16 [36480/50000]\tLoss: 1.0062\tLR: 0.000337\n",
      "Training Epoch: 16 [36608/50000]\tLoss: 1.2172\tLR: 0.000337\n",
      "Training Epoch: 16 [36736/50000]\tLoss: 1.0946\tLR: 0.000337\n",
      "Training Epoch: 16 [36864/50000]\tLoss: 1.0176\tLR: 0.000337\n",
      "Training Epoch: 16 [36992/50000]\tLoss: 0.9906\tLR: 0.000337\n",
      "Training Epoch: 16 [37120/50000]\tLoss: 0.9187\tLR: 0.000337\n",
      "Training Epoch: 16 [37248/50000]\tLoss: 1.1363\tLR: 0.000337\n",
      "Training Epoch: 16 [37376/50000]\tLoss: 1.1809\tLR: 0.000337\n",
      "Training Epoch: 16 [37504/50000]\tLoss: 1.1729\tLR: 0.000337\n",
      "Training Epoch: 16 [37632/50000]\tLoss: 1.2139\tLR: 0.000337\n",
      "Training Epoch: 16 [37760/50000]\tLoss: 1.0204\tLR: 0.000337\n",
      "Training Epoch: 16 [37888/50000]\tLoss: 0.8790\tLR: 0.000337\n",
      "Training Epoch: 16 [38016/50000]\tLoss: 1.1062\tLR: 0.000337\n",
      "Training Epoch: 16 [38144/50000]\tLoss: 1.2417\tLR: 0.000337\n",
      "Training Epoch: 16 [38272/50000]\tLoss: 1.0453\tLR: 0.000337\n",
      "Training Epoch: 16 [38400/50000]\tLoss: 1.2972\tLR: 0.000337\n",
      "Training Epoch: 16 [38528/50000]\tLoss: 1.0971\tLR: 0.000337\n",
      "Training Epoch: 16 [38656/50000]\tLoss: 1.1079\tLR: 0.000337\n",
      "Training Epoch: 16 [38784/50000]\tLoss: 1.1474\tLR: 0.000337\n",
      "Training Epoch: 16 [38912/50000]\tLoss: 1.0004\tLR: 0.000337\n",
      "Training Epoch: 16 [39040/50000]\tLoss: 1.1843\tLR: 0.000337\n",
      "Training Epoch: 16 [39168/50000]\tLoss: 0.9697\tLR: 0.000337\n",
      "Training Epoch: 16 [39296/50000]\tLoss: 1.1376\tLR: 0.000337\n",
      "Training Epoch: 16 [39424/50000]\tLoss: 1.1617\tLR: 0.000337\n",
      "Training Epoch: 16 [39552/50000]\tLoss: 1.0195\tLR: 0.000337\n",
      "Training Epoch: 16 [39680/50000]\tLoss: 1.1779\tLR: 0.000337\n",
      "Training Epoch: 16 [39808/50000]\tLoss: 0.9688\tLR: 0.000337\n",
      "Training Epoch: 16 [39936/50000]\tLoss: 1.1471\tLR: 0.000337\n",
      "Training Epoch: 16 [40064/50000]\tLoss: 1.1383\tLR: 0.000337\n",
      "Training Epoch: 16 [40192/50000]\tLoss: 1.2360\tLR: 0.000337\n",
      "Training Epoch: 16 [40320/50000]\tLoss: 1.0160\tLR: 0.000337\n",
      "Training Epoch: 16 [40448/50000]\tLoss: 1.1356\tLR: 0.000337\n",
      "Training Epoch: 16 [40576/50000]\tLoss: 1.3445\tLR: 0.000337\n",
      "Training Epoch: 16 [40704/50000]\tLoss: 1.1078\tLR: 0.000337\n",
      "Training Epoch: 16 [40832/50000]\tLoss: 1.3667\tLR: 0.000337\n",
      "Training Epoch: 16 [40960/50000]\tLoss: 1.0745\tLR: 0.000337\n",
      "Training Epoch: 16 [41088/50000]\tLoss: 0.9118\tLR: 0.000337\n",
      "Training Epoch: 16 [41216/50000]\tLoss: 1.1469\tLR: 0.000337\n",
      "Training Epoch: 16 [41344/50000]\tLoss: 1.0625\tLR: 0.000337\n",
      "Training Epoch: 16 [41472/50000]\tLoss: 1.0237\tLR: 0.000337\n",
      "Training Epoch: 16 [41600/50000]\tLoss: 1.2611\tLR: 0.000337\n",
      "Training Epoch: 16 [41728/50000]\tLoss: 1.0350\tLR: 0.000337\n",
      "Training Epoch: 16 [41856/50000]\tLoss: 0.9432\tLR: 0.000337\n",
      "Training Epoch: 16 [41984/50000]\tLoss: 1.1057\tLR: 0.000337\n",
      "Training Epoch: 16 [42112/50000]\tLoss: 1.1100\tLR: 0.000337\n",
      "Training Epoch: 16 [42240/50000]\tLoss: 1.0357\tLR: 0.000337\n",
      "Training Epoch: 16 [42368/50000]\tLoss: 1.3984\tLR: 0.000337\n",
      "Training Epoch: 16 [42496/50000]\tLoss: 1.3637\tLR: 0.000337\n",
      "Training Epoch: 16 [42624/50000]\tLoss: 1.1919\tLR: 0.000337\n",
      "Training Epoch: 16 [42752/50000]\tLoss: 0.9704\tLR: 0.000337\n",
      "Training Epoch: 16 [42880/50000]\tLoss: 1.0691\tLR: 0.000337\n",
      "Training Epoch: 16 [43008/50000]\tLoss: 1.1951\tLR: 0.000337\n",
      "Training Epoch: 16 [43136/50000]\tLoss: 1.0117\tLR: 0.000337\n",
      "Training Epoch: 16 [43264/50000]\tLoss: 0.9527\tLR: 0.000337\n",
      "Training Epoch: 16 [43392/50000]\tLoss: 1.2016\tLR: 0.000337\n",
      "Training Epoch: 16 [43520/50000]\tLoss: 1.4373\tLR: 0.000337\n",
      "Training Epoch: 16 [43648/50000]\tLoss: 1.0622\tLR: 0.000337\n",
      "Training Epoch: 16 [43776/50000]\tLoss: 1.3174\tLR: 0.000337\n",
      "Training Epoch: 16 [43904/50000]\tLoss: 1.1644\tLR: 0.000337\n",
      "Training Epoch: 16 [44032/50000]\tLoss: 1.0793\tLR: 0.000337\n",
      "Training Epoch: 16 [44160/50000]\tLoss: 1.3392\tLR: 0.000337\n",
      "Training Epoch: 16 [44288/50000]\tLoss: 1.1080\tLR: 0.000337\n",
      "Training Epoch: 16 [44416/50000]\tLoss: 1.1847\tLR: 0.000337\n",
      "Training Epoch: 16 [44544/50000]\tLoss: 1.1124\tLR: 0.000337\n",
      "Training Epoch: 16 [44672/50000]\tLoss: 1.0081\tLR: 0.000337\n",
      "Training Epoch: 16 [44800/50000]\tLoss: 1.1709\tLR: 0.000337\n",
      "Training Epoch: 16 [44928/50000]\tLoss: 1.1687\tLR: 0.000337\n",
      "Training Epoch: 16 [45056/50000]\tLoss: 1.3413\tLR: 0.000337\n",
      "Training Epoch: 16 [45184/50000]\tLoss: 1.1530\tLR: 0.000337\n",
      "Training Epoch: 16 [45312/50000]\tLoss: 1.0868\tLR: 0.000337\n",
      "Training Epoch: 16 [45440/50000]\tLoss: 0.9652\tLR: 0.000337\n",
      "Training Epoch: 16 [45568/50000]\tLoss: 0.9664\tLR: 0.000337\n",
      "Training Epoch: 16 [45696/50000]\tLoss: 1.1574\tLR: 0.000337\n",
      "Training Epoch: 16 [45824/50000]\tLoss: 1.1058\tLR: 0.000337\n",
      "Training Epoch: 16 [45952/50000]\tLoss: 1.1859\tLR: 0.000337\n",
      "Training Epoch: 16 [46080/50000]\tLoss: 1.1995\tLR: 0.000337\n",
      "Training Epoch: 16 [46208/50000]\tLoss: 1.0091\tLR: 0.000337\n",
      "Training Epoch: 16 [46336/50000]\tLoss: 1.2288\tLR: 0.000337\n",
      "Training Epoch: 16 [46464/50000]\tLoss: 1.1078\tLR: 0.000337\n",
      "Training Epoch: 16 [46592/50000]\tLoss: 1.0013\tLR: 0.000337\n",
      "Training Epoch: 16 [46720/50000]\tLoss: 1.3253\tLR: 0.000337\n",
      "Training Epoch: 16 [46848/50000]\tLoss: 1.3185\tLR: 0.000337\n",
      "Training Epoch: 16 [46976/50000]\tLoss: 1.0981\tLR: 0.000337\n",
      "Training Epoch: 16 [47104/50000]\tLoss: 1.1886\tLR: 0.000337\n",
      "Training Epoch: 16 [47232/50000]\tLoss: 1.2447\tLR: 0.000337\n",
      "Training Epoch: 16 [47360/50000]\tLoss: 1.1644\tLR: 0.000337\n",
      "Training Epoch: 16 [47488/50000]\tLoss: 1.1732\tLR: 0.000337\n",
      "Training Epoch: 16 [47616/50000]\tLoss: 1.1916\tLR: 0.000337\n",
      "Training Epoch: 16 [47744/50000]\tLoss: 1.1758\tLR: 0.000337\n",
      "Training Epoch: 16 [47872/50000]\tLoss: 0.9262\tLR: 0.000337\n",
      "Training Epoch: 16 [48000/50000]\tLoss: 1.0960\tLR: 0.000337\n",
      "Training Epoch: 16 [48128/50000]\tLoss: 1.2080\tLR: 0.000337\n",
      "Training Epoch: 16 [48256/50000]\tLoss: 0.9611\tLR: 0.000337\n",
      "Training Epoch: 16 [48384/50000]\tLoss: 0.9448\tLR: 0.000337\n",
      "Training Epoch: 16 [48512/50000]\tLoss: 1.1053\tLR: 0.000337\n",
      "Training Epoch: 16 [48640/50000]\tLoss: 1.1848\tLR: 0.000337\n",
      "Training Epoch: 16 [48768/50000]\tLoss: 1.2612\tLR: 0.000337\n",
      "Training Epoch: 16 [48896/50000]\tLoss: 1.1863\tLR: 0.000337\n",
      "Training Epoch: 16 [49024/50000]\tLoss: 1.0676\tLR: 0.000337\n",
      "Training Epoch: 16 [49152/50000]\tLoss: 1.2924\tLR: 0.000337\n",
      "Training Epoch: 16 [49280/50000]\tLoss: 1.1608\tLR: 0.000337\n",
      "Training Epoch: 16 [49408/50000]\tLoss: 1.1061\tLR: 0.000337\n",
      "Training Epoch: 16 [49536/50000]\tLoss: 1.1386\tLR: 0.000337\n",
      "Training Epoch: 16 [49664/50000]\tLoss: 1.1274\tLR: 0.000337\n",
      "Training Epoch: 16 [49792/50000]\tLoss: 0.9771\tLR: 0.000337\n",
      "Training Epoch: 16 [49920/50000]\tLoss: 1.2549\tLR: 0.000337\n",
      "Training Epoch: 16 [50000/50000]\tLoss: 1.2391\tLR: 0.000337\n",
      "Test set: Average loss: 0.0104, Accuracy: 0.6254\n",
      "\n",
      "Training Epoch: 17 [128/50000]\tLoss: 1.3567\tLR: 0.000337\n",
      "Training Epoch: 17 [256/50000]\tLoss: 1.0379\tLR: 0.000337\n",
      "Training Epoch: 17 [384/50000]\tLoss: 1.0951\tLR: 0.000337\n",
      "Training Epoch: 17 [512/50000]\tLoss: 1.1399\tLR: 0.000337\n",
      "Training Epoch: 17 [640/50000]\tLoss: 1.1113\tLR: 0.000337\n",
      "Training Epoch: 17 [768/50000]\tLoss: 1.1890\tLR: 0.000337\n",
      "Training Epoch: 17 [896/50000]\tLoss: 1.1447\tLR: 0.000337\n",
      "Training Epoch: 17 [1024/50000]\tLoss: 1.1031\tLR: 0.000337\n",
      "Training Epoch: 17 [1152/50000]\tLoss: 0.8963\tLR: 0.000337\n",
      "Training Epoch: 17 [1280/50000]\tLoss: 0.9515\tLR: 0.000337\n",
      "Training Epoch: 17 [1408/50000]\tLoss: 1.0660\tLR: 0.000337\n",
      "Training Epoch: 17 [1536/50000]\tLoss: 1.2015\tLR: 0.000337\n",
      "Training Epoch: 17 [1664/50000]\tLoss: 1.0393\tLR: 0.000337\n",
      "Training Epoch: 17 [1792/50000]\tLoss: 1.1857\tLR: 0.000337\n",
      "Training Epoch: 17 [1920/50000]\tLoss: 1.1379\tLR: 0.000337\n",
      "Training Epoch: 17 [2048/50000]\tLoss: 1.1517\tLR: 0.000337\n",
      "Training Epoch: 17 [2176/50000]\tLoss: 1.2534\tLR: 0.000337\n",
      "Training Epoch: 17 [2304/50000]\tLoss: 1.1608\tLR: 0.000337\n",
      "Training Epoch: 17 [2432/50000]\tLoss: 1.1395\tLR: 0.000337\n",
      "Training Epoch: 17 [2560/50000]\tLoss: 0.9301\tLR: 0.000337\n",
      "Training Epoch: 17 [2688/50000]\tLoss: 1.2048\tLR: 0.000337\n",
      "Training Epoch: 17 [2816/50000]\tLoss: 1.0169\tLR: 0.000337\n",
      "Training Epoch: 17 [2944/50000]\tLoss: 1.1578\tLR: 0.000337\n",
      "Training Epoch: 17 [3072/50000]\tLoss: 1.0470\tLR: 0.000337\n",
      "Training Epoch: 17 [3200/50000]\tLoss: 1.0677\tLR: 0.000337\n",
      "Training Epoch: 17 [3328/50000]\tLoss: 1.3766\tLR: 0.000337\n",
      "Training Epoch: 17 [3456/50000]\tLoss: 1.3675\tLR: 0.000337\n",
      "Training Epoch: 17 [3584/50000]\tLoss: 1.1591\tLR: 0.000337\n",
      "Training Epoch: 17 [3712/50000]\tLoss: 0.9535\tLR: 0.000337\n",
      "Training Epoch: 17 [3840/50000]\tLoss: 1.2557\tLR: 0.000337\n",
      "Training Epoch: 17 [3968/50000]\tLoss: 0.8909\tLR: 0.000337\n",
      "Training Epoch: 17 [4096/50000]\tLoss: 1.0310\tLR: 0.000337\n",
      "Training Epoch: 17 [4224/50000]\tLoss: 1.2022\tLR: 0.000337\n",
      "Training Epoch: 17 [4352/50000]\tLoss: 1.1987\tLR: 0.000337\n",
      "Training Epoch: 17 [4480/50000]\tLoss: 1.1277\tLR: 0.000337\n",
      "Training Epoch: 17 [4608/50000]\tLoss: 1.0898\tLR: 0.000337\n",
      "Training Epoch: 17 [4736/50000]\tLoss: 1.0309\tLR: 0.000337\n",
      "Training Epoch: 17 [4864/50000]\tLoss: 1.0981\tLR: 0.000337\n",
      "Training Epoch: 17 [4992/50000]\tLoss: 1.1185\tLR: 0.000337\n",
      "Training Epoch: 17 [5120/50000]\tLoss: 1.1873\tLR: 0.000337\n",
      "Training Epoch: 17 [5248/50000]\tLoss: 1.1092\tLR: 0.000337\n",
      "Training Epoch: 17 [5376/50000]\tLoss: 1.2253\tLR: 0.000337\n",
      "Training Epoch: 17 [5504/50000]\tLoss: 1.1881\tLR: 0.000337\n",
      "Training Epoch: 17 [5632/50000]\tLoss: 1.2677\tLR: 0.000337\n",
      "Training Epoch: 17 [5760/50000]\tLoss: 1.0792\tLR: 0.000337\n",
      "Training Epoch: 17 [5888/50000]\tLoss: 1.0370\tLR: 0.000337\n",
      "Training Epoch: 17 [6016/50000]\tLoss: 1.0412\tLR: 0.000337\n",
      "Training Epoch: 17 [6144/50000]\tLoss: 1.1265\tLR: 0.000337\n",
      "Training Epoch: 17 [6272/50000]\tLoss: 1.1279\tLR: 0.000337\n",
      "Training Epoch: 17 [6400/50000]\tLoss: 1.0260\tLR: 0.000337\n",
      "Training Epoch: 17 [6528/50000]\tLoss: 0.9945\tLR: 0.000337\n",
      "Training Epoch: 17 [6656/50000]\tLoss: 0.9600\tLR: 0.000337\n",
      "Training Epoch: 17 [6784/50000]\tLoss: 0.9329\tLR: 0.000337\n",
      "Training Epoch: 17 [6912/50000]\tLoss: 1.2783\tLR: 0.000337\n",
      "Training Epoch: 17 [7040/50000]\tLoss: 1.1628\tLR: 0.000337\n",
      "Training Epoch: 17 [7168/50000]\tLoss: 0.9953\tLR: 0.000337\n",
      "Training Epoch: 17 [7296/50000]\tLoss: 1.1848\tLR: 0.000337\n",
      "Training Epoch: 17 [7424/50000]\tLoss: 1.1680\tLR: 0.000337\n",
      "Training Epoch: 17 [7552/50000]\tLoss: 1.0263\tLR: 0.000337\n",
      "Training Epoch: 17 [7680/50000]\tLoss: 1.1331\tLR: 0.000337\n",
      "Training Epoch: 17 [7808/50000]\tLoss: 0.8690\tLR: 0.000337\n",
      "Training Epoch: 17 [7936/50000]\tLoss: 1.0441\tLR: 0.000337\n",
      "Training Epoch: 17 [8064/50000]\tLoss: 1.2616\tLR: 0.000337\n",
      "Training Epoch: 17 [8192/50000]\tLoss: 1.0973\tLR: 0.000337\n",
      "Training Epoch: 17 [8320/50000]\tLoss: 1.1117\tLR: 0.000337\n",
      "Training Epoch: 17 [8448/50000]\tLoss: 1.0203\tLR: 0.000337\n",
      "Training Epoch: 17 [8576/50000]\tLoss: 0.9737\tLR: 0.000337\n",
      "Training Epoch: 17 [8704/50000]\tLoss: 1.2576\tLR: 0.000337\n",
      "Training Epoch: 17 [8832/50000]\tLoss: 0.9059\tLR: 0.000337\n",
      "Training Epoch: 17 [8960/50000]\tLoss: 0.9382\tLR: 0.000337\n",
      "Training Epoch: 17 [9088/50000]\tLoss: 0.9558\tLR: 0.000337\n",
      "Training Epoch: 17 [9216/50000]\tLoss: 1.1108\tLR: 0.000337\n",
      "Training Epoch: 17 [9344/50000]\tLoss: 1.0661\tLR: 0.000337\n",
      "Training Epoch: 17 [9472/50000]\tLoss: 0.9776\tLR: 0.000337\n",
      "Training Epoch: 17 [9600/50000]\tLoss: 1.1182\tLR: 0.000337\n",
      "Training Epoch: 17 [9728/50000]\tLoss: 1.0569\tLR: 0.000337\n",
      "Training Epoch: 17 [9856/50000]\tLoss: 1.1771\tLR: 0.000337\n",
      "Training Epoch: 17 [9984/50000]\tLoss: 1.0994\tLR: 0.000337\n",
      "Training Epoch: 17 [10112/50000]\tLoss: 1.1067\tLR: 0.000337\n",
      "Training Epoch: 17 [10240/50000]\tLoss: 1.2081\tLR: 0.000337\n",
      "Training Epoch: 17 [10368/50000]\tLoss: 1.0221\tLR: 0.000337\n",
      "Training Epoch: 17 [10496/50000]\tLoss: 1.1332\tLR: 0.000337\n",
      "Training Epoch: 17 [10624/50000]\tLoss: 1.4451\tLR: 0.000337\n",
      "Training Epoch: 17 [10752/50000]\tLoss: 1.1678\tLR: 0.000337\n",
      "Training Epoch: 17 [10880/50000]\tLoss: 1.0897\tLR: 0.000337\n",
      "Training Epoch: 17 [11008/50000]\tLoss: 1.2486\tLR: 0.000337\n",
      "Training Epoch: 17 [11136/50000]\tLoss: 1.0585\tLR: 0.000337\n",
      "Training Epoch: 17 [11264/50000]\tLoss: 1.2935\tLR: 0.000337\n",
      "Training Epoch: 17 [11392/50000]\tLoss: 1.0311\tLR: 0.000337\n",
      "Training Epoch: 17 [11520/50000]\tLoss: 1.1627\tLR: 0.000337\n",
      "Training Epoch: 17 [11648/50000]\tLoss: 1.2162\tLR: 0.000337\n",
      "Training Epoch: 17 [11776/50000]\tLoss: 1.1635\tLR: 0.000337\n",
      "Training Epoch: 17 [11904/50000]\tLoss: 1.0130\tLR: 0.000337\n",
      "Training Epoch: 17 [12032/50000]\tLoss: 1.0799\tLR: 0.000337\n",
      "Training Epoch: 17 [12160/50000]\tLoss: 1.1153\tLR: 0.000337\n",
      "Training Epoch: 17 [12288/50000]\tLoss: 1.2644\tLR: 0.000337\n",
      "Training Epoch: 17 [12416/50000]\tLoss: 1.1659\tLR: 0.000337\n",
      "Training Epoch: 17 [12544/50000]\tLoss: 1.1784\tLR: 0.000337\n",
      "Training Epoch: 17 [12672/50000]\tLoss: 0.9262\tLR: 0.000337\n",
      "Training Epoch: 17 [12800/50000]\tLoss: 1.1775\tLR: 0.000337\n",
      "Training Epoch: 17 [12928/50000]\tLoss: 1.1145\tLR: 0.000337\n",
      "Training Epoch: 17 [13056/50000]\tLoss: 0.9123\tLR: 0.000337\n",
      "Training Epoch: 17 [13184/50000]\tLoss: 1.3124\tLR: 0.000337\n",
      "Training Epoch: 17 [13312/50000]\tLoss: 1.0556\tLR: 0.000337\n",
      "Training Epoch: 17 [13440/50000]\tLoss: 1.0411\tLR: 0.000337\n",
      "Training Epoch: 17 [13568/50000]\tLoss: 1.2094\tLR: 0.000337\n",
      "Training Epoch: 17 [13696/50000]\tLoss: 0.9496\tLR: 0.000337\n",
      "Training Epoch: 17 [13824/50000]\tLoss: 1.1834\tLR: 0.000337\n",
      "Training Epoch: 17 [13952/50000]\tLoss: 1.0179\tLR: 0.000337\n",
      "Training Epoch: 17 [14080/50000]\tLoss: 1.1528\tLR: 0.000337\n",
      "Training Epoch: 17 [14208/50000]\tLoss: 1.3102\tLR: 0.000337\n",
      "Training Epoch: 17 [14336/50000]\tLoss: 0.9914\tLR: 0.000337\n",
      "Training Epoch: 17 [14464/50000]\tLoss: 1.0825\tLR: 0.000337\n",
      "Training Epoch: 17 [14592/50000]\tLoss: 1.0665\tLR: 0.000337\n",
      "Training Epoch: 17 [14720/50000]\tLoss: 1.2879\tLR: 0.000337\n",
      "Training Epoch: 17 [14848/50000]\tLoss: 1.0681\tLR: 0.000337\n",
      "Training Epoch: 17 [14976/50000]\tLoss: 1.2081\tLR: 0.000337\n",
      "Training Epoch: 17 [15104/50000]\tLoss: 1.1809\tLR: 0.000337\n",
      "Training Epoch: 17 [15232/50000]\tLoss: 1.2945\tLR: 0.000337\n",
      "Training Epoch: 17 [15360/50000]\tLoss: 1.0426\tLR: 0.000337\n",
      "Training Epoch: 17 [15488/50000]\tLoss: 1.0243\tLR: 0.000337\n",
      "Training Epoch: 17 [15616/50000]\tLoss: 1.1517\tLR: 0.000337\n",
      "Training Epoch: 17 [15744/50000]\tLoss: 1.2124\tLR: 0.000337\n",
      "Training Epoch: 17 [15872/50000]\tLoss: 1.1097\tLR: 0.000337\n",
      "Training Epoch: 17 [16000/50000]\tLoss: 1.0278\tLR: 0.000337\n",
      "Training Epoch: 17 [16128/50000]\tLoss: 1.2162\tLR: 0.000337\n",
      "Training Epoch: 17 [16256/50000]\tLoss: 1.0429\tLR: 0.000337\n",
      "Training Epoch: 17 [16384/50000]\tLoss: 1.1512\tLR: 0.000337\n",
      "Training Epoch: 17 [16512/50000]\tLoss: 1.2578\tLR: 0.000337\n",
      "Training Epoch: 17 [16640/50000]\tLoss: 1.0690\tLR: 0.000337\n",
      "Training Epoch: 17 [16768/50000]\tLoss: 1.1861\tLR: 0.000337\n",
      "Training Epoch: 17 [16896/50000]\tLoss: 1.0520\tLR: 0.000337\n",
      "Training Epoch: 17 [17024/50000]\tLoss: 0.9292\tLR: 0.000337\n",
      "Training Epoch: 17 [17152/50000]\tLoss: 1.1200\tLR: 0.000337\n",
      "Training Epoch: 17 [17280/50000]\tLoss: 1.2074\tLR: 0.000337\n",
      "Training Epoch: 17 [17408/50000]\tLoss: 1.1727\tLR: 0.000337\n",
      "Training Epoch: 17 [17536/50000]\tLoss: 0.9670\tLR: 0.000337\n",
      "Training Epoch: 17 [17664/50000]\tLoss: 1.0439\tLR: 0.000337\n",
      "Training Epoch: 17 [17792/50000]\tLoss: 1.2349\tLR: 0.000337\n",
      "Training Epoch: 17 [17920/50000]\tLoss: 0.9381\tLR: 0.000337\n",
      "Training Epoch: 17 [18048/50000]\tLoss: 1.0949\tLR: 0.000337\n",
      "Training Epoch: 17 [18176/50000]\tLoss: 0.9261\tLR: 0.000337\n",
      "Training Epoch: 17 [18304/50000]\tLoss: 1.2327\tLR: 0.000337\n",
      "Training Epoch: 17 [18432/50000]\tLoss: 1.2348\tLR: 0.000337\n",
      "Training Epoch: 17 [18560/50000]\tLoss: 0.9374\tLR: 0.000337\n",
      "Training Epoch: 17 [18688/50000]\tLoss: 1.2468\tLR: 0.000337\n",
      "Training Epoch: 17 [18816/50000]\tLoss: 1.3608\tLR: 0.000337\n",
      "Training Epoch: 17 [18944/50000]\tLoss: 1.1425\tLR: 0.000337\n",
      "Training Epoch: 17 [19072/50000]\tLoss: 1.2041\tLR: 0.000337\n",
      "Training Epoch: 17 [19200/50000]\tLoss: 1.0495\tLR: 0.000337\n",
      "Training Epoch: 17 [19328/50000]\tLoss: 1.1341\tLR: 0.000337\n",
      "Training Epoch: 17 [19456/50000]\tLoss: 1.1829\tLR: 0.000337\n",
      "Training Epoch: 17 [19584/50000]\tLoss: 0.9242\tLR: 0.000337\n",
      "Training Epoch: 17 [19712/50000]\tLoss: 1.0129\tLR: 0.000337\n",
      "Training Epoch: 17 [19840/50000]\tLoss: 1.0682\tLR: 0.000337\n",
      "Training Epoch: 17 [19968/50000]\tLoss: 1.2058\tLR: 0.000337\n",
      "Training Epoch: 17 [20096/50000]\tLoss: 1.1168\tLR: 0.000337\n",
      "Training Epoch: 17 [20224/50000]\tLoss: 1.1366\tLR: 0.000337\n",
      "Training Epoch: 17 [20352/50000]\tLoss: 1.3439\tLR: 0.000337\n",
      "Training Epoch: 17 [20480/50000]\tLoss: 1.0956\tLR: 0.000337\n",
      "Training Epoch: 17 [20608/50000]\tLoss: 0.9723\tLR: 0.000337\n",
      "Training Epoch: 17 [20736/50000]\tLoss: 1.1606\tLR: 0.000337\n",
      "Training Epoch: 17 [20864/50000]\tLoss: 1.0870\tLR: 0.000337\n",
      "Training Epoch: 17 [20992/50000]\tLoss: 1.2455\tLR: 0.000337\n",
      "Training Epoch: 17 [21120/50000]\tLoss: 1.2863\tLR: 0.000337\n",
      "Training Epoch: 17 [21248/50000]\tLoss: 1.3505\tLR: 0.000337\n",
      "Training Epoch: 17 [21376/50000]\tLoss: 1.1540\tLR: 0.000337\n",
      "Training Epoch: 17 [21504/50000]\tLoss: 0.9230\tLR: 0.000337\n",
      "Training Epoch: 17 [21632/50000]\tLoss: 1.1205\tLR: 0.000337\n",
      "Training Epoch: 17 [21760/50000]\tLoss: 1.1570\tLR: 0.000337\n",
      "Training Epoch: 17 [21888/50000]\tLoss: 1.0539\tLR: 0.000337\n",
      "Training Epoch: 17 [22016/50000]\tLoss: 1.1993\tLR: 0.000337\n",
      "Training Epoch: 17 [22144/50000]\tLoss: 1.2224\tLR: 0.000337\n",
      "Training Epoch: 17 [22272/50000]\tLoss: 1.2175\tLR: 0.000337\n",
      "Training Epoch: 17 [22400/50000]\tLoss: 0.9863\tLR: 0.000337\n",
      "Training Epoch: 17 [22528/50000]\tLoss: 0.9913\tLR: 0.000337\n",
      "Training Epoch: 17 [22656/50000]\tLoss: 1.1101\tLR: 0.000337\n",
      "Training Epoch: 17 [22784/50000]\tLoss: 1.2005\tLR: 0.000337\n",
      "Training Epoch: 17 [22912/50000]\tLoss: 0.9897\tLR: 0.000337\n",
      "Training Epoch: 17 [23040/50000]\tLoss: 1.0864\tLR: 0.000337\n",
      "Training Epoch: 17 [23168/50000]\tLoss: 1.0267\tLR: 0.000337\n",
      "Training Epoch: 17 [23296/50000]\tLoss: 1.0949\tLR: 0.000337\n",
      "Training Epoch: 17 [23424/50000]\tLoss: 1.1682\tLR: 0.000337\n",
      "Training Epoch: 17 [23552/50000]\tLoss: 1.3206\tLR: 0.000337\n",
      "Training Epoch: 17 [23680/50000]\tLoss: 1.1449\tLR: 0.000337\n",
      "Training Epoch: 17 [23808/50000]\tLoss: 1.3624\tLR: 0.000337\n",
      "Training Epoch: 17 [23936/50000]\tLoss: 1.2178\tLR: 0.000337\n",
      "Training Epoch: 17 [24064/50000]\tLoss: 1.0012\tLR: 0.000337\n",
      "Training Epoch: 17 [24192/50000]\tLoss: 1.0368\tLR: 0.000337\n",
      "Training Epoch: 17 [24320/50000]\tLoss: 1.0596\tLR: 0.000337\n",
      "Training Epoch: 17 [24448/50000]\tLoss: 1.1705\tLR: 0.000337\n",
      "Training Epoch: 17 [24576/50000]\tLoss: 1.2219\tLR: 0.000337\n",
      "Training Epoch: 17 [24704/50000]\tLoss: 1.0502\tLR: 0.000337\n",
      "Training Epoch: 17 [24832/50000]\tLoss: 0.9723\tLR: 0.000337\n",
      "Training Epoch: 17 [24960/50000]\tLoss: 1.1289\tLR: 0.000337\n",
      "Training Epoch: 17 [25088/50000]\tLoss: 0.9591\tLR: 0.000337\n",
      "Training Epoch: 17 [25216/50000]\tLoss: 1.0389\tLR: 0.000337\n",
      "Training Epoch: 17 [25344/50000]\tLoss: 1.0897\tLR: 0.000337\n",
      "Training Epoch: 17 [25472/50000]\tLoss: 1.1789\tLR: 0.000337\n",
      "Training Epoch: 17 [25600/50000]\tLoss: 0.9819\tLR: 0.000337\n",
      "Training Epoch: 17 [25728/50000]\tLoss: 1.0916\tLR: 0.000337\n",
      "Training Epoch: 17 [25856/50000]\tLoss: 1.2336\tLR: 0.000337\n",
      "Training Epoch: 17 [25984/50000]\tLoss: 1.1463\tLR: 0.000337\n",
      "Training Epoch: 17 [26112/50000]\tLoss: 1.1835\tLR: 0.000337\n",
      "Training Epoch: 17 [26240/50000]\tLoss: 1.1682\tLR: 0.000337\n",
      "Training Epoch: 17 [26368/50000]\tLoss: 1.0368\tLR: 0.000337\n",
      "Training Epoch: 17 [26496/50000]\tLoss: 0.9546\tLR: 0.000337\n",
      "Training Epoch: 17 [26624/50000]\tLoss: 0.9579\tLR: 0.000337\n",
      "Training Epoch: 17 [26752/50000]\tLoss: 0.9958\tLR: 0.000337\n",
      "Training Epoch: 17 [26880/50000]\tLoss: 1.1318\tLR: 0.000337\n",
      "Training Epoch: 17 [27008/50000]\tLoss: 1.0500\tLR: 0.000337\n",
      "Training Epoch: 17 [27136/50000]\tLoss: 1.1727\tLR: 0.000337\n",
      "Training Epoch: 17 [27264/50000]\tLoss: 1.2178\tLR: 0.000337\n",
      "Training Epoch: 17 [27392/50000]\tLoss: 1.1718\tLR: 0.000337\n",
      "Training Epoch: 17 [27520/50000]\tLoss: 1.1304\tLR: 0.000337\n",
      "Training Epoch: 17 [27648/50000]\tLoss: 1.1685\tLR: 0.000337\n",
      "Training Epoch: 17 [27776/50000]\tLoss: 1.2850\tLR: 0.000337\n",
      "Training Epoch: 17 [27904/50000]\tLoss: 0.9616\tLR: 0.000337\n",
      "Training Epoch: 17 [28032/50000]\tLoss: 0.9859\tLR: 0.000337\n",
      "Training Epoch: 17 [28160/50000]\tLoss: 1.2416\tLR: 0.000337\n",
      "Training Epoch: 17 [28288/50000]\tLoss: 1.0440\tLR: 0.000337\n",
      "Training Epoch: 17 [28416/50000]\tLoss: 1.0794\tLR: 0.000337\n",
      "Training Epoch: 17 [28544/50000]\tLoss: 1.0511\tLR: 0.000337\n",
      "Training Epoch: 17 [28672/50000]\tLoss: 1.0446\tLR: 0.000337\n",
      "Training Epoch: 17 [28800/50000]\tLoss: 1.2060\tLR: 0.000337\n",
      "Training Epoch: 17 [28928/50000]\tLoss: 1.0476\tLR: 0.000337\n",
      "Training Epoch: 17 [29056/50000]\tLoss: 1.0501\tLR: 0.000337\n",
      "Training Epoch: 17 [29184/50000]\tLoss: 0.9747\tLR: 0.000337\n",
      "Training Epoch: 17 [29312/50000]\tLoss: 1.2747\tLR: 0.000337\n",
      "Training Epoch: 17 [29440/50000]\tLoss: 1.0364\tLR: 0.000337\n",
      "Training Epoch: 17 [29568/50000]\tLoss: 1.1986\tLR: 0.000337\n",
      "Training Epoch: 17 [29696/50000]\tLoss: 1.0403\tLR: 0.000337\n",
      "Training Epoch: 17 [29824/50000]\tLoss: 1.1100\tLR: 0.000337\n",
      "Training Epoch: 17 [29952/50000]\tLoss: 1.0563\tLR: 0.000337\n",
      "Training Epoch: 17 [30080/50000]\tLoss: 0.9299\tLR: 0.000337\n",
      "Training Epoch: 17 [30208/50000]\tLoss: 1.2334\tLR: 0.000337\n",
      "Training Epoch: 17 [30336/50000]\tLoss: 1.0190\tLR: 0.000337\n",
      "Training Epoch: 17 [30464/50000]\tLoss: 1.0382\tLR: 0.000337\n",
      "Training Epoch: 17 [30592/50000]\tLoss: 1.1821\tLR: 0.000337\n",
      "Training Epoch: 17 [30720/50000]\tLoss: 1.2594\tLR: 0.000337\n",
      "Training Epoch: 17 [30848/50000]\tLoss: 1.1103\tLR: 0.000337\n",
      "Training Epoch: 17 [30976/50000]\tLoss: 1.1389\tLR: 0.000337\n",
      "Training Epoch: 17 [31104/50000]\tLoss: 1.0121\tLR: 0.000337\n",
      "Training Epoch: 17 [31232/50000]\tLoss: 1.1845\tLR: 0.000337\n",
      "Training Epoch: 17 [31360/50000]\tLoss: 1.1949\tLR: 0.000337\n",
      "Training Epoch: 17 [31488/50000]\tLoss: 1.0759\tLR: 0.000337\n",
      "Training Epoch: 17 [31616/50000]\tLoss: 1.2642\tLR: 0.000337\n",
      "Training Epoch: 17 [31744/50000]\tLoss: 1.1728\tLR: 0.000337\n",
      "Training Epoch: 17 [31872/50000]\tLoss: 1.0738\tLR: 0.000337\n",
      "Training Epoch: 17 [32000/50000]\tLoss: 1.2205\tLR: 0.000337\n",
      "Training Epoch: 17 [32128/50000]\tLoss: 1.0451\tLR: 0.000337\n",
      "Training Epoch: 17 [32256/50000]\tLoss: 1.0063\tLR: 0.000337\n",
      "Training Epoch: 17 [32384/50000]\tLoss: 0.9951\tLR: 0.000337\n",
      "Training Epoch: 17 [32512/50000]\tLoss: 1.0699\tLR: 0.000337\n",
      "Training Epoch: 17 [32640/50000]\tLoss: 1.1023\tLR: 0.000337\n",
      "Training Epoch: 17 [32768/50000]\tLoss: 0.9023\tLR: 0.000337\n",
      "Training Epoch: 17 [32896/50000]\tLoss: 1.0803\tLR: 0.000337\n",
      "Training Epoch: 17 [33024/50000]\tLoss: 1.0554\tLR: 0.000337\n",
      "Training Epoch: 17 [33152/50000]\tLoss: 1.1075\tLR: 0.000337\n",
      "Training Epoch: 17 [33280/50000]\tLoss: 1.1323\tLR: 0.000337\n",
      "Training Epoch: 17 [33408/50000]\tLoss: 1.3920\tLR: 0.000337\n",
      "Training Epoch: 17 [33536/50000]\tLoss: 0.9607\tLR: 0.000337\n",
      "Training Epoch: 17 [33664/50000]\tLoss: 1.0636\tLR: 0.000337\n",
      "Training Epoch: 17 [33792/50000]\tLoss: 1.2154\tLR: 0.000337\n",
      "Training Epoch: 17 [33920/50000]\tLoss: 1.1193\tLR: 0.000337\n",
      "Training Epoch: 17 [34048/50000]\tLoss: 1.2796\tLR: 0.000337\n",
      "Training Epoch: 17 [34176/50000]\tLoss: 1.5102\tLR: 0.000337\n",
      "Training Epoch: 17 [34304/50000]\tLoss: 1.0915\tLR: 0.000337\n",
      "Training Epoch: 17 [34432/50000]\tLoss: 1.0777\tLR: 0.000337\n",
      "Training Epoch: 17 [34560/50000]\tLoss: 1.1665\tLR: 0.000337\n",
      "Training Epoch: 17 [34688/50000]\tLoss: 1.1377\tLR: 0.000337\n",
      "Training Epoch: 17 [34816/50000]\tLoss: 1.2445\tLR: 0.000337\n",
      "Training Epoch: 17 [34944/50000]\tLoss: 0.9903\tLR: 0.000337\n",
      "Training Epoch: 17 [35072/50000]\tLoss: 1.0110\tLR: 0.000337\n",
      "Training Epoch: 17 [35200/50000]\tLoss: 1.0241\tLR: 0.000337\n",
      "Training Epoch: 17 [35328/50000]\tLoss: 1.3017\tLR: 0.000337\n",
      "Training Epoch: 17 [35456/50000]\tLoss: 0.9938\tLR: 0.000337\n",
      "Training Epoch: 17 [35584/50000]\tLoss: 1.0952\tLR: 0.000337\n",
      "Training Epoch: 17 [35712/50000]\tLoss: 0.9862\tLR: 0.000337\n",
      "Training Epoch: 17 [35840/50000]\tLoss: 1.1932\tLR: 0.000337\n",
      "Training Epoch: 17 [35968/50000]\tLoss: 1.0776\tLR: 0.000337\n",
      "Training Epoch: 17 [36096/50000]\tLoss: 0.9592\tLR: 0.000337\n",
      "Training Epoch: 17 [36224/50000]\tLoss: 1.2397\tLR: 0.000337\n",
      "Training Epoch: 17 [36352/50000]\tLoss: 1.0109\tLR: 0.000337\n",
      "Training Epoch: 17 [36480/50000]\tLoss: 1.0735\tLR: 0.000337\n",
      "Training Epoch: 17 [36608/50000]\tLoss: 1.2553\tLR: 0.000337\n",
      "Training Epoch: 17 [36736/50000]\tLoss: 0.9915\tLR: 0.000337\n",
      "Training Epoch: 17 [36864/50000]\tLoss: 1.0636\tLR: 0.000337\n",
      "Training Epoch: 17 [36992/50000]\tLoss: 1.2537\tLR: 0.000337\n",
      "Training Epoch: 17 [37120/50000]\tLoss: 1.1806\tLR: 0.000337\n",
      "Training Epoch: 17 [37248/50000]\tLoss: 1.2109\tLR: 0.000337\n",
      "Training Epoch: 17 [37376/50000]\tLoss: 1.3209\tLR: 0.000337\n",
      "Training Epoch: 17 [37504/50000]\tLoss: 1.0152\tLR: 0.000337\n",
      "Training Epoch: 17 [37632/50000]\tLoss: 0.9857\tLR: 0.000337\n",
      "Training Epoch: 17 [37760/50000]\tLoss: 1.2130\tLR: 0.000337\n",
      "Training Epoch: 17 [37888/50000]\tLoss: 1.3134\tLR: 0.000337\n",
      "Training Epoch: 17 [38016/50000]\tLoss: 0.9749\tLR: 0.000337\n",
      "Training Epoch: 17 [38144/50000]\tLoss: 1.0084\tLR: 0.000337\n",
      "Training Epoch: 17 [38272/50000]\tLoss: 1.1683\tLR: 0.000337\n",
      "Training Epoch: 17 [38400/50000]\tLoss: 0.9201\tLR: 0.000337\n",
      "Training Epoch: 17 [38528/50000]\tLoss: 1.0933\tLR: 0.000337\n",
      "Training Epoch: 17 [38656/50000]\tLoss: 1.1490\tLR: 0.000337\n",
      "Training Epoch: 17 [38784/50000]\tLoss: 0.9757\tLR: 0.000337\n",
      "Training Epoch: 17 [38912/50000]\tLoss: 1.0829\tLR: 0.000337\n",
      "Training Epoch: 17 [39040/50000]\tLoss: 0.9511\tLR: 0.000337\n",
      "Training Epoch: 17 [39168/50000]\tLoss: 1.1747\tLR: 0.000337\n",
      "Training Epoch: 17 [39296/50000]\tLoss: 1.3062\tLR: 0.000337\n",
      "Training Epoch: 17 [39424/50000]\tLoss: 1.2101\tLR: 0.000337\n",
      "Training Epoch: 17 [39552/50000]\tLoss: 1.1867\tLR: 0.000337\n",
      "Training Epoch: 17 [39680/50000]\tLoss: 0.9361\tLR: 0.000337\n",
      "Training Epoch: 17 [39808/50000]\tLoss: 1.0548\tLR: 0.000337\n",
      "Training Epoch: 17 [39936/50000]\tLoss: 0.9717\tLR: 0.000337\n",
      "Training Epoch: 17 [40064/50000]\tLoss: 1.0131\tLR: 0.000337\n",
      "Training Epoch: 17 [40192/50000]\tLoss: 1.1122\tLR: 0.000337\n",
      "Training Epoch: 17 [40320/50000]\tLoss: 0.9867\tLR: 0.000337\n",
      "Training Epoch: 17 [40448/50000]\tLoss: 1.1157\tLR: 0.000337\n",
      "Training Epoch: 17 [40576/50000]\tLoss: 1.0763\tLR: 0.000337\n",
      "Training Epoch: 17 [40704/50000]\tLoss: 1.2016\tLR: 0.000337\n",
      "Training Epoch: 17 [40832/50000]\tLoss: 1.2855\tLR: 0.000337\n",
      "Training Epoch: 17 [40960/50000]\tLoss: 1.1556\tLR: 0.000337\n",
      "Training Epoch: 17 [41088/50000]\tLoss: 1.0916\tLR: 0.000337\n",
      "Training Epoch: 17 [41216/50000]\tLoss: 1.1979\tLR: 0.000337\n",
      "Training Epoch: 17 [41344/50000]\tLoss: 0.9995\tLR: 0.000337\n",
      "Training Epoch: 17 [41472/50000]\tLoss: 0.9472\tLR: 0.000337\n",
      "Training Epoch: 17 [41600/50000]\tLoss: 1.2325\tLR: 0.000337\n",
      "Training Epoch: 17 [41728/50000]\tLoss: 1.0701\tLR: 0.000337\n",
      "Training Epoch: 17 [41856/50000]\tLoss: 0.8012\tLR: 0.000337\n",
      "Training Epoch: 17 [41984/50000]\tLoss: 1.3185\tLR: 0.000337\n",
      "Training Epoch: 17 [42112/50000]\tLoss: 1.0896\tLR: 0.000337\n",
      "Training Epoch: 17 [42240/50000]\tLoss: 0.9498\tLR: 0.000337\n",
      "Training Epoch: 17 [42368/50000]\tLoss: 0.9260\tLR: 0.000337\n",
      "Training Epoch: 17 [42496/50000]\tLoss: 1.2623\tLR: 0.000337\n",
      "Training Epoch: 17 [42624/50000]\tLoss: 0.9349\tLR: 0.000337\n",
      "Training Epoch: 17 [42752/50000]\tLoss: 1.0741\tLR: 0.000337\n",
      "Training Epoch: 17 [42880/50000]\tLoss: 1.1198\tLR: 0.000337\n",
      "Training Epoch: 17 [43008/50000]\tLoss: 1.2821\tLR: 0.000337\n",
      "Training Epoch: 17 [43136/50000]\tLoss: 1.2205\tLR: 0.000337\n",
      "Training Epoch: 17 [43264/50000]\tLoss: 1.2034\tLR: 0.000337\n",
      "Training Epoch: 17 [43392/50000]\tLoss: 1.1289\tLR: 0.000337\n",
      "Training Epoch: 17 [43520/50000]\tLoss: 1.1608\tLR: 0.000337\n",
      "Training Epoch: 17 [43648/50000]\tLoss: 1.0502\tLR: 0.000337\n",
      "Training Epoch: 17 [43776/50000]\tLoss: 1.2240\tLR: 0.000337\n",
      "Training Epoch: 17 [43904/50000]\tLoss: 1.0907\tLR: 0.000337\n",
      "Training Epoch: 17 [44032/50000]\tLoss: 1.1153\tLR: 0.000337\n",
      "Training Epoch: 17 [44160/50000]\tLoss: 1.3779\tLR: 0.000337\n",
      "Training Epoch: 17 [44288/50000]\tLoss: 1.0911\tLR: 0.000337\n",
      "Training Epoch: 17 [44416/50000]\tLoss: 1.1023\tLR: 0.000337\n",
      "Training Epoch: 17 [44544/50000]\tLoss: 0.9992\tLR: 0.000337\n",
      "Training Epoch: 17 [44672/50000]\tLoss: 1.0199\tLR: 0.000337\n",
      "Training Epoch: 17 [44800/50000]\tLoss: 1.1436\tLR: 0.000337\n",
      "Training Epoch: 17 [44928/50000]\tLoss: 1.3441\tLR: 0.000337\n",
      "Training Epoch: 17 [45056/50000]\tLoss: 1.1266\tLR: 0.000337\n",
      "Training Epoch: 17 [45184/50000]\tLoss: 0.9804\tLR: 0.000337\n",
      "Training Epoch: 17 [45312/50000]\tLoss: 1.1165\tLR: 0.000337\n",
      "Training Epoch: 17 [45440/50000]\tLoss: 1.1104\tLR: 0.000337\n",
      "Training Epoch: 17 [45568/50000]\tLoss: 0.8964\tLR: 0.000337\n",
      "Training Epoch: 17 [45696/50000]\tLoss: 1.2661\tLR: 0.000337\n",
      "Training Epoch: 17 [45824/50000]\tLoss: 0.9252\tLR: 0.000337\n",
      "Training Epoch: 17 [45952/50000]\tLoss: 1.1248\tLR: 0.000337\n",
      "Training Epoch: 17 [46080/50000]\tLoss: 1.1006\tLR: 0.000337\n",
      "Training Epoch: 17 [46208/50000]\tLoss: 0.9518\tLR: 0.000337\n",
      "Training Epoch: 17 [46336/50000]\tLoss: 1.2129\tLR: 0.000337\n",
      "Training Epoch: 17 [46464/50000]\tLoss: 0.9706\tLR: 0.000337\n",
      "Training Epoch: 17 [46592/50000]\tLoss: 1.1562\tLR: 0.000337\n",
      "Training Epoch: 17 [46720/50000]\tLoss: 1.1629\tLR: 0.000337\n",
      "Training Epoch: 17 [46848/50000]\tLoss: 1.1063\tLR: 0.000337\n",
      "Training Epoch: 17 [46976/50000]\tLoss: 1.2965\tLR: 0.000337\n",
      "Training Epoch: 17 [47104/50000]\tLoss: 1.2352\tLR: 0.000337\n",
      "Training Epoch: 17 [47232/50000]\tLoss: 1.2012\tLR: 0.000337\n",
      "Training Epoch: 17 [47360/50000]\tLoss: 1.1107\tLR: 0.000337\n",
      "Training Epoch: 17 [47488/50000]\tLoss: 1.2917\tLR: 0.000337\n",
      "Training Epoch: 17 [47616/50000]\tLoss: 1.0757\tLR: 0.000337\n",
      "Training Epoch: 17 [47744/50000]\tLoss: 1.3066\tLR: 0.000337\n",
      "Training Epoch: 17 [47872/50000]\tLoss: 0.9394\tLR: 0.000337\n",
      "Training Epoch: 17 [48000/50000]\tLoss: 0.9982\tLR: 0.000337\n",
      "Training Epoch: 17 [48128/50000]\tLoss: 1.0994\tLR: 0.000337\n",
      "Training Epoch: 17 [48256/50000]\tLoss: 0.9383\tLR: 0.000337\n",
      "Training Epoch: 17 [48384/50000]\tLoss: 1.1109\tLR: 0.000337\n",
      "Training Epoch: 17 [48512/50000]\tLoss: 1.3271\tLR: 0.000337\n",
      "Training Epoch: 17 [48640/50000]\tLoss: 1.2932\tLR: 0.000337\n",
      "Training Epoch: 17 [48768/50000]\tLoss: 1.2288\tLR: 0.000337\n",
      "Training Epoch: 17 [48896/50000]\tLoss: 1.1280\tLR: 0.000337\n",
      "Training Epoch: 17 [49024/50000]\tLoss: 1.3041\tLR: 0.000337\n",
      "Training Epoch: 17 [49152/50000]\tLoss: 1.1879\tLR: 0.000337\n",
      "Training Epoch: 17 [49280/50000]\tLoss: 1.1842\tLR: 0.000337\n",
      "Training Epoch: 17 [49408/50000]\tLoss: 1.1880\tLR: 0.000337\n",
      "Training Epoch: 17 [49536/50000]\tLoss: 1.1117\tLR: 0.000337\n",
      "Training Epoch: 17 [49664/50000]\tLoss: 1.2368\tLR: 0.000337\n",
      "Training Epoch: 17 [49792/50000]\tLoss: 1.0715\tLR: 0.000337\n",
      "Training Epoch: 17 [49920/50000]\tLoss: 1.0150\tLR: 0.000337\n",
      "Training Epoch: 17 [50000/50000]\tLoss: 1.3626\tLR: 0.000337\n",
      "Test set: Average loss: 0.0104, Accuracy: 0.6278\n",
      "\n",
      "Training Epoch: 18 [128/50000]\tLoss: 1.1690\tLR: 0.000337\n",
      "Training Epoch: 18 [256/50000]\tLoss: 1.1908\tLR: 0.000337\n",
      "Training Epoch: 18 [384/50000]\tLoss: 1.1301\tLR: 0.000337\n",
      "Training Epoch: 18 [512/50000]\tLoss: 1.1257\tLR: 0.000337\n",
      "Training Epoch: 18 [640/50000]\tLoss: 0.9357\tLR: 0.000337\n",
      "Training Epoch: 18 [768/50000]\tLoss: 1.1404\tLR: 0.000337\n",
      "Training Epoch: 18 [896/50000]\tLoss: 1.1768\tLR: 0.000337\n",
      "Training Epoch: 18 [1024/50000]\tLoss: 1.1813\tLR: 0.000337\n",
      "Training Epoch: 18 [1152/50000]\tLoss: 1.2799\tLR: 0.000337\n",
      "Training Epoch: 18 [1280/50000]\tLoss: 1.0970\tLR: 0.000337\n",
      "Training Epoch: 18 [1408/50000]\tLoss: 1.2539\tLR: 0.000337\n",
      "Training Epoch: 18 [1536/50000]\tLoss: 1.0443\tLR: 0.000337\n",
      "Training Epoch: 18 [1664/50000]\tLoss: 1.2573\tLR: 0.000337\n",
      "Training Epoch: 18 [1792/50000]\tLoss: 1.0227\tLR: 0.000337\n",
      "Training Epoch: 18 [1920/50000]\tLoss: 1.3331\tLR: 0.000337\n",
      "Training Epoch: 18 [2048/50000]\tLoss: 0.9959\tLR: 0.000337\n",
      "Training Epoch: 18 [2176/50000]\tLoss: 1.1810\tLR: 0.000337\n",
      "Training Epoch: 18 [2304/50000]\tLoss: 1.0325\tLR: 0.000337\n",
      "Training Epoch: 18 [2432/50000]\tLoss: 1.1386\tLR: 0.000337\n",
      "Training Epoch: 18 [2560/50000]\tLoss: 0.8564\tLR: 0.000337\n",
      "Training Epoch: 18 [2688/50000]\tLoss: 1.0128\tLR: 0.000337\n",
      "Training Epoch: 18 [2816/50000]\tLoss: 1.2197\tLR: 0.000337\n",
      "Training Epoch: 18 [2944/50000]\tLoss: 1.0152\tLR: 0.000337\n",
      "Training Epoch: 18 [3072/50000]\tLoss: 1.1167\tLR: 0.000337\n",
      "Training Epoch: 18 [3200/50000]\tLoss: 1.1011\tLR: 0.000337\n",
      "Training Epoch: 18 [3328/50000]\tLoss: 1.2694\tLR: 0.000337\n",
      "Training Epoch: 18 [3456/50000]\tLoss: 1.1687\tLR: 0.000337\n",
      "Training Epoch: 18 [3584/50000]\tLoss: 1.2024\tLR: 0.000337\n",
      "Training Epoch: 18 [3712/50000]\tLoss: 1.2328\tLR: 0.000337\n",
      "Training Epoch: 18 [3840/50000]\tLoss: 1.1493\tLR: 0.000337\n",
      "Training Epoch: 18 [3968/50000]\tLoss: 1.0758\tLR: 0.000337\n",
      "Training Epoch: 18 [4096/50000]\tLoss: 1.0392\tLR: 0.000337\n",
      "Training Epoch: 18 [4224/50000]\tLoss: 0.8878\tLR: 0.000337\n",
      "Training Epoch: 18 [4352/50000]\tLoss: 1.0910\tLR: 0.000337\n",
      "Training Epoch: 18 [4480/50000]\tLoss: 1.1312\tLR: 0.000337\n",
      "Training Epoch: 18 [4608/50000]\tLoss: 1.1329\tLR: 0.000337\n",
      "Training Epoch: 18 [4736/50000]\tLoss: 1.2212\tLR: 0.000337\n",
      "Training Epoch: 18 [4864/50000]\tLoss: 1.0934\tLR: 0.000337\n",
      "Training Epoch: 18 [4992/50000]\tLoss: 1.4479\tLR: 0.000337\n",
      "Training Epoch: 18 [5120/50000]\tLoss: 1.1912\tLR: 0.000337\n",
      "Training Epoch: 18 [5248/50000]\tLoss: 1.2835\tLR: 0.000337\n",
      "Training Epoch: 18 [5376/50000]\tLoss: 1.0959\tLR: 0.000337\n",
      "Training Epoch: 18 [5504/50000]\tLoss: 1.0148\tLR: 0.000337\n",
      "Training Epoch: 18 [5632/50000]\tLoss: 0.8925\tLR: 0.000337\n",
      "Training Epoch: 18 [5760/50000]\tLoss: 1.0732\tLR: 0.000337\n",
      "Training Epoch: 18 [5888/50000]\tLoss: 1.2380\tLR: 0.000337\n",
      "Training Epoch: 18 [6016/50000]\tLoss: 1.0991\tLR: 0.000337\n",
      "Training Epoch: 18 [6144/50000]\tLoss: 1.2343\tLR: 0.000337\n",
      "Training Epoch: 18 [6272/50000]\tLoss: 1.3162\tLR: 0.000337\n",
      "Training Epoch: 18 [6400/50000]\tLoss: 1.1295\tLR: 0.000337\n",
      "Training Epoch: 18 [6528/50000]\tLoss: 0.9329\tLR: 0.000337\n",
      "Training Epoch: 18 [6656/50000]\tLoss: 0.8909\tLR: 0.000337\n",
      "Training Epoch: 18 [6784/50000]\tLoss: 0.9560\tLR: 0.000337\n",
      "Training Epoch: 18 [6912/50000]\tLoss: 1.2997\tLR: 0.000337\n",
      "Training Epoch: 18 [7040/50000]\tLoss: 0.9945\tLR: 0.000337\n",
      "Training Epoch: 18 [7168/50000]\tLoss: 1.2350\tLR: 0.000337\n",
      "Training Epoch: 18 [7296/50000]\tLoss: 1.0779\tLR: 0.000337\n",
      "Training Epoch: 18 [7424/50000]\tLoss: 0.9000\tLR: 0.000337\n",
      "Training Epoch: 18 [7552/50000]\tLoss: 0.9809\tLR: 0.000337\n",
      "Training Epoch: 18 [7680/50000]\tLoss: 0.9893\tLR: 0.000337\n",
      "Training Epoch: 18 [7808/50000]\tLoss: 1.1239\tLR: 0.000337\n",
      "Training Epoch: 18 [7936/50000]\tLoss: 1.1992\tLR: 0.000337\n",
      "Training Epoch: 18 [8064/50000]\tLoss: 0.9344\tLR: 0.000337\n",
      "Training Epoch: 18 [8192/50000]\tLoss: 1.0687\tLR: 0.000337\n",
      "Training Epoch: 18 [8320/50000]\tLoss: 1.1569\tLR: 0.000337\n",
      "Training Epoch: 18 [8448/50000]\tLoss: 0.9386\tLR: 0.000337\n",
      "Training Epoch: 18 [8576/50000]\tLoss: 1.3039\tLR: 0.000337\n",
      "Training Epoch: 18 [8704/50000]\tLoss: 1.2109\tLR: 0.000337\n",
      "Training Epoch: 18 [8832/50000]\tLoss: 0.9183\tLR: 0.000337\n",
      "Training Epoch: 18 [8960/50000]\tLoss: 0.8867\tLR: 0.000337\n",
      "Training Epoch: 18 [9088/50000]\tLoss: 1.2492\tLR: 0.000337\n",
      "Training Epoch: 18 [9216/50000]\tLoss: 0.9843\tLR: 0.000337\n",
      "Training Epoch: 18 [9344/50000]\tLoss: 1.0020\tLR: 0.000337\n",
      "Training Epoch: 18 [9472/50000]\tLoss: 1.0228\tLR: 0.000337\n",
      "Training Epoch: 18 [9600/50000]\tLoss: 1.0252\tLR: 0.000337\n",
      "Training Epoch: 18 [9728/50000]\tLoss: 1.1770\tLR: 0.000337\n",
      "Training Epoch: 18 [9856/50000]\tLoss: 1.2858\tLR: 0.000337\n",
      "Training Epoch: 18 [9984/50000]\tLoss: 0.9460\tLR: 0.000337\n",
      "Training Epoch: 18 [10112/50000]\tLoss: 1.0574\tLR: 0.000337\n",
      "Training Epoch: 18 [10240/50000]\tLoss: 1.2115\tLR: 0.000337\n",
      "Training Epoch: 18 [10368/50000]\tLoss: 1.1215\tLR: 0.000337\n",
      "Training Epoch: 18 [10496/50000]\tLoss: 1.2017\tLR: 0.000337\n",
      "Training Epoch: 18 [10624/50000]\tLoss: 1.2527\tLR: 0.000337\n",
      "Training Epoch: 18 [10752/50000]\tLoss: 1.1610\tLR: 0.000337\n",
      "Training Epoch: 18 [10880/50000]\tLoss: 1.0847\tLR: 0.000337\n",
      "Training Epoch: 18 [11008/50000]\tLoss: 1.0931\tLR: 0.000337\n",
      "Training Epoch: 18 [11136/50000]\tLoss: 1.0966\tLR: 0.000337\n",
      "Training Epoch: 18 [11264/50000]\tLoss: 1.1673\tLR: 0.000337\n",
      "Training Epoch: 18 [11392/50000]\tLoss: 1.2195\tLR: 0.000337\n",
      "Training Epoch: 18 [11520/50000]\tLoss: 1.2583\tLR: 0.000337\n",
      "Training Epoch: 18 [11648/50000]\tLoss: 0.8740\tLR: 0.000337\n",
      "Training Epoch: 18 [11776/50000]\tLoss: 1.1185\tLR: 0.000337\n",
      "Training Epoch: 18 [11904/50000]\tLoss: 1.1071\tLR: 0.000337\n",
      "Training Epoch: 18 [12032/50000]\tLoss: 0.9147\tLR: 0.000337\n",
      "Training Epoch: 18 [12160/50000]\tLoss: 0.9794\tLR: 0.000337\n",
      "Training Epoch: 18 [12288/50000]\tLoss: 1.0059\tLR: 0.000337\n",
      "Training Epoch: 18 [12416/50000]\tLoss: 1.2667\tLR: 0.000337\n",
      "Training Epoch: 18 [12544/50000]\tLoss: 1.0799\tLR: 0.000337\n",
      "Training Epoch: 18 [12672/50000]\tLoss: 0.9755\tLR: 0.000337\n",
      "Training Epoch: 18 [12800/50000]\tLoss: 1.1067\tLR: 0.000337\n",
      "Training Epoch: 18 [12928/50000]\tLoss: 1.2135\tLR: 0.000337\n",
      "Training Epoch: 18 [13056/50000]\tLoss: 1.1237\tLR: 0.000337\n",
      "Training Epoch: 18 [13184/50000]\tLoss: 1.0327\tLR: 0.000337\n",
      "Training Epoch: 18 [13312/50000]\tLoss: 0.8848\tLR: 0.000337\n",
      "Training Epoch: 18 [13440/50000]\tLoss: 1.0218\tLR: 0.000337\n",
      "Training Epoch: 18 [13568/50000]\tLoss: 1.1518\tLR: 0.000337\n",
      "Training Epoch: 18 [13696/50000]\tLoss: 1.0006\tLR: 0.000337\n",
      "Training Epoch: 18 [13824/50000]\tLoss: 1.1233\tLR: 0.000337\n",
      "Training Epoch: 18 [13952/50000]\tLoss: 1.1031\tLR: 0.000337\n",
      "Training Epoch: 18 [14080/50000]\tLoss: 0.9995\tLR: 0.000337\n",
      "Training Epoch: 18 [14208/50000]\tLoss: 1.1010\tLR: 0.000337\n",
      "Training Epoch: 18 [14336/50000]\tLoss: 1.1612\tLR: 0.000337\n",
      "Training Epoch: 18 [14464/50000]\tLoss: 1.2701\tLR: 0.000337\n",
      "Training Epoch: 18 [14592/50000]\tLoss: 0.8908\tLR: 0.000337\n",
      "Training Epoch: 18 [14720/50000]\tLoss: 1.1735\tLR: 0.000337\n",
      "Training Epoch: 18 [14848/50000]\tLoss: 1.3118\tLR: 0.000337\n",
      "Training Epoch: 18 [14976/50000]\tLoss: 1.0276\tLR: 0.000337\n",
      "Training Epoch: 18 [15104/50000]\tLoss: 0.9164\tLR: 0.000337\n",
      "Training Epoch: 18 [15232/50000]\tLoss: 1.3200\tLR: 0.000337\n",
      "Training Epoch: 18 [15360/50000]\tLoss: 1.0448\tLR: 0.000337\n",
      "Training Epoch: 18 [15488/50000]\tLoss: 1.2931\tLR: 0.000337\n",
      "Training Epoch: 18 [15616/50000]\tLoss: 1.1293\tLR: 0.000337\n",
      "Training Epoch: 18 [15744/50000]\tLoss: 0.9789\tLR: 0.000337\n",
      "Training Epoch: 18 [15872/50000]\tLoss: 1.0709\tLR: 0.000337\n",
      "Training Epoch: 18 [16000/50000]\tLoss: 0.9428\tLR: 0.000337\n",
      "Training Epoch: 18 [16128/50000]\tLoss: 1.0548\tLR: 0.000337\n",
      "Training Epoch: 18 [16256/50000]\tLoss: 0.9387\tLR: 0.000337\n",
      "Training Epoch: 18 [16384/50000]\tLoss: 1.1310\tLR: 0.000337\n",
      "Training Epoch: 18 [16512/50000]\tLoss: 1.2234\tLR: 0.000337\n",
      "Training Epoch: 18 [16640/50000]\tLoss: 1.1127\tLR: 0.000337\n",
      "Training Epoch: 18 [16768/50000]\tLoss: 1.1396\tLR: 0.000337\n",
      "Training Epoch: 18 [16896/50000]\tLoss: 1.1960\tLR: 0.000337\n",
      "Training Epoch: 18 [17024/50000]\tLoss: 1.0905\tLR: 0.000337\n",
      "Training Epoch: 18 [17152/50000]\tLoss: 1.0418\tLR: 0.000337\n",
      "Training Epoch: 18 [17280/50000]\tLoss: 1.1589\tLR: 0.000337\n",
      "Training Epoch: 18 [17408/50000]\tLoss: 0.9477\tLR: 0.000337\n",
      "Training Epoch: 18 [17536/50000]\tLoss: 1.1114\tLR: 0.000337\n",
      "Training Epoch: 18 [17664/50000]\tLoss: 0.9136\tLR: 0.000337\n",
      "Training Epoch: 18 [17792/50000]\tLoss: 1.1013\tLR: 0.000337\n",
      "Training Epoch: 18 [17920/50000]\tLoss: 1.1631\tLR: 0.000337\n",
      "Training Epoch: 18 [18048/50000]\tLoss: 0.9966\tLR: 0.000337\n",
      "Training Epoch: 18 [18176/50000]\tLoss: 0.9953\tLR: 0.000337\n",
      "Training Epoch: 18 [18304/50000]\tLoss: 1.2390\tLR: 0.000337\n",
      "Training Epoch: 18 [18432/50000]\tLoss: 1.0414\tLR: 0.000337\n",
      "Training Epoch: 18 [18560/50000]\tLoss: 1.0878\tLR: 0.000337\n",
      "Training Epoch: 18 [18688/50000]\tLoss: 1.1643\tLR: 0.000337\n",
      "Training Epoch: 18 [18816/50000]\tLoss: 1.2702\tLR: 0.000337\n",
      "Training Epoch: 18 [18944/50000]\tLoss: 1.0566\tLR: 0.000337\n",
      "Training Epoch: 18 [19072/50000]\tLoss: 1.0555\tLR: 0.000337\n",
      "Training Epoch: 18 [19200/50000]\tLoss: 1.2756\tLR: 0.000337\n",
      "Training Epoch: 18 [19328/50000]\tLoss: 1.2776\tLR: 0.000337\n",
      "Training Epoch: 18 [19456/50000]\tLoss: 1.2038\tLR: 0.000337\n",
      "Training Epoch: 18 [19584/50000]\tLoss: 1.1743\tLR: 0.000337\n",
      "Training Epoch: 18 [19712/50000]\tLoss: 1.4167\tLR: 0.000337\n",
      "Training Epoch: 18 [19840/50000]\tLoss: 1.0544\tLR: 0.000337\n",
      "Training Epoch: 18 [19968/50000]\tLoss: 1.0837\tLR: 0.000337\n",
      "Training Epoch: 18 [20096/50000]\tLoss: 1.3449\tLR: 0.000337\n",
      "Training Epoch: 18 [20224/50000]\tLoss: 0.9756\tLR: 0.000337\n",
      "Training Epoch: 18 [20352/50000]\tLoss: 1.0866\tLR: 0.000337\n",
      "Training Epoch: 18 [20480/50000]\tLoss: 1.1057\tLR: 0.000337\n",
      "Training Epoch: 18 [20608/50000]\tLoss: 0.9922\tLR: 0.000337\n",
      "Training Epoch: 18 [20736/50000]\tLoss: 1.2899\tLR: 0.000337\n",
      "Training Epoch: 18 [20864/50000]\tLoss: 1.2654\tLR: 0.000337\n",
      "Training Epoch: 18 [20992/50000]\tLoss: 1.0645\tLR: 0.000337\n",
      "Training Epoch: 18 [21120/50000]\tLoss: 1.1091\tLR: 0.000337\n",
      "Training Epoch: 18 [21248/50000]\tLoss: 1.0030\tLR: 0.000337\n",
      "Training Epoch: 18 [21376/50000]\tLoss: 1.2086\tLR: 0.000337\n",
      "Training Epoch: 18 [21504/50000]\tLoss: 1.1179\tLR: 0.000337\n",
      "Training Epoch: 18 [21632/50000]\tLoss: 1.2547\tLR: 0.000337\n",
      "Training Epoch: 18 [21760/50000]\tLoss: 1.0746\tLR: 0.000337\n",
      "Training Epoch: 18 [21888/50000]\tLoss: 1.1002\tLR: 0.000337\n",
      "Training Epoch: 18 [22016/50000]\tLoss: 0.9762\tLR: 0.000337\n",
      "Training Epoch: 18 [22144/50000]\tLoss: 1.0583\tLR: 0.000337\n",
      "Training Epoch: 18 [22272/50000]\tLoss: 1.1141\tLR: 0.000337\n",
      "Training Epoch: 18 [22400/50000]\tLoss: 1.3062\tLR: 0.000337\n",
      "Training Epoch: 18 [22528/50000]\tLoss: 1.0055\tLR: 0.000337\n",
      "Training Epoch: 18 [22656/50000]\tLoss: 1.2349\tLR: 0.000337\n",
      "Training Epoch: 18 [22784/50000]\tLoss: 0.9854\tLR: 0.000337\n",
      "Training Epoch: 18 [22912/50000]\tLoss: 0.9282\tLR: 0.000337\n",
      "Training Epoch: 18 [23040/50000]\tLoss: 1.0336\tLR: 0.000337\n",
      "Training Epoch: 18 [23168/50000]\tLoss: 1.1335\tLR: 0.000337\n",
      "Training Epoch: 18 [23296/50000]\tLoss: 1.2377\tLR: 0.000337\n",
      "Training Epoch: 18 [23424/50000]\tLoss: 1.2540\tLR: 0.000337\n",
      "Training Epoch: 18 [23552/50000]\tLoss: 1.1329\tLR: 0.000337\n",
      "Training Epoch: 18 [23680/50000]\tLoss: 1.3490\tLR: 0.000337\n",
      "Training Epoch: 18 [23808/50000]\tLoss: 1.2368\tLR: 0.000337\n",
      "Training Epoch: 18 [23936/50000]\tLoss: 1.1533\tLR: 0.000337\n",
      "Training Epoch: 18 [24064/50000]\tLoss: 1.2067\tLR: 0.000337\n",
      "Training Epoch: 18 [24192/50000]\tLoss: 1.1921\tLR: 0.000337\n",
      "Training Epoch: 18 [24320/50000]\tLoss: 1.0236\tLR: 0.000337\n",
      "Training Epoch: 18 [24448/50000]\tLoss: 1.0867\tLR: 0.000337\n",
      "Training Epoch: 18 [24576/50000]\tLoss: 1.1215\tLR: 0.000337\n",
      "Training Epoch: 18 [24704/50000]\tLoss: 1.1241\tLR: 0.000337\n",
      "Training Epoch: 18 [24832/50000]\tLoss: 1.1286\tLR: 0.000337\n",
      "Training Epoch: 18 [24960/50000]\tLoss: 0.9417\tLR: 0.000337\n",
      "Training Epoch: 18 [25088/50000]\tLoss: 1.0916\tLR: 0.000337\n",
      "Training Epoch: 18 [25216/50000]\tLoss: 1.1673\tLR: 0.000337\n",
      "Training Epoch: 18 [25344/50000]\tLoss: 1.1164\tLR: 0.000337\n",
      "Training Epoch: 18 [25472/50000]\tLoss: 1.1255\tLR: 0.000337\n",
      "Training Epoch: 18 [25600/50000]\tLoss: 1.2012\tLR: 0.000337\n",
      "Training Epoch: 18 [25728/50000]\tLoss: 0.9800\tLR: 0.000337\n",
      "Training Epoch: 18 [25856/50000]\tLoss: 1.1875\tLR: 0.000337\n",
      "Training Epoch: 18 [25984/50000]\tLoss: 1.0555\tLR: 0.000337\n",
      "Training Epoch: 18 [26112/50000]\tLoss: 1.0317\tLR: 0.000337\n",
      "Training Epoch: 18 [26240/50000]\tLoss: 1.0775\tLR: 0.000337\n",
      "Training Epoch: 18 [26368/50000]\tLoss: 1.2702\tLR: 0.000337\n",
      "Training Epoch: 18 [26496/50000]\tLoss: 0.9291\tLR: 0.000337\n",
      "Training Epoch: 18 [26624/50000]\tLoss: 1.0004\tLR: 0.000337\n",
      "Training Epoch: 18 [26752/50000]\tLoss: 0.8756\tLR: 0.000337\n",
      "Training Epoch: 18 [26880/50000]\tLoss: 1.1263\tLR: 0.000337\n",
      "Training Epoch: 18 [27008/50000]\tLoss: 1.3242\tLR: 0.000337\n",
      "Training Epoch: 18 [27136/50000]\tLoss: 1.3357\tLR: 0.000337\n",
      "Training Epoch: 18 [27264/50000]\tLoss: 1.2157\tLR: 0.000337\n",
      "Training Epoch: 18 [27392/50000]\tLoss: 1.2307\tLR: 0.000337\n",
      "Training Epoch: 18 [27520/50000]\tLoss: 1.1809\tLR: 0.000337\n",
      "Training Epoch: 18 [27648/50000]\tLoss: 1.0498\tLR: 0.000337\n",
      "Training Epoch: 18 [27776/50000]\tLoss: 1.4562\tLR: 0.000337\n",
      "Training Epoch: 18 [27904/50000]\tLoss: 1.3154\tLR: 0.000337\n",
      "Training Epoch: 18 [28032/50000]\tLoss: 1.2555\tLR: 0.000337\n",
      "Training Epoch: 18 [28160/50000]\tLoss: 1.0943\tLR: 0.000337\n",
      "Training Epoch: 18 [28288/50000]\tLoss: 1.0834\tLR: 0.000337\n",
      "Training Epoch: 18 [28416/50000]\tLoss: 1.1604\tLR: 0.000337\n",
      "Training Epoch: 18 [28544/50000]\tLoss: 1.0944\tLR: 0.000337\n",
      "Training Epoch: 18 [28672/50000]\tLoss: 1.2632\tLR: 0.000337\n",
      "Training Epoch: 18 [28800/50000]\tLoss: 1.2192\tLR: 0.000337\n",
      "Training Epoch: 18 [28928/50000]\tLoss: 1.2219\tLR: 0.000337\n",
      "Training Epoch: 18 [29056/50000]\tLoss: 1.1220\tLR: 0.000337\n",
      "Training Epoch: 18 [29184/50000]\tLoss: 0.9684\tLR: 0.000337\n",
      "Training Epoch: 18 [29312/50000]\tLoss: 1.1548\tLR: 0.000337\n",
      "Training Epoch: 18 [29440/50000]\tLoss: 1.0924\tLR: 0.000337\n",
      "Training Epoch: 18 [29568/50000]\tLoss: 1.0310\tLR: 0.000337\n",
      "Training Epoch: 18 [29696/50000]\tLoss: 1.0171\tLR: 0.000337\n",
      "Training Epoch: 18 [29824/50000]\tLoss: 1.1833\tLR: 0.000337\n",
      "Training Epoch: 18 [29952/50000]\tLoss: 1.0046\tLR: 0.000337\n",
      "Training Epoch: 18 [30080/50000]\tLoss: 1.0779\tLR: 0.000337\n",
      "Training Epoch: 18 [30208/50000]\tLoss: 1.0016\tLR: 0.000337\n",
      "Training Epoch: 18 [30336/50000]\tLoss: 1.0414\tLR: 0.000337\n",
      "Training Epoch: 18 [30464/50000]\tLoss: 0.7580\tLR: 0.000337\n",
      "Training Epoch: 18 [30592/50000]\tLoss: 1.1053\tLR: 0.000337\n",
      "Training Epoch: 18 [30720/50000]\tLoss: 1.1281\tLR: 0.000337\n",
      "Training Epoch: 18 [30848/50000]\tLoss: 1.0925\tLR: 0.000337\n",
      "Training Epoch: 18 [30976/50000]\tLoss: 1.0648\tLR: 0.000337\n",
      "Training Epoch: 18 [31104/50000]\tLoss: 1.1695\tLR: 0.000337\n",
      "Training Epoch: 18 [31232/50000]\tLoss: 1.0515\tLR: 0.000337\n",
      "Training Epoch: 18 [31360/50000]\tLoss: 1.1392\tLR: 0.000337\n",
      "Training Epoch: 18 [31488/50000]\tLoss: 1.1390\tLR: 0.000337\n",
      "Training Epoch: 18 [31616/50000]\tLoss: 1.1848\tLR: 0.000337\n",
      "Training Epoch: 18 [31744/50000]\tLoss: 1.0196\tLR: 0.000337\n",
      "Training Epoch: 18 [31872/50000]\tLoss: 1.0239\tLR: 0.000337\n",
      "Training Epoch: 18 [32000/50000]\tLoss: 1.1216\tLR: 0.000337\n",
      "Training Epoch: 18 [32128/50000]\tLoss: 0.9962\tLR: 0.000337\n",
      "Training Epoch: 18 [32256/50000]\tLoss: 1.1605\tLR: 0.000337\n",
      "Training Epoch: 18 [32384/50000]\tLoss: 1.2113\tLR: 0.000337\n",
      "Training Epoch: 18 [32512/50000]\tLoss: 0.9417\tLR: 0.000337\n",
      "Training Epoch: 18 [32640/50000]\tLoss: 1.2831\tLR: 0.000337\n",
      "Training Epoch: 18 [32768/50000]\tLoss: 1.0954\tLR: 0.000337\n",
      "Training Epoch: 18 [32896/50000]\tLoss: 1.1486\tLR: 0.000337\n",
      "Training Epoch: 18 [33024/50000]\tLoss: 1.0906\tLR: 0.000337\n",
      "Training Epoch: 18 [33152/50000]\tLoss: 1.2672\tLR: 0.000337\n",
      "Training Epoch: 18 [33280/50000]\tLoss: 1.1348\tLR: 0.000337\n",
      "Training Epoch: 18 [33408/50000]\tLoss: 1.1225\tLR: 0.000337\n",
      "Training Epoch: 18 [33536/50000]\tLoss: 1.1554\tLR: 0.000337\n",
      "Training Epoch: 18 [33664/50000]\tLoss: 1.2267\tLR: 0.000337\n",
      "Training Epoch: 18 [33792/50000]\tLoss: 1.0798\tLR: 0.000337\n",
      "Training Epoch: 18 [33920/50000]\tLoss: 1.3022\tLR: 0.000337\n",
      "Training Epoch: 18 [34048/50000]\tLoss: 1.2680\tLR: 0.000337\n",
      "Training Epoch: 18 [34176/50000]\tLoss: 1.1417\tLR: 0.000337\n",
      "Training Epoch: 18 [34304/50000]\tLoss: 1.1759\tLR: 0.000337\n",
      "Training Epoch: 18 [34432/50000]\tLoss: 1.1550\tLR: 0.000337\n",
      "Training Epoch: 18 [34560/50000]\tLoss: 1.0683\tLR: 0.000337\n",
      "Training Epoch: 18 [34688/50000]\tLoss: 0.9455\tLR: 0.000337\n",
      "Training Epoch: 18 [34816/50000]\tLoss: 1.0228\tLR: 0.000337\n",
      "Training Epoch: 18 [34944/50000]\tLoss: 0.9961\tLR: 0.000337\n",
      "Training Epoch: 18 [35072/50000]\tLoss: 1.1533\tLR: 0.000337\n",
      "Training Epoch: 18 [35200/50000]\tLoss: 1.0775\tLR: 0.000337\n",
      "Training Epoch: 18 [35328/50000]\tLoss: 1.1673\tLR: 0.000337\n",
      "Training Epoch: 18 [35456/50000]\tLoss: 1.1542\tLR: 0.000337\n",
      "Training Epoch: 18 [35584/50000]\tLoss: 1.2074\tLR: 0.000337\n",
      "Training Epoch: 18 [35712/50000]\tLoss: 0.9758\tLR: 0.000337\n",
      "Training Epoch: 18 [35840/50000]\tLoss: 0.9743\tLR: 0.000337\n",
      "Training Epoch: 18 [35968/50000]\tLoss: 1.1546\tLR: 0.000337\n",
      "Training Epoch: 18 [36096/50000]\tLoss: 1.1060\tLR: 0.000337\n",
      "Training Epoch: 18 [36224/50000]\tLoss: 0.8950\tLR: 0.000337\n",
      "Training Epoch: 18 [36352/50000]\tLoss: 1.0886\tLR: 0.000337\n",
      "Training Epoch: 18 [36480/50000]\tLoss: 0.9734\tLR: 0.000337\n",
      "Training Epoch: 18 [36608/50000]\tLoss: 0.9632\tLR: 0.000337\n",
      "Training Epoch: 18 [36736/50000]\tLoss: 1.0795\tLR: 0.000337\n",
      "Training Epoch: 18 [36864/50000]\tLoss: 1.0016\tLR: 0.000337\n",
      "Training Epoch: 18 [36992/50000]\tLoss: 1.0417\tLR: 0.000337\n",
      "Training Epoch: 18 [37120/50000]\tLoss: 1.1903\tLR: 0.000337\n",
      "Training Epoch: 18 [37248/50000]\tLoss: 1.1056\tLR: 0.000337\n",
      "Training Epoch: 18 [37376/50000]\tLoss: 1.0791\tLR: 0.000337\n",
      "Training Epoch: 18 [37504/50000]\tLoss: 1.1822\tLR: 0.000337\n",
      "Training Epoch: 18 [37632/50000]\tLoss: 1.0678\tLR: 0.000337\n",
      "Training Epoch: 18 [37760/50000]\tLoss: 0.9389\tLR: 0.000337\n",
      "Training Epoch: 18 [37888/50000]\tLoss: 1.1590\tLR: 0.000337\n",
      "Training Epoch: 18 [38016/50000]\tLoss: 1.2816\tLR: 0.000337\n",
      "Training Epoch: 18 [38144/50000]\tLoss: 1.2810\tLR: 0.000337\n",
      "Training Epoch: 18 [38272/50000]\tLoss: 1.2483\tLR: 0.000337\n",
      "Training Epoch: 18 [38400/50000]\tLoss: 0.9558\tLR: 0.000337\n",
      "Training Epoch: 18 [38528/50000]\tLoss: 1.2257\tLR: 0.000337\n",
      "Training Epoch: 18 [38656/50000]\tLoss: 1.1192\tLR: 0.000337\n",
      "Training Epoch: 18 [38784/50000]\tLoss: 1.0758\tLR: 0.000337\n",
      "Training Epoch: 18 [38912/50000]\tLoss: 1.0123\tLR: 0.000337\n",
      "Training Epoch: 18 [39040/50000]\tLoss: 0.9861\tLR: 0.000337\n",
      "Training Epoch: 18 [39168/50000]\tLoss: 1.2621\tLR: 0.000337\n",
      "Training Epoch: 18 [39296/50000]\tLoss: 1.0858\tLR: 0.000337\n",
      "Training Epoch: 18 [39424/50000]\tLoss: 1.0568\tLR: 0.000337\n",
      "Training Epoch: 18 [39552/50000]\tLoss: 1.2468\tLR: 0.000337\n",
      "Training Epoch: 18 [39680/50000]\tLoss: 1.1121\tLR: 0.000337\n",
      "Training Epoch: 18 [39808/50000]\tLoss: 1.0948\tLR: 0.000337\n",
      "Training Epoch: 18 [39936/50000]\tLoss: 1.0855\tLR: 0.000337\n",
      "Training Epoch: 18 [40064/50000]\tLoss: 0.9235\tLR: 0.000337\n",
      "Training Epoch: 18 [40192/50000]\tLoss: 1.3906\tLR: 0.000337\n",
      "Training Epoch: 18 [40320/50000]\tLoss: 1.1068\tLR: 0.000337\n",
      "Training Epoch: 18 [40448/50000]\tLoss: 1.1053\tLR: 0.000337\n",
      "Training Epoch: 18 [40576/50000]\tLoss: 0.8284\tLR: 0.000337\n",
      "Training Epoch: 18 [40704/50000]\tLoss: 1.1170\tLR: 0.000337\n",
      "Training Epoch: 18 [40832/50000]\tLoss: 1.0735\tLR: 0.000337\n",
      "Training Epoch: 18 [40960/50000]\tLoss: 1.1524\tLR: 0.000337\n",
      "Training Epoch: 18 [41088/50000]\tLoss: 1.0848\tLR: 0.000337\n",
      "Training Epoch: 18 [41216/50000]\tLoss: 0.9990\tLR: 0.000337\n",
      "Training Epoch: 18 [41344/50000]\tLoss: 0.9487\tLR: 0.000337\n",
      "Training Epoch: 18 [41472/50000]\tLoss: 0.9760\tLR: 0.000337\n",
      "Training Epoch: 18 [41600/50000]\tLoss: 1.1569\tLR: 0.000337\n",
      "Training Epoch: 18 [41728/50000]\tLoss: 1.0553\tLR: 0.000337\n",
      "Training Epoch: 18 [41856/50000]\tLoss: 1.2622\tLR: 0.000337\n",
      "Training Epoch: 18 [41984/50000]\tLoss: 1.0894\tLR: 0.000337\n",
      "Training Epoch: 18 [42112/50000]\tLoss: 1.1349\tLR: 0.000337\n",
      "Training Epoch: 18 [42240/50000]\tLoss: 1.0017\tLR: 0.000337\n",
      "Training Epoch: 18 [42368/50000]\tLoss: 1.1813\tLR: 0.000337\n",
      "Training Epoch: 18 [42496/50000]\tLoss: 0.9867\tLR: 0.000337\n",
      "Training Epoch: 18 [42624/50000]\tLoss: 0.9374\tLR: 0.000337\n",
      "Training Epoch: 18 [42752/50000]\tLoss: 1.1904\tLR: 0.000337\n",
      "Training Epoch: 18 [42880/50000]\tLoss: 1.2924\tLR: 0.000337\n",
      "Training Epoch: 18 [43008/50000]\tLoss: 1.2822\tLR: 0.000337\n",
      "Training Epoch: 18 [43136/50000]\tLoss: 1.3025\tLR: 0.000337\n",
      "Training Epoch: 18 [43264/50000]\tLoss: 1.1541\tLR: 0.000337\n",
      "Training Epoch: 18 [43392/50000]\tLoss: 1.1758\tLR: 0.000337\n",
      "Training Epoch: 18 [43520/50000]\tLoss: 1.0163\tLR: 0.000337\n",
      "Training Epoch: 18 [43648/50000]\tLoss: 1.2355\tLR: 0.000337\n",
      "Training Epoch: 18 [43776/50000]\tLoss: 1.0787\tLR: 0.000337\n",
      "Training Epoch: 18 [43904/50000]\tLoss: 1.2254\tLR: 0.000337\n",
      "Training Epoch: 18 [44032/50000]\tLoss: 1.0118\tLR: 0.000337\n",
      "Training Epoch: 18 [44160/50000]\tLoss: 0.9953\tLR: 0.000337\n",
      "Training Epoch: 18 [44288/50000]\tLoss: 1.0875\tLR: 0.000337\n",
      "Training Epoch: 18 [44416/50000]\tLoss: 1.0342\tLR: 0.000337\n",
      "Training Epoch: 18 [44544/50000]\tLoss: 1.1149\tLR: 0.000337\n",
      "Training Epoch: 18 [44672/50000]\tLoss: 1.2655\tLR: 0.000337\n",
      "Training Epoch: 18 [44800/50000]\tLoss: 1.0779\tLR: 0.000337\n",
      "Training Epoch: 18 [44928/50000]\tLoss: 1.0095\tLR: 0.000337\n",
      "Training Epoch: 18 [45056/50000]\tLoss: 1.0122\tLR: 0.000337\n",
      "Training Epoch: 18 [45184/50000]\tLoss: 1.3093\tLR: 0.000337\n",
      "Training Epoch: 18 [45312/50000]\tLoss: 1.0728\tLR: 0.000337\n",
      "Training Epoch: 18 [45440/50000]\tLoss: 1.1230\tLR: 0.000337\n",
      "Training Epoch: 18 [45568/50000]\tLoss: 0.9755\tLR: 0.000337\n",
      "Training Epoch: 18 [45696/50000]\tLoss: 0.9930\tLR: 0.000337\n",
      "Training Epoch: 18 [45824/50000]\tLoss: 1.3458\tLR: 0.000337\n",
      "Training Epoch: 18 [45952/50000]\tLoss: 1.0728\tLR: 0.000337\n",
      "Training Epoch: 18 [46080/50000]\tLoss: 1.2667\tLR: 0.000337\n",
      "Training Epoch: 18 [46208/50000]\tLoss: 1.0241\tLR: 0.000337\n",
      "Training Epoch: 18 [46336/50000]\tLoss: 1.1327\tLR: 0.000337\n",
      "Training Epoch: 18 [46464/50000]\tLoss: 1.0685\tLR: 0.000337\n",
      "Training Epoch: 18 [46592/50000]\tLoss: 0.9520\tLR: 0.000337\n",
      "Training Epoch: 18 [46720/50000]\tLoss: 1.0552\tLR: 0.000337\n",
      "Training Epoch: 18 [46848/50000]\tLoss: 0.9723\tLR: 0.000337\n",
      "Training Epoch: 18 [46976/50000]\tLoss: 1.2145\tLR: 0.000337\n",
      "Training Epoch: 18 [47104/50000]\tLoss: 1.3664\tLR: 0.000337\n",
      "Training Epoch: 18 [47232/50000]\tLoss: 1.1645\tLR: 0.000337\n",
      "Training Epoch: 18 [47360/50000]\tLoss: 0.9437\tLR: 0.000337\n",
      "Training Epoch: 18 [47488/50000]\tLoss: 1.3336\tLR: 0.000337\n",
      "Training Epoch: 18 [47616/50000]\tLoss: 1.2173\tLR: 0.000337\n",
      "Training Epoch: 18 [47744/50000]\tLoss: 1.1004\tLR: 0.000337\n",
      "Training Epoch: 18 [47872/50000]\tLoss: 0.9937\tLR: 0.000337\n",
      "Training Epoch: 18 [48000/50000]\tLoss: 0.9957\tLR: 0.000337\n",
      "Training Epoch: 18 [48128/50000]\tLoss: 1.1463\tLR: 0.000337\n",
      "Training Epoch: 18 [48256/50000]\tLoss: 1.1563\tLR: 0.000337\n",
      "Training Epoch: 18 [48384/50000]\tLoss: 1.0401\tLR: 0.000337\n",
      "Training Epoch: 18 [48512/50000]\tLoss: 0.8894\tLR: 0.000337\n",
      "Training Epoch: 18 [48640/50000]\tLoss: 1.3066\tLR: 0.000337\n",
      "Training Epoch: 18 [48768/50000]\tLoss: 1.0560\tLR: 0.000337\n",
      "Training Epoch: 18 [48896/50000]\tLoss: 1.0792\tLR: 0.000337\n",
      "Training Epoch: 18 [49024/50000]\tLoss: 1.2754\tLR: 0.000337\n",
      "Training Epoch: 18 [49152/50000]\tLoss: 1.3293\tLR: 0.000337\n",
      "Training Epoch: 18 [49280/50000]\tLoss: 0.9846\tLR: 0.000337\n",
      "Training Epoch: 18 [49408/50000]\tLoss: 1.3363\tLR: 0.000337\n",
      "Training Epoch: 18 [49536/50000]\tLoss: 1.0619\tLR: 0.000337\n",
      "Training Epoch: 18 [49664/50000]\tLoss: 1.1206\tLR: 0.000337\n",
      "Training Epoch: 18 [49792/50000]\tLoss: 1.1608\tLR: 0.000337\n",
      "Training Epoch: 18 [49920/50000]\tLoss: 1.0536\tLR: 0.000337\n",
      "Training Epoch: 18 [50000/50000]\tLoss: 1.3747\tLR: 0.000337\n",
      "Test set: Average loss: 0.0104, Accuracy: 0.6268\n",
      "\n",
      "Training Epoch: 19 [128/50000]\tLoss: 0.8915\tLR: 0.000337\n",
      "Training Epoch: 19 [256/50000]\tLoss: 1.0514\tLR: 0.000337\n",
      "Training Epoch: 19 [384/50000]\tLoss: 1.1193\tLR: 0.000337\n",
      "Training Epoch: 19 [512/50000]\tLoss: 1.0693\tLR: 0.000337\n",
      "Training Epoch: 19 [640/50000]\tLoss: 1.2582\tLR: 0.000337\n",
      "Training Epoch: 19 [768/50000]\tLoss: 1.0776\tLR: 0.000337\n",
      "Training Epoch: 19 [896/50000]\tLoss: 1.0109\tLR: 0.000337\n",
      "Training Epoch: 19 [1024/50000]\tLoss: 1.1952\tLR: 0.000337\n",
      "Training Epoch: 19 [1152/50000]\tLoss: 1.1662\tLR: 0.000337\n",
      "Training Epoch: 19 [1280/50000]\tLoss: 1.0632\tLR: 0.000337\n",
      "Training Epoch: 19 [1408/50000]\tLoss: 1.2063\tLR: 0.000337\n",
      "Training Epoch: 19 [1536/50000]\tLoss: 1.3395\tLR: 0.000337\n",
      "Training Epoch: 19 [1664/50000]\tLoss: 0.9308\tLR: 0.000337\n",
      "Training Epoch: 19 [1792/50000]\tLoss: 1.1277\tLR: 0.000337\n",
      "Training Epoch: 19 [1920/50000]\tLoss: 0.8361\tLR: 0.000337\n",
      "Training Epoch: 19 [2048/50000]\tLoss: 1.0427\tLR: 0.000337\n",
      "Training Epoch: 19 [2176/50000]\tLoss: 1.1226\tLR: 0.000337\n",
      "Training Epoch: 19 [2304/50000]\tLoss: 0.9978\tLR: 0.000337\n",
      "Training Epoch: 19 [2432/50000]\tLoss: 1.2038\tLR: 0.000337\n",
      "Training Epoch: 19 [2560/50000]\tLoss: 0.9636\tLR: 0.000337\n",
      "Training Epoch: 19 [2688/50000]\tLoss: 1.1085\tLR: 0.000337\n",
      "Training Epoch: 19 [2816/50000]\tLoss: 1.1684\tLR: 0.000337\n",
      "Training Epoch: 19 [2944/50000]\tLoss: 1.0745\tLR: 0.000337\n",
      "Training Epoch: 19 [3072/50000]\tLoss: 1.2517\tLR: 0.000337\n",
      "Training Epoch: 19 [3200/50000]\tLoss: 1.2497\tLR: 0.000337\n",
      "Training Epoch: 19 [3328/50000]\tLoss: 1.1015\tLR: 0.000337\n",
      "Training Epoch: 19 [3456/50000]\tLoss: 0.9773\tLR: 0.000337\n",
      "Training Epoch: 19 [3584/50000]\tLoss: 0.9431\tLR: 0.000337\n",
      "Training Epoch: 19 [3712/50000]\tLoss: 1.2653\tLR: 0.000337\n",
      "Training Epoch: 19 [3840/50000]\tLoss: 1.0800\tLR: 0.000337\n",
      "Training Epoch: 19 [3968/50000]\tLoss: 1.2326\tLR: 0.000337\n",
      "Training Epoch: 19 [4096/50000]\tLoss: 1.0402\tLR: 0.000337\n",
      "Training Epoch: 19 [4224/50000]\tLoss: 1.0600\tLR: 0.000337\n",
      "Training Epoch: 19 [4352/50000]\tLoss: 0.9104\tLR: 0.000337\n",
      "Training Epoch: 19 [4480/50000]\tLoss: 0.9542\tLR: 0.000337\n",
      "Training Epoch: 19 [4608/50000]\tLoss: 1.1336\tLR: 0.000337\n",
      "Training Epoch: 19 [4736/50000]\tLoss: 1.2016\tLR: 0.000337\n",
      "Training Epoch: 19 [4864/50000]\tLoss: 1.1891\tLR: 0.000337\n",
      "Training Epoch: 19 [4992/50000]\tLoss: 0.9823\tLR: 0.000337\n",
      "Training Epoch: 19 [5120/50000]\tLoss: 1.1273\tLR: 0.000337\n",
      "Training Epoch: 19 [5248/50000]\tLoss: 1.2540\tLR: 0.000337\n",
      "Training Epoch: 19 [5376/50000]\tLoss: 0.9463\tLR: 0.000337\n",
      "Training Epoch: 19 [5504/50000]\tLoss: 1.0449\tLR: 0.000337\n",
      "Training Epoch: 19 [5632/50000]\tLoss: 1.1975\tLR: 0.000337\n",
      "Training Epoch: 19 [5760/50000]\tLoss: 1.2309\tLR: 0.000337\n",
      "Training Epoch: 19 [5888/50000]\tLoss: 1.1599\tLR: 0.000337\n",
      "Training Epoch: 19 [6016/50000]\tLoss: 0.9691\tLR: 0.000337\n",
      "Training Epoch: 19 [6144/50000]\tLoss: 1.1276\tLR: 0.000337\n",
      "Training Epoch: 19 [6272/50000]\tLoss: 0.9970\tLR: 0.000337\n",
      "Training Epoch: 19 [6400/50000]\tLoss: 1.1711\tLR: 0.000337\n",
      "Training Epoch: 19 [6528/50000]\tLoss: 1.1931\tLR: 0.000337\n",
      "Training Epoch: 19 [6656/50000]\tLoss: 1.0374\tLR: 0.000337\n",
      "Training Epoch: 19 [6784/50000]\tLoss: 1.0829\tLR: 0.000337\n",
      "Training Epoch: 19 [6912/50000]\tLoss: 0.9993\tLR: 0.000337\n",
      "Training Epoch: 19 [7040/50000]\tLoss: 1.1888\tLR: 0.000337\n",
      "Training Epoch: 19 [7168/50000]\tLoss: 1.1189\tLR: 0.000337\n",
      "Training Epoch: 19 [7296/50000]\tLoss: 1.1366\tLR: 0.000337\n",
      "Training Epoch: 19 [7424/50000]\tLoss: 1.0405\tLR: 0.000337\n",
      "Training Epoch: 19 [7552/50000]\tLoss: 1.0999\tLR: 0.000337\n",
      "Training Epoch: 19 [7680/50000]\tLoss: 1.2855\tLR: 0.000337\n",
      "Training Epoch: 19 [7808/50000]\tLoss: 1.1551\tLR: 0.000337\n",
      "Training Epoch: 19 [7936/50000]\tLoss: 1.2484\tLR: 0.000337\n",
      "Training Epoch: 19 [8064/50000]\tLoss: 1.0866\tLR: 0.000337\n",
      "Training Epoch: 19 [8192/50000]\tLoss: 1.0246\tLR: 0.000337\n",
      "Training Epoch: 19 [8320/50000]\tLoss: 1.2016\tLR: 0.000337\n",
      "Training Epoch: 19 [8448/50000]\tLoss: 1.0681\tLR: 0.000337\n",
      "Training Epoch: 19 [8576/50000]\tLoss: 1.0126\tLR: 0.000337\n",
      "Training Epoch: 19 [8704/50000]\tLoss: 1.1594\tLR: 0.000337\n",
      "Training Epoch: 19 [8832/50000]\tLoss: 1.1197\tLR: 0.000337\n",
      "Training Epoch: 19 [8960/50000]\tLoss: 1.1307\tLR: 0.000337\n",
      "Training Epoch: 19 [9088/50000]\tLoss: 1.1020\tLR: 0.000337\n",
      "Training Epoch: 19 [9216/50000]\tLoss: 0.8647\tLR: 0.000337\n",
      "Training Epoch: 19 [9344/50000]\tLoss: 1.2195\tLR: 0.000337\n",
      "Training Epoch: 19 [9472/50000]\tLoss: 1.1433\tLR: 0.000337\n",
      "Training Epoch: 19 [9600/50000]\tLoss: 1.1332\tLR: 0.000337\n",
      "Training Epoch: 19 [9728/50000]\tLoss: 1.2993\tLR: 0.000337\n",
      "Training Epoch: 19 [9856/50000]\tLoss: 1.3602\tLR: 0.000337\n",
      "Training Epoch: 19 [9984/50000]\tLoss: 1.1109\tLR: 0.000337\n",
      "Training Epoch: 19 [10112/50000]\tLoss: 1.1190\tLR: 0.000337\n",
      "Training Epoch: 19 [10240/50000]\tLoss: 1.0371\tLR: 0.000337\n",
      "Training Epoch: 19 [10368/50000]\tLoss: 1.0641\tLR: 0.000337\n",
      "Training Epoch: 19 [10496/50000]\tLoss: 1.0803\tLR: 0.000337\n",
      "Training Epoch: 19 [10624/50000]\tLoss: 1.3179\tLR: 0.000337\n",
      "Training Epoch: 19 [10752/50000]\tLoss: 1.1378\tLR: 0.000337\n",
      "Training Epoch: 19 [10880/50000]\tLoss: 1.0955\tLR: 0.000337\n",
      "Training Epoch: 19 [11008/50000]\tLoss: 1.1240\tLR: 0.000337\n",
      "Training Epoch: 19 [11136/50000]\tLoss: 1.0218\tLR: 0.000337\n",
      "Training Epoch: 19 [11264/50000]\tLoss: 0.8849\tLR: 0.000337\n",
      "Training Epoch: 19 [11392/50000]\tLoss: 0.9511\tLR: 0.000337\n",
      "Training Epoch: 19 [11520/50000]\tLoss: 1.3021\tLR: 0.000337\n",
      "Training Epoch: 19 [11648/50000]\tLoss: 1.0419\tLR: 0.000337\n",
      "Training Epoch: 19 [11776/50000]\tLoss: 1.1302\tLR: 0.000337\n",
      "Training Epoch: 19 [11904/50000]\tLoss: 1.1176\tLR: 0.000337\n",
      "Training Epoch: 19 [12032/50000]\tLoss: 1.1540\tLR: 0.000337\n",
      "Training Epoch: 19 [12160/50000]\tLoss: 1.0139\tLR: 0.000337\n",
      "Training Epoch: 19 [12288/50000]\tLoss: 1.1052\tLR: 0.000337\n",
      "Training Epoch: 19 [12416/50000]\tLoss: 1.0205\tLR: 0.000337\n",
      "Training Epoch: 19 [12544/50000]\tLoss: 1.0536\tLR: 0.000337\n",
      "Training Epoch: 19 [12672/50000]\tLoss: 1.1707\tLR: 0.000337\n",
      "Training Epoch: 19 [12800/50000]\tLoss: 1.0817\tLR: 0.000337\n",
      "Training Epoch: 19 [12928/50000]\tLoss: 1.0994\tLR: 0.000337\n",
      "Training Epoch: 19 [13056/50000]\tLoss: 1.0556\tLR: 0.000337\n",
      "Training Epoch: 19 [13184/50000]\tLoss: 1.3482\tLR: 0.000337\n",
      "Training Epoch: 19 [13312/50000]\tLoss: 1.0906\tLR: 0.000337\n",
      "Training Epoch: 19 [13440/50000]\tLoss: 0.9722\tLR: 0.000337\n",
      "Training Epoch: 19 [13568/50000]\tLoss: 0.8453\tLR: 0.000337\n",
      "Training Epoch: 19 [13696/50000]\tLoss: 1.0516\tLR: 0.000337\n",
      "Training Epoch: 19 [13824/50000]\tLoss: 1.1794\tLR: 0.000337\n",
      "Training Epoch: 19 [13952/50000]\tLoss: 1.1420\tLR: 0.000337\n",
      "Training Epoch: 19 [14080/50000]\tLoss: 1.0520\tLR: 0.000337\n",
      "Training Epoch: 19 [14208/50000]\tLoss: 0.9312\tLR: 0.000337\n",
      "Training Epoch: 19 [14336/50000]\tLoss: 1.1355\tLR: 0.000337\n",
      "Training Epoch: 19 [14464/50000]\tLoss: 1.2626\tLR: 0.000337\n",
      "Training Epoch: 19 [14592/50000]\tLoss: 1.0408\tLR: 0.000337\n",
      "Training Epoch: 19 [14720/50000]\tLoss: 1.1191\tLR: 0.000337\n",
      "Training Epoch: 19 [14848/50000]\tLoss: 1.1459\tLR: 0.000337\n",
      "Training Epoch: 19 [14976/50000]\tLoss: 1.2542\tLR: 0.000337\n",
      "Training Epoch: 19 [15104/50000]\tLoss: 1.1124\tLR: 0.000337\n",
      "Training Epoch: 19 [15232/50000]\tLoss: 1.1614\tLR: 0.000337\n",
      "Training Epoch: 19 [15360/50000]\tLoss: 1.2137\tLR: 0.000337\n",
      "Training Epoch: 19 [15488/50000]\tLoss: 1.1225\tLR: 0.000337\n",
      "Training Epoch: 19 [15616/50000]\tLoss: 1.0467\tLR: 0.000337\n",
      "Training Epoch: 19 [15744/50000]\tLoss: 1.4090\tLR: 0.000337\n",
      "Training Epoch: 19 [15872/50000]\tLoss: 1.1110\tLR: 0.000337\n",
      "Training Epoch: 19 [16000/50000]\tLoss: 1.1977\tLR: 0.000337\n",
      "Training Epoch: 19 [16128/50000]\tLoss: 0.9661\tLR: 0.000337\n",
      "Training Epoch: 19 [16256/50000]\tLoss: 1.0602\tLR: 0.000337\n",
      "Training Epoch: 19 [16384/50000]\tLoss: 1.1096\tLR: 0.000337\n",
      "Training Epoch: 19 [16512/50000]\tLoss: 1.0901\tLR: 0.000337\n",
      "Training Epoch: 19 [16640/50000]\tLoss: 0.9661\tLR: 0.000337\n",
      "Training Epoch: 19 [16768/50000]\tLoss: 1.1296\tLR: 0.000337\n",
      "Training Epoch: 19 [16896/50000]\tLoss: 0.9004\tLR: 0.000337\n",
      "Training Epoch: 19 [17024/50000]\tLoss: 1.0710\tLR: 0.000337\n",
      "Training Epoch: 19 [17152/50000]\tLoss: 1.1919\tLR: 0.000337\n",
      "Training Epoch: 19 [17280/50000]\tLoss: 1.1814\tLR: 0.000337\n",
      "Training Epoch: 19 [17408/50000]\tLoss: 1.1012\tLR: 0.000337\n",
      "Training Epoch: 19 [17536/50000]\tLoss: 1.1222\tLR: 0.000337\n",
      "Training Epoch: 19 [17664/50000]\tLoss: 1.0554\tLR: 0.000337\n",
      "Training Epoch: 19 [17792/50000]\tLoss: 1.1259\tLR: 0.000337\n",
      "Training Epoch: 19 [17920/50000]\tLoss: 1.2234\tLR: 0.000337\n",
      "Training Epoch: 19 [18048/50000]\tLoss: 0.9935\tLR: 0.000337\n",
      "Training Epoch: 19 [18176/50000]\tLoss: 1.3029\tLR: 0.000337\n",
      "Training Epoch: 19 [18304/50000]\tLoss: 1.1558\tLR: 0.000337\n",
      "Training Epoch: 19 [18432/50000]\tLoss: 1.0103\tLR: 0.000337\n",
      "Training Epoch: 19 [18560/50000]\tLoss: 1.2588\tLR: 0.000337\n",
      "Training Epoch: 19 [18688/50000]\tLoss: 1.3156\tLR: 0.000337\n",
      "Training Epoch: 19 [18816/50000]\tLoss: 1.0519\tLR: 0.000337\n",
      "Training Epoch: 19 [18944/50000]\tLoss: 1.1146\tLR: 0.000337\n",
      "Training Epoch: 19 [19072/50000]\tLoss: 1.1349\tLR: 0.000337\n",
      "Training Epoch: 19 [19200/50000]\tLoss: 1.2601\tLR: 0.000337\n",
      "Training Epoch: 19 [19328/50000]\tLoss: 1.1197\tLR: 0.000337\n",
      "Training Epoch: 19 [19456/50000]\tLoss: 0.9490\tLR: 0.000337\n",
      "Training Epoch: 19 [19584/50000]\tLoss: 1.1777\tLR: 0.000337\n",
      "Training Epoch: 19 [19712/50000]\tLoss: 1.1176\tLR: 0.000337\n",
      "Training Epoch: 19 [19840/50000]\tLoss: 1.2901\tLR: 0.000337\n",
      "Training Epoch: 19 [19968/50000]\tLoss: 1.2052\tLR: 0.000337\n",
      "Training Epoch: 19 [20096/50000]\tLoss: 1.0850\tLR: 0.000337\n",
      "Training Epoch: 19 [20224/50000]\tLoss: 1.1712\tLR: 0.000337\n",
      "Training Epoch: 19 [20352/50000]\tLoss: 1.0331\tLR: 0.000337\n",
      "Training Epoch: 19 [20480/50000]\tLoss: 0.9881\tLR: 0.000337\n",
      "Training Epoch: 19 [20608/50000]\tLoss: 1.0867\tLR: 0.000337\n",
      "Training Epoch: 19 [20736/50000]\tLoss: 1.2129\tLR: 0.000337\n",
      "Training Epoch: 19 [20864/50000]\tLoss: 1.0853\tLR: 0.000337\n",
      "Training Epoch: 19 [20992/50000]\tLoss: 1.1525\tLR: 0.000337\n",
      "Training Epoch: 19 [21120/50000]\tLoss: 1.1516\tLR: 0.000337\n",
      "Training Epoch: 19 [21248/50000]\tLoss: 1.1613\tLR: 0.000337\n",
      "Training Epoch: 19 [21376/50000]\tLoss: 1.0314\tLR: 0.000337\n",
      "Training Epoch: 19 [21504/50000]\tLoss: 1.0667\tLR: 0.000337\n",
      "Training Epoch: 19 [21632/50000]\tLoss: 1.2420\tLR: 0.000337\n",
      "Training Epoch: 19 [21760/50000]\tLoss: 1.1331\tLR: 0.000337\n",
      "Training Epoch: 19 [21888/50000]\tLoss: 0.9004\tLR: 0.000337\n",
      "Training Epoch: 19 [22016/50000]\tLoss: 1.1768\tLR: 0.000337\n",
      "Training Epoch: 19 [22144/50000]\tLoss: 1.1938\tLR: 0.000337\n",
      "Training Epoch: 19 [22272/50000]\tLoss: 0.9596\tLR: 0.000337\n",
      "Training Epoch: 19 [22400/50000]\tLoss: 0.9796\tLR: 0.000337\n",
      "Training Epoch: 19 [22528/50000]\tLoss: 1.3485\tLR: 0.000337\n",
      "Training Epoch: 19 [22656/50000]\tLoss: 0.9528\tLR: 0.000337\n",
      "Training Epoch: 19 [22784/50000]\tLoss: 1.0057\tLR: 0.000337\n",
      "Training Epoch: 19 [22912/50000]\tLoss: 0.9997\tLR: 0.000337\n",
      "Training Epoch: 19 [23040/50000]\tLoss: 1.1646\tLR: 0.000337\n",
      "Training Epoch: 19 [23168/50000]\tLoss: 1.2476\tLR: 0.000337\n",
      "Training Epoch: 19 [23296/50000]\tLoss: 1.2960\tLR: 0.000337\n",
      "Training Epoch: 19 [23424/50000]\tLoss: 0.9753\tLR: 0.000337\n",
      "Training Epoch: 19 [23552/50000]\tLoss: 1.1983\tLR: 0.000337\n",
      "Training Epoch: 19 [23680/50000]\tLoss: 1.1142\tLR: 0.000337\n",
      "Training Epoch: 19 [23808/50000]\tLoss: 0.9851\tLR: 0.000337\n",
      "Training Epoch: 19 [23936/50000]\tLoss: 1.1644\tLR: 0.000337\n",
      "Training Epoch: 19 [24064/50000]\tLoss: 1.0877\tLR: 0.000337\n",
      "Training Epoch: 19 [24192/50000]\tLoss: 1.3768\tLR: 0.000337\n",
      "Training Epoch: 19 [24320/50000]\tLoss: 1.0130\tLR: 0.000337\n",
      "Training Epoch: 19 [24448/50000]\tLoss: 1.0000\tLR: 0.000337\n",
      "Training Epoch: 19 [24576/50000]\tLoss: 1.1295\tLR: 0.000337\n",
      "Training Epoch: 19 [24704/50000]\tLoss: 1.1552\tLR: 0.000337\n",
      "Training Epoch: 19 [24832/50000]\tLoss: 0.8979\tLR: 0.000337\n",
      "Training Epoch: 19 [24960/50000]\tLoss: 1.0176\tLR: 0.000337\n",
      "Training Epoch: 19 [25088/50000]\tLoss: 1.1642\tLR: 0.000337\n",
      "Training Epoch: 19 [25216/50000]\tLoss: 1.0730\tLR: 0.000337\n",
      "Training Epoch: 19 [25344/50000]\tLoss: 1.2075\tLR: 0.000337\n",
      "Training Epoch: 19 [25472/50000]\tLoss: 1.1047\tLR: 0.000337\n",
      "Training Epoch: 19 [25600/50000]\tLoss: 1.1145\tLR: 0.000337\n",
      "Training Epoch: 19 [25728/50000]\tLoss: 1.2189\tLR: 0.000337\n",
      "Training Epoch: 19 [25856/50000]\tLoss: 1.3455\tLR: 0.000337\n",
      "Training Epoch: 19 [25984/50000]\tLoss: 1.0727\tLR: 0.000337\n",
      "Training Epoch: 19 [26112/50000]\tLoss: 1.1248\tLR: 0.000337\n",
      "Training Epoch: 19 [26240/50000]\tLoss: 1.2342\tLR: 0.000337\n",
      "Training Epoch: 19 [26368/50000]\tLoss: 1.3495\tLR: 0.000337\n",
      "Training Epoch: 19 [26496/50000]\tLoss: 0.8588\tLR: 0.000337\n",
      "Training Epoch: 19 [26624/50000]\tLoss: 1.0862\tLR: 0.000337\n",
      "Training Epoch: 19 [26752/50000]\tLoss: 1.0900\tLR: 0.000337\n",
      "Training Epoch: 19 [26880/50000]\tLoss: 1.1560\tLR: 0.000337\n",
      "Training Epoch: 19 [27008/50000]\tLoss: 1.1328\tLR: 0.000337\n",
      "Training Epoch: 19 [27136/50000]\tLoss: 0.9638\tLR: 0.000337\n",
      "Training Epoch: 19 [27264/50000]\tLoss: 1.1797\tLR: 0.000337\n",
      "Training Epoch: 19 [27392/50000]\tLoss: 1.0270\tLR: 0.000337\n",
      "Training Epoch: 19 [27520/50000]\tLoss: 1.2413\tLR: 0.000337\n",
      "Training Epoch: 19 [27648/50000]\tLoss: 0.9771\tLR: 0.000337\n",
      "Training Epoch: 19 [27776/50000]\tLoss: 0.9091\tLR: 0.000337\n",
      "Training Epoch: 19 [27904/50000]\tLoss: 1.0325\tLR: 0.000337\n",
      "Training Epoch: 19 [28032/50000]\tLoss: 0.8929\tLR: 0.000337\n",
      "Training Epoch: 19 [28160/50000]\tLoss: 0.9568\tLR: 0.000337\n",
      "Training Epoch: 19 [28288/50000]\tLoss: 0.8733\tLR: 0.000337\n",
      "Training Epoch: 19 [28416/50000]\tLoss: 1.1970\tLR: 0.000337\n",
      "Training Epoch: 19 [28544/50000]\tLoss: 1.0622\tLR: 0.000337\n",
      "Training Epoch: 19 [28672/50000]\tLoss: 1.1515\tLR: 0.000337\n",
      "Training Epoch: 19 [28800/50000]\tLoss: 0.9202\tLR: 0.000337\n",
      "Training Epoch: 19 [28928/50000]\tLoss: 1.2540\tLR: 0.000337\n",
      "Training Epoch: 19 [29056/50000]\tLoss: 1.2605\tLR: 0.000337\n",
      "Training Epoch: 19 [29184/50000]\tLoss: 1.0765\tLR: 0.000337\n",
      "Training Epoch: 19 [29312/50000]\tLoss: 0.9157\tLR: 0.000337\n",
      "Training Epoch: 19 [29440/50000]\tLoss: 0.9995\tLR: 0.000337\n",
      "Training Epoch: 19 [29568/50000]\tLoss: 1.0280\tLR: 0.000337\n",
      "Training Epoch: 19 [29696/50000]\tLoss: 1.0399\tLR: 0.000337\n",
      "Training Epoch: 19 [29824/50000]\tLoss: 1.0007\tLR: 0.000337\n",
      "Training Epoch: 19 [29952/50000]\tLoss: 1.1238\tLR: 0.000337\n",
      "Training Epoch: 19 [30080/50000]\tLoss: 0.9676\tLR: 0.000337\n",
      "Training Epoch: 19 [30208/50000]\tLoss: 0.9732\tLR: 0.000337\n",
      "Training Epoch: 19 [30336/50000]\tLoss: 0.6638\tLR: 0.000337\n",
      "Training Epoch: 19 [30464/50000]\tLoss: 0.9987\tLR: 0.000337\n",
      "Training Epoch: 19 [30592/50000]\tLoss: 1.2677\tLR: 0.000337\n",
      "Training Epoch: 19 [30720/50000]\tLoss: 1.1684\tLR: 0.000337\n",
      "Training Epoch: 19 [30848/50000]\tLoss: 1.2136\tLR: 0.000337\n",
      "Training Epoch: 19 [30976/50000]\tLoss: 1.2772\tLR: 0.000337\n",
      "Training Epoch: 19 [31104/50000]\tLoss: 1.0115\tLR: 0.000337\n",
      "Training Epoch: 19 [31232/50000]\tLoss: 1.2925\tLR: 0.000337\n",
      "Training Epoch: 19 [31360/50000]\tLoss: 0.9755\tLR: 0.000337\n",
      "Training Epoch: 19 [31488/50000]\tLoss: 1.0440\tLR: 0.000337\n",
      "Training Epoch: 19 [31616/50000]\tLoss: 1.2692\tLR: 0.000337\n",
      "Training Epoch: 19 [31744/50000]\tLoss: 1.0959\tLR: 0.000337\n",
      "Training Epoch: 19 [31872/50000]\tLoss: 1.1181\tLR: 0.000337\n",
      "Training Epoch: 19 [32000/50000]\tLoss: 1.2053\tLR: 0.000337\n",
      "Training Epoch: 19 [32128/50000]\tLoss: 1.1136\tLR: 0.000337\n",
      "Training Epoch: 19 [32256/50000]\tLoss: 1.0722\tLR: 0.000337\n",
      "Training Epoch: 19 [32384/50000]\tLoss: 1.0942\tLR: 0.000337\n",
      "Training Epoch: 19 [32512/50000]\tLoss: 1.2346\tLR: 0.000337\n",
      "Training Epoch: 19 [32640/50000]\tLoss: 1.0677\tLR: 0.000337\n",
      "Training Epoch: 19 [32768/50000]\tLoss: 1.3108\tLR: 0.000337\n",
      "Training Epoch: 19 [32896/50000]\tLoss: 1.1542\tLR: 0.000337\n",
      "Training Epoch: 19 [33024/50000]\tLoss: 1.0595\tLR: 0.000337\n",
      "Training Epoch: 19 [33152/50000]\tLoss: 1.2205\tLR: 0.000337\n",
      "Training Epoch: 19 [33280/50000]\tLoss: 1.0694\tLR: 0.000337\n",
      "Training Epoch: 19 [33408/50000]\tLoss: 0.9808\tLR: 0.000337\n",
      "Training Epoch: 19 [33536/50000]\tLoss: 1.0279\tLR: 0.000337\n",
      "Training Epoch: 19 [33664/50000]\tLoss: 0.9957\tLR: 0.000337\n",
      "Training Epoch: 19 [33792/50000]\tLoss: 1.0726\tLR: 0.000337\n",
      "Training Epoch: 19 [33920/50000]\tLoss: 1.2658\tLR: 0.000337\n",
      "Training Epoch: 19 [34048/50000]\tLoss: 1.1475\tLR: 0.000337\n",
      "Training Epoch: 19 [34176/50000]\tLoss: 1.0364\tLR: 0.000337\n",
      "Training Epoch: 19 [34304/50000]\tLoss: 1.0575\tLR: 0.000337\n",
      "Training Epoch: 19 [34432/50000]\tLoss: 1.1418\tLR: 0.000337\n",
      "Training Epoch: 19 [34560/50000]\tLoss: 1.1007\tLR: 0.000337\n",
      "Training Epoch: 19 [34688/50000]\tLoss: 1.1568\tLR: 0.000337\n",
      "Training Epoch: 19 [34816/50000]\tLoss: 1.1391\tLR: 0.000337\n",
      "Training Epoch: 19 [34944/50000]\tLoss: 1.0858\tLR: 0.000337\n",
      "Training Epoch: 19 [35072/50000]\tLoss: 1.1826\tLR: 0.000337\n",
      "Training Epoch: 19 [35200/50000]\tLoss: 1.3164\tLR: 0.000337\n",
      "Training Epoch: 19 [35328/50000]\tLoss: 1.1105\tLR: 0.000337\n",
      "Training Epoch: 19 [35456/50000]\tLoss: 1.1169\tLR: 0.000337\n",
      "Training Epoch: 19 [35584/50000]\tLoss: 1.0131\tLR: 0.000337\n",
      "Training Epoch: 19 [35712/50000]\tLoss: 1.1513\tLR: 0.000337\n",
      "Training Epoch: 19 [35840/50000]\tLoss: 0.9317\tLR: 0.000337\n",
      "Training Epoch: 19 [35968/50000]\tLoss: 0.8902\tLR: 0.000337\n",
      "Training Epoch: 19 [36096/50000]\tLoss: 1.3341\tLR: 0.000337\n",
      "Training Epoch: 19 [36224/50000]\tLoss: 1.2711\tLR: 0.000337\n",
      "Training Epoch: 19 [36352/50000]\tLoss: 1.1599\tLR: 0.000337\n",
      "Training Epoch: 19 [36480/50000]\tLoss: 1.1498\tLR: 0.000337\n",
      "Training Epoch: 19 [36608/50000]\tLoss: 1.1466\tLR: 0.000337\n",
      "Training Epoch: 19 [36736/50000]\tLoss: 0.9084\tLR: 0.000337\n",
      "Training Epoch: 19 [36864/50000]\tLoss: 1.1980\tLR: 0.000337\n",
      "Training Epoch: 19 [36992/50000]\tLoss: 1.0707\tLR: 0.000337\n",
      "Training Epoch: 19 [37120/50000]\tLoss: 1.0707\tLR: 0.000337\n",
      "Training Epoch: 19 [37248/50000]\tLoss: 1.2666\tLR: 0.000337\n",
      "Training Epoch: 19 [37376/50000]\tLoss: 0.9612\tLR: 0.000337\n",
      "Training Epoch: 19 [37504/50000]\tLoss: 1.0280\tLR: 0.000337\n",
      "Training Epoch: 19 [37632/50000]\tLoss: 1.3033\tLR: 0.000337\n",
      "Training Epoch: 19 [37760/50000]\tLoss: 1.1135\tLR: 0.000337\n",
      "Training Epoch: 19 [37888/50000]\tLoss: 1.1514\tLR: 0.000337\n",
      "Training Epoch: 19 [38016/50000]\tLoss: 1.1231\tLR: 0.000337\n",
      "Training Epoch: 19 [38144/50000]\tLoss: 1.1280\tLR: 0.000337\n",
      "Training Epoch: 19 [38272/50000]\tLoss: 1.0443\tLR: 0.000337\n",
      "Training Epoch: 19 [38400/50000]\tLoss: 1.2011\tLR: 0.000337\n",
      "Training Epoch: 19 [38528/50000]\tLoss: 1.0092\tLR: 0.000337\n",
      "Training Epoch: 19 [38656/50000]\tLoss: 1.0205\tLR: 0.000337\n",
      "Training Epoch: 19 [38784/50000]\tLoss: 1.1996\tLR: 0.000337\n",
      "Training Epoch: 19 [38912/50000]\tLoss: 1.0166\tLR: 0.000337\n",
      "Training Epoch: 19 [39040/50000]\tLoss: 0.8832\tLR: 0.000337\n",
      "Training Epoch: 19 [39168/50000]\tLoss: 0.9286\tLR: 0.000337\n",
      "Training Epoch: 19 [39296/50000]\tLoss: 1.5483\tLR: 0.000337\n",
      "Training Epoch: 19 [39424/50000]\tLoss: 0.9722\tLR: 0.000337\n",
      "Training Epoch: 19 [39552/50000]\tLoss: 0.9150\tLR: 0.000337\n",
      "Training Epoch: 19 [39680/50000]\tLoss: 0.9512\tLR: 0.000337\n",
      "Training Epoch: 19 [39808/50000]\tLoss: 1.1946\tLR: 0.000337\n",
      "Training Epoch: 19 [39936/50000]\tLoss: 1.1079\tLR: 0.000337\n",
      "Training Epoch: 19 [40064/50000]\tLoss: 1.1749\tLR: 0.000337\n",
      "Training Epoch: 19 [40192/50000]\tLoss: 0.9486\tLR: 0.000337\n",
      "Training Epoch: 19 [40320/50000]\tLoss: 0.8767\tLR: 0.000337\n",
      "Training Epoch: 19 [40448/50000]\tLoss: 1.0655\tLR: 0.000337\n",
      "Training Epoch: 19 [40576/50000]\tLoss: 1.2260\tLR: 0.000337\n",
      "Training Epoch: 19 [40704/50000]\tLoss: 0.8841\tLR: 0.000337\n",
      "Training Epoch: 19 [40832/50000]\tLoss: 1.0596\tLR: 0.000337\n",
      "Training Epoch: 19 [40960/50000]\tLoss: 1.0988\tLR: 0.000337\n",
      "Training Epoch: 19 [41088/50000]\tLoss: 1.0678\tLR: 0.000337\n",
      "Training Epoch: 19 [41216/50000]\tLoss: 1.2550\tLR: 0.000337\n",
      "Training Epoch: 19 [41344/50000]\tLoss: 1.1564\tLR: 0.000337\n",
      "Training Epoch: 19 [41472/50000]\tLoss: 1.0853\tLR: 0.000337\n",
      "Training Epoch: 19 [41600/50000]\tLoss: 1.1339\tLR: 0.000337\n",
      "Training Epoch: 19 [41728/50000]\tLoss: 0.8810\tLR: 0.000337\n",
      "Training Epoch: 19 [41856/50000]\tLoss: 1.1554\tLR: 0.000337\n",
      "Training Epoch: 19 [41984/50000]\tLoss: 1.1790\tLR: 0.000337\n",
      "Training Epoch: 19 [42112/50000]\tLoss: 1.1985\tLR: 0.000337\n",
      "Training Epoch: 19 [42240/50000]\tLoss: 1.1795\tLR: 0.000337\n",
      "Training Epoch: 19 [42368/50000]\tLoss: 1.2885\tLR: 0.000337\n",
      "Training Epoch: 19 [42496/50000]\tLoss: 1.0327\tLR: 0.000337\n",
      "Training Epoch: 19 [42624/50000]\tLoss: 1.1469\tLR: 0.000337\n",
      "Training Epoch: 19 [42752/50000]\tLoss: 1.2046\tLR: 0.000337\n",
      "Training Epoch: 19 [42880/50000]\tLoss: 1.2013\tLR: 0.000337\n",
      "Training Epoch: 19 [43008/50000]\tLoss: 0.8448\tLR: 0.000337\n",
      "Training Epoch: 19 [43136/50000]\tLoss: 0.9320\tLR: 0.000337\n",
      "Training Epoch: 19 [43264/50000]\tLoss: 0.9993\tLR: 0.000337\n",
      "Training Epoch: 19 [43392/50000]\tLoss: 1.3321\tLR: 0.000337\n",
      "Training Epoch: 19 [43520/50000]\tLoss: 1.0901\tLR: 0.000337\n",
      "Training Epoch: 19 [43648/50000]\tLoss: 1.0214\tLR: 0.000337\n",
      "Training Epoch: 19 [43776/50000]\tLoss: 0.9729\tLR: 0.000337\n",
      "Training Epoch: 19 [43904/50000]\tLoss: 0.9801\tLR: 0.000337\n",
      "Training Epoch: 19 [44032/50000]\tLoss: 0.9245\tLR: 0.000337\n",
      "Training Epoch: 19 [44160/50000]\tLoss: 1.1628\tLR: 0.000337\n",
      "Training Epoch: 19 [44288/50000]\tLoss: 1.1408\tLR: 0.000337\n",
      "Training Epoch: 19 [44416/50000]\tLoss: 1.1970\tLR: 0.000337\n",
      "Training Epoch: 19 [44544/50000]\tLoss: 1.2062\tLR: 0.000337\n",
      "Training Epoch: 19 [44672/50000]\tLoss: 1.0412\tLR: 0.000337\n",
      "Training Epoch: 19 [44800/50000]\tLoss: 1.1726\tLR: 0.000337\n",
      "Training Epoch: 19 [44928/50000]\tLoss: 1.0277\tLR: 0.000337\n",
      "Training Epoch: 19 [45056/50000]\tLoss: 1.0993\tLR: 0.000337\n",
      "Training Epoch: 19 [45184/50000]\tLoss: 1.0776\tLR: 0.000337\n",
      "Training Epoch: 19 [45312/50000]\tLoss: 1.0938\tLR: 0.000337\n",
      "Training Epoch: 19 [45440/50000]\tLoss: 1.1475\tLR: 0.000337\n",
      "Training Epoch: 19 [45568/50000]\tLoss: 1.0661\tLR: 0.000337\n",
      "Training Epoch: 19 [45696/50000]\tLoss: 1.0715\tLR: 0.000337\n",
      "Training Epoch: 19 [45824/50000]\tLoss: 1.1253\tLR: 0.000337\n",
      "Training Epoch: 19 [45952/50000]\tLoss: 1.1143\tLR: 0.000337\n",
      "Training Epoch: 19 [46080/50000]\tLoss: 1.2178\tLR: 0.000337\n",
      "Training Epoch: 19 [46208/50000]\tLoss: 1.0759\tLR: 0.000337\n",
      "Training Epoch: 19 [46336/50000]\tLoss: 1.0734\tLR: 0.000337\n",
      "Training Epoch: 19 [46464/50000]\tLoss: 1.0191\tLR: 0.000337\n",
      "Training Epoch: 19 [46592/50000]\tLoss: 1.3200\tLR: 0.000337\n",
      "Training Epoch: 19 [46720/50000]\tLoss: 1.2678\tLR: 0.000337\n",
      "Training Epoch: 19 [46848/50000]\tLoss: 1.2776\tLR: 0.000337\n",
      "Training Epoch: 19 [46976/50000]\tLoss: 1.1374\tLR: 0.000337\n",
      "Training Epoch: 19 [47104/50000]\tLoss: 0.9502\tLR: 0.000337\n",
      "Training Epoch: 19 [47232/50000]\tLoss: 1.2178\tLR: 0.000337\n",
      "Training Epoch: 19 [47360/50000]\tLoss: 1.2393\tLR: 0.000337\n",
      "Training Epoch: 19 [47488/50000]\tLoss: 1.0541\tLR: 0.000337\n",
      "Training Epoch: 19 [47616/50000]\tLoss: 0.9759\tLR: 0.000337\n",
      "Training Epoch: 19 [47744/50000]\tLoss: 1.0801\tLR: 0.000337\n",
      "Training Epoch: 19 [47872/50000]\tLoss: 0.9932\tLR: 0.000337\n",
      "Training Epoch: 19 [48000/50000]\tLoss: 1.1476\tLR: 0.000337\n",
      "Training Epoch: 19 [48128/50000]\tLoss: 1.2148\tLR: 0.000337\n",
      "Training Epoch: 19 [48256/50000]\tLoss: 1.1556\tLR: 0.000337\n",
      "Training Epoch: 19 [48384/50000]\tLoss: 1.0888\tLR: 0.000337\n",
      "Training Epoch: 19 [48512/50000]\tLoss: 1.3235\tLR: 0.000337\n",
      "Training Epoch: 19 [48640/50000]\tLoss: 0.9589\tLR: 0.000337\n",
      "Training Epoch: 19 [48768/50000]\tLoss: 1.0108\tLR: 0.000337\n",
      "Training Epoch: 19 [48896/50000]\tLoss: 0.9580\tLR: 0.000337\n",
      "Training Epoch: 19 [49024/50000]\tLoss: 1.2578\tLR: 0.000337\n",
      "Training Epoch: 19 [49152/50000]\tLoss: 1.1425\tLR: 0.000337\n",
      "Training Epoch: 19 [49280/50000]\tLoss: 1.0525\tLR: 0.000337\n",
      "Training Epoch: 19 [49408/50000]\tLoss: 0.9777\tLR: 0.000337\n",
      "Training Epoch: 19 [49536/50000]\tLoss: 1.1150\tLR: 0.000337\n",
      "Training Epoch: 19 [49664/50000]\tLoss: 1.0895\tLR: 0.000337\n",
      "Training Epoch: 19 [49792/50000]\tLoss: 0.9629\tLR: 0.000337\n",
      "Training Epoch: 19 [49920/50000]\tLoss: 1.0297\tLR: 0.000337\n",
      "Training Epoch: 19 [50000/50000]\tLoss: 1.2452\tLR: 0.000337\n",
      "Test set: Average loss: 0.0103, Accuracy: 0.6294\n",
      "\n",
      "Training Epoch: 20 [128/50000]\tLoss: 1.1186\tLR: 0.000337\n",
      "Training Epoch: 20 [256/50000]\tLoss: 1.0705\tLR: 0.000337\n",
      "Training Epoch: 20 [384/50000]\tLoss: 0.9818\tLR: 0.000337\n",
      "Training Epoch: 20 [512/50000]\tLoss: 1.1596\tLR: 0.000337\n",
      "Training Epoch: 20 [640/50000]\tLoss: 1.0559\tLR: 0.000337\n",
      "Training Epoch: 20 [768/50000]\tLoss: 1.1143\tLR: 0.000337\n",
      "Training Epoch: 20 [896/50000]\tLoss: 1.2541\tLR: 0.000337\n",
      "Training Epoch: 20 [1024/50000]\tLoss: 1.1725\tLR: 0.000337\n",
      "Training Epoch: 20 [1152/50000]\tLoss: 1.0862\tLR: 0.000337\n",
      "Training Epoch: 20 [1280/50000]\tLoss: 1.1259\tLR: 0.000337\n",
      "Training Epoch: 20 [1408/50000]\tLoss: 1.0287\tLR: 0.000337\n",
      "Training Epoch: 20 [1536/50000]\tLoss: 1.2093\tLR: 0.000337\n",
      "Training Epoch: 20 [1664/50000]\tLoss: 1.0021\tLR: 0.000337\n",
      "Training Epoch: 20 [1792/50000]\tLoss: 1.1644\tLR: 0.000337\n",
      "Training Epoch: 20 [1920/50000]\tLoss: 1.0956\tLR: 0.000337\n",
      "Training Epoch: 20 [2048/50000]\tLoss: 1.1888\tLR: 0.000337\n",
      "Training Epoch: 20 [2176/50000]\tLoss: 1.3065\tLR: 0.000337\n",
      "Training Epoch: 20 [2304/50000]\tLoss: 1.1707\tLR: 0.000337\n",
      "Training Epoch: 20 [2432/50000]\tLoss: 0.9765\tLR: 0.000337\n",
      "Training Epoch: 20 [2560/50000]\tLoss: 1.0584\tLR: 0.000337\n",
      "Training Epoch: 20 [2688/50000]\tLoss: 0.9160\tLR: 0.000337\n",
      "Training Epoch: 20 [2816/50000]\tLoss: 1.1565\tLR: 0.000337\n",
      "Training Epoch: 20 [2944/50000]\tLoss: 1.0176\tLR: 0.000337\n",
      "Training Epoch: 20 [3072/50000]\tLoss: 1.0206\tLR: 0.000337\n",
      "Training Epoch: 20 [3200/50000]\tLoss: 0.9522\tLR: 0.000337\n",
      "Training Epoch: 20 [3328/50000]\tLoss: 1.1805\tLR: 0.000337\n",
      "Training Epoch: 20 [3456/50000]\tLoss: 1.1261\tLR: 0.000337\n",
      "Training Epoch: 20 [3584/50000]\tLoss: 1.1094\tLR: 0.000337\n",
      "Training Epoch: 20 [3712/50000]\tLoss: 0.9181\tLR: 0.000337\n",
      "Training Epoch: 20 [3840/50000]\tLoss: 1.2426\tLR: 0.000337\n",
      "Training Epoch: 20 [3968/50000]\tLoss: 1.2779\tLR: 0.000337\n",
      "Training Epoch: 20 [4096/50000]\tLoss: 1.0880\tLR: 0.000337\n",
      "Training Epoch: 20 [4224/50000]\tLoss: 1.3835\tLR: 0.000337\n",
      "Training Epoch: 20 [4352/50000]\tLoss: 1.1160\tLR: 0.000337\n",
      "Training Epoch: 20 [4480/50000]\tLoss: 1.0329\tLR: 0.000337\n",
      "Training Epoch: 20 [4608/50000]\tLoss: 1.3202\tLR: 0.000337\n",
      "Training Epoch: 20 [4736/50000]\tLoss: 0.9602\tLR: 0.000337\n",
      "Training Epoch: 20 [4864/50000]\tLoss: 1.1967\tLR: 0.000337\n",
      "Training Epoch: 20 [4992/50000]\tLoss: 1.1109\tLR: 0.000337\n",
      "Training Epoch: 20 [5120/50000]\tLoss: 1.0747\tLR: 0.000337\n",
      "Training Epoch: 20 [5248/50000]\tLoss: 1.0612\tLR: 0.000337\n",
      "Training Epoch: 20 [5376/50000]\tLoss: 1.0355\tLR: 0.000337\n",
      "Training Epoch: 20 [5504/50000]\tLoss: 1.0923\tLR: 0.000337\n",
      "Training Epoch: 20 [5632/50000]\tLoss: 1.1868\tLR: 0.000337\n",
      "Training Epoch: 20 [5760/50000]\tLoss: 1.0623\tLR: 0.000337\n",
      "Training Epoch: 20 [5888/50000]\tLoss: 1.1187\tLR: 0.000337\n",
      "Training Epoch: 20 [6016/50000]\tLoss: 1.1659\tLR: 0.000337\n",
      "Training Epoch: 20 [6144/50000]\tLoss: 1.0357\tLR: 0.000337\n",
      "Training Epoch: 20 [6272/50000]\tLoss: 0.9904\tLR: 0.000337\n",
      "Training Epoch: 20 [6400/50000]\tLoss: 1.1313\tLR: 0.000337\n",
      "Training Epoch: 20 [6528/50000]\tLoss: 1.1654\tLR: 0.000337\n",
      "Training Epoch: 20 [6656/50000]\tLoss: 0.9181\tLR: 0.000337\n",
      "Training Epoch: 20 [6784/50000]\tLoss: 0.9691\tLR: 0.000337\n",
      "Training Epoch: 20 [6912/50000]\tLoss: 1.0249\tLR: 0.000337\n",
      "Training Epoch: 20 [7040/50000]\tLoss: 0.9243\tLR: 0.000337\n",
      "Training Epoch: 20 [7168/50000]\tLoss: 1.0912\tLR: 0.000337\n",
      "Training Epoch: 20 [7296/50000]\tLoss: 1.1271\tLR: 0.000337\n",
      "Training Epoch: 20 [7424/50000]\tLoss: 0.9406\tLR: 0.000337\n",
      "Training Epoch: 20 [7552/50000]\tLoss: 1.0218\tLR: 0.000337\n",
      "Training Epoch: 20 [7680/50000]\tLoss: 1.1007\tLR: 0.000337\n",
      "Training Epoch: 20 [7808/50000]\tLoss: 1.1374\tLR: 0.000337\n",
      "Training Epoch: 20 [7936/50000]\tLoss: 1.3428\tLR: 0.000337\n",
      "Training Epoch: 20 [8064/50000]\tLoss: 1.1734\tLR: 0.000337\n",
      "Training Epoch: 20 [8192/50000]\tLoss: 1.0376\tLR: 0.000337\n",
      "Training Epoch: 20 [8320/50000]\tLoss: 1.1521\tLR: 0.000337\n",
      "Training Epoch: 20 [8448/50000]\tLoss: 1.1586\tLR: 0.000337\n",
      "Training Epoch: 20 [8576/50000]\tLoss: 1.1088\tLR: 0.000337\n",
      "Training Epoch: 20 [8704/50000]\tLoss: 1.1266\tLR: 0.000337\n",
      "Training Epoch: 20 [8832/50000]\tLoss: 1.2307\tLR: 0.000337\n",
      "Training Epoch: 20 [8960/50000]\tLoss: 1.0501\tLR: 0.000337\n",
      "Training Epoch: 20 [9088/50000]\tLoss: 1.0756\tLR: 0.000337\n",
      "Training Epoch: 20 [9216/50000]\tLoss: 1.2324\tLR: 0.000337\n",
      "Training Epoch: 20 [9344/50000]\tLoss: 1.2189\tLR: 0.000337\n",
      "Training Epoch: 20 [9472/50000]\tLoss: 1.0653\tLR: 0.000337\n",
      "Training Epoch: 20 [9600/50000]\tLoss: 0.9725\tLR: 0.000337\n",
      "Training Epoch: 20 [9728/50000]\tLoss: 1.2229\tLR: 0.000337\n",
      "Training Epoch: 20 [9856/50000]\tLoss: 1.3158\tLR: 0.000337\n",
      "Training Epoch: 20 [9984/50000]\tLoss: 1.1222\tLR: 0.000337\n",
      "Training Epoch: 20 [10112/50000]\tLoss: 1.1045\tLR: 0.000337\n",
      "Training Epoch: 20 [10240/50000]\tLoss: 1.2316\tLR: 0.000337\n",
      "Training Epoch: 20 [10368/50000]\tLoss: 1.1578\tLR: 0.000337\n",
      "Training Epoch: 20 [10496/50000]\tLoss: 1.1481\tLR: 0.000337\n",
      "Training Epoch: 20 [10624/50000]\tLoss: 1.0702\tLR: 0.000337\n",
      "Training Epoch: 20 [10752/50000]\tLoss: 1.0659\tLR: 0.000337\n",
      "Training Epoch: 20 [10880/50000]\tLoss: 1.0723\tLR: 0.000337\n",
      "Training Epoch: 20 [11008/50000]\tLoss: 1.1132\tLR: 0.000337\n",
      "Training Epoch: 20 [11136/50000]\tLoss: 1.0329\tLR: 0.000337\n",
      "Training Epoch: 20 [11264/50000]\tLoss: 1.0163\tLR: 0.000337\n",
      "Training Epoch: 20 [11392/50000]\tLoss: 1.1046\tLR: 0.000337\n",
      "Training Epoch: 20 [11520/50000]\tLoss: 1.1719\tLR: 0.000337\n",
      "Training Epoch: 20 [11648/50000]\tLoss: 0.9325\tLR: 0.000337\n",
      "Training Epoch: 20 [11776/50000]\tLoss: 1.1187\tLR: 0.000337\n",
      "Training Epoch: 20 [11904/50000]\tLoss: 0.9978\tLR: 0.000337\n",
      "Training Epoch: 20 [12032/50000]\tLoss: 1.2755\tLR: 0.000337\n",
      "Training Epoch: 20 [12160/50000]\tLoss: 1.1410\tLR: 0.000337\n",
      "Training Epoch: 20 [12288/50000]\tLoss: 1.1417\tLR: 0.000337\n",
      "Training Epoch: 20 [12416/50000]\tLoss: 1.0655\tLR: 0.000337\n",
      "Training Epoch: 20 [12544/50000]\tLoss: 1.1255\tLR: 0.000337\n",
      "Training Epoch: 20 [12672/50000]\tLoss: 1.1013\tLR: 0.000337\n",
      "Training Epoch: 20 [12800/50000]\tLoss: 1.0907\tLR: 0.000337\n",
      "Training Epoch: 20 [12928/50000]\tLoss: 0.9778\tLR: 0.000337\n",
      "Training Epoch: 20 [13056/50000]\tLoss: 1.0996\tLR: 0.000337\n",
      "Training Epoch: 20 [13184/50000]\tLoss: 1.0267\tLR: 0.000337\n",
      "Training Epoch: 20 [13312/50000]\tLoss: 1.0991\tLR: 0.000337\n",
      "Training Epoch: 20 [13440/50000]\tLoss: 1.1242\tLR: 0.000337\n",
      "Training Epoch: 20 [13568/50000]\tLoss: 1.1345\tLR: 0.000337\n",
      "Training Epoch: 20 [13696/50000]\tLoss: 1.2864\tLR: 0.000337\n",
      "Training Epoch: 20 [13824/50000]\tLoss: 0.9500\tLR: 0.000337\n",
      "Training Epoch: 20 [13952/50000]\tLoss: 1.0683\tLR: 0.000337\n",
      "Training Epoch: 20 [14080/50000]\tLoss: 1.0978\tLR: 0.000337\n",
      "Training Epoch: 20 [14208/50000]\tLoss: 1.1468\tLR: 0.000337\n",
      "Training Epoch: 20 [14336/50000]\tLoss: 0.9195\tLR: 0.000337\n",
      "Training Epoch: 20 [14464/50000]\tLoss: 1.0244\tLR: 0.000337\n",
      "Training Epoch: 20 [14592/50000]\tLoss: 1.1099\tLR: 0.000337\n",
      "Training Epoch: 20 [14720/50000]\tLoss: 1.0149\tLR: 0.000337\n",
      "Training Epoch: 20 [14848/50000]\tLoss: 0.9669\tLR: 0.000337\n",
      "Training Epoch: 20 [14976/50000]\tLoss: 0.9950\tLR: 0.000337\n",
      "Training Epoch: 20 [15104/50000]\tLoss: 1.0035\tLR: 0.000337\n",
      "Training Epoch: 20 [15232/50000]\tLoss: 1.1421\tLR: 0.000337\n",
      "Training Epoch: 20 [15360/50000]\tLoss: 0.9700\tLR: 0.000337\n",
      "Training Epoch: 20 [15488/50000]\tLoss: 1.3246\tLR: 0.000337\n",
      "Training Epoch: 20 [15616/50000]\tLoss: 0.9937\tLR: 0.000337\n",
      "Training Epoch: 20 [15744/50000]\tLoss: 0.9966\tLR: 0.000337\n",
      "Training Epoch: 20 [15872/50000]\tLoss: 1.0616\tLR: 0.000337\n",
      "Training Epoch: 20 [16000/50000]\tLoss: 1.0007\tLR: 0.000337\n",
      "Training Epoch: 20 [16128/50000]\tLoss: 1.3121\tLR: 0.000337\n",
      "Training Epoch: 20 [16256/50000]\tLoss: 0.9865\tLR: 0.000337\n",
      "Training Epoch: 20 [16384/50000]\tLoss: 1.0595\tLR: 0.000337\n",
      "Training Epoch: 20 [16512/50000]\tLoss: 0.8459\tLR: 0.000337\n",
      "Training Epoch: 20 [16640/50000]\tLoss: 1.1444\tLR: 0.000337\n",
      "Training Epoch: 20 [16768/50000]\tLoss: 1.1618\tLR: 0.000337\n",
      "Training Epoch: 20 [16896/50000]\tLoss: 1.0679\tLR: 0.000337\n",
      "Training Epoch: 20 [17024/50000]\tLoss: 0.8823\tLR: 0.000337\n",
      "Training Epoch: 20 [17152/50000]\tLoss: 1.2560\tLR: 0.000337\n",
      "Training Epoch: 20 [17280/50000]\tLoss: 1.1206\tLR: 0.000337\n",
      "Training Epoch: 20 [17408/50000]\tLoss: 1.1508\tLR: 0.000337\n",
      "Training Epoch: 20 [17536/50000]\tLoss: 1.0754\tLR: 0.000337\n",
      "Training Epoch: 20 [17664/50000]\tLoss: 1.1109\tLR: 0.000337\n",
      "Training Epoch: 20 [17792/50000]\tLoss: 1.2865\tLR: 0.000337\n",
      "Training Epoch: 20 [17920/50000]\tLoss: 1.1245\tLR: 0.000337\n",
      "Training Epoch: 20 [18048/50000]\tLoss: 1.0532\tLR: 0.000337\n",
      "Training Epoch: 20 [18176/50000]\tLoss: 1.1839\tLR: 0.000337\n",
      "Training Epoch: 20 [18304/50000]\tLoss: 0.9622\tLR: 0.000337\n",
      "Training Epoch: 20 [18432/50000]\tLoss: 1.0710\tLR: 0.000337\n",
      "Training Epoch: 20 [18560/50000]\tLoss: 0.9230\tLR: 0.000337\n",
      "Training Epoch: 20 [18688/50000]\tLoss: 0.9990\tLR: 0.000337\n",
      "Training Epoch: 20 [18816/50000]\tLoss: 0.9075\tLR: 0.000337\n",
      "Training Epoch: 20 [18944/50000]\tLoss: 0.9284\tLR: 0.000337\n",
      "Training Epoch: 20 [19072/50000]\tLoss: 1.2670\tLR: 0.000337\n",
      "Training Epoch: 20 [19200/50000]\tLoss: 0.9306\tLR: 0.000337\n",
      "Training Epoch: 20 [19328/50000]\tLoss: 1.0516\tLR: 0.000337\n",
      "Training Epoch: 20 [19456/50000]\tLoss: 1.2477\tLR: 0.000337\n",
      "Training Epoch: 20 [19584/50000]\tLoss: 1.1120\tLR: 0.000337\n",
      "Training Epoch: 20 [19712/50000]\tLoss: 0.9804\tLR: 0.000337\n",
      "Training Epoch: 20 [19840/50000]\tLoss: 0.9206\tLR: 0.000337\n",
      "Training Epoch: 20 [19968/50000]\tLoss: 1.0550\tLR: 0.000337\n",
      "Training Epoch: 20 [20096/50000]\tLoss: 1.1759\tLR: 0.000337\n",
      "Training Epoch: 20 [20224/50000]\tLoss: 1.0651\tLR: 0.000337\n",
      "Training Epoch: 20 [20352/50000]\tLoss: 1.2548\tLR: 0.000337\n",
      "Training Epoch: 20 [20480/50000]\tLoss: 1.1413\tLR: 0.000337\n",
      "Training Epoch: 20 [20608/50000]\tLoss: 0.9407\tLR: 0.000337\n",
      "Training Epoch: 20 [20736/50000]\tLoss: 1.3830\tLR: 0.000337\n",
      "Training Epoch: 20 [20864/50000]\tLoss: 1.2267\tLR: 0.000337\n",
      "Training Epoch: 20 [20992/50000]\tLoss: 0.9814\tLR: 0.000337\n",
      "Training Epoch: 20 [21120/50000]\tLoss: 1.2798\tLR: 0.000337\n",
      "Training Epoch: 20 [21248/50000]\tLoss: 1.0945\tLR: 0.000337\n",
      "Training Epoch: 20 [21376/50000]\tLoss: 1.0206\tLR: 0.000337\n",
      "Training Epoch: 20 [21504/50000]\tLoss: 0.9723\tLR: 0.000337\n",
      "Training Epoch: 20 [21632/50000]\tLoss: 1.0979\tLR: 0.000337\n",
      "Training Epoch: 20 [21760/50000]\tLoss: 1.4253\tLR: 0.000337\n",
      "Training Epoch: 20 [21888/50000]\tLoss: 0.9203\tLR: 0.000337\n",
      "Training Epoch: 20 [22016/50000]\tLoss: 1.0832\tLR: 0.000337\n",
      "Training Epoch: 20 [22144/50000]\tLoss: 0.9257\tLR: 0.000337\n",
      "Training Epoch: 20 [22272/50000]\tLoss: 0.9272\tLR: 0.000337\n",
      "Training Epoch: 20 [22400/50000]\tLoss: 1.0343\tLR: 0.000337\n",
      "Training Epoch: 20 [22528/50000]\tLoss: 1.1281\tLR: 0.000337\n",
      "Training Epoch: 20 [22656/50000]\tLoss: 1.0753\tLR: 0.000337\n",
      "Training Epoch: 20 [22784/50000]\tLoss: 1.2153\tLR: 0.000337\n",
      "Training Epoch: 20 [22912/50000]\tLoss: 1.3643\tLR: 0.000337\n",
      "Training Epoch: 20 [23040/50000]\tLoss: 1.1136\tLR: 0.000337\n",
      "Training Epoch: 20 [23168/50000]\tLoss: 1.1520\tLR: 0.000337\n",
      "Training Epoch: 20 [23296/50000]\tLoss: 1.0330\tLR: 0.000337\n",
      "Training Epoch: 20 [23424/50000]\tLoss: 1.0555\tLR: 0.000337\n",
      "Training Epoch: 20 [23552/50000]\tLoss: 1.2214\tLR: 0.000337\n",
      "Training Epoch: 20 [23680/50000]\tLoss: 1.2328\tLR: 0.000337\n",
      "Training Epoch: 20 [23808/50000]\tLoss: 1.0876\tLR: 0.000337\n",
      "Training Epoch: 20 [23936/50000]\tLoss: 1.0684\tLR: 0.000337\n",
      "Training Epoch: 20 [24064/50000]\tLoss: 1.2188\tLR: 0.000337\n",
      "Training Epoch: 20 [24192/50000]\tLoss: 1.1951\tLR: 0.000337\n",
      "Training Epoch: 20 [24320/50000]\tLoss: 0.9109\tLR: 0.000337\n",
      "Training Epoch: 20 [24448/50000]\tLoss: 1.1073\tLR: 0.000337\n",
      "Training Epoch: 20 [24576/50000]\tLoss: 1.2197\tLR: 0.000337\n",
      "Training Epoch: 20 [24704/50000]\tLoss: 1.0321\tLR: 0.000337\n",
      "Training Epoch: 20 [24832/50000]\tLoss: 1.0366\tLR: 0.000337\n",
      "Training Epoch: 20 [24960/50000]\tLoss: 0.8801\tLR: 0.000337\n",
      "Training Epoch: 20 [25088/50000]\tLoss: 1.1204\tLR: 0.000337\n",
      "Training Epoch: 20 [25216/50000]\tLoss: 1.1450\tLR: 0.000337\n",
      "Training Epoch: 20 [25344/50000]\tLoss: 1.1211\tLR: 0.000337\n",
      "Training Epoch: 20 [25472/50000]\tLoss: 0.9968\tLR: 0.000337\n",
      "Training Epoch: 20 [25600/50000]\tLoss: 1.1626\tLR: 0.000337\n",
      "Training Epoch: 20 [25728/50000]\tLoss: 1.2050\tLR: 0.000337\n",
      "Training Epoch: 20 [25856/50000]\tLoss: 1.0874\tLR: 0.000337\n",
      "Training Epoch: 20 [25984/50000]\tLoss: 1.2087\tLR: 0.000337\n",
      "Training Epoch: 20 [26112/50000]\tLoss: 0.9741\tLR: 0.000337\n",
      "Training Epoch: 20 [26240/50000]\tLoss: 1.2023\tLR: 0.000337\n",
      "Training Epoch: 20 [26368/50000]\tLoss: 1.0273\tLR: 0.000337\n",
      "Training Epoch: 20 [26496/50000]\tLoss: 1.2667\tLR: 0.000337\n",
      "Training Epoch: 20 [26624/50000]\tLoss: 1.1166\tLR: 0.000337\n",
      "Training Epoch: 20 [26752/50000]\tLoss: 1.1101\tLR: 0.000337\n",
      "Training Epoch: 20 [26880/50000]\tLoss: 1.2029\tLR: 0.000337\n",
      "Training Epoch: 20 [27008/50000]\tLoss: 0.9180\tLR: 0.000337\n",
      "Training Epoch: 20 [27136/50000]\tLoss: 1.0808\tLR: 0.000337\n",
      "Training Epoch: 20 [27264/50000]\tLoss: 1.0767\tLR: 0.000337\n",
      "Training Epoch: 20 [27392/50000]\tLoss: 1.3270\tLR: 0.000337\n",
      "Training Epoch: 20 [27520/50000]\tLoss: 1.2936\tLR: 0.000337\n",
      "Training Epoch: 20 [27648/50000]\tLoss: 1.0780\tLR: 0.000337\n",
      "Training Epoch: 20 [27776/50000]\tLoss: 1.1521\tLR: 0.000337\n",
      "Training Epoch: 20 [27904/50000]\tLoss: 1.1008\tLR: 0.000337\n",
      "Training Epoch: 20 [28032/50000]\tLoss: 1.1271\tLR: 0.000337\n",
      "Training Epoch: 20 [28160/50000]\tLoss: 0.9923\tLR: 0.000337\n",
      "Training Epoch: 20 [28288/50000]\tLoss: 1.0751\tLR: 0.000337\n",
      "Training Epoch: 20 [28416/50000]\tLoss: 1.0252\tLR: 0.000337\n",
      "Training Epoch: 20 [28544/50000]\tLoss: 0.8807\tLR: 0.000337\n",
      "Training Epoch: 20 [28672/50000]\tLoss: 1.0699\tLR: 0.000337\n",
      "Training Epoch: 20 [28800/50000]\tLoss: 1.0869\tLR: 0.000337\n",
      "Training Epoch: 20 [28928/50000]\tLoss: 1.2311\tLR: 0.000337\n",
      "Training Epoch: 20 [29056/50000]\tLoss: 1.0731\tLR: 0.000337\n",
      "Training Epoch: 20 [29184/50000]\tLoss: 0.9987\tLR: 0.000337\n",
      "Training Epoch: 20 [29312/50000]\tLoss: 0.9883\tLR: 0.000337\n",
      "Training Epoch: 20 [29440/50000]\tLoss: 1.1967\tLR: 0.000337\n",
      "Training Epoch: 20 [29568/50000]\tLoss: 1.0469\tLR: 0.000337\n",
      "Training Epoch: 20 [29696/50000]\tLoss: 0.8361\tLR: 0.000337\n",
      "Training Epoch: 20 [29824/50000]\tLoss: 0.9944\tLR: 0.000337\n",
      "Training Epoch: 20 [29952/50000]\tLoss: 1.0488\tLR: 0.000337\n",
      "Training Epoch: 20 [30080/50000]\tLoss: 0.9162\tLR: 0.000337\n",
      "Training Epoch: 20 [30208/50000]\tLoss: 0.9274\tLR: 0.000337\n",
      "Training Epoch: 20 [30336/50000]\tLoss: 1.2585\tLR: 0.000337\n",
      "Training Epoch: 20 [30464/50000]\tLoss: 0.9753\tLR: 0.000337\n",
      "Training Epoch: 20 [30592/50000]\tLoss: 1.0226\tLR: 0.000337\n",
      "Training Epoch: 20 [30720/50000]\tLoss: 1.1195\tLR: 0.000337\n",
      "Training Epoch: 20 [30848/50000]\tLoss: 1.1775\tLR: 0.000337\n",
      "Training Epoch: 20 [30976/50000]\tLoss: 1.0238\tLR: 0.000337\n",
      "Training Epoch: 20 [31104/50000]\tLoss: 1.0344\tLR: 0.000337\n",
      "Training Epoch: 20 [31232/50000]\tLoss: 0.8941\tLR: 0.000337\n",
      "Training Epoch: 20 [31360/50000]\tLoss: 1.0125\tLR: 0.000337\n",
      "Training Epoch: 20 [31488/50000]\tLoss: 1.2030\tLR: 0.000337\n",
      "Training Epoch: 20 [31616/50000]\tLoss: 1.2531\tLR: 0.000337\n",
      "Training Epoch: 20 [31744/50000]\tLoss: 1.2692\tLR: 0.000337\n",
      "Training Epoch: 20 [31872/50000]\tLoss: 1.2524\tLR: 0.000337\n",
      "Training Epoch: 20 [32000/50000]\tLoss: 1.1141\tLR: 0.000337\n",
      "Training Epoch: 20 [32128/50000]\tLoss: 0.8354\tLR: 0.000337\n",
      "Training Epoch: 20 [32256/50000]\tLoss: 1.1703\tLR: 0.000337\n",
      "Training Epoch: 20 [32384/50000]\tLoss: 1.2257\tLR: 0.000337\n",
      "Training Epoch: 20 [32512/50000]\tLoss: 1.2723\tLR: 0.000337\n",
      "Training Epoch: 20 [32640/50000]\tLoss: 1.1979\tLR: 0.000337\n",
      "Training Epoch: 20 [32768/50000]\tLoss: 0.9063\tLR: 0.000337\n",
      "Training Epoch: 20 [32896/50000]\tLoss: 0.9941\tLR: 0.000337\n",
      "Training Epoch: 20 [33024/50000]\tLoss: 1.0576\tLR: 0.000337\n",
      "Training Epoch: 20 [33152/50000]\tLoss: 1.1600\tLR: 0.000337\n",
      "Training Epoch: 20 [33280/50000]\tLoss: 1.1449\tLR: 0.000337\n",
      "Training Epoch: 20 [33408/50000]\tLoss: 1.1127\tLR: 0.000337\n",
      "Training Epoch: 20 [33536/50000]\tLoss: 1.2990\tLR: 0.000337\n",
      "Training Epoch: 20 [33664/50000]\tLoss: 1.1424\tLR: 0.000337\n",
      "Training Epoch: 20 [33792/50000]\tLoss: 0.8885\tLR: 0.000337\n",
      "Training Epoch: 20 [33920/50000]\tLoss: 1.4459\tLR: 0.000337\n",
      "Training Epoch: 20 [34048/50000]\tLoss: 1.1678\tLR: 0.000337\n",
      "Training Epoch: 20 [34176/50000]\tLoss: 0.9683\tLR: 0.000337\n",
      "Training Epoch: 20 [34304/50000]\tLoss: 1.0235\tLR: 0.000337\n",
      "Training Epoch: 20 [34432/50000]\tLoss: 1.1376\tLR: 0.000337\n",
      "Training Epoch: 20 [34560/50000]\tLoss: 1.1582\tLR: 0.000337\n",
      "Training Epoch: 20 [34688/50000]\tLoss: 1.2457\tLR: 0.000337\n",
      "Training Epoch: 20 [34816/50000]\tLoss: 1.1191\tLR: 0.000337\n",
      "Training Epoch: 20 [34944/50000]\tLoss: 1.0861\tLR: 0.000337\n",
      "Training Epoch: 20 [35072/50000]\tLoss: 0.8224\tLR: 0.000337\n",
      "Training Epoch: 20 [35200/50000]\tLoss: 1.1075\tLR: 0.000337\n",
      "Training Epoch: 20 [35328/50000]\tLoss: 1.0238\tLR: 0.000337\n",
      "Training Epoch: 20 [35456/50000]\tLoss: 1.0419\tLR: 0.000337\n",
      "Training Epoch: 20 [35584/50000]\tLoss: 1.2296\tLR: 0.000337\n",
      "Training Epoch: 20 [35712/50000]\tLoss: 0.9408\tLR: 0.000337\n",
      "Training Epoch: 20 [35840/50000]\tLoss: 0.9215\tLR: 0.000337\n",
      "Training Epoch: 20 [35968/50000]\tLoss: 1.2000\tLR: 0.000337\n",
      "Training Epoch: 20 [36096/50000]\tLoss: 1.1236\tLR: 0.000337\n",
      "Training Epoch: 20 [36224/50000]\tLoss: 0.9875\tLR: 0.000337\n",
      "Training Epoch: 20 [36352/50000]\tLoss: 1.0634\tLR: 0.000337\n",
      "Training Epoch: 20 [36480/50000]\tLoss: 1.0690\tLR: 0.000337\n",
      "Training Epoch: 20 [36608/50000]\tLoss: 0.8724\tLR: 0.000337\n",
      "Training Epoch: 20 [36736/50000]\tLoss: 1.0148\tLR: 0.000337\n",
      "Training Epoch: 20 [36864/50000]\tLoss: 1.0726\tLR: 0.000337\n",
      "Training Epoch: 20 [36992/50000]\tLoss: 0.9389\tLR: 0.000337\n",
      "Training Epoch: 20 [37120/50000]\tLoss: 0.9708\tLR: 0.000337\n",
      "Training Epoch: 20 [37248/50000]\tLoss: 1.0777\tLR: 0.000337\n",
      "Training Epoch: 20 [37376/50000]\tLoss: 1.2438\tLR: 0.000337\n",
      "Training Epoch: 20 [37504/50000]\tLoss: 1.1057\tLR: 0.000337\n",
      "Training Epoch: 20 [37632/50000]\tLoss: 1.1794\tLR: 0.000337\n",
      "Training Epoch: 20 [37760/50000]\tLoss: 1.2398\tLR: 0.000337\n",
      "Training Epoch: 20 [37888/50000]\tLoss: 1.2100\tLR: 0.000337\n",
      "Training Epoch: 20 [38016/50000]\tLoss: 1.3488\tLR: 0.000337\n",
      "Training Epoch: 20 [38144/50000]\tLoss: 1.1042\tLR: 0.000337\n",
      "Training Epoch: 20 [38272/50000]\tLoss: 1.2679\tLR: 0.000337\n",
      "Training Epoch: 20 [38400/50000]\tLoss: 1.0124\tLR: 0.000337\n",
      "Training Epoch: 20 [38528/50000]\tLoss: 1.1367\tLR: 0.000337\n",
      "Training Epoch: 20 [38656/50000]\tLoss: 1.1413\tLR: 0.000337\n",
      "Training Epoch: 20 [38784/50000]\tLoss: 0.9561\tLR: 0.000337\n",
      "Training Epoch: 20 [38912/50000]\tLoss: 1.1260\tLR: 0.000337\n",
      "Training Epoch: 20 [39040/50000]\tLoss: 1.0916\tLR: 0.000337\n",
      "Training Epoch: 20 [39168/50000]\tLoss: 1.3038\tLR: 0.000337\n",
      "Training Epoch: 20 [39296/50000]\tLoss: 1.2666\tLR: 0.000337\n",
      "Training Epoch: 20 [39424/50000]\tLoss: 1.0097\tLR: 0.000337\n",
      "Training Epoch: 20 [39552/50000]\tLoss: 1.0637\tLR: 0.000337\n",
      "Training Epoch: 20 [39680/50000]\tLoss: 1.0372\tLR: 0.000337\n",
      "Training Epoch: 20 [39808/50000]\tLoss: 1.1802\tLR: 0.000337\n",
      "Training Epoch: 20 [39936/50000]\tLoss: 0.9651\tLR: 0.000337\n",
      "Training Epoch: 20 [40064/50000]\tLoss: 1.1307\tLR: 0.000337\n",
      "Training Epoch: 20 [40192/50000]\tLoss: 1.1546\tLR: 0.000337\n",
      "Training Epoch: 20 [40320/50000]\tLoss: 1.1781\tLR: 0.000337\n",
      "Training Epoch: 20 [40448/50000]\tLoss: 0.8613\tLR: 0.000337\n",
      "Training Epoch: 20 [40576/50000]\tLoss: 1.0325\tLR: 0.000337\n",
      "Training Epoch: 20 [40704/50000]\tLoss: 1.1453\tLR: 0.000337\n",
      "Training Epoch: 20 [40832/50000]\tLoss: 1.2039\tLR: 0.000337\n",
      "Training Epoch: 20 [40960/50000]\tLoss: 1.0997\tLR: 0.000337\n",
      "Training Epoch: 20 [41088/50000]\tLoss: 1.3248\tLR: 0.000337\n",
      "Training Epoch: 20 [41216/50000]\tLoss: 1.1156\tLR: 0.000337\n",
      "Training Epoch: 20 [41344/50000]\tLoss: 1.1703\tLR: 0.000337\n",
      "Training Epoch: 20 [41472/50000]\tLoss: 1.2196\tLR: 0.000337\n",
      "Training Epoch: 20 [41600/50000]\tLoss: 1.0498\tLR: 0.000337\n",
      "Training Epoch: 20 [41728/50000]\tLoss: 1.1666\tLR: 0.000337\n",
      "Training Epoch: 20 [41856/50000]\tLoss: 0.9907\tLR: 0.000337\n",
      "Training Epoch: 20 [41984/50000]\tLoss: 1.2018\tLR: 0.000337\n",
      "Training Epoch: 20 [42112/50000]\tLoss: 1.1000\tLR: 0.000337\n",
      "Training Epoch: 20 [42240/50000]\tLoss: 1.0531\tLR: 0.000337\n",
      "Training Epoch: 20 [42368/50000]\tLoss: 0.9621\tLR: 0.000337\n",
      "Training Epoch: 20 [42496/50000]\tLoss: 1.0361\tLR: 0.000337\n",
      "Training Epoch: 20 [42624/50000]\tLoss: 1.1565\tLR: 0.000337\n",
      "Training Epoch: 20 [42752/50000]\tLoss: 1.1363\tLR: 0.000337\n",
      "Training Epoch: 20 [42880/50000]\tLoss: 1.0434\tLR: 0.000337\n",
      "Training Epoch: 20 [43008/50000]\tLoss: 1.1607\tLR: 0.000337\n",
      "Training Epoch: 20 [43136/50000]\tLoss: 1.0781\tLR: 0.000337\n",
      "Training Epoch: 20 [43264/50000]\tLoss: 1.1883\tLR: 0.000337\n",
      "Training Epoch: 20 [43392/50000]\tLoss: 1.2002\tLR: 0.000337\n",
      "Training Epoch: 20 [43520/50000]\tLoss: 1.2781\tLR: 0.000337\n",
      "Training Epoch: 20 [43648/50000]\tLoss: 0.8498\tLR: 0.000337\n",
      "Training Epoch: 20 [43776/50000]\tLoss: 1.1806\tLR: 0.000337\n",
      "Training Epoch: 20 [43904/50000]\tLoss: 1.2642\tLR: 0.000337\n",
      "Training Epoch: 20 [44032/50000]\tLoss: 0.9231\tLR: 0.000337\n",
      "Training Epoch: 20 [44160/50000]\tLoss: 1.1672\tLR: 0.000337\n",
      "Training Epoch: 20 [44288/50000]\tLoss: 1.0961\tLR: 0.000337\n",
      "Training Epoch: 20 [44416/50000]\tLoss: 1.2719\tLR: 0.000337\n",
      "Training Epoch: 20 [44544/50000]\tLoss: 1.1217\tLR: 0.000337\n",
      "Training Epoch: 20 [44672/50000]\tLoss: 0.9037\tLR: 0.000337\n",
      "Training Epoch: 20 [44800/50000]\tLoss: 1.1776\tLR: 0.000337\n",
      "Training Epoch: 20 [44928/50000]\tLoss: 0.9463\tLR: 0.000337\n",
      "Training Epoch: 20 [45056/50000]\tLoss: 0.9200\tLR: 0.000337\n",
      "Training Epoch: 20 [45184/50000]\tLoss: 1.0359\tLR: 0.000337\n",
      "Training Epoch: 20 [45312/50000]\tLoss: 0.8939\tLR: 0.000337\n",
      "Training Epoch: 20 [45440/50000]\tLoss: 1.1607\tLR: 0.000337\n",
      "Training Epoch: 20 [45568/50000]\tLoss: 0.9345\tLR: 0.000337\n",
      "Training Epoch: 20 [45696/50000]\tLoss: 1.0972\tLR: 0.000337\n",
      "Training Epoch: 20 [45824/50000]\tLoss: 1.2127\tLR: 0.000337\n",
      "Training Epoch: 20 [45952/50000]\tLoss: 1.3008\tLR: 0.000337\n",
      "Training Epoch: 20 [46080/50000]\tLoss: 1.0104\tLR: 0.000337\n",
      "Training Epoch: 20 [46208/50000]\tLoss: 1.2146\tLR: 0.000337\n",
      "Training Epoch: 20 [46336/50000]\tLoss: 1.2505\tLR: 0.000337\n",
      "Training Epoch: 20 [46464/50000]\tLoss: 1.1369\tLR: 0.000337\n",
      "Training Epoch: 20 [46592/50000]\tLoss: 1.3333\tLR: 0.000337\n",
      "Training Epoch: 20 [46720/50000]\tLoss: 0.9854\tLR: 0.000337\n",
      "Training Epoch: 20 [46848/50000]\tLoss: 1.1306\tLR: 0.000337\n",
      "Training Epoch: 20 [46976/50000]\tLoss: 1.0360\tLR: 0.000337\n",
      "Training Epoch: 20 [47104/50000]\tLoss: 1.1767\tLR: 0.000337\n",
      "Training Epoch: 20 [47232/50000]\tLoss: 1.2585\tLR: 0.000337\n",
      "Training Epoch: 20 [47360/50000]\tLoss: 1.1126\tLR: 0.000337\n",
      "Training Epoch: 20 [47488/50000]\tLoss: 1.1174\tLR: 0.000337\n",
      "Training Epoch: 20 [47616/50000]\tLoss: 1.2199\tLR: 0.000337\n",
      "Training Epoch: 20 [47744/50000]\tLoss: 0.9199\tLR: 0.000337\n",
      "Training Epoch: 20 [47872/50000]\tLoss: 1.3217\tLR: 0.000337\n",
      "Training Epoch: 20 [48000/50000]\tLoss: 1.1270\tLR: 0.000337\n",
      "Training Epoch: 20 [48128/50000]\tLoss: 0.9582\tLR: 0.000337\n",
      "Training Epoch: 20 [48256/50000]\tLoss: 1.0445\tLR: 0.000337\n",
      "Training Epoch: 20 [48384/50000]\tLoss: 1.0030\tLR: 0.000337\n",
      "Training Epoch: 20 [48512/50000]\tLoss: 1.1550\tLR: 0.000337\n",
      "Training Epoch: 20 [48640/50000]\tLoss: 1.1232\tLR: 0.000337\n",
      "Training Epoch: 20 [48768/50000]\tLoss: 1.0283\tLR: 0.000337\n",
      "Training Epoch: 20 [48896/50000]\tLoss: 1.1169\tLR: 0.000337\n",
      "Training Epoch: 20 [49024/50000]\tLoss: 1.1165\tLR: 0.000337\n",
      "Training Epoch: 20 [49152/50000]\tLoss: 1.1826\tLR: 0.000337\n",
      "Training Epoch: 20 [49280/50000]\tLoss: 0.9222\tLR: 0.000337\n",
      "Training Epoch: 20 [49408/50000]\tLoss: 1.1083\tLR: 0.000337\n",
      "Training Epoch: 20 [49536/50000]\tLoss: 1.1005\tLR: 0.000337\n",
      "Training Epoch: 20 [49664/50000]\tLoss: 1.1210\tLR: 0.000337\n",
      "Training Epoch: 20 [49792/50000]\tLoss: 1.0708\tLR: 0.000337\n",
      "Training Epoch: 20 [49920/50000]\tLoss: 0.9675\tLR: 0.000337\n",
      "Training Epoch: 20 [50000/50000]\tLoss: 1.1675\tLR: 0.000337\n",
      "Test set: Average loss: 0.0103, Accuracy: 0.6260\n",
      "\n",
      "Training Epoch: 21 [128/50000]\tLoss: 1.1305\tLR: 0.000337\n",
      "Training Epoch: 21 [256/50000]\tLoss: 1.0148\tLR: 0.000337\n",
      "Training Epoch: 21 [384/50000]\tLoss: 1.3361\tLR: 0.000337\n",
      "Training Epoch: 21 [512/50000]\tLoss: 1.0415\tLR: 0.000337\n",
      "Training Epoch: 21 [640/50000]\tLoss: 0.8830\tLR: 0.000337\n",
      "Training Epoch: 21 [768/50000]\tLoss: 0.9963\tLR: 0.000337\n",
      "Training Epoch: 21 [896/50000]\tLoss: 1.2098\tLR: 0.000337\n",
      "Training Epoch: 21 [1024/50000]\tLoss: 1.0743\tLR: 0.000337\n",
      "Training Epoch: 21 [1152/50000]\tLoss: 1.2403\tLR: 0.000337\n",
      "Training Epoch: 21 [1280/50000]\tLoss: 1.1805\tLR: 0.000337\n",
      "Training Epoch: 21 [1408/50000]\tLoss: 1.1664\tLR: 0.000337\n",
      "Training Epoch: 21 [1536/50000]\tLoss: 1.0636\tLR: 0.000337\n",
      "Training Epoch: 21 [1664/50000]\tLoss: 1.0066\tLR: 0.000337\n",
      "Training Epoch: 21 [1792/50000]\tLoss: 1.1893\tLR: 0.000337\n",
      "Training Epoch: 21 [1920/50000]\tLoss: 1.0909\tLR: 0.000337\n",
      "Training Epoch: 21 [2048/50000]\tLoss: 0.9950\tLR: 0.000337\n",
      "Training Epoch: 21 [2176/50000]\tLoss: 1.0433\tLR: 0.000337\n",
      "Training Epoch: 21 [2304/50000]\tLoss: 1.4520\tLR: 0.000337\n",
      "Training Epoch: 21 [2432/50000]\tLoss: 1.3446\tLR: 0.000337\n",
      "Training Epoch: 21 [2560/50000]\tLoss: 1.0373\tLR: 0.000337\n",
      "Training Epoch: 21 [2688/50000]\tLoss: 1.0780\tLR: 0.000337\n",
      "Training Epoch: 21 [2816/50000]\tLoss: 1.1355\tLR: 0.000337\n",
      "Training Epoch: 21 [2944/50000]\tLoss: 1.1545\tLR: 0.000337\n",
      "Training Epoch: 21 [3072/50000]\tLoss: 1.1101\tLR: 0.000337\n",
      "Training Epoch: 21 [3200/50000]\tLoss: 1.1852\tLR: 0.000337\n",
      "Training Epoch: 21 [3328/50000]\tLoss: 1.0171\tLR: 0.000337\n",
      "Training Epoch: 21 [3456/50000]\tLoss: 1.1667\tLR: 0.000337\n",
      "Training Epoch: 21 [3584/50000]\tLoss: 1.3388\tLR: 0.000337\n",
      "Training Epoch: 21 [3712/50000]\tLoss: 0.9795\tLR: 0.000337\n",
      "Training Epoch: 21 [3840/50000]\tLoss: 1.2366\tLR: 0.000337\n",
      "Training Epoch: 21 [3968/50000]\tLoss: 1.0734\tLR: 0.000337\n",
      "Training Epoch: 21 [4096/50000]\tLoss: 1.2329\tLR: 0.000337\n",
      "Training Epoch: 21 [4224/50000]\tLoss: 1.0175\tLR: 0.000337\n",
      "Training Epoch: 21 [4352/50000]\tLoss: 1.0614\tLR: 0.000337\n",
      "Training Epoch: 21 [4480/50000]\tLoss: 1.0662\tLR: 0.000337\n",
      "Training Epoch: 21 [4608/50000]\tLoss: 1.0051\tLR: 0.000337\n",
      "Training Epoch: 21 [4736/50000]\tLoss: 0.8566\tLR: 0.000337\n",
      "Training Epoch: 21 [4864/50000]\tLoss: 1.0925\tLR: 0.000337\n",
      "Training Epoch: 21 [4992/50000]\tLoss: 1.3003\tLR: 0.000337\n",
      "Training Epoch: 21 [5120/50000]\tLoss: 1.2012\tLR: 0.000337\n",
      "Training Epoch: 21 [5248/50000]\tLoss: 1.1251\tLR: 0.000337\n",
      "Training Epoch: 21 [5376/50000]\tLoss: 1.0516\tLR: 0.000337\n",
      "Training Epoch: 21 [5504/50000]\tLoss: 1.1318\tLR: 0.000337\n",
      "Training Epoch: 21 [5632/50000]\tLoss: 0.9150\tLR: 0.000337\n",
      "Training Epoch: 21 [5760/50000]\tLoss: 1.0111\tLR: 0.000337\n",
      "Training Epoch: 21 [5888/50000]\tLoss: 0.9699\tLR: 0.000337\n",
      "Training Epoch: 21 [6016/50000]\tLoss: 0.9577\tLR: 0.000337\n",
      "Training Epoch: 21 [6144/50000]\tLoss: 0.8948\tLR: 0.000337\n",
      "Training Epoch: 21 [6272/50000]\tLoss: 1.1354\tLR: 0.000337\n",
      "Training Epoch: 21 [6400/50000]\tLoss: 1.0725\tLR: 0.000337\n",
      "Training Epoch: 21 [6528/50000]\tLoss: 1.0086\tLR: 0.000337\n",
      "Training Epoch: 21 [6656/50000]\tLoss: 1.1153\tLR: 0.000337\n",
      "Training Epoch: 21 [6784/50000]\tLoss: 0.9559\tLR: 0.000337\n",
      "Training Epoch: 21 [6912/50000]\tLoss: 0.9893\tLR: 0.000337\n",
      "Training Epoch: 21 [7040/50000]\tLoss: 1.0735\tLR: 0.000337\n",
      "Training Epoch: 21 [7168/50000]\tLoss: 1.0445\tLR: 0.000337\n",
      "Training Epoch: 21 [7296/50000]\tLoss: 0.9535\tLR: 0.000337\n",
      "Training Epoch: 21 [7424/50000]\tLoss: 1.0713\tLR: 0.000337\n",
      "Training Epoch: 21 [7552/50000]\tLoss: 1.0164\tLR: 0.000337\n",
      "Training Epoch: 21 [7680/50000]\tLoss: 1.1161\tLR: 0.000337\n",
      "Training Epoch: 21 [7808/50000]\tLoss: 1.1266\tLR: 0.000337\n",
      "Training Epoch: 21 [7936/50000]\tLoss: 1.1478\tLR: 0.000337\n",
      "Training Epoch: 21 [8064/50000]\tLoss: 0.8793\tLR: 0.000337\n",
      "Training Epoch: 21 [8192/50000]\tLoss: 1.1192\tLR: 0.000337\n",
      "Training Epoch: 21 [8320/50000]\tLoss: 1.1251\tLR: 0.000337\n",
      "Training Epoch: 21 [8448/50000]\tLoss: 1.2007\tLR: 0.000337\n",
      "Training Epoch: 21 [8576/50000]\tLoss: 1.0984\tLR: 0.000337\n",
      "Training Epoch: 21 [8704/50000]\tLoss: 1.0615\tLR: 0.000337\n",
      "Training Epoch: 21 [8832/50000]\tLoss: 0.7718\tLR: 0.000337\n",
      "Training Epoch: 21 [8960/50000]\tLoss: 1.0471\tLR: 0.000337\n",
      "Training Epoch: 21 [9088/50000]\tLoss: 1.2130\tLR: 0.000337\n",
      "Training Epoch: 21 [9216/50000]\tLoss: 1.1900\tLR: 0.000337\n",
      "Training Epoch: 21 [9344/50000]\tLoss: 1.0873\tLR: 0.000337\n",
      "Training Epoch: 21 [9472/50000]\tLoss: 1.0266\tLR: 0.000337\n",
      "Training Epoch: 21 [9600/50000]\tLoss: 1.3094\tLR: 0.000337\n",
      "Training Epoch: 21 [9728/50000]\tLoss: 1.1996\tLR: 0.000337\n",
      "Training Epoch: 21 [9856/50000]\tLoss: 1.2000\tLR: 0.000337\n",
      "Training Epoch: 21 [9984/50000]\tLoss: 1.1207\tLR: 0.000337\n",
      "Training Epoch: 21 [10112/50000]\tLoss: 1.1125\tLR: 0.000337\n",
      "Training Epoch: 21 [10240/50000]\tLoss: 1.0276\tLR: 0.000337\n",
      "Training Epoch: 21 [10368/50000]\tLoss: 1.1765\tLR: 0.000337\n",
      "Training Epoch: 21 [10496/50000]\tLoss: 1.2479\tLR: 0.000337\n",
      "Training Epoch: 21 [10624/50000]\tLoss: 1.0205\tLR: 0.000337\n",
      "Training Epoch: 21 [10752/50000]\tLoss: 1.0780\tLR: 0.000337\n",
      "Training Epoch: 21 [10880/50000]\tLoss: 1.1526\tLR: 0.000337\n",
      "Training Epoch: 21 [11008/50000]\tLoss: 1.0092\tLR: 0.000337\n",
      "Training Epoch: 21 [11136/50000]\tLoss: 1.1257\tLR: 0.000337\n",
      "Training Epoch: 21 [11264/50000]\tLoss: 1.2202\tLR: 0.000337\n",
      "Training Epoch: 21 [11392/50000]\tLoss: 1.0918\tLR: 0.000337\n",
      "Training Epoch: 21 [11520/50000]\tLoss: 1.0880\tLR: 0.000337\n",
      "Training Epoch: 21 [11648/50000]\tLoss: 1.2039\tLR: 0.000337\n",
      "Training Epoch: 21 [11776/50000]\tLoss: 1.1112\tLR: 0.000337\n",
      "Training Epoch: 21 [11904/50000]\tLoss: 1.0601\tLR: 0.000337\n",
      "Training Epoch: 21 [12032/50000]\tLoss: 1.1357\tLR: 0.000337\n",
      "Training Epoch: 21 [12160/50000]\tLoss: 1.1476\tLR: 0.000337\n",
      "Training Epoch: 21 [12288/50000]\tLoss: 1.1667\tLR: 0.000337\n",
      "Training Epoch: 21 [12416/50000]\tLoss: 0.9413\tLR: 0.000337\n",
      "Training Epoch: 21 [12544/50000]\tLoss: 1.2159\tLR: 0.000337\n",
      "Training Epoch: 21 [12672/50000]\tLoss: 0.9329\tLR: 0.000337\n",
      "Training Epoch: 21 [12800/50000]\tLoss: 1.1712\tLR: 0.000337\n",
      "Training Epoch: 21 [12928/50000]\tLoss: 1.0754\tLR: 0.000337\n",
      "Training Epoch: 21 [13056/50000]\tLoss: 1.2312\tLR: 0.000337\n",
      "Training Epoch: 21 [13184/50000]\tLoss: 1.0909\tLR: 0.000337\n",
      "Training Epoch: 21 [13312/50000]\tLoss: 0.9400\tLR: 0.000337\n",
      "Training Epoch: 21 [13440/50000]\tLoss: 1.1147\tLR: 0.000337\n",
      "Training Epoch: 21 [13568/50000]\tLoss: 1.1451\tLR: 0.000337\n",
      "Training Epoch: 21 [13696/50000]\tLoss: 1.0644\tLR: 0.000337\n",
      "Training Epoch: 21 [13824/50000]\tLoss: 1.0798\tLR: 0.000337\n",
      "Training Epoch: 21 [13952/50000]\tLoss: 0.9213\tLR: 0.000337\n",
      "Training Epoch: 21 [14080/50000]\tLoss: 1.1001\tLR: 0.000337\n",
      "Training Epoch: 21 [14208/50000]\tLoss: 1.0763\tLR: 0.000337\n",
      "Training Epoch: 21 [14336/50000]\tLoss: 1.0856\tLR: 0.000337\n",
      "Training Epoch: 21 [14464/50000]\tLoss: 1.1935\tLR: 0.000337\n",
      "Training Epoch: 21 [14592/50000]\tLoss: 1.1623\tLR: 0.000337\n",
      "Training Epoch: 21 [14720/50000]\tLoss: 1.0594\tLR: 0.000337\n",
      "Training Epoch: 21 [14848/50000]\tLoss: 1.1652\tLR: 0.000337\n",
      "Training Epoch: 21 [14976/50000]\tLoss: 1.0665\tLR: 0.000337\n",
      "Training Epoch: 21 [15104/50000]\tLoss: 1.0078\tLR: 0.000337\n",
      "Training Epoch: 21 [15232/50000]\tLoss: 1.0763\tLR: 0.000337\n",
      "Training Epoch: 21 [15360/50000]\tLoss: 0.9685\tLR: 0.000337\n",
      "Training Epoch: 21 [15488/50000]\tLoss: 1.1789\tLR: 0.000337\n",
      "Training Epoch: 21 [15616/50000]\tLoss: 1.1760\tLR: 0.000337\n",
      "Training Epoch: 21 [15744/50000]\tLoss: 1.0009\tLR: 0.000337\n",
      "Training Epoch: 21 [15872/50000]\tLoss: 0.8844\tLR: 0.000337\n",
      "Training Epoch: 21 [16000/50000]\tLoss: 0.9924\tLR: 0.000337\n",
      "Training Epoch: 21 [16128/50000]\tLoss: 1.0596\tLR: 0.000337\n",
      "Training Epoch: 21 [16256/50000]\tLoss: 1.0627\tLR: 0.000337\n",
      "Training Epoch: 21 [16384/50000]\tLoss: 1.0845\tLR: 0.000337\n",
      "Training Epoch: 21 [16512/50000]\tLoss: 0.9543\tLR: 0.000337\n",
      "Training Epoch: 21 [16640/50000]\tLoss: 1.1059\tLR: 0.000337\n",
      "Training Epoch: 21 [16768/50000]\tLoss: 1.1486\tLR: 0.000337\n",
      "Training Epoch: 21 [16896/50000]\tLoss: 1.2589\tLR: 0.000337\n",
      "Training Epoch: 21 [17024/50000]\tLoss: 1.0047\tLR: 0.000337\n",
      "Training Epoch: 21 [17152/50000]\tLoss: 0.9994\tLR: 0.000337\n",
      "Training Epoch: 21 [17280/50000]\tLoss: 1.0677\tLR: 0.000337\n",
      "Training Epoch: 21 [17408/50000]\tLoss: 1.2270\tLR: 0.000337\n",
      "Training Epoch: 21 [17536/50000]\tLoss: 0.8789\tLR: 0.000337\n",
      "Training Epoch: 21 [17664/50000]\tLoss: 1.1797\tLR: 0.000337\n",
      "Training Epoch: 21 [17792/50000]\tLoss: 1.1878\tLR: 0.000337\n",
      "Training Epoch: 21 [17920/50000]\tLoss: 1.0269\tLR: 0.000337\n",
      "Training Epoch: 21 [18048/50000]\tLoss: 1.1864\tLR: 0.000337\n",
      "Training Epoch: 21 [18176/50000]\tLoss: 1.2853\tLR: 0.000337\n",
      "Training Epoch: 21 [18304/50000]\tLoss: 1.1381\tLR: 0.000337\n",
      "Training Epoch: 21 [18432/50000]\tLoss: 1.1548\tLR: 0.000337\n",
      "Training Epoch: 21 [18560/50000]\tLoss: 1.0938\tLR: 0.000337\n",
      "Training Epoch: 21 [18688/50000]\tLoss: 1.3875\tLR: 0.000337\n",
      "Training Epoch: 21 [18816/50000]\tLoss: 1.0458\tLR: 0.000337\n",
      "Training Epoch: 21 [18944/50000]\tLoss: 1.0241\tLR: 0.000337\n",
      "Training Epoch: 21 [19072/50000]\tLoss: 1.0272\tLR: 0.000337\n",
      "Training Epoch: 21 [19200/50000]\tLoss: 1.2435\tLR: 0.000337\n",
      "Training Epoch: 21 [19328/50000]\tLoss: 0.9449\tLR: 0.000337\n",
      "Training Epoch: 21 [19456/50000]\tLoss: 1.2518\tLR: 0.000337\n",
      "Training Epoch: 21 [19584/50000]\tLoss: 1.0585\tLR: 0.000337\n",
      "Training Epoch: 21 [19712/50000]\tLoss: 1.0115\tLR: 0.000337\n",
      "Training Epoch: 21 [19840/50000]\tLoss: 1.0930\tLR: 0.000337\n",
      "Training Epoch: 21 [19968/50000]\tLoss: 1.1277\tLR: 0.000337\n",
      "Training Epoch: 21 [20096/50000]\tLoss: 1.1455\tLR: 0.000337\n",
      "Training Epoch: 21 [20224/50000]\tLoss: 1.2168\tLR: 0.000337\n",
      "Training Epoch: 21 [20352/50000]\tLoss: 1.2406\tLR: 0.000337\n",
      "Training Epoch: 21 [20480/50000]\tLoss: 1.1906\tLR: 0.000337\n",
      "Training Epoch: 21 [20608/50000]\tLoss: 1.1647\tLR: 0.000337\n",
      "Training Epoch: 21 [20736/50000]\tLoss: 1.0806\tLR: 0.000337\n",
      "Training Epoch: 21 [20864/50000]\tLoss: 1.1729\tLR: 0.000337\n",
      "Training Epoch: 21 [20992/50000]\tLoss: 0.9530\tLR: 0.000337\n",
      "Training Epoch: 21 [21120/50000]\tLoss: 1.1874\tLR: 0.000337\n",
      "Training Epoch: 21 [21248/50000]\tLoss: 1.0893\tLR: 0.000337\n",
      "Training Epoch: 21 [21376/50000]\tLoss: 1.1956\tLR: 0.000337\n",
      "Training Epoch: 21 [21504/50000]\tLoss: 1.0919\tLR: 0.000337\n",
      "Training Epoch: 21 [21632/50000]\tLoss: 1.1665\tLR: 0.000337\n",
      "Training Epoch: 21 [21760/50000]\tLoss: 1.1851\tLR: 0.000337\n",
      "Training Epoch: 21 [21888/50000]\tLoss: 1.0676\tLR: 0.000337\n",
      "Training Epoch: 21 [22016/50000]\tLoss: 1.1264\tLR: 0.000337\n",
      "Training Epoch: 21 [22144/50000]\tLoss: 1.1974\tLR: 0.000337\n",
      "Training Epoch: 21 [22272/50000]\tLoss: 0.8448\tLR: 0.000337\n",
      "Training Epoch: 21 [22400/50000]\tLoss: 1.4149\tLR: 0.000337\n",
      "Training Epoch: 21 [22528/50000]\tLoss: 1.0559\tLR: 0.000337\n",
      "Training Epoch: 21 [22656/50000]\tLoss: 1.1447\tLR: 0.000337\n",
      "Training Epoch: 21 [22784/50000]\tLoss: 1.1073\tLR: 0.000337\n",
      "Training Epoch: 21 [22912/50000]\tLoss: 1.0349\tLR: 0.000337\n",
      "Training Epoch: 21 [23040/50000]\tLoss: 1.1918\tLR: 0.000337\n",
      "Training Epoch: 21 [23168/50000]\tLoss: 1.1703\tLR: 0.000337\n",
      "Training Epoch: 21 [23296/50000]\tLoss: 1.2796\tLR: 0.000337\n",
      "Training Epoch: 21 [23424/50000]\tLoss: 1.1030\tLR: 0.000337\n",
      "Training Epoch: 21 [23552/50000]\tLoss: 1.1539\tLR: 0.000337\n",
      "Training Epoch: 21 [23680/50000]\tLoss: 0.9994\tLR: 0.000337\n",
      "Training Epoch: 21 [23808/50000]\tLoss: 1.0747\tLR: 0.000337\n",
      "Training Epoch: 21 [23936/50000]\tLoss: 1.2900\tLR: 0.000337\n",
      "Training Epoch: 21 [24064/50000]\tLoss: 1.1013\tLR: 0.000337\n",
      "Training Epoch: 21 [24192/50000]\tLoss: 0.9825\tLR: 0.000337\n",
      "Training Epoch: 21 [24320/50000]\tLoss: 0.9268\tLR: 0.000337\n",
      "Training Epoch: 21 [24448/50000]\tLoss: 1.1742\tLR: 0.000337\n",
      "Training Epoch: 21 [24576/50000]\tLoss: 1.2923\tLR: 0.000337\n",
      "Training Epoch: 21 [24704/50000]\tLoss: 1.2839\tLR: 0.000337\n",
      "Training Epoch: 21 [24832/50000]\tLoss: 1.2150\tLR: 0.000337\n",
      "Training Epoch: 21 [24960/50000]\tLoss: 1.1166\tLR: 0.000337\n",
      "Training Epoch: 21 [25088/50000]\tLoss: 1.0619\tLR: 0.000337\n",
      "Training Epoch: 21 [25216/50000]\tLoss: 0.9196\tLR: 0.000337\n",
      "Training Epoch: 21 [25344/50000]\tLoss: 1.1937\tLR: 0.000337\n",
      "Training Epoch: 21 [25472/50000]\tLoss: 1.1993\tLR: 0.000337\n",
      "Training Epoch: 21 [25600/50000]\tLoss: 1.1658\tLR: 0.000337\n",
      "Training Epoch: 21 [25728/50000]\tLoss: 0.9590\tLR: 0.000337\n",
      "Training Epoch: 21 [25856/50000]\tLoss: 1.3628\tLR: 0.000337\n",
      "Training Epoch: 21 [25984/50000]\tLoss: 0.9310\tLR: 0.000337\n",
      "Training Epoch: 21 [26112/50000]\tLoss: 1.0698\tLR: 0.000337\n",
      "Training Epoch: 21 [26240/50000]\tLoss: 1.3896\tLR: 0.000337\n",
      "Training Epoch: 21 [26368/50000]\tLoss: 0.8331\tLR: 0.000337\n",
      "Training Epoch: 21 [26496/50000]\tLoss: 0.9698\tLR: 0.000337\n",
      "Training Epoch: 21 [26624/50000]\tLoss: 1.1333\tLR: 0.000337\n",
      "Training Epoch: 21 [26752/50000]\tLoss: 0.7989\tLR: 0.000337\n",
      "Training Epoch: 21 [26880/50000]\tLoss: 1.0848\tLR: 0.000337\n",
      "Training Epoch: 21 [27008/50000]\tLoss: 0.9574\tLR: 0.000337\n",
      "Training Epoch: 21 [27136/50000]\tLoss: 1.1361\tLR: 0.000337\n",
      "Training Epoch: 21 [27264/50000]\tLoss: 0.9437\tLR: 0.000337\n",
      "Training Epoch: 21 [27392/50000]\tLoss: 1.1658\tLR: 0.000337\n",
      "Training Epoch: 21 [27520/50000]\tLoss: 0.9881\tLR: 0.000337\n",
      "Training Epoch: 21 [27648/50000]\tLoss: 1.0127\tLR: 0.000337\n",
      "Training Epoch: 21 [27776/50000]\tLoss: 1.1469\tLR: 0.000337\n",
      "Training Epoch: 21 [27904/50000]\tLoss: 1.1072\tLR: 0.000337\n",
      "Training Epoch: 21 [28032/50000]\tLoss: 1.1135\tLR: 0.000337\n",
      "Training Epoch: 21 [28160/50000]\tLoss: 1.2741\tLR: 0.000337\n",
      "Training Epoch: 21 [28288/50000]\tLoss: 1.1835\tLR: 0.000337\n",
      "Training Epoch: 21 [28416/50000]\tLoss: 1.5188\tLR: 0.000337\n",
      "Training Epoch: 21 [28544/50000]\tLoss: 1.1859\tLR: 0.000337\n",
      "Training Epoch: 21 [28672/50000]\tLoss: 1.0221\tLR: 0.000337\n",
      "Training Epoch: 21 [28800/50000]\tLoss: 0.9716\tLR: 0.000337\n",
      "Training Epoch: 21 [28928/50000]\tLoss: 1.1166\tLR: 0.000337\n",
      "Training Epoch: 21 [29056/50000]\tLoss: 1.0979\tLR: 0.000337\n",
      "Training Epoch: 21 [29184/50000]\tLoss: 1.0262\tLR: 0.000337\n",
      "Training Epoch: 21 [29312/50000]\tLoss: 1.1244\tLR: 0.000337\n",
      "Training Epoch: 21 [29440/50000]\tLoss: 1.2555\tLR: 0.000337\n",
      "Training Epoch: 21 [29568/50000]\tLoss: 1.0025\tLR: 0.000337\n",
      "Training Epoch: 21 [29696/50000]\tLoss: 0.9864\tLR: 0.000337\n",
      "Training Epoch: 21 [29824/50000]\tLoss: 1.0435\tLR: 0.000337\n",
      "Training Epoch: 21 [29952/50000]\tLoss: 1.1118\tLR: 0.000337\n",
      "Training Epoch: 21 [30080/50000]\tLoss: 1.3321\tLR: 0.000337\n",
      "Training Epoch: 21 [30208/50000]\tLoss: 1.2311\tLR: 0.000337\n",
      "Training Epoch: 21 [30336/50000]\tLoss: 1.0724\tLR: 0.000337\n",
      "Training Epoch: 21 [30464/50000]\tLoss: 1.0310\tLR: 0.000337\n",
      "Training Epoch: 21 [30592/50000]\tLoss: 0.9842\tLR: 0.000337\n",
      "Training Epoch: 21 [30720/50000]\tLoss: 1.0030\tLR: 0.000337\n",
      "Training Epoch: 21 [30848/50000]\tLoss: 1.2173\tLR: 0.000337\n",
      "Training Epoch: 21 [30976/50000]\tLoss: 0.8646\tLR: 0.000337\n",
      "Training Epoch: 21 [31104/50000]\tLoss: 1.2822\tLR: 0.000337\n",
      "Training Epoch: 21 [31232/50000]\tLoss: 1.2092\tLR: 0.000337\n",
      "Training Epoch: 21 [31360/50000]\tLoss: 0.9429\tLR: 0.000337\n",
      "Training Epoch: 21 [31488/50000]\tLoss: 0.9260\tLR: 0.000337\n",
      "Training Epoch: 21 [31616/50000]\tLoss: 1.1628\tLR: 0.000337\n",
      "Training Epoch: 21 [31744/50000]\tLoss: 0.9003\tLR: 0.000337\n",
      "Training Epoch: 21 [31872/50000]\tLoss: 1.2005\tLR: 0.000337\n",
      "Training Epoch: 21 [32000/50000]\tLoss: 0.8713\tLR: 0.000337\n",
      "Training Epoch: 21 [32128/50000]\tLoss: 1.0106\tLR: 0.000337\n",
      "Training Epoch: 21 [32256/50000]\tLoss: 1.2301\tLR: 0.000337\n",
      "Training Epoch: 21 [32384/50000]\tLoss: 1.0717\tLR: 0.000337\n",
      "Training Epoch: 21 [32512/50000]\tLoss: 1.0236\tLR: 0.000337\n",
      "Training Epoch: 21 [32640/50000]\tLoss: 1.0963\tLR: 0.000337\n",
      "Training Epoch: 21 [32768/50000]\tLoss: 0.9580\tLR: 0.000337\n",
      "Training Epoch: 21 [32896/50000]\tLoss: 1.2962\tLR: 0.000337\n",
      "Training Epoch: 21 [33024/50000]\tLoss: 1.2151\tLR: 0.000337\n",
      "Training Epoch: 21 [33152/50000]\tLoss: 1.0339\tLR: 0.000337\n",
      "Training Epoch: 21 [33280/50000]\tLoss: 1.0563\tLR: 0.000337\n",
      "Training Epoch: 21 [33408/50000]\tLoss: 1.1723\tLR: 0.000337\n",
      "Training Epoch: 21 [33536/50000]\tLoss: 1.1036\tLR: 0.000337\n",
      "Training Epoch: 21 [33664/50000]\tLoss: 1.1226\tLR: 0.000337\n",
      "Training Epoch: 21 [33792/50000]\tLoss: 1.1305\tLR: 0.000337\n",
      "Training Epoch: 21 [33920/50000]\tLoss: 1.1941\tLR: 0.000337\n",
      "Training Epoch: 21 [34048/50000]\tLoss: 1.0914\tLR: 0.000337\n",
      "Training Epoch: 21 [34176/50000]\tLoss: 1.2161\tLR: 0.000337\n",
      "Training Epoch: 21 [34304/50000]\tLoss: 0.9266\tLR: 0.000337\n",
      "Training Epoch: 21 [34432/50000]\tLoss: 0.9861\tLR: 0.000337\n",
      "Training Epoch: 21 [34560/50000]\tLoss: 1.2295\tLR: 0.000337\n",
      "Training Epoch: 21 [34688/50000]\tLoss: 1.0962\tLR: 0.000337\n",
      "Training Epoch: 21 [34816/50000]\tLoss: 1.1641\tLR: 0.000337\n",
      "Training Epoch: 21 [34944/50000]\tLoss: 1.1079\tLR: 0.000337\n",
      "Training Epoch: 21 [35072/50000]\tLoss: 1.0757\tLR: 0.000337\n",
      "Training Epoch: 21 [35200/50000]\tLoss: 0.9079\tLR: 0.000337\n",
      "Training Epoch: 21 [35328/50000]\tLoss: 1.1978\tLR: 0.000337\n",
      "Training Epoch: 21 [35456/50000]\tLoss: 1.1328\tLR: 0.000337\n",
      "Training Epoch: 21 [35584/50000]\tLoss: 1.0501\tLR: 0.000337\n",
      "Training Epoch: 21 [35712/50000]\tLoss: 1.0322\tLR: 0.000337\n",
      "Training Epoch: 21 [35840/50000]\tLoss: 0.9191\tLR: 0.000337\n",
      "Training Epoch: 21 [35968/50000]\tLoss: 1.0212\tLR: 0.000337\n",
      "Training Epoch: 21 [36096/50000]\tLoss: 1.0213\tLR: 0.000337\n",
      "Training Epoch: 21 [36224/50000]\tLoss: 1.0172\tLR: 0.000337\n",
      "Training Epoch: 21 [36352/50000]\tLoss: 1.0277\tLR: 0.000337\n",
      "Training Epoch: 21 [36480/50000]\tLoss: 1.2554\tLR: 0.000337\n",
      "Training Epoch: 21 [36608/50000]\tLoss: 0.9386\tLR: 0.000337\n",
      "Training Epoch: 21 [36736/50000]\tLoss: 1.1166\tLR: 0.000337\n",
      "Training Epoch: 21 [36864/50000]\tLoss: 1.0194\tLR: 0.000337\n",
      "Training Epoch: 21 [36992/50000]\tLoss: 1.1988\tLR: 0.000337\n",
      "Training Epoch: 21 [37120/50000]\tLoss: 1.3279\tLR: 0.000337\n",
      "Training Epoch: 21 [37248/50000]\tLoss: 1.0499\tLR: 0.000337\n",
      "Training Epoch: 21 [37376/50000]\tLoss: 1.0952\tLR: 0.000337\n",
      "Training Epoch: 21 [37504/50000]\tLoss: 1.2864\tLR: 0.000337\n",
      "Training Epoch: 21 [37632/50000]\tLoss: 1.1780\tLR: 0.000337\n",
      "Training Epoch: 21 [37760/50000]\tLoss: 1.2969\tLR: 0.000337\n",
      "Training Epoch: 21 [37888/50000]\tLoss: 1.3009\tLR: 0.000337\n",
      "Training Epoch: 21 [38016/50000]\tLoss: 1.1475\tLR: 0.000337\n",
      "Training Epoch: 21 [38144/50000]\tLoss: 0.9709\tLR: 0.000337\n",
      "Training Epoch: 21 [38272/50000]\tLoss: 1.1652\tLR: 0.000337\n",
      "Training Epoch: 21 [38400/50000]\tLoss: 1.1524\tLR: 0.000337\n",
      "Training Epoch: 21 [38528/50000]\tLoss: 1.0383\tLR: 0.000337\n",
      "Training Epoch: 21 [38656/50000]\tLoss: 1.1685\tLR: 0.000337\n",
      "Training Epoch: 21 [38784/50000]\tLoss: 1.0647\tLR: 0.000337\n",
      "Training Epoch: 21 [38912/50000]\tLoss: 1.3185\tLR: 0.000337\n",
      "Training Epoch: 21 [39040/50000]\tLoss: 1.1148\tLR: 0.000337\n",
      "Training Epoch: 21 [39168/50000]\tLoss: 1.1577\tLR: 0.000337\n",
      "Training Epoch: 21 [39296/50000]\tLoss: 1.2341\tLR: 0.000337\n",
      "Training Epoch: 21 [39424/50000]\tLoss: 0.9841\tLR: 0.000337\n",
      "Training Epoch: 21 [39552/50000]\tLoss: 1.3612\tLR: 0.000337\n",
      "Training Epoch: 21 [39680/50000]\tLoss: 1.0554\tLR: 0.000337\n",
      "Training Epoch: 21 [39808/50000]\tLoss: 1.1614\tLR: 0.000337\n",
      "Training Epoch: 21 [39936/50000]\tLoss: 1.2192\tLR: 0.000337\n",
      "Training Epoch: 21 [40064/50000]\tLoss: 1.0864\tLR: 0.000337\n",
      "Training Epoch: 21 [40192/50000]\tLoss: 1.1195\tLR: 0.000337\n",
      "Training Epoch: 21 [40320/50000]\tLoss: 0.8944\tLR: 0.000337\n",
      "Training Epoch: 21 [40448/50000]\tLoss: 1.2021\tLR: 0.000337\n",
      "Training Epoch: 21 [40576/50000]\tLoss: 1.1730\tLR: 0.000337\n",
      "Training Epoch: 21 [40704/50000]\tLoss: 1.0658\tLR: 0.000337\n",
      "Training Epoch: 21 [40832/50000]\tLoss: 0.9915\tLR: 0.000337\n",
      "Training Epoch: 21 [40960/50000]\tLoss: 0.9908\tLR: 0.000337\n",
      "Training Epoch: 21 [41088/50000]\tLoss: 1.1707\tLR: 0.000337\n",
      "Training Epoch: 21 [41216/50000]\tLoss: 1.2382\tLR: 0.000337\n",
      "Training Epoch: 21 [41344/50000]\tLoss: 1.0062\tLR: 0.000337\n",
      "Training Epoch: 21 [41472/50000]\tLoss: 1.0341\tLR: 0.000337\n",
      "Training Epoch: 21 [41600/50000]\tLoss: 0.9702\tLR: 0.000337\n",
      "Training Epoch: 21 [41728/50000]\tLoss: 1.0835\tLR: 0.000337\n",
      "Training Epoch: 21 [41856/50000]\tLoss: 1.1251\tLR: 0.000337\n",
      "Training Epoch: 21 [41984/50000]\tLoss: 1.1671\tLR: 0.000337\n",
      "Training Epoch: 21 [42112/50000]\tLoss: 1.0350\tLR: 0.000337\n",
      "Training Epoch: 21 [42240/50000]\tLoss: 1.2542\tLR: 0.000337\n",
      "Training Epoch: 21 [42368/50000]\tLoss: 1.0990\tLR: 0.000337\n",
      "Training Epoch: 21 [42496/50000]\tLoss: 1.2280\tLR: 0.000337\n",
      "Training Epoch: 21 [42624/50000]\tLoss: 0.8887\tLR: 0.000337\n",
      "Training Epoch: 21 [42752/50000]\tLoss: 1.0974\tLR: 0.000337\n",
      "Training Epoch: 21 [42880/50000]\tLoss: 1.2127\tLR: 0.000337\n",
      "Training Epoch: 21 [43008/50000]\tLoss: 0.9662\tLR: 0.000337\n",
      "Training Epoch: 21 [43136/50000]\tLoss: 1.0719\tLR: 0.000337\n",
      "Training Epoch: 21 [43264/50000]\tLoss: 1.3696\tLR: 0.000337\n",
      "Training Epoch: 21 [43392/50000]\tLoss: 0.9673\tLR: 0.000337\n",
      "Training Epoch: 21 [43520/50000]\tLoss: 1.1955\tLR: 0.000337\n",
      "Training Epoch: 21 [43648/50000]\tLoss: 1.1365\tLR: 0.000337\n",
      "Training Epoch: 21 [43776/50000]\tLoss: 1.1516\tLR: 0.000337\n",
      "Training Epoch: 21 [43904/50000]\tLoss: 1.1330\tLR: 0.000337\n",
      "Training Epoch: 21 [44032/50000]\tLoss: 1.1421\tLR: 0.000337\n",
      "Training Epoch: 21 [44160/50000]\tLoss: 1.1037\tLR: 0.000337\n",
      "Training Epoch: 21 [44288/50000]\tLoss: 1.2427\tLR: 0.000337\n",
      "Training Epoch: 21 [44416/50000]\tLoss: 1.1912\tLR: 0.000337\n",
      "Training Epoch: 21 [44544/50000]\tLoss: 1.0562\tLR: 0.000337\n",
      "Training Epoch: 21 [44672/50000]\tLoss: 1.0218\tLR: 0.000337\n",
      "Training Epoch: 21 [44800/50000]\tLoss: 1.0615\tLR: 0.000337\n",
      "Training Epoch: 21 [44928/50000]\tLoss: 1.0117\tLR: 0.000337\n",
      "Training Epoch: 21 [45056/50000]\tLoss: 1.1188\tLR: 0.000337\n",
      "Training Epoch: 21 [45184/50000]\tLoss: 1.2004\tLR: 0.000337\n",
      "Training Epoch: 21 [45312/50000]\tLoss: 1.1593\tLR: 0.000337\n",
      "Training Epoch: 21 [45440/50000]\tLoss: 1.0719\tLR: 0.000337\n",
      "Training Epoch: 21 [45568/50000]\tLoss: 1.1508\tLR: 0.000337\n",
      "Training Epoch: 21 [45696/50000]\tLoss: 1.0917\tLR: 0.000337\n",
      "Training Epoch: 21 [45824/50000]\tLoss: 1.2689\tLR: 0.000337\n",
      "Training Epoch: 21 [45952/50000]\tLoss: 1.1981\tLR: 0.000337\n",
      "Training Epoch: 21 [46080/50000]\tLoss: 0.9829\tLR: 0.000337\n",
      "Training Epoch: 21 [46208/50000]\tLoss: 1.0029\tLR: 0.000337\n",
      "Training Epoch: 21 [46336/50000]\tLoss: 1.0807\tLR: 0.000337\n",
      "Training Epoch: 21 [46464/50000]\tLoss: 1.0482\tLR: 0.000337\n",
      "Training Epoch: 21 [46592/50000]\tLoss: 0.9204\tLR: 0.000337\n",
      "Training Epoch: 21 [46720/50000]\tLoss: 1.1549\tLR: 0.000337\n",
      "Training Epoch: 21 [46848/50000]\tLoss: 0.9174\tLR: 0.000337\n",
      "Training Epoch: 21 [46976/50000]\tLoss: 1.1111\tLR: 0.000337\n",
      "Training Epoch: 21 [47104/50000]\tLoss: 1.0948\tLR: 0.000337\n",
      "Training Epoch: 21 [47232/50000]\tLoss: 1.1204\tLR: 0.000337\n",
      "Training Epoch: 21 [47360/50000]\tLoss: 1.0965\tLR: 0.000337\n",
      "Training Epoch: 21 [47488/50000]\tLoss: 0.9419\tLR: 0.000337\n",
      "Training Epoch: 21 [47616/50000]\tLoss: 1.0579\tLR: 0.000337\n",
      "Training Epoch: 21 [47744/50000]\tLoss: 1.1405\tLR: 0.000337\n",
      "Training Epoch: 21 [47872/50000]\tLoss: 1.0744\tLR: 0.000337\n",
      "Training Epoch: 21 [48000/50000]\tLoss: 1.0238\tLR: 0.000337\n",
      "Training Epoch: 21 [48128/50000]\tLoss: 1.0243\tLR: 0.000337\n",
      "Training Epoch: 21 [48256/50000]\tLoss: 1.1448\tLR: 0.000337\n",
      "Training Epoch: 21 [48384/50000]\tLoss: 1.0564\tLR: 0.000337\n",
      "Training Epoch: 21 [48512/50000]\tLoss: 1.0999\tLR: 0.000337\n",
      "Training Epoch: 21 [48640/50000]\tLoss: 0.8714\tLR: 0.000337\n",
      "Training Epoch: 21 [48768/50000]\tLoss: 1.2572\tLR: 0.000337\n",
      "Training Epoch: 21 [48896/50000]\tLoss: 1.0657\tLR: 0.000337\n",
      "Training Epoch: 21 [49024/50000]\tLoss: 1.1068\tLR: 0.000337\n",
      "Training Epoch: 21 [49152/50000]\tLoss: 1.1105\tLR: 0.000337\n",
      "Training Epoch: 21 [49280/50000]\tLoss: 1.0000\tLR: 0.000337\n",
      "Training Epoch: 21 [49408/50000]\tLoss: 1.1845\tLR: 0.000337\n",
      "Training Epoch: 21 [49536/50000]\tLoss: 1.1202\tLR: 0.000337\n",
      "Training Epoch: 21 [49664/50000]\tLoss: 1.0891\tLR: 0.000337\n",
      "Training Epoch: 21 [49792/50000]\tLoss: 1.0977\tLR: 0.000337\n",
      "Training Epoch: 21 [49920/50000]\tLoss: 1.1328\tLR: 0.000337\n",
      "Training Epoch: 21 [50000/50000]\tLoss: 1.0189\tLR: 0.000337\n",
      "Test set: Average loss: 0.0103, Accuracy: 0.6280\n",
      "\n",
      "Training Epoch: 22 [128/50000]\tLoss: 1.0124\tLR: 0.000337\n",
      "Training Epoch: 22 [256/50000]\tLoss: 1.1207\tLR: 0.000337\n",
      "Training Epoch: 22 [384/50000]\tLoss: 1.2566\tLR: 0.000337\n",
      "Training Epoch: 22 [512/50000]\tLoss: 1.0095\tLR: 0.000337\n",
      "Training Epoch: 22 [640/50000]\tLoss: 0.8648\tLR: 0.000337\n",
      "Training Epoch: 22 [768/50000]\tLoss: 1.0867\tLR: 0.000337\n",
      "Training Epoch: 22 [896/50000]\tLoss: 1.0527\tLR: 0.000337\n",
      "Training Epoch: 22 [1024/50000]\tLoss: 0.9635\tLR: 0.000337\n",
      "Training Epoch: 22 [1152/50000]\tLoss: 1.1202\tLR: 0.000337\n",
      "Training Epoch: 22 [1280/50000]\tLoss: 0.9897\tLR: 0.000337\n",
      "Training Epoch: 22 [1408/50000]\tLoss: 0.9591\tLR: 0.000337\n",
      "Training Epoch: 22 [1536/50000]\tLoss: 1.2336\tLR: 0.000337\n",
      "Training Epoch: 22 [1664/50000]\tLoss: 1.0402\tLR: 0.000337\n",
      "Training Epoch: 22 [1792/50000]\tLoss: 1.0711\tLR: 0.000337\n",
      "Training Epoch: 22 [1920/50000]\tLoss: 1.1824\tLR: 0.000337\n",
      "Training Epoch: 22 [2048/50000]\tLoss: 1.1765\tLR: 0.000337\n",
      "Training Epoch: 22 [2176/50000]\tLoss: 0.9251\tLR: 0.000337\n",
      "Training Epoch: 22 [2304/50000]\tLoss: 1.1318\tLR: 0.000337\n",
      "Training Epoch: 22 [2432/50000]\tLoss: 1.1076\tLR: 0.000337\n",
      "Training Epoch: 22 [2560/50000]\tLoss: 1.0148\tLR: 0.000337\n",
      "Training Epoch: 22 [2688/50000]\tLoss: 1.0942\tLR: 0.000337\n",
      "Training Epoch: 22 [2816/50000]\tLoss: 0.9890\tLR: 0.000337\n",
      "Training Epoch: 22 [2944/50000]\tLoss: 1.1666\tLR: 0.000337\n",
      "Training Epoch: 22 [3072/50000]\tLoss: 1.2852\tLR: 0.000337\n",
      "Training Epoch: 22 [3200/50000]\tLoss: 1.2123\tLR: 0.000337\n",
      "Training Epoch: 22 [3328/50000]\tLoss: 1.1584\tLR: 0.000337\n",
      "Training Epoch: 22 [3456/50000]\tLoss: 1.1666\tLR: 0.000337\n",
      "Training Epoch: 22 [3584/50000]\tLoss: 1.0757\tLR: 0.000337\n",
      "Training Epoch: 22 [3712/50000]\tLoss: 1.1626\tLR: 0.000337\n",
      "Training Epoch: 22 [3840/50000]\tLoss: 1.0127\tLR: 0.000337\n",
      "Training Epoch: 22 [3968/50000]\tLoss: 1.2178\tLR: 0.000337\n",
      "Training Epoch: 22 [4096/50000]\tLoss: 1.2355\tLR: 0.000337\n",
      "Training Epoch: 22 [4224/50000]\tLoss: 1.3875\tLR: 0.000337\n",
      "Training Epoch: 22 [4352/50000]\tLoss: 1.1382\tLR: 0.000337\n",
      "Training Epoch: 22 [4480/50000]\tLoss: 1.0783\tLR: 0.000337\n",
      "Training Epoch: 22 [4608/50000]\tLoss: 0.9629\tLR: 0.000337\n",
      "Training Epoch: 22 [4736/50000]\tLoss: 1.0513\tLR: 0.000337\n",
      "Training Epoch: 22 [4864/50000]\tLoss: 1.0273\tLR: 0.000337\n",
      "Training Epoch: 22 [4992/50000]\tLoss: 1.1223\tLR: 0.000337\n",
      "Training Epoch: 22 [5120/50000]\tLoss: 1.1984\tLR: 0.000337\n",
      "Training Epoch: 22 [5248/50000]\tLoss: 1.0293\tLR: 0.000337\n",
      "Training Epoch: 22 [5376/50000]\tLoss: 0.8762\tLR: 0.000337\n",
      "Training Epoch: 22 [5504/50000]\tLoss: 1.0872\tLR: 0.000337\n",
      "Training Epoch: 22 [5632/50000]\tLoss: 1.1204\tLR: 0.000337\n",
      "Training Epoch: 22 [5760/50000]\tLoss: 1.1255\tLR: 0.000337\n",
      "Training Epoch: 22 [5888/50000]\tLoss: 1.0830\tLR: 0.000337\n",
      "Training Epoch: 22 [6016/50000]\tLoss: 1.2795\tLR: 0.000337\n",
      "Training Epoch: 22 [6144/50000]\tLoss: 1.0697\tLR: 0.000337\n",
      "Training Epoch: 22 [6272/50000]\tLoss: 1.2398\tLR: 0.000337\n",
      "Training Epoch: 22 [6400/50000]\tLoss: 0.9386\tLR: 0.000337\n",
      "Training Epoch: 22 [6528/50000]\tLoss: 1.1780\tLR: 0.000337\n",
      "Training Epoch: 22 [6656/50000]\tLoss: 1.2702\tLR: 0.000337\n",
      "Training Epoch: 22 [6784/50000]\tLoss: 1.0842\tLR: 0.000337\n",
      "Training Epoch: 22 [6912/50000]\tLoss: 1.1385\tLR: 0.000337\n",
      "Training Epoch: 22 [7040/50000]\tLoss: 0.9606\tLR: 0.000337\n",
      "Training Epoch: 22 [7168/50000]\tLoss: 1.1371\tLR: 0.000337\n",
      "Training Epoch: 22 [7296/50000]\tLoss: 1.0625\tLR: 0.000337\n",
      "Training Epoch: 22 [7424/50000]\tLoss: 0.9728\tLR: 0.000337\n",
      "Training Epoch: 22 [7552/50000]\tLoss: 1.0206\tLR: 0.000337\n",
      "Training Epoch: 22 [7680/50000]\tLoss: 1.2376\tLR: 0.000337\n",
      "Training Epoch: 22 [7808/50000]\tLoss: 1.1151\tLR: 0.000337\n",
      "Training Epoch: 22 [7936/50000]\tLoss: 1.0250\tLR: 0.000337\n",
      "Training Epoch: 22 [8064/50000]\tLoss: 1.0120\tLR: 0.000337\n",
      "Training Epoch: 22 [8192/50000]\tLoss: 1.1010\tLR: 0.000337\n",
      "Training Epoch: 22 [8320/50000]\tLoss: 1.1151\tLR: 0.000337\n",
      "Training Epoch: 22 [8448/50000]\tLoss: 1.1222\tLR: 0.000337\n",
      "Training Epoch: 22 [8576/50000]\tLoss: 1.1220\tLR: 0.000337\n",
      "Training Epoch: 22 [8704/50000]\tLoss: 1.1773\tLR: 0.000337\n",
      "Training Epoch: 22 [8832/50000]\tLoss: 1.0700\tLR: 0.000337\n",
      "Training Epoch: 22 [8960/50000]\tLoss: 1.0032\tLR: 0.000337\n",
      "Training Epoch: 22 [9088/50000]\tLoss: 0.9625\tLR: 0.000337\n",
      "Training Epoch: 22 [9216/50000]\tLoss: 1.2583\tLR: 0.000337\n",
      "Training Epoch: 22 [9344/50000]\tLoss: 1.1118\tLR: 0.000337\n",
      "Training Epoch: 22 [9472/50000]\tLoss: 1.0864\tLR: 0.000337\n",
      "Training Epoch: 22 [9600/50000]\tLoss: 1.1313\tLR: 0.000337\n",
      "Training Epoch: 22 [9728/50000]\tLoss: 0.8935\tLR: 0.000337\n",
      "Training Epoch: 22 [9856/50000]\tLoss: 1.0895\tLR: 0.000337\n",
      "Training Epoch: 22 [9984/50000]\tLoss: 1.1739\tLR: 0.000337\n",
      "Training Epoch: 22 [10112/50000]\tLoss: 1.1103\tLR: 0.000337\n",
      "Training Epoch: 22 [10240/50000]\tLoss: 1.0854\tLR: 0.000337\n",
      "Training Epoch: 22 [10368/50000]\tLoss: 1.1117\tLR: 0.000337\n",
      "Training Epoch: 22 [10496/50000]\tLoss: 0.9983\tLR: 0.000337\n",
      "Training Epoch: 22 [10624/50000]\tLoss: 1.1461\tLR: 0.000337\n",
      "Training Epoch: 22 [10752/50000]\tLoss: 1.1069\tLR: 0.000337\n",
      "Training Epoch: 22 [10880/50000]\tLoss: 1.2237\tLR: 0.000337\n",
      "Training Epoch: 22 [11008/50000]\tLoss: 1.1607\tLR: 0.000337\n",
      "Training Epoch: 22 [11136/50000]\tLoss: 1.1026\tLR: 0.000337\n",
      "Training Epoch: 22 [11264/50000]\tLoss: 1.0822\tLR: 0.000337\n",
      "Training Epoch: 22 [11392/50000]\tLoss: 0.8298\tLR: 0.000337\n",
      "Training Epoch: 22 [11520/50000]\tLoss: 1.1546\tLR: 0.000337\n",
      "Training Epoch: 22 [11648/50000]\tLoss: 1.2890\tLR: 0.000337\n",
      "Training Epoch: 22 [11776/50000]\tLoss: 0.9620\tLR: 0.000337\n",
      "Training Epoch: 22 [11904/50000]\tLoss: 0.9949\tLR: 0.000337\n",
      "Training Epoch: 22 [12032/50000]\tLoss: 1.0898\tLR: 0.000337\n",
      "Training Epoch: 22 [12160/50000]\tLoss: 1.1831\tLR: 0.000337\n",
      "Training Epoch: 22 [12288/50000]\tLoss: 0.9449\tLR: 0.000337\n",
      "Training Epoch: 22 [12416/50000]\tLoss: 0.9869\tLR: 0.000337\n",
      "Training Epoch: 22 [12544/50000]\tLoss: 1.0919\tLR: 0.000337\n",
      "Training Epoch: 22 [12672/50000]\tLoss: 1.0507\tLR: 0.000337\n",
      "Training Epoch: 22 [12800/50000]\tLoss: 1.0832\tLR: 0.000337\n",
      "Training Epoch: 22 [12928/50000]\tLoss: 0.8863\tLR: 0.000337\n",
      "Training Epoch: 22 [13056/50000]\tLoss: 1.1836\tLR: 0.000337\n",
      "Training Epoch: 22 [13184/50000]\tLoss: 1.1132\tLR: 0.000337\n",
      "Training Epoch: 22 [13312/50000]\tLoss: 1.2115\tLR: 0.000337\n",
      "Training Epoch: 22 [13440/50000]\tLoss: 1.1117\tLR: 0.000337\n",
      "Training Epoch: 22 [13568/50000]\tLoss: 1.2224\tLR: 0.000337\n",
      "Training Epoch: 22 [13696/50000]\tLoss: 0.9560\tLR: 0.000337\n",
      "Training Epoch: 22 [13824/50000]\tLoss: 1.2504\tLR: 0.000337\n",
      "Training Epoch: 22 [13952/50000]\tLoss: 1.2164\tLR: 0.000337\n",
      "Training Epoch: 22 [14080/50000]\tLoss: 0.9920\tLR: 0.000337\n",
      "Training Epoch: 22 [14208/50000]\tLoss: 1.1185\tLR: 0.000337\n",
      "Training Epoch: 22 [14336/50000]\tLoss: 1.1159\tLR: 0.000337\n",
      "Training Epoch: 22 [14464/50000]\tLoss: 1.1635\tLR: 0.000337\n",
      "Training Epoch: 22 [14592/50000]\tLoss: 1.1938\tLR: 0.000337\n",
      "Training Epoch: 22 [14720/50000]\tLoss: 0.9058\tLR: 0.000337\n",
      "Training Epoch: 22 [14848/50000]\tLoss: 1.1143\tLR: 0.000337\n",
      "Training Epoch: 22 [14976/50000]\tLoss: 0.9498\tLR: 0.000337\n",
      "Training Epoch: 22 [15104/50000]\tLoss: 1.0464\tLR: 0.000337\n",
      "Training Epoch: 22 [15232/50000]\tLoss: 1.1145\tLR: 0.000337\n",
      "Training Epoch: 22 [15360/50000]\tLoss: 1.0545\tLR: 0.000337\n",
      "Training Epoch: 22 [15488/50000]\tLoss: 0.9089\tLR: 0.000337\n",
      "Training Epoch: 22 [15616/50000]\tLoss: 0.9381\tLR: 0.000337\n",
      "Training Epoch: 22 [15744/50000]\tLoss: 1.0986\tLR: 0.000337\n",
      "Training Epoch: 22 [15872/50000]\tLoss: 0.9133\tLR: 0.000337\n",
      "Training Epoch: 22 [16000/50000]\tLoss: 1.0097\tLR: 0.000337\n",
      "Training Epoch: 22 [16128/50000]\tLoss: 1.1224\tLR: 0.000337\n",
      "Training Epoch: 22 [16256/50000]\tLoss: 1.0787\tLR: 0.000337\n",
      "Training Epoch: 22 [16384/50000]\tLoss: 0.9537\tLR: 0.000337\n",
      "Training Epoch: 22 [16512/50000]\tLoss: 1.0427\tLR: 0.000337\n",
      "Training Epoch: 22 [16640/50000]\tLoss: 1.0020\tLR: 0.000337\n",
      "Training Epoch: 22 [16768/50000]\tLoss: 1.0933\tLR: 0.000337\n",
      "Training Epoch: 22 [16896/50000]\tLoss: 1.2209\tLR: 0.000337\n",
      "Training Epoch: 22 [17024/50000]\tLoss: 1.0507\tLR: 0.000337\n",
      "Training Epoch: 22 [17152/50000]\tLoss: 1.0910\tLR: 0.000337\n",
      "Training Epoch: 22 [17280/50000]\tLoss: 1.1154\tLR: 0.000337\n",
      "Training Epoch: 22 [17408/50000]\tLoss: 0.9831\tLR: 0.000337\n",
      "Training Epoch: 22 [17536/50000]\tLoss: 1.0437\tLR: 0.000337\n",
      "Training Epoch: 22 [17664/50000]\tLoss: 1.0651\tLR: 0.000337\n",
      "Training Epoch: 22 [17792/50000]\tLoss: 1.0496\tLR: 0.000337\n",
      "Training Epoch: 22 [17920/50000]\tLoss: 1.1048\tLR: 0.000337\n",
      "Training Epoch: 22 [18048/50000]\tLoss: 1.0152\tLR: 0.000337\n",
      "Training Epoch: 22 [18176/50000]\tLoss: 1.0928\tLR: 0.000337\n",
      "Training Epoch: 22 [18304/50000]\tLoss: 1.2689\tLR: 0.000337\n",
      "Training Epoch: 22 [18432/50000]\tLoss: 1.1155\tLR: 0.000337\n",
      "Training Epoch: 22 [18560/50000]\tLoss: 1.2017\tLR: 0.000337\n",
      "Training Epoch: 22 [18688/50000]\tLoss: 1.0094\tLR: 0.000337\n",
      "Training Epoch: 22 [18816/50000]\tLoss: 1.0323\tLR: 0.000337\n",
      "Training Epoch: 22 [18944/50000]\tLoss: 1.1069\tLR: 0.000337\n",
      "Training Epoch: 22 [19072/50000]\tLoss: 0.9546\tLR: 0.000337\n",
      "Training Epoch: 22 [19200/50000]\tLoss: 1.2431\tLR: 0.000337\n",
      "Training Epoch: 22 [19328/50000]\tLoss: 1.0690\tLR: 0.000337\n",
      "Training Epoch: 22 [19456/50000]\tLoss: 1.1543\tLR: 0.000337\n",
      "Training Epoch: 22 [19584/50000]\tLoss: 1.1763\tLR: 0.000337\n",
      "Training Epoch: 22 [19712/50000]\tLoss: 1.1195\tLR: 0.000337\n",
      "Training Epoch: 22 [19840/50000]\tLoss: 0.9640\tLR: 0.000337\n",
      "Training Epoch: 22 [19968/50000]\tLoss: 1.1283\tLR: 0.000337\n",
      "Training Epoch: 22 [20096/50000]\tLoss: 0.7706\tLR: 0.000337\n",
      "Training Epoch: 22 [20224/50000]\tLoss: 1.0977\tLR: 0.000337\n",
      "Training Epoch: 22 [20352/50000]\tLoss: 0.9739\tLR: 0.000337\n",
      "Training Epoch: 22 [20480/50000]\tLoss: 1.1403\tLR: 0.000337\n",
      "Training Epoch: 22 [20608/50000]\tLoss: 1.1682\tLR: 0.000337\n",
      "Training Epoch: 22 [20736/50000]\tLoss: 1.2201\tLR: 0.000337\n",
      "Training Epoch: 22 [20864/50000]\tLoss: 1.0885\tLR: 0.000337\n",
      "Training Epoch: 22 [20992/50000]\tLoss: 1.3622\tLR: 0.000337\n",
      "Training Epoch: 22 [21120/50000]\tLoss: 1.1459\tLR: 0.000337\n",
      "Training Epoch: 22 [21248/50000]\tLoss: 0.9673\tLR: 0.000337\n",
      "Training Epoch: 22 [21376/50000]\tLoss: 0.9510\tLR: 0.000337\n",
      "Training Epoch: 22 [21504/50000]\tLoss: 1.1170\tLR: 0.000337\n",
      "Training Epoch: 22 [21632/50000]\tLoss: 1.1118\tLR: 0.000337\n",
      "Training Epoch: 22 [21760/50000]\tLoss: 0.8777\tLR: 0.000337\n",
      "Training Epoch: 22 [21888/50000]\tLoss: 1.2192\tLR: 0.000337\n",
      "Training Epoch: 22 [22016/50000]\tLoss: 1.0755\tLR: 0.000337\n",
      "Training Epoch: 22 [22144/50000]\tLoss: 1.0718\tLR: 0.000337\n",
      "Training Epoch: 22 [22272/50000]\tLoss: 1.1801\tLR: 0.000337\n",
      "Training Epoch: 22 [22400/50000]\tLoss: 1.0172\tLR: 0.000337\n",
      "Training Epoch: 22 [22528/50000]\tLoss: 0.9156\tLR: 0.000337\n",
      "Training Epoch: 22 [22656/50000]\tLoss: 1.3542\tLR: 0.000337\n",
      "Training Epoch: 22 [22784/50000]\tLoss: 1.1271\tLR: 0.000337\n",
      "Training Epoch: 22 [22912/50000]\tLoss: 1.0159\tLR: 0.000337\n",
      "Training Epoch: 22 [23040/50000]\tLoss: 0.9059\tLR: 0.000337\n",
      "Training Epoch: 22 [23168/50000]\tLoss: 0.9725\tLR: 0.000337\n",
      "Training Epoch: 22 [23296/50000]\tLoss: 1.0848\tLR: 0.000337\n",
      "Training Epoch: 22 [23424/50000]\tLoss: 1.2216\tLR: 0.000337\n",
      "Training Epoch: 22 [23552/50000]\tLoss: 1.0961\tLR: 0.000337\n",
      "Training Epoch: 22 [23680/50000]\tLoss: 1.3046\tLR: 0.000337\n",
      "Training Epoch: 22 [23808/50000]\tLoss: 0.9685\tLR: 0.000337\n",
      "Training Epoch: 22 [23936/50000]\tLoss: 1.0352\tLR: 0.000337\n",
      "Training Epoch: 22 [24064/50000]\tLoss: 1.0339\tLR: 0.000337\n",
      "Training Epoch: 22 [24192/50000]\tLoss: 1.0241\tLR: 0.000337\n",
      "Training Epoch: 22 [24320/50000]\tLoss: 1.1298\tLR: 0.000337\n",
      "Training Epoch: 22 [24448/50000]\tLoss: 1.0627\tLR: 0.000337\n",
      "Training Epoch: 22 [24576/50000]\tLoss: 1.1331\tLR: 0.000337\n",
      "Training Epoch: 22 [24704/50000]\tLoss: 1.1027\tLR: 0.000337\n",
      "Training Epoch: 22 [24832/50000]\tLoss: 1.2067\tLR: 0.000337\n",
      "Training Epoch: 22 [24960/50000]\tLoss: 1.0588\tLR: 0.000337\n",
      "Training Epoch: 22 [25088/50000]\tLoss: 0.9200\tLR: 0.000337\n",
      "Training Epoch: 22 [25216/50000]\tLoss: 0.9973\tLR: 0.000337\n",
      "Training Epoch: 22 [25344/50000]\tLoss: 1.2155\tLR: 0.000337\n",
      "Training Epoch: 22 [25472/50000]\tLoss: 1.0415\tLR: 0.000337\n",
      "Training Epoch: 22 [25600/50000]\tLoss: 0.9111\tLR: 0.000337\n",
      "Training Epoch: 22 [25728/50000]\tLoss: 1.0437\tLR: 0.000337\n",
      "Training Epoch: 22 [25856/50000]\tLoss: 1.1052\tLR: 0.000337\n",
      "Training Epoch: 22 [25984/50000]\tLoss: 1.1536\tLR: 0.000337\n",
      "Training Epoch: 22 [26112/50000]\tLoss: 1.0644\tLR: 0.000337\n",
      "Training Epoch: 22 [26240/50000]\tLoss: 1.1431\tLR: 0.000337\n",
      "Training Epoch: 22 [26368/50000]\tLoss: 1.1040\tLR: 0.000337\n",
      "Training Epoch: 22 [26496/50000]\tLoss: 1.0760\tLR: 0.000337\n",
      "Training Epoch: 22 [26624/50000]\tLoss: 1.0136\tLR: 0.000337\n",
      "Training Epoch: 22 [26752/50000]\tLoss: 1.0174\tLR: 0.000337\n",
      "Training Epoch: 22 [26880/50000]\tLoss: 1.1396\tLR: 0.000337\n",
      "Training Epoch: 22 [27008/50000]\tLoss: 1.1133\tLR: 0.000337\n",
      "Training Epoch: 22 [27136/50000]\tLoss: 1.0948\tLR: 0.000337\n",
      "Training Epoch: 22 [27264/50000]\tLoss: 1.1176\tLR: 0.000337\n",
      "Training Epoch: 22 [27392/50000]\tLoss: 1.2140\tLR: 0.000337\n",
      "Training Epoch: 22 [27520/50000]\tLoss: 1.1102\tLR: 0.000337\n",
      "Training Epoch: 22 [27648/50000]\tLoss: 0.9320\tLR: 0.000337\n",
      "Training Epoch: 22 [27776/50000]\tLoss: 1.0811\tLR: 0.000337\n",
      "Training Epoch: 22 [27904/50000]\tLoss: 1.2444\tLR: 0.000337\n",
      "Training Epoch: 22 [28032/50000]\tLoss: 0.9398\tLR: 0.000337\n",
      "Training Epoch: 22 [28160/50000]\tLoss: 1.0499\tLR: 0.000337\n",
      "Training Epoch: 22 [28288/50000]\tLoss: 1.0085\tLR: 0.000337\n",
      "Training Epoch: 22 [28416/50000]\tLoss: 1.2303\tLR: 0.000337\n",
      "Training Epoch: 22 [28544/50000]\tLoss: 1.2404\tLR: 0.000337\n",
      "Training Epoch: 22 [28672/50000]\tLoss: 1.1804\tLR: 0.000337\n",
      "Training Epoch: 22 [28800/50000]\tLoss: 0.9856\tLR: 0.000337\n",
      "Training Epoch: 22 [28928/50000]\tLoss: 1.2293\tLR: 0.000337\n",
      "Training Epoch: 22 [29056/50000]\tLoss: 1.0568\tLR: 0.000337\n",
      "Training Epoch: 22 [29184/50000]\tLoss: 1.1118\tLR: 0.000337\n",
      "Training Epoch: 22 [29312/50000]\tLoss: 1.0780\tLR: 0.000337\n",
      "Training Epoch: 22 [29440/50000]\tLoss: 1.1242\tLR: 0.000337\n",
      "Training Epoch: 22 [29568/50000]\tLoss: 1.1206\tLR: 0.000337\n",
      "Training Epoch: 22 [29696/50000]\tLoss: 1.1309\tLR: 0.000337\n",
      "Training Epoch: 22 [29824/50000]\tLoss: 1.0321\tLR: 0.000337\n",
      "Training Epoch: 22 [29952/50000]\tLoss: 0.9860\tLR: 0.000337\n",
      "Training Epoch: 22 [30080/50000]\tLoss: 1.0276\tLR: 0.000337\n",
      "Training Epoch: 22 [30208/50000]\tLoss: 1.0499\tLR: 0.000337\n",
      "Training Epoch: 22 [30336/50000]\tLoss: 1.1055\tLR: 0.000337\n",
      "Training Epoch: 22 [30464/50000]\tLoss: 1.0227\tLR: 0.000337\n",
      "Training Epoch: 22 [30592/50000]\tLoss: 1.1375\tLR: 0.000337\n",
      "Training Epoch: 22 [30720/50000]\tLoss: 1.3472\tLR: 0.000337\n",
      "Training Epoch: 22 [30848/50000]\tLoss: 1.0726\tLR: 0.000337\n",
      "Training Epoch: 22 [30976/50000]\tLoss: 1.2695\tLR: 0.000337\n",
      "Training Epoch: 22 [31104/50000]\tLoss: 0.9837\tLR: 0.000337\n",
      "Training Epoch: 22 [31232/50000]\tLoss: 1.0930\tLR: 0.000337\n",
      "Training Epoch: 22 [31360/50000]\tLoss: 1.0155\tLR: 0.000337\n",
      "Training Epoch: 22 [31488/50000]\tLoss: 0.9186\tLR: 0.000337\n",
      "Training Epoch: 22 [31616/50000]\tLoss: 1.2148\tLR: 0.000337\n",
      "Training Epoch: 22 [31744/50000]\tLoss: 1.1306\tLR: 0.000337\n",
      "Training Epoch: 22 [31872/50000]\tLoss: 1.2171\tLR: 0.000337\n",
      "Training Epoch: 22 [32000/50000]\tLoss: 1.2303\tLR: 0.000337\n",
      "Training Epoch: 22 [32128/50000]\tLoss: 1.0498\tLR: 0.000337\n",
      "Training Epoch: 22 [32256/50000]\tLoss: 1.2021\tLR: 0.000337\n",
      "Training Epoch: 22 [32384/50000]\tLoss: 1.2131\tLR: 0.000337\n",
      "Training Epoch: 22 [32512/50000]\tLoss: 1.0332\tLR: 0.000337\n",
      "Training Epoch: 22 [32640/50000]\tLoss: 1.1692\tLR: 0.000337\n",
      "Training Epoch: 22 [32768/50000]\tLoss: 1.0633\tLR: 0.000337\n",
      "Training Epoch: 22 [32896/50000]\tLoss: 1.1774\tLR: 0.000337\n",
      "Training Epoch: 22 [33024/50000]\tLoss: 1.1129\tLR: 0.000337\n",
      "Training Epoch: 22 [33152/50000]\tLoss: 1.0050\tLR: 0.000337\n",
      "Training Epoch: 22 [33280/50000]\tLoss: 1.0631\tLR: 0.000337\n",
      "Training Epoch: 22 [33408/50000]\tLoss: 0.9438\tLR: 0.000337\n",
      "Training Epoch: 22 [33536/50000]\tLoss: 1.1589\tLR: 0.000337\n",
      "Training Epoch: 22 [33664/50000]\tLoss: 0.9757\tLR: 0.000337\n",
      "Training Epoch: 22 [33792/50000]\tLoss: 1.0958\tLR: 0.000337\n",
      "Training Epoch: 22 [33920/50000]\tLoss: 1.5342\tLR: 0.000337\n",
      "Training Epoch: 22 [34048/50000]\tLoss: 1.0415\tLR: 0.000337\n",
      "Training Epoch: 22 [34176/50000]\tLoss: 1.3371\tLR: 0.000337\n",
      "Training Epoch: 22 [34304/50000]\tLoss: 0.8957\tLR: 0.000337\n",
      "Training Epoch: 22 [34432/50000]\tLoss: 1.0013\tLR: 0.000337\n",
      "Training Epoch: 22 [34560/50000]\tLoss: 1.0544\tLR: 0.000337\n",
      "Training Epoch: 22 [34688/50000]\tLoss: 1.0422\tLR: 0.000337\n",
      "Training Epoch: 22 [34816/50000]\tLoss: 1.1531\tLR: 0.000337\n",
      "Training Epoch: 22 [34944/50000]\tLoss: 0.9670\tLR: 0.000337\n",
      "Training Epoch: 22 [35072/50000]\tLoss: 1.0719\tLR: 0.000337\n",
      "Training Epoch: 22 [35200/50000]\tLoss: 1.2790\tLR: 0.000337\n",
      "Training Epoch: 22 [35328/50000]\tLoss: 0.9310\tLR: 0.000337\n",
      "Training Epoch: 22 [35456/50000]\tLoss: 1.3284\tLR: 0.000337\n",
      "Training Epoch: 22 [35584/50000]\tLoss: 0.9736\tLR: 0.000337\n",
      "Training Epoch: 22 [35712/50000]\tLoss: 0.9363\tLR: 0.000337\n",
      "Training Epoch: 22 [35840/50000]\tLoss: 1.0688\tLR: 0.000337\n",
      "Training Epoch: 22 [35968/50000]\tLoss: 0.9818\tLR: 0.000337\n",
      "Training Epoch: 22 [36096/50000]\tLoss: 1.0664\tLR: 0.000337\n",
      "Training Epoch: 22 [36224/50000]\tLoss: 0.9931\tLR: 0.000337\n",
      "Training Epoch: 22 [36352/50000]\tLoss: 1.2453\tLR: 0.000337\n",
      "Training Epoch: 22 [36480/50000]\tLoss: 1.1729\tLR: 0.000337\n",
      "Training Epoch: 22 [36608/50000]\tLoss: 1.0059\tLR: 0.000337\n",
      "Training Epoch: 22 [36736/50000]\tLoss: 1.0397\tLR: 0.000337\n",
      "Training Epoch: 22 [36864/50000]\tLoss: 1.1271\tLR: 0.000337\n",
      "Training Epoch: 22 [36992/50000]\tLoss: 0.9840\tLR: 0.000337\n",
      "Training Epoch: 22 [37120/50000]\tLoss: 1.2061\tLR: 0.000337\n",
      "Training Epoch: 22 [37248/50000]\tLoss: 0.8756\tLR: 0.000337\n",
      "Training Epoch: 22 [37376/50000]\tLoss: 1.3708\tLR: 0.000337\n",
      "Training Epoch: 22 [37504/50000]\tLoss: 1.0813\tLR: 0.000337\n",
      "Training Epoch: 22 [37632/50000]\tLoss: 1.1164\tLR: 0.000337\n",
      "Training Epoch: 22 [37760/50000]\tLoss: 1.4014\tLR: 0.000337\n",
      "Training Epoch: 22 [37888/50000]\tLoss: 1.0656\tLR: 0.000337\n",
      "Training Epoch: 22 [38016/50000]\tLoss: 1.1965\tLR: 0.000337\n",
      "Training Epoch: 22 [38144/50000]\tLoss: 1.0468\tLR: 0.000337\n",
      "Training Epoch: 22 [38272/50000]\tLoss: 1.0823\tLR: 0.000337\n",
      "Training Epoch: 22 [38400/50000]\tLoss: 1.1923\tLR: 0.000337\n",
      "Training Epoch: 22 [38528/50000]\tLoss: 0.9782\tLR: 0.000337\n",
      "Training Epoch: 22 [38656/50000]\tLoss: 1.0165\tLR: 0.000337\n",
      "Training Epoch: 22 [38784/50000]\tLoss: 1.1937\tLR: 0.000337\n",
      "Training Epoch: 22 [38912/50000]\tLoss: 1.2280\tLR: 0.000337\n",
      "Training Epoch: 22 [39040/50000]\tLoss: 0.8950\tLR: 0.000337\n",
      "Training Epoch: 22 [39168/50000]\tLoss: 1.2146\tLR: 0.000337\n",
      "Training Epoch: 22 [39296/50000]\tLoss: 1.0376\tLR: 0.000337\n",
      "Training Epoch: 22 [39424/50000]\tLoss: 1.0564\tLR: 0.000337\n",
      "Training Epoch: 22 [39552/50000]\tLoss: 1.0508\tLR: 0.000337\n",
      "Training Epoch: 22 [39680/50000]\tLoss: 1.0379\tLR: 0.000337\n",
      "Training Epoch: 22 [39808/50000]\tLoss: 1.1650\tLR: 0.000337\n",
      "Training Epoch: 22 [39936/50000]\tLoss: 1.1137\tLR: 0.000337\n",
      "Training Epoch: 22 [40064/50000]\tLoss: 1.0997\tLR: 0.000337\n",
      "Training Epoch: 22 [40192/50000]\tLoss: 1.3035\tLR: 0.000337\n",
      "Training Epoch: 22 [40320/50000]\tLoss: 1.1298\tLR: 0.000337\n",
      "Training Epoch: 22 [40448/50000]\tLoss: 1.0728\tLR: 0.000337\n",
      "Training Epoch: 22 [40576/50000]\tLoss: 1.1685\tLR: 0.000337\n",
      "Training Epoch: 22 [40704/50000]\tLoss: 1.0559\tLR: 0.000337\n",
      "Training Epoch: 22 [40832/50000]\tLoss: 1.1572\tLR: 0.000337\n",
      "Training Epoch: 22 [40960/50000]\tLoss: 1.0814\tLR: 0.000337\n",
      "Training Epoch: 22 [41088/50000]\tLoss: 1.1408\tLR: 0.000337\n",
      "Training Epoch: 22 [41216/50000]\tLoss: 0.9688\tLR: 0.000337\n",
      "Training Epoch: 22 [41344/50000]\tLoss: 1.0078\tLR: 0.000337\n",
      "Training Epoch: 22 [41472/50000]\tLoss: 1.0619\tLR: 0.000337\n",
      "Training Epoch: 22 [41600/50000]\tLoss: 1.1438\tLR: 0.000337\n",
      "Training Epoch: 22 [41728/50000]\tLoss: 0.9913\tLR: 0.000337\n",
      "Training Epoch: 22 [41856/50000]\tLoss: 1.0701\tLR: 0.000337\n",
      "Training Epoch: 22 [41984/50000]\tLoss: 1.0286\tLR: 0.000337\n",
      "Training Epoch: 22 [42112/50000]\tLoss: 1.1064\tLR: 0.000337\n",
      "Training Epoch: 22 [42240/50000]\tLoss: 1.0502\tLR: 0.000337\n",
      "Training Epoch: 22 [42368/50000]\tLoss: 1.2909\tLR: 0.000337\n",
      "Training Epoch: 22 [42496/50000]\tLoss: 1.0470\tLR: 0.000337\n",
      "Training Epoch: 22 [42624/50000]\tLoss: 0.9773\tLR: 0.000337\n",
      "Training Epoch: 22 [42752/50000]\tLoss: 1.1781\tLR: 0.000337\n",
      "Training Epoch: 22 [42880/50000]\tLoss: 0.9926\tLR: 0.000337\n",
      "Training Epoch: 22 [43008/50000]\tLoss: 1.0162\tLR: 0.000337\n",
      "Training Epoch: 22 [43136/50000]\tLoss: 1.0515\tLR: 0.000337\n",
      "Training Epoch: 22 [43264/50000]\tLoss: 1.2460\tLR: 0.000337\n",
      "Training Epoch: 22 [43392/50000]\tLoss: 0.9815\tLR: 0.000337\n",
      "Training Epoch: 22 [43520/50000]\tLoss: 1.0289\tLR: 0.000337\n",
      "Training Epoch: 22 [43648/50000]\tLoss: 1.1723\tLR: 0.000337\n",
      "Training Epoch: 22 [43776/50000]\tLoss: 0.9707\tLR: 0.000337\n",
      "Training Epoch: 22 [43904/50000]\tLoss: 0.9855\tLR: 0.000337\n",
      "Training Epoch: 22 [44032/50000]\tLoss: 1.0097\tLR: 0.000337\n",
      "Training Epoch: 22 [44160/50000]\tLoss: 1.2525\tLR: 0.000337\n",
      "Training Epoch: 22 [44288/50000]\tLoss: 1.1417\tLR: 0.000337\n",
      "Training Epoch: 22 [44416/50000]\tLoss: 0.9919\tLR: 0.000337\n",
      "Training Epoch: 22 [44544/50000]\tLoss: 1.0063\tLR: 0.000337\n",
      "Training Epoch: 22 [44672/50000]\tLoss: 1.1136\tLR: 0.000337\n",
      "Training Epoch: 22 [44800/50000]\tLoss: 1.1636\tLR: 0.000337\n",
      "Training Epoch: 22 [44928/50000]\tLoss: 0.9200\tLR: 0.000337\n",
      "Training Epoch: 22 [45056/50000]\tLoss: 1.0299\tLR: 0.000337\n",
      "Training Epoch: 22 [45184/50000]\tLoss: 1.2892\tLR: 0.000337\n",
      "Training Epoch: 22 [45312/50000]\tLoss: 1.2130\tLR: 0.000337\n",
      "Training Epoch: 22 [45440/50000]\tLoss: 0.9091\tLR: 0.000337\n",
      "Training Epoch: 22 [45568/50000]\tLoss: 1.2508\tLR: 0.000337\n",
      "Training Epoch: 22 [45696/50000]\tLoss: 1.3041\tLR: 0.000337\n",
      "Training Epoch: 22 [45824/50000]\tLoss: 1.2225\tLR: 0.000337\n",
      "Training Epoch: 22 [45952/50000]\tLoss: 1.0866\tLR: 0.000337\n",
      "Training Epoch: 22 [46080/50000]\tLoss: 1.0836\tLR: 0.000337\n",
      "Training Epoch: 22 [46208/50000]\tLoss: 1.3176\tLR: 0.000337\n",
      "Training Epoch: 22 [46336/50000]\tLoss: 1.1721\tLR: 0.000337\n",
      "Training Epoch: 22 [46464/50000]\tLoss: 1.1172\tLR: 0.000337\n",
      "Training Epoch: 22 [46592/50000]\tLoss: 1.1132\tLR: 0.000337\n",
      "Training Epoch: 22 [46720/50000]\tLoss: 0.9138\tLR: 0.000337\n",
      "Training Epoch: 22 [46848/50000]\tLoss: 1.0816\tLR: 0.000337\n",
      "Training Epoch: 22 [46976/50000]\tLoss: 0.9334\tLR: 0.000337\n",
      "Training Epoch: 22 [47104/50000]\tLoss: 1.2625\tLR: 0.000337\n",
      "Training Epoch: 22 [47232/50000]\tLoss: 1.4339\tLR: 0.000337\n",
      "Training Epoch: 22 [47360/50000]\tLoss: 1.0528\tLR: 0.000337\n",
      "Training Epoch: 22 [47488/50000]\tLoss: 1.1340\tLR: 0.000337\n",
      "Training Epoch: 22 [47616/50000]\tLoss: 1.2340\tLR: 0.000337\n",
      "Training Epoch: 22 [47744/50000]\tLoss: 1.0716\tLR: 0.000337\n",
      "Training Epoch: 22 [47872/50000]\tLoss: 1.2704\tLR: 0.000337\n",
      "Training Epoch: 22 [48000/50000]\tLoss: 1.1517\tLR: 0.000337\n",
      "Training Epoch: 22 [48128/50000]\tLoss: 1.0082\tLR: 0.000337\n",
      "Training Epoch: 22 [48256/50000]\tLoss: 1.0702\tLR: 0.000337\n",
      "Training Epoch: 22 [48384/50000]\tLoss: 1.2383\tLR: 0.000337\n",
      "Training Epoch: 22 [48512/50000]\tLoss: 1.1810\tLR: 0.000337\n",
      "Training Epoch: 22 [48640/50000]\tLoss: 1.0230\tLR: 0.000337\n",
      "Training Epoch: 22 [48768/50000]\tLoss: 1.0201\tLR: 0.000337\n",
      "Training Epoch: 22 [48896/50000]\tLoss: 1.0208\tLR: 0.000337\n",
      "Training Epoch: 22 [49024/50000]\tLoss: 1.2070\tLR: 0.000337\n",
      "Training Epoch: 22 [49152/50000]\tLoss: 1.2192\tLR: 0.000337\n",
      "Training Epoch: 22 [49280/50000]\tLoss: 0.9174\tLR: 0.000337\n",
      "Training Epoch: 22 [49408/50000]\tLoss: 1.0844\tLR: 0.000337\n",
      "Training Epoch: 22 [49536/50000]\tLoss: 1.1483\tLR: 0.000337\n",
      "Training Epoch: 22 [49664/50000]\tLoss: 0.9256\tLR: 0.000337\n",
      "Training Epoch: 22 [49792/50000]\tLoss: 1.0868\tLR: 0.000337\n",
      "Training Epoch: 22 [49920/50000]\tLoss: 1.0565\tLR: 0.000337\n",
      "Training Epoch: 22 [50000/50000]\tLoss: 1.0960\tLR: 0.000337\n",
      "Test set: Average loss: 0.0104, Accuracy: 0.6296\n",
      "\n",
      "Training Epoch: 23 [128/50000]\tLoss: 0.8961\tLR: 0.000337\n",
      "Training Epoch: 23 [256/50000]\tLoss: 1.1284\tLR: 0.000337\n",
      "Training Epoch: 23 [384/50000]\tLoss: 1.0889\tLR: 0.000337\n",
      "Training Epoch: 23 [512/50000]\tLoss: 1.0964\tLR: 0.000337\n",
      "Training Epoch: 23 [640/50000]\tLoss: 0.9932\tLR: 0.000337\n",
      "Training Epoch: 23 [768/50000]\tLoss: 1.0732\tLR: 0.000337\n",
      "Training Epoch: 23 [896/50000]\tLoss: 1.2203\tLR: 0.000337\n",
      "Training Epoch: 23 [1024/50000]\tLoss: 0.9853\tLR: 0.000337\n",
      "Training Epoch: 23 [1152/50000]\tLoss: 1.1208\tLR: 0.000337\n",
      "Training Epoch: 23 [1280/50000]\tLoss: 0.8696\tLR: 0.000337\n",
      "Training Epoch: 23 [1408/50000]\tLoss: 0.9559\tLR: 0.000337\n",
      "Training Epoch: 23 [1536/50000]\tLoss: 1.0908\tLR: 0.000337\n",
      "Training Epoch: 23 [1664/50000]\tLoss: 1.1382\tLR: 0.000337\n",
      "Training Epoch: 23 [1792/50000]\tLoss: 1.0688\tLR: 0.000337\n",
      "Training Epoch: 23 [1920/50000]\tLoss: 1.1140\tLR: 0.000337\n",
      "Training Epoch: 23 [2048/50000]\tLoss: 0.9884\tLR: 0.000337\n",
      "Training Epoch: 23 [2176/50000]\tLoss: 1.0680\tLR: 0.000337\n",
      "Training Epoch: 23 [2304/50000]\tLoss: 1.0841\tLR: 0.000337\n",
      "Training Epoch: 23 [2432/50000]\tLoss: 1.1368\tLR: 0.000337\n",
      "Training Epoch: 23 [2560/50000]\tLoss: 0.8422\tLR: 0.000337\n",
      "Training Epoch: 23 [2688/50000]\tLoss: 1.1842\tLR: 0.000337\n",
      "Training Epoch: 23 [2816/50000]\tLoss: 1.2695\tLR: 0.000337\n",
      "Training Epoch: 23 [2944/50000]\tLoss: 0.9774\tLR: 0.000337\n",
      "Training Epoch: 23 [3072/50000]\tLoss: 1.1360\tLR: 0.000337\n",
      "Training Epoch: 23 [3200/50000]\tLoss: 1.1067\tLR: 0.000337\n",
      "Training Epoch: 23 [3328/50000]\tLoss: 1.1843\tLR: 0.000337\n",
      "Training Epoch: 23 [3456/50000]\tLoss: 1.1098\tLR: 0.000337\n",
      "Training Epoch: 23 [3584/50000]\tLoss: 1.1531\tLR: 0.000337\n",
      "Training Epoch: 23 [3712/50000]\tLoss: 1.1860\tLR: 0.000337\n",
      "Training Epoch: 23 [3840/50000]\tLoss: 1.0051\tLR: 0.000337\n",
      "Training Epoch: 23 [3968/50000]\tLoss: 1.0902\tLR: 0.000337\n",
      "Training Epoch: 23 [4096/50000]\tLoss: 0.9572\tLR: 0.000337\n",
      "Training Epoch: 23 [4224/50000]\tLoss: 1.2760\tLR: 0.000337\n",
      "Training Epoch: 23 [4352/50000]\tLoss: 0.8574\tLR: 0.000337\n",
      "Training Epoch: 23 [4480/50000]\tLoss: 1.1253\tLR: 0.000337\n",
      "Training Epoch: 23 [4608/50000]\tLoss: 1.1550\tLR: 0.000337\n",
      "Training Epoch: 23 [4736/50000]\tLoss: 1.0368\tLR: 0.000337\n",
      "Training Epoch: 23 [4864/50000]\tLoss: 1.0551\tLR: 0.000337\n",
      "Training Epoch: 23 [4992/50000]\tLoss: 1.1736\tLR: 0.000337\n",
      "Training Epoch: 23 [5120/50000]\tLoss: 0.9531\tLR: 0.000337\n",
      "Training Epoch: 23 [5248/50000]\tLoss: 1.0848\tLR: 0.000337\n",
      "Training Epoch: 23 [5376/50000]\tLoss: 1.0018\tLR: 0.000337\n",
      "Training Epoch: 23 [5504/50000]\tLoss: 1.1079\tLR: 0.000337\n",
      "Training Epoch: 23 [5632/50000]\tLoss: 1.1329\tLR: 0.000337\n",
      "Training Epoch: 23 [5760/50000]\tLoss: 1.1467\tLR: 0.000337\n",
      "Training Epoch: 23 [5888/50000]\tLoss: 1.3564\tLR: 0.000337\n",
      "Training Epoch: 23 [6016/50000]\tLoss: 1.0444\tLR: 0.000337\n",
      "Training Epoch: 23 [6144/50000]\tLoss: 0.9199\tLR: 0.000337\n",
      "Training Epoch: 23 [6272/50000]\tLoss: 0.9383\tLR: 0.000337\n",
      "Training Epoch: 23 [6400/50000]\tLoss: 1.0425\tLR: 0.000337\n",
      "Training Epoch: 23 [6528/50000]\tLoss: 1.1841\tLR: 0.000337\n",
      "Training Epoch: 23 [6656/50000]\tLoss: 0.9416\tLR: 0.000337\n",
      "Training Epoch: 23 [6784/50000]\tLoss: 1.0365\tLR: 0.000337\n",
      "Training Epoch: 23 [6912/50000]\tLoss: 1.1885\tLR: 0.000337\n",
      "Training Epoch: 23 [7040/50000]\tLoss: 1.0960\tLR: 0.000337\n",
      "Training Epoch: 23 [7168/50000]\tLoss: 0.9237\tLR: 0.000337\n",
      "Training Epoch: 23 [7296/50000]\tLoss: 0.9839\tLR: 0.000337\n",
      "Training Epoch: 23 [7424/50000]\tLoss: 1.0871\tLR: 0.000337\n",
      "Training Epoch: 23 [7552/50000]\tLoss: 1.1884\tLR: 0.000337\n",
      "Training Epoch: 23 [7680/50000]\tLoss: 1.0876\tLR: 0.000337\n",
      "Training Epoch: 23 [7808/50000]\tLoss: 1.1012\tLR: 0.000337\n",
      "Training Epoch: 23 [7936/50000]\tLoss: 0.9708\tLR: 0.000337\n",
      "Training Epoch: 23 [8064/50000]\tLoss: 1.1578\tLR: 0.000337\n",
      "Training Epoch: 23 [8192/50000]\tLoss: 1.0652\tLR: 0.000337\n",
      "Training Epoch: 23 [8320/50000]\tLoss: 1.0681\tLR: 0.000337\n",
      "Training Epoch: 23 [8448/50000]\tLoss: 0.9708\tLR: 0.000337\n",
      "Training Epoch: 23 [8576/50000]\tLoss: 1.1732\tLR: 0.000337\n",
      "Training Epoch: 23 [8704/50000]\tLoss: 1.0342\tLR: 0.000337\n",
      "Training Epoch: 23 [8832/50000]\tLoss: 0.9363\tLR: 0.000337\n",
      "Training Epoch: 23 [8960/50000]\tLoss: 1.0144\tLR: 0.000337\n",
      "Training Epoch: 23 [9088/50000]\tLoss: 1.0010\tLR: 0.000337\n",
      "Training Epoch: 23 [9216/50000]\tLoss: 1.1309\tLR: 0.000337\n",
      "Training Epoch: 23 [9344/50000]\tLoss: 1.1011\tLR: 0.000337\n",
      "Training Epoch: 23 [9472/50000]\tLoss: 1.2438\tLR: 0.000337\n",
      "Training Epoch: 23 [9600/50000]\tLoss: 1.0799\tLR: 0.000337\n",
      "Training Epoch: 23 [9728/50000]\tLoss: 1.1841\tLR: 0.000337\n",
      "Training Epoch: 23 [9856/50000]\tLoss: 0.9605\tLR: 0.000337\n",
      "Training Epoch: 23 [9984/50000]\tLoss: 0.8525\tLR: 0.000337\n",
      "Training Epoch: 23 [10112/50000]\tLoss: 1.1209\tLR: 0.000337\n",
      "Training Epoch: 23 [10240/50000]\tLoss: 1.2270\tLR: 0.000337\n",
      "Training Epoch: 23 [10368/50000]\tLoss: 1.0849\tLR: 0.000337\n",
      "Training Epoch: 23 [10496/50000]\tLoss: 0.9614\tLR: 0.000337\n",
      "Training Epoch: 23 [10624/50000]\tLoss: 1.0921\tLR: 0.000337\n",
      "Training Epoch: 23 [10752/50000]\tLoss: 1.0856\tLR: 0.000337\n",
      "Training Epoch: 23 [10880/50000]\tLoss: 1.1006\tLR: 0.000337\n",
      "Training Epoch: 23 [11008/50000]\tLoss: 1.0428\tLR: 0.000337\n",
      "Training Epoch: 23 [11136/50000]\tLoss: 1.1007\tLR: 0.000337\n",
      "Training Epoch: 23 [11264/50000]\tLoss: 1.0751\tLR: 0.000337\n",
      "Training Epoch: 23 [11392/50000]\tLoss: 1.0712\tLR: 0.000337\n",
      "Training Epoch: 23 [11520/50000]\tLoss: 1.1040\tLR: 0.000337\n",
      "Training Epoch: 23 [11648/50000]\tLoss: 1.3534\tLR: 0.000337\n",
      "Training Epoch: 23 [11776/50000]\tLoss: 1.1149\tLR: 0.000337\n",
      "Training Epoch: 23 [11904/50000]\tLoss: 1.0902\tLR: 0.000337\n",
      "Training Epoch: 23 [12032/50000]\tLoss: 1.2765\tLR: 0.000337\n",
      "Training Epoch: 23 [12160/50000]\tLoss: 1.0017\tLR: 0.000337\n",
      "Training Epoch: 23 [12288/50000]\tLoss: 1.0963\tLR: 0.000337\n",
      "Training Epoch: 23 [12416/50000]\tLoss: 1.0395\tLR: 0.000337\n",
      "Training Epoch: 23 [12544/50000]\tLoss: 1.0004\tLR: 0.000337\n",
      "Training Epoch: 23 [12672/50000]\tLoss: 1.0273\tLR: 0.000337\n",
      "Training Epoch: 23 [12800/50000]\tLoss: 1.4405\tLR: 0.000337\n",
      "Training Epoch: 23 [12928/50000]\tLoss: 1.2365\tLR: 0.000337\n",
      "Training Epoch: 23 [13056/50000]\tLoss: 1.0465\tLR: 0.000337\n",
      "Training Epoch: 23 [13184/50000]\tLoss: 1.1074\tLR: 0.000337\n",
      "Training Epoch: 23 [13312/50000]\tLoss: 1.0787\tLR: 0.000337\n",
      "Training Epoch: 23 [13440/50000]\tLoss: 1.2648\tLR: 0.000337\n",
      "Training Epoch: 23 [13568/50000]\tLoss: 1.1062\tLR: 0.000337\n",
      "Training Epoch: 23 [13696/50000]\tLoss: 0.9273\tLR: 0.000337\n",
      "Training Epoch: 23 [13824/50000]\tLoss: 1.1998\tLR: 0.000337\n",
      "Training Epoch: 23 [13952/50000]\tLoss: 1.1026\tLR: 0.000337\n",
      "Training Epoch: 23 [14080/50000]\tLoss: 1.0208\tLR: 0.000337\n",
      "Training Epoch: 23 [14208/50000]\tLoss: 1.1852\tLR: 0.000337\n",
      "Training Epoch: 23 [14336/50000]\tLoss: 1.2762\tLR: 0.000337\n",
      "Training Epoch: 23 [14464/50000]\tLoss: 0.9309\tLR: 0.000337\n",
      "Training Epoch: 23 [14592/50000]\tLoss: 1.0384\tLR: 0.000337\n",
      "Training Epoch: 23 [14720/50000]\tLoss: 1.1269\tLR: 0.000337\n",
      "Training Epoch: 23 [14848/50000]\tLoss: 1.1170\tLR: 0.000337\n",
      "Training Epoch: 23 [14976/50000]\tLoss: 1.1500\tLR: 0.000337\n",
      "Training Epoch: 23 [15104/50000]\tLoss: 1.1868\tLR: 0.000337\n",
      "Training Epoch: 23 [15232/50000]\tLoss: 1.3072\tLR: 0.000337\n",
      "Training Epoch: 23 [15360/50000]\tLoss: 1.2511\tLR: 0.000337\n",
      "Training Epoch: 23 [15488/50000]\tLoss: 1.0561\tLR: 0.000337\n",
      "Training Epoch: 23 [15616/50000]\tLoss: 1.0611\tLR: 0.000337\n",
      "Training Epoch: 23 [15744/50000]\tLoss: 1.0815\tLR: 0.000337\n",
      "Training Epoch: 23 [15872/50000]\tLoss: 1.0976\tLR: 0.000337\n",
      "Training Epoch: 23 [16000/50000]\tLoss: 0.8417\tLR: 0.000337\n",
      "Training Epoch: 23 [16128/50000]\tLoss: 0.9583\tLR: 0.000337\n",
      "Training Epoch: 23 [16256/50000]\tLoss: 1.2028\tLR: 0.000337\n",
      "Training Epoch: 23 [16384/50000]\tLoss: 1.1565\tLR: 0.000337\n",
      "Training Epoch: 23 [16512/50000]\tLoss: 1.0122\tLR: 0.000337\n",
      "Training Epoch: 23 [16640/50000]\tLoss: 1.0636\tLR: 0.000337\n",
      "Training Epoch: 23 [16768/50000]\tLoss: 1.2509\tLR: 0.000337\n",
      "Training Epoch: 23 [16896/50000]\tLoss: 1.0419\tLR: 0.000337\n",
      "Training Epoch: 23 [17024/50000]\tLoss: 1.1501\tLR: 0.000337\n",
      "Training Epoch: 23 [17152/50000]\tLoss: 1.0838\tLR: 0.000337\n",
      "Training Epoch: 23 [17280/50000]\tLoss: 1.1770\tLR: 0.000337\n",
      "Training Epoch: 23 [17408/50000]\tLoss: 0.9138\tLR: 0.000337\n",
      "Training Epoch: 23 [17536/50000]\tLoss: 1.1546\tLR: 0.000337\n",
      "Training Epoch: 23 [17664/50000]\tLoss: 1.0440\tLR: 0.000337\n",
      "Training Epoch: 23 [17792/50000]\tLoss: 1.0248\tLR: 0.000337\n",
      "Training Epoch: 23 [17920/50000]\tLoss: 1.1075\tLR: 0.000337\n",
      "Training Epoch: 23 [18048/50000]\tLoss: 1.0298\tLR: 0.000337\n",
      "Training Epoch: 23 [18176/50000]\tLoss: 1.0755\tLR: 0.000337\n",
      "Training Epoch: 23 [18304/50000]\tLoss: 0.9624\tLR: 0.000337\n",
      "Training Epoch: 23 [18432/50000]\tLoss: 1.0725\tLR: 0.000337\n",
      "Training Epoch: 23 [18560/50000]\tLoss: 1.0359\tLR: 0.000337\n",
      "Training Epoch: 23 [18688/50000]\tLoss: 0.9216\tLR: 0.000337\n",
      "Training Epoch: 23 [18816/50000]\tLoss: 1.1487\tLR: 0.000337\n",
      "Training Epoch: 23 [18944/50000]\tLoss: 1.1965\tLR: 0.000337\n",
      "Training Epoch: 23 [19072/50000]\tLoss: 0.8278\tLR: 0.000337\n",
      "Training Epoch: 23 [19200/50000]\tLoss: 0.9903\tLR: 0.000337\n",
      "Training Epoch: 23 [19328/50000]\tLoss: 1.0926\tLR: 0.000337\n",
      "Training Epoch: 23 [19456/50000]\tLoss: 1.0517\tLR: 0.000337\n",
      "Training Epoch: 23 [19584/50000]\tLoss: 0.9753\tLR: 0.000337\n",
      "Training Epoch: 23 [19712/50000]\tLoss: 1.0022\tLR: 0.000337\n",
      "Training Epoch: 23 [19840/50000]\tLoss: 0.9671\tLR: 0.000337\n",
      "Training Epoch: 23 [19968/50000]\tLoss: 1.2768\tLR: 0.000337\n",
      "Training Epoch: 23 [20096/50000]\tLoss: 0.9681\tLR: 0.000337\n",
      "Training Epoch: 23 [20224/50000]\tLoss: 1.0832\tLR: 0.000337\n",
      "Training Epoch: 23 [20352/50000]\tLoss: 1.0176\tLR: 0.000337\n",
      "Training Epoch: 23 [20480/50000]\tLoss: 1.0051\tLR: 0.000337\n",
      "Training Epoch: 23 [20608/50000]\tLoss: 0.9396\tLR: 0.000337\n",
      "Training Epoch: 23 [20736/50000]\tLoss: 1.1829\tLR: 0.000337\n",
      "Training Epoch: 23 [20864/50000]\tLoss: 1.0608\tLR: 0.000337\n",
      "Training Epoch: 23 [20992/50000]\tLoss: 1.0168\tLR: 0.000337\n",
      "Training Epoch: 23 [21120/50000]\tLoss: 1.0304\tLR: 0.000337\n",
      "Training Epoch: 23 [21248/50000]\tLoss: 1.1057\tLR: 0.000337\n",
      "Training Epoch: 23 [21376/50000]\tLoss: 1.2669\tLR: 0.000337\n",
      "Training Epoch: 23 [21504/50000]\tLoss: 1.1314\tLR: 0.000337\n",
      "Training Epoch: 23 [21632/50000]\tLoss: 1.2514\tLR: 0.000337\n",
      "Training Epoch: 23 [21760/50000]\tLoss: 1.0817\tLR: 0.000337\n",
      "Training Epoch: 23 [21888/50000]\tLoss: 1.0040\tLR: 0.000337\n",
      "Training Epoch: 23 [22016/50000]\tLoss: 0.9266\tLR: 0.000337\n",
      "Training Epoch: 23 [22144/50000]\tLoss: 0.9771\tLR: 0.000337\n",
      "Training Epoch: 23 [22272/50000]\tLoss: 1.0458\tLR: 0.000337\n",
      "Training Epoch: 23 [22400/50000]\tLoss: 1.1024\tLR: 0.000337\n",
      "Training Epoch: 23 [22528/50000]\tLoss: 0.7857\tLR: 0.000337\n",
      "Training Epoch: 23 [22656/50000]\tLoss: 1.0788\tLR: 0.000337\n",
      "Training Epoch: 23 [22784/50000]\tLoss: 0.9827\tLR: 0.000337\n",
      "Training Epoch: 23 [22912/50000]\tLoss: 0.8669\tLR: 0.000337\n",
      "Training Epoch: 23 [23040/50000]\tLoss: 1.2944\tLR: 0.000337\n",
      "Training Epoch: 23 [23168/50000]\tLoss: 0.9754\tLR: 0.000337\n",
      "Training Epoch: 23 [23296/50000]\tLoss: 1.0428\tLR: 0.000337\n",
      "Training Epoch: 23 [23424/50000]\tLoss: 1.2339\tLR: 0.000337\n",
      "Training Epoch: 23 [23552/50000]\tLoss: 1.0946\tLR: 0.000337\n",
      "Training Epoch: 23 [23680/50000]\tLoss: 1.0281\tLR: 0.000337\n",
      "Training Epoch: 23 [23808/50000]\tLoss: 0.9156\tLR: 0.000337\n",
      "Training Epoch: 23 [23936/50000]\tLoss: 1.1087\tLR: 0.000337\n",
      "Training Epoch: 23 [24064/50000]\tLoss: 1.1759\tLR: 0.000337\n",
      "Training Epoch: 23 [24192/50000]\tLoss: 1.0970\tLR: 0.000337\n",
      "Training Epoch: 23 [24320/50000]\tLoss: 1.3261\tLR: 0.000337\n",
      "Training Epoch: 23 [24448/50000]\tLoss: 1.1943\tLR: 0.000337\n",
      "Training Epoch: 23 [24576/50000]\tLoss: 1.1450\tLR: 0.000337\n",
      "Training Epoch: 23 [24704/50000]\tLoss: 1.2729\tLR: 0.000337\n",
      "Training Epoch: 23 [24832/50000]\tLoss: 1.0448\tLR: 0.000337\n",
      "Training Epoch: 23 [24960/50000]\tLoss: 1.1265\tLR: 0.000337\n",
      "Training Epoch: 23 [25088/50000]\tLoss: 1.0858\tLR: 0.000337\n",
      "Training Epoch: 23 [25216/50000]\tLoss: 1.0889\tLR: 0.000337\n",
      "Training Epoch: 23 [25344/50000]\tLoss: 1.2407\tLR: 0.000337\n",
      "Training Epoch: 23 [25472/50000]\tLoss: 0.9859\tLR: 0.000337\n",
      "Training Epoch: 23 [25600/50000]\tLoss: 1.1775\tLR: 0.000337\n",
      "Training Epoch: 23 [25728/50000]\tLoss: 1.1221\tLR: 0.000337\n",
      "Training Epoch: 23 [25856/50000]\tLoss: 1.2386\tLR: 0.000337\n",
      "Training Epoch: 23 [25984/50000]\tLoss: 1.2375\tLR: 0.000337\n",
      "Training Epoch: 23 [26112/50000]\tLoss: 1.1796\tLR: 0.000337\n",
      "Training Epoch: 23 [26240/50000]\tLoss: 1.1383\tLR: 0.000337\n",
      "Training Epoch: 23 [26368/50000]\tLoss: 1.1995\tLR: 0.000337\n",
      "Training Epoch: 23 [26496/50000]\tLoss: 1.2162\tLR: 0.000337\n",
      "Training Epoch: 23 [26624/50000]\tLoss: 0.9735\tLR: 0.000337\n",
      "Training Epoch: 23 [26752/50000]\tLoss: 1.0307\tLR: 0.000337\n",
      "Training Epoch: 23 [26880/50000]\tLoss: 1.0489\tLR: 0.000337\n",
      "Training Epoch: 23 [27008/50000]\tLoss: 0.9811\tLR: 0.000337\n",
      "Training Epoch: 23 [27136/50000]\tLoss: 1.1768\tLR: 0.000337\n",
      "Training Epoch: 23 [27264/50000]\tLoss: 1.1156\tLR: 0.000337\n",
      "Training Epoch: 23 [27392/50000]\tLoss: 0.9732\tLR: 0.000337\n",
      "Training Epoch: 23 [27520/50000]\tLoss: 1.0350\tLR: 0.000337\n",
      "Training Epoch: 23 [27648/50000]\tLoss: 1.1213\tLR: 0.000337\n",
      "Training Epoch: 23 [27776/50000]\tLoss: 0.9500\tLR: 0.000337\n",
      "Training Epoch: 23 [27904/50000]\tLoss: 1.1995\tLR: 0.000337\n",
      "Training Epoch: 23 [28032/50000]\tLoss: 1.1467\tLR: 0.000337\n",
      "Training Epoch: 23 [28160/50000]\tLoss: 0.9832\tLR: 0.000337\n",
      "Training Epoch: 23 [28288/50000]\tLoss: 1.1245\tLR: 0.000337\n",
      "Training Epoch: 23 [28416/50000]\tLoss: 0.9374\tLR: 0.000337\n",
      "Training Epoch: 23 [28544/50000]\tLoss: 1.1175\tLR: 0.000337\n",
      "Training Epoch: 23 [28672/50000]\tLoss: 1.0187\tLR: 0.000337\n",
      "Training Epoch: 23 [28800/50000]\tLoss: 1.2043\tLR: 0.000337\n",
      "Training Epoch: 23 [28928/50000]\tLoss: 1.2307\tLR: 0.000337\n",
      "Training Epoch: 23 [29056/50000]\tLoss: 0.9896\tLR: 0.000337\n",
      "Training Epoch: 23 [29184/50000]\tLoss: 1.2593\tLR: 0.000337\n",
      "Training Epoch: 23 [29312/50000]\tLoss: 1.1021\tLR: 0.000337\n",
      "Training Epoch: 23 [29440/50000]\tLoss: 1.1510\tLR: 0.000337\n",
      "Training Epoch: 23 [29568/50000]\tLoss: 1.0474\tLR: 0.000337\n",
      "Training Epoch: 23 [29696/50000]\tLoss: 0.8116\tLR: 0.000337\n",
      "Training Epoch: 23 [29824/50000]\tLoss: 1.1857\tLR: 0.000337\n",
      "Training Epoch: 23 [29952/50000]\tLoss: 1.2055\tLR: 0.000337\n",
      "Training Epoch: 23 [30080/50000]\tLoss: 1.1659\tLR: 0.000337\n",
      "Training Epoch: 23 [30208/50000]\tLoss: 1.0121\tLR: 0.000337\n",
      "Training Epoch: 23 [30336/50000]\tLoss: 1.0927\tLR: 0.000337\n",
      "Training Epoch: 23 [30464/50000]\tLoss: 1.1734\tLR: 0.000337\n",
      "Training Epoch: 23 [30592/50000]\tLoss: 0.9776\tLR: 0.000337\n",
      "Training Epoch: 23 [30720/50000]\tLoss: 1.2834\tLR: 0.000337\n",
      "Training Epoch: 23 [30848/50000]\tLoss: 1.1338\tLR: 0.000337\n",
      "Training Epoch: 23 [30976/50000]\tLoss: 0.9990\tLR: 0.000337\n",
      "Training Epoch: 23 [31104/50000]\tLoss: 1.0675\tLR: 0.000337\n",
      "Training Epoch: 23 [31232/50000]\tLoss: 1.0473\tLR: 0.000337\n",
      "Training Epoch: 23 [31360/50000]\tLoss: 1.2984\tLR: 0.000337\n",
      "Training Epoch: 23 [31488/50000]\tLoss: 0.8806\tLR: 0.000337\n",
      "Training Epoch: 23 [31616/50000]\tLoss: 1.3164\tLR: 0.000337\n",
      "Training Epoch: 23 [31744/50000]\tLoss: 1.2182\tLR: 0.000337\n",
      "Training Epoch: 23 [31872/50000]\tLoss: 1.0596\tLR: 0.000337\n",
      "Training Epoch: 23 [32000/50000]\tLoss: 1.1880\tLR: 0.000337\n",
      "Training Epoch: 23 [32128/50000]\tLoss: 0.9505\tLR: 0.000337\n",
      "Training Epoch: 23 [32256/50000]\tLoss: 1.0908\tLR: 0.000337\n",
      "Training Epoch: 23 [32384/50000]\tLoss: 0.8717\tLR: 0.000337\n",
      "Training Epoch: 23 [32512/50000]\tLoss: 0.9810\tLR: 0.000337\n",
      "Training Epoch: 23 [32640/50000]\tLoss: 1.1688\tLR: 0.000337\n",
      "Training Epoch: 23 [32768/50000]\tLoss: 1.1428\tLR: 0.000337\n",
      "Training Epoch: 23 [32896/50000]\tLoss: 0.9281\tLR: 0.000337\n",
      "Training Epoch: 23 [33024/50000]\tLoss: 0.9271\tLR: 0.000337\n",
      "Training Epoch: 23 [33152/50000]\tLoss: 1.2243\tLR: 0.000337\n",
      "Training Epoch: 23 [33280/50000]\tLoss: 1.2154\tLR: 0.000337\n",
      "Training Epoch: 23 [33408/50000]\tLoss: 0.9473\tLR: 0.000337\n",
      "Training Epoch: 23 [33536/50000]\tLoss: 1.0771\tLR: 0.000337\n",
      "Training Epoch: 23 [33664/50000]\tLoss: 1.0686\tLR: 0.000337\n",
      "Training Epoch: 23 [33792/50000]\tLoss: 1.2100\tLR: 0.000337\n",
      "Training Epoch: 23 [33920/50000]\tLoss: 1.2319\tLR: 0.000337\n",
      "Training Epoch: 23 [34048/50000]\tLoss: 0.9799\tLR: 0.000337\n",
      "Training Epoch: 23 [34176/50000]\tLoss: 1.0016\tLR: 0.000337\n",
      "Training Epoch: 23 [34304/50000]\tLoss: 1.1165\tLR: 0.000337\n",
      "Training Epoch: 23 [34432/50000]\tLoss: 1.0574\tLR: 0.000337\n",
      "Training Epoch: 23 [34560/50000]\tLoss: 1.1461\tLR: 0.000337\n",
      "Training Epoch: 23 [34688/50000]\tLoss: 1.0668\tLR: 0.000337\n",
      "Training Epoch: 23 [34816/50000]\tLoss: 1.1190\tLR: 0.000337\n",
      "Training Epoch: 23 [34944/50000]\tLoss: 1.0348\tLR: 0.000337\n",
      "Training Epoch: 23 [35072/50000]\tLoss: 1.0100\tLR: 0.000337\n",
      "Training Epoch: 23 [35200/50000]\tLoss: 1.3162\tLR: 0.000337\n",
      "Training Epoch: 23 [35328/50000]\tLoss: 1.1646\tLR: 0.000337\n",
      "Training Epoch: 23 [35456/50000]\tLoss: 0.9598\tLR: 0.000337\n",
      "Training Epoch: 23 [35584/50000]\tLoss: 1.1992\tLR: 0.000337\n",
      "Training Epoch: 23 [35712/50000]\tLoss: 1.0516\tLR: 0.000337\n",
      "Training Epoch: 23 [35840/50000]\tLoss: 0.9545\tLR: 0.000337\n",
      "Training Epoch: 23 [35968/50000]\tLoss: 0.9975\tLR: 0.000337\n",
      "Training Epoch: 23 [36096/50000]\tLoss: 1.0649\tLR: 0.000337\n",
      "Training Epoch: 23 [36224/50000]\tLoss: 1.0597\tLR: 0.000337\n",
      "Training Epoch: 23 [36352/50000]\tLoss: 1.2607\tLR: 0.000337\n",
      "Training Epoch: 23 [36480/50000]\tLoss: 1.3055\tLR: 0.000337\n",
      "Training Epoch: 23 [36608/50000]\tLoss: 1.1933\tLR: 0.000337\n",
      "Training Epoch: 23 [36736/50000]\tLoss: 1.0533\tLR: 0.000337\n",
      "Training Epoch: 23 [36864/50000]\tLoss: 1.0519\tLR: 0.000337\n",
      "Training Epoch: 23 [36992/50000]\tLoss: 1.3273\tLR: 0.000337\n",
      "Training Epoch: 23 [37120/50000]\tLoss: 1.1478\tLR: 0.000337\n",
      "Training Epoch: 23 [37248/50000]\tLoss: 1.0827\tLR: 0.000337\n",
      "Training Epoch: 23 [37376/50000]\tLoss: 1.1583\tLR: 0.000337\n",
      "Training Epoch: 23 [37504/50000]\tLoss: 1.0711\tLR: 0.000337\n",
      "Training Epoch: 23 [37632/50000]\tLoss: 1.0945\tLR: 0.000337\n",
      "Training Epoch: 23 [37760/50000]\tLoss: 1.0909\tLR: 0.000337\n",
      "Training Epoch: 23 [37888/50000]\tLoss: 0.8925\tLR: 0.000337\n",
      "Training Epoch: 23 [38016/50000]\tLoss: 1.1310\tLR: 0.000337\n",
      "Training Epoch: 23 [38144/50000]\tLoss: 0.9946\tLR: 0.000337\n",
      "Training Epoch: 23 [38272/50000]\tLoss: 1.0937\tLR: 0.000337\n",
      "Training Epoch: 23 [38400/50000]\tLoss: 1.1563\tLR: 0.000337\n",
      "Training Epoch: 23 [38528/50000]\tLoss: 0.9721\tLR: 0.000337\n",
      "Training Epoch: 23 [38656/50000]\tLoss: 1.0911\tLR: 0.000337\n",
      "Training Epoch: 23 [38784/50000]\tLoss: 1.1573\tLR: 0.000337\n",
      "Training Epoch: 23 [38912/50000]\tLoss: 1.2200\tLR: 0.000337\n",
      "Training Epoch: 23 [39040/50000]\tLoss: 1.0538\tLR: 0.000337\n",
      "Training Epoch: 23 [39168/50000]\tLoss: 1.0789\tLR: 0.000337\n",
      "Training Epoch: 23 [39296/50000]\tLoss: 0.9214\tLR: 0.000337\n",
      "Training Epoch: 23 [39424/50000]\tLoss: 1.0663\tLR: 0.000337\n",
      "Training Epoch: 23 [39552/50000]\tLoss: 1.0018\tLR: 0.000337\n",
      "Training Epoch: 23 [39680/50000]\tLoss: 1.2826\tLR: 0.000337\n",
      "Training Epoch: 23 [39808/50000]\tLoss: 1.0258\tLR: 0.000337\n",
      "Training Epoch: 23 [39936/50000]\tLoss: 1.1969\tLR: 0.000337\n",
      "Training Epoch: 23 [40064/50000]\tLoss: 1.2628\tLR: 0.000337\n",
      "Training Epoch: 23 [40192/50000]\tLoss: 1.3035\tLR: 0.000337\n",
      "Training Epoch: 23 [40320/50000]\tLoss: 1.3016\tLR: 0.000337\n",
      "Training Epoch: 23 [40448/50000]\tLoss: 1.0940\tLR: 0.000337\n",
      "Training Epoch: 23 [40576/50000]\tLoss: 1.0793\tLR: 0.000337\n",
      "Training Epoch: 23 [40704/50000]\tLoss: 1.1190\tLR: 0.000337\n",
      "Training Epoch: 23 [40832/50000]\tLoss: 0.8800\tLR: 0.000337\n",
      "Training Epoch: 23 [40960/50000]\tLoss: 1.1487\tLR: 0.000337\n",
      "Training Epoch: 23 [41088/50000]\tLoss: 1.1258\tLR: 0.000337\n",
      "Training Epoch: 23 [41216/50000]\tLoss: 1.0388\tLR: 0.000337\n",
      "Training Epoch: 23 [41344/50000]\tLoss: 1.2486\tLR: 0.000337\n",
      "Training Epoch: 23 [41472/50000]\tLoss: 0.9353\tLR: 0.000337\n",
      "Training Epoch: 23 [41600/50000]\tLoss: 1.0882\tLR: 0.000337\n",
      "Training Epoch: 23 [41728/50000]\tLoss: 1.1887\tLR: 0.000337\n",
      "Training Epoch: 23 [41856/50000]\tLoss: 1.0697\tLR: 0.000337\n",
      "Training Epoch: 23 [41984/50000]\tLoss: 0.8019\tLR: 0.000337\n",
      "Training Epoch: 23 [42112/50000]\tLoss: 1.1463\tLR: 0.000337\n",
      "Training Epoch: 23 [42240/50000]\tLoss: 1.0146\tLR: 0.000337\n",
      "Training Epoch: 23 [42368/50000]\tLoss: 0.9079\tLR: 0.000337\n",
      "Training Epoch: 23 [42496/50000]\tLoss: 1.0544\tLR: 0.000337\n",
      "Training Epoch: 23 [42624/50000]\tLoss: 0.9022\tLR: 0.000337\n",
      "Training Epoch: 23 [42752/50000]\tLoss: 0.9934\tLR: 0.000337\n",
      "Training Epoch: 23 [42880/50000]\tLoss: 1.1724\tLR: 0.000337\n",
      "Training Epoch: 23 [43008/50000]\tLoss: 1.2586\tLR: 0.000337\n",
      "Training Epoch: 23 [43136/50000]\tLoss: 1.2260\tLR: 0.000337\n",
      "Training Epoch: 23 [43264/50000]\tLoss: 1.1645\tLR: 0.000337\n",
      "Training Epoch: 23 [43392/50000]\tLoss: 0.9778\tLR: 0.000337\n",
      "Training Epoch: 23 [43520/50000]\tLoss: 1.0705\tLR: 0.000337\n",
      "Training Epoch: 23 [43648/50000]\tLoss: 1.0842\tLR: 0.000337\n",
      "Training Epoch: 23 [43776/50000]\tLoss: 1.0385\tLR: 0.000337\n",
      "Training Epoch: 23 [43904/50000]\tLoss: 1.2720\tLR: 0.000337\n",
      "Training Epoch: 23 [44032/50000]\tLoss: 1.1124\tLR: 0.000337\n",
      "Training Epoch: 23 [44160/50000]\tLoss: 1.2949\tLR: 0.000337\n",
      "Training Epoch: 23 [44288/50000]\tLoss: 1.2729\tLR: 0.000337\n",
      "Training Epoch: 23 [44416/50000]\tLoss: 1.0169\tLR: 0.000337\n",
      "Training Epoch: 23 [44544/50000]\tLoss: 0.8735\tLR: 0.000337\n",
      "Training Epoch: 23 [44672/50000]\tLoss: 1.0250\tLR: 0.000337\n",
      "Training Epoch: 23 [44800/50000]\tLoss: 0.9372\tLR: 0.000337\n",
      "Training Epoch: 23 [44928/50000]\tLoss: 1.5169\tLR: 0.000337\n",
      "Training Epoch: 23 [45056/50000]\tLoss: 1.1085\tLR: 0.000337\n",
      "Training Epoch: 23 [45184/50000]\tLoss: 0.9849\tLR: 0.000337\n",
      "Training Epoch: 23 [45312/50000]\tLoss: 1.0463\tLR: 0.000337\n",
      "Training Epoch: 23 [45440/50000]\tLoss: 1.0392\tLR: 0.000337\n",
      "Training Epoch: 23 [45568/50000]\tLoss: 1.0237\tLR: 0.000337\n",
      "Training Epoch: 23 [45696/50000]\tLoss: 1.0484\tLR: 0.000337\n",
      "Training Epoch: 23 [45824/50000]\tLoss: 1.1058\tLR: 0.000337\n",
      "Training Epoch: 23 [45952/50000]\tLoss: 1.0416\tLR: 0.000337\n",
      "Training Epoch: 23 [46080/50000]\tLoss: 0.9699\tLR: 0.000337\n",
      "Training Epoch: 23 [46208/50000]\tLoss: 1.1724\tLR: 0.000337\n",
      "Training Epoch: 23 [46336/50000]\tLoss: 1.0804\tLR: 0.000337\n",
      "Training Epoch: 23 [46464/50000]\tLoss: 1.0851\tLR: 0.000337\n",
      "Training Epoch: 23 [46592/50000]\tLoss: 1.0556\tLR: 0.000337\n",
      "Training Epoch: 23 [46720/50000]\tLoss: 1.2111\tLR: 0.000337\n",
      "Training Epoch: 23 [46848/50000]\tLoss: 1.0480\tLR: 0.000337\n",
      "Training Epoch: 23 [46976/50000]\tLoss: 0.9424\tLR: 0.000337\n",
      "Training Epoch: 23 [47104/50000]\tLoss: 1.2064\tLR: 0.000337\n",
      "Training Epoch: 23 [47232/50000]\tLoss: 1.2587\tLR: 0.000337\n",
      "Training Epoch: 23 [47360/50000]\tLoss: 1.0390\tLR: 0.000337\n",
      "Training Epoch: 23 [47488/50000]\tLoss: 1.1265\tLR: 0.000337\n",
      "Training Epoch: 23 [47616/50000]\tLoss: 1.2736\tLR: 0.000337\n",
      "Training Epoch: 23 [47744/50000]\tLoss: 1.1321\tLR: 0.000337\n",
      "Training Epoch: 23 [47872/50000]\tLoss: 1.1039\tLR: 0.000337\n",
      "Training Epoch: 23 [48000/50000]\tLoss: 1.2382\tLR: 0.000337\n",
      "Training Epoch: 23 [48128/50000]\tLoss: 1.1443\tLR: 0.000337\n",
      "Training Epoch: 23 [48256/50000]\tLoss: 0.9741\tLR: 0.000337\n",
      "Training Epoch: 23 [48384/50000]\tLoss: 1.2321\tLR: 0.000337\n",
      "Training Epoch: 23 [48512/50000]\tLoss: 1.1195\tLR: 0.000337\n",
      "Training Epoch: 23 [48640/50000]\tLoss: 1.2247\tLR: 0.000337\n",
      "Training Epoch: 23 [48768/50000]\tLoss: 1.0191\tLR: 0.000337\n",
      "Training Epoch: 23 [48896/50000]\tLoss: 0.9590\tLR: 0.000337\n",
      "Training Epoch: 23 [49024/50000]\tLoss: 0.9545\tLR: 0.000337\n",
      "Training Epoch: 23 [49152/50000]\tLoss: 1.2315\tLR: 0.000337\n",
      "Training Epoch: 23 [49280/50000]\tLoss: 1.1414\tLR: 0.000337\n",
      "Training Epoch: 23 [49408/50000]\tLoss: 1.0001\tLR: 0.000337\n",
      "Training Epoch: 23 [49536/50000]\tLoss: 1.1333\tLR: 0.000337\n",
      "Training Epoch: 23 [49664/50000]\tLoss: 0.9612\tLR: 0.000337\n",
      "Training Epoch: 23 [49792/50000]\tLoss: 1.1154\tLR: 0.000337\n",
      "Training Epoch: 23 [49920/50000]\tLoss: 1.1939\tLR: 0.000337\n",
      "Training Epoch: 23 [50000/50000]\tLoss: 1.1901\tLR: 0.000337\n",
      "Test set: Average loss: 0.0103, Accuracy: 0.6302\n",
      "\n",
      "Training Epoch: 24 [128/50000]\tLoss: 1.1889\tLR: 0.000337\n",
      "Training Epoch: 24 [256/50000]\tLoss: 1.1560\tLR: 0.000337\n",
      "Training Epoch: 24 [384/50000]\tLoss: 0.9526\tLR: 0.000337\n",
      "Training Epoch: 24 [512/50000]\tLoss: 0.9767\tLR: 0.000337\n",
      "Training Epoch: 24 [640/50000]\tLoss: 0.9770\tLR: 0.000337\n",
      "Training Epoch: 24 [768/50000]\tLoss: 0.9748\tLR: 0.000337\n",
      "Training Epoch: 24 [896/50000]\tLoss: 0.9581\tLR: 0.000337\n",
      "Training Epoch: 24 [1024/50000]\tLoss: 0.7549\tLR: 0.000337\n",
      "Training Epoch: 24 [1152/50000]\tLoss: 1.0343\tLR: 0.000337\n",
      "Training Epoch: 24 [1280/50000]\tLoss: 1.2940\tLR: 0.000337\n",
      "Training Epoch: 24 [1408/50000]\tLoss: 0.9231\tLR: 0.000337\n",
      "Training Epoch: 24 [1536/50000]\tLoss: 1.1253\tLR: 0.000337\n",
      "Training Epoch: 24 [1664/50000]\tLoss: 1.2184\tLR: 0.000337\n",
      "Training Epoch: 24 [1792/50000]\tLoss: 1.2405\tLR: 0.000337\n",
      "Training Epoch: 24 [1920/50000]\tLoss: 1.2033\tLR: 0.000337\n",
      "Training Epoch: 24 [2048/50000]\tLoss: 1.1977\tLR: 0.000337\n",
      "Training Epoch: 24 [2176/50000]\tLoss: 1.1338\tLR: 0.000337\n",
      "Training Epoch: 24 [2304/50000]\tLoss: 1.2834\tLR: 0.000337\n",
      "Training Epoch: 24 [2432/50000]\tLoss: 1.1684\tLR: 0.000337\n",
      "Training Epoch: 24 [2560/50000]\tLoss: 1.1685\tLR: 0.000337\n",
      "Training Epoch: 24 [2688/50000]\tLoss: 1.2165\tLR: 0.000337\n",
      "Training Epoch: 24 [2816/50000]\tLoss: 0.9732\tLR: 0.000337\n",
      "Training Epoch: 24 [2944/50000]\tLoss: 0.9331\tLR: 0.000337\n",
      "Training Epoch: 24 [3072/50000]\tLoss: 1.1527\tLR: 0.000337\n",
      "Training Epoch: 24 [3200/50000]\tLoss: 1.0222\tLR: 0.000337\n",
      "Training Epoch: 24 [3328/50000]\tLoss: 1.0185\tLR: 0.000337\n",
      "Training Epoch: 24 [3456/50000]\tLoss: 1.0234\tLR: 0.000337\n",
      "Training Epoch: 24 [3584/50000]\tLoss: 1.1659\tLR: 0.000337\n",
      "Training Epoch: 24 [3712/50000]\tLoss: 1.1759\tLR: 0.000337\n",
      "Training Epoch: 24 [3840/50000]\tLoss: 0.8949\tLR: 0.000337\n",
      "Training Epoch: 24 [3968/50000]\tLoss: 1.1024\tLR: 0.000337\n",
      "Training Epoch: 24 [4096/50000]\tLoss: 1.0960\tLR: 0.000337\n",
      "Training Epoch: 24 [4224/50000]\tLoss: 1.1286\tLR: 0.000337\n",
      "Training Epoch: 24 [4352/50000]\tLoss: 1.0085\tLR: 0.000337\n",
      "Training Epoch: 24 [4480/50000]\tLoss: 1.0850\tLR: 0.000337\n",
      "Training Epoch: 24 [4608/50000]\tLoss: 1.0999\tLR: 0.000337\n",
      "Training Epoch: 24 [4736/50000]\tLoss: 1.0334\tLR: 0.000337\n",
      "Training Epoch: 24 [4864/50000]\tLoss: 1.2003\tLR: 0.000337\n",
      "Training Epoch: 24 [4992/50000]\tLoss: 1.1245\tLR: 0.000337\n",
      "Training Epoch: 24 [5120/50000]\tLoss: 0.9697\tLR: 0.000337\n",
      "Training Epoch: 24 [5248/50000]\tLoss: 1.1443\tLR: 0.000337\n",
      "Training Epoch: 24 [5376/50000]\tLoss: 1.0179\tLR: 0.000337\n",
      "Training Epoch: 24 [5504/50000]\tLoss: 1.0550\tLR: 0.000337\n",
      "Training Epoch: 24 [5632/50000]\tLoss: 1.0984\tLR: 0.000337\n",
      "Training Epoch: 24 [5760/50000]\tLoss: 1.1055\tLR: 0.000337\n",
      "Training Epoch: 24 [5888/50000]\tLoss: 1.0746\tLR: 0.000337\n",
      "Training Epoch: 24 [6016/50000]\tLoss: 0.9344\tLR: 0.000337\n",
      "Training Epoch: 24 [6144/50000]\tLoss: 0.8627\tLR: 0.000337\n",
      "Training Epoch: 24 [6272/50000]\tLoss: 1.1230\tLR: 0.000337\n",
      "Training Epoch: 24 [6400/50000]\tLoss: 1.1903\tLR: 0.000337\n",
      "Training Epoch: 24 [6528/50000]\tLoss: 0.9240\tLR: 0.000337\n",
      "Training Epoch: 24 [6656/50000]\tLoss: 1.0357\tLR: 0.000337\n",
      "Training Epoch: 24 [6784/50000]\tLoss: 0.9187\tLR: 0.000337\n",
      "Training Epoch: 24 [6912/50000]\tLoss: 0.9916\tLR: 0.000337\n",
      "Training Epoch: 24 [7040/50000]\tLoss: 1.1292\tLR: 0.000337\n",
      "Training Epoch: 24 [7168/50000]\tLoss: 0.9624\tLR: 0.000337\n",
      "Training Epoch: 24 [7296/50000]\tLoss: 1.0975\tLR: 0.000337\n",
      "Training Epoch: 24 [7424/50000]\tLoss: 1.2848\tLR: 0.000337\n",
      "Training Epoch: 24 [7552/50000]\tLoss: 0.9817\tLR: 0.000337\n",
      "Training Epoch: 24 [7680/50000]\tLoss: 1.0076\tLR: 0.000337\n",
      "Training Epoch: 24 [7808/50000]\tLoss: 1.0907\tLR: 0.000337\n",
      "Training Epoch: 24 [7936/50000]\tLoss: 0.9143\tLR: 0.000337\n",
      "Training Epoch: 24 [8064/50000]\tLoss: 1.0494\tLR: 0.000337\n",
      "Training Epoch: 24 [8192/50000]\tLoss: 1.2295\tLR: 0.000337\n",
      "Training Epoch: 24 [8320/50000]\tLoss: 1.0690\tLR: 0.000337\n",
      "Training Epoch: 24 [8448/50000]\tLoss: 1.0814\tLR: 0.000337\n",
      "Training Epoch: 24 [8576/50000]\tLoss: 0.9998\tLR: 0.000337\n",
      "Training Epoch: 24 [8704/50000]\tLoss: 1.1622\tLR: 0.000337\n",
      "Training Epoch: 24 [8832/50000]\tLoss: 0.9411\tLR: 0.000337\n",
      "Training Epoch: 24 [8960/50000]\tLoss: 1.2698\tLR: 0.000337\n",
      "Training Epoch: 24 [9088/50000]\tLoss: 1.1101\tLR: 0.000337\n",
      "Training Epoch: 24 [9216/50000]\tLoss: 1.0487\tLR: 0.000337\n",
      "Training Epoch: 24 [9344/50000]\tLoss: 1.0608\tLR: 0.000337\n",
      "Training Epoch: 24 [9472/50000]\tLoss: 0.9294\tLR: 0.000337\n",
      "Training Epoch: 24 [9600/50000]\tLoss: 1.0525\tLR: 0.000337\n",
      "Training Epoch: 24 [9728/50000]\tLoss: 1.3549\tLR: 0.000337\n",
      "Training Epoch: 24 [9856/50000]\tLoss: 1.0827\tLR: 0.000337\n",
      "Training Epoch: 24 [9984/50000]\tLoss: 0.8488\tLR: 0.000337\n",
      "Training Epoch: 24 [10112/50000]\tLoss: 1.1742\tLR: 0.000337\n",
      "Training Epoch: 24 [10240/50000]\tLoss: 1.1939\tLR: 0.000337\n",
      "Training Epoch: 24 [10368/50000]\tLoss: 1.0877\tLR: 0.000337\n",
      "Training Epoch: 24 [10496/50000]\tLoss: 1.1804\tLR: 0.000337\n",
      "Training Epoch: 24 [10624/50000]\tLoss: 0.9840\tLR: 0.000337\n",
      "Training Epoch: 24 [10752/50000]\tLoss: 1.1078\tLR: 0.000337\n",
      "Training Epoch: 24 [10880/50000]\tLoss: 1.0477\tLR: 0.000337\n",
      "Training Epoch: 24 [11008/50000]\tLoss: 1.0901\tLR: 0.000337\n",
      "Training Epoch: 24 [11136/50000]\tLoss: 1.1835\tLR: 0.000337\n",
      "Training Epoch: 24 [11264/50000]\tLoss: 0.9384\tLR: 0.000337\n",
      "Training Epoch: 24 [11392/50000]\tLoss: 1.0114\tLR: 0.000337\n",
      "Training Epoch: 24 [11520/50000]\tLoss: 0.8850\tLR: 0.000337\n",
      "Training Epoch: 24 [11648/50000]\tLoss: 1.1149\tLR: 0.000337\n",
      "Training Epoch: 24 [11776/50000]\tLoss: 1.1186\tLR: 0.000337\n",
      "Training Epoch: 24 [11904/50000]\tLoss: 1.0836\tLR: 0.000337\n",
      "Training Epoch: 24 [12032/50000]\tLoss: 0.9545\tLR: 0.000337\n",
      "Training Epoch: 24 [12160/50000]\tLoss: 1.1586\tLR: 0.000337\n",
      "Training Epoch: 24 [12288/50000]\tLoss: 1.1254\tLR: 0.000337\n",
      "Training Epoch: 24 [12416/50000]\tLoss: 0.9244\tLR: 0.000337\n",
      "Training Epoch: 24 [12544/50000]\tLoss: 1.0516\tLR: 0.000337\n",
      "Training Epoch: 24 [12672/50000]\tLoss: 1.0822\tLR: 0.000337\n",
      "Training Epoch: 24 [12800/50000]\tLoss: 1.0253\tLR: 0.000337\n",
      "Training Epoch: 24 [12928/50000]\tLoss: 0.9887\tLR: 0.000337\n",
      "Training Epoch: 24 [13056/50000]\tLoss: 1.0820\tLR: 0.000337\n",
      "Training Epoch: 24 [13184/50000]\tLoss: 0.9740\tLR: 0.000337\n",
      "Training Epoch: 24 [13312/50000]\tLoss: 1.2180\tLR: 0.000337\n",
      "Training Epoch: 24 [13440/50000]\tLoss: 1.3184\tLR: 0.000337\n",
      "Training Epoch: 24 [13568/50000]\tLoss: 1.2245\tLR: 0.000337\n",
      "Training Epoch: 24 [13696/50000]\tLoss: 1.0414\tLR: 0.000337\n",
      "Training Epoch: 24 [13824/50000]\tLoss: 1.0821\tLR: 0.000337\n",
      "Training Epoch: 24 [13952/50000]\tLoss: 1.1432\tLR: 0.000337\n",
      "Training Epoch: 24 [14080/50000]\tLoss: 1.0036\tLR: 0.000337\n",
      "Training Epoch: 24 [14208/50000]\tLoss: 1.0000\tLR: 0.000337\n",
      "Training Epoch: 24 [14336/50000]\tLoss: 1.0615\tLR: 0.000337\n",
      "Training Epoch: 24 [14464/50000]\tLoss: 1.0830\tLR: 0.000337\n",
      "Training Epoch: 24 [14592/50000]\tLoss: 1.0748\tLR: 0.000337\n",
      "Training Epoch: 24 [14720/50000]\tLoss: 1.3806\tLR: 0.000337\n",
      "Training Epoch: 24 [14848/50000]\tLoss: 1.2813\tLR: 0.000337\n",
      "Training Epoch: 24 [14976/50000]\tLoss: 1.1179\tLR: 0.000337\n",
      "Training Epoch: 24 [15104/50000]\tLoss: 1.1240\tLR: 0.000337\n",
      "Training Epoch: 24 [15232/50000]\tLoss: 1.0942\tLR: 0.000337\n",
      "Training Epoch: 24 [15360/50000]\tLoss: 1.2060\tLR: 0.000337\n",
      "Training Epoch: 24 [15488/50000]\tLoss: 1.0550\tLR: 0.000337\n",
      "Training Epoch: 24 [15616/50000]\tLoss: 1.1364\tLR: 0.000337\n",
      "Training Epoch: 24 [15744/50000]\tLoss: 0.9187\tLR: 0.000337\n",
      "Training Epoch: 24 [15872/50000]\tLoss: 1.0106\tLR: 0.000337\n",
      "Training Epoch: 24 [16000/50000]\tLoss: 1.2295\tLR: 0.000337\n",
      "Training Epoch: 24 [16128/50000]\tLoss: 0.9061\tLR: 0.000337\n",
      "Training Epoch: 24 [16256/50000]\tLoss: 0.8636\tLR: 0.000337\n",
      "Training Epoch: 24 [16384/50000]\tLoss: 1.3187\tLR: 0.000337\n",
      "Training Epoch: 24 [16512/50000]\tLoss: 0.8565\tLR: 0.000337\n",
      "Training Epoch: 24 [16640/50000]\tLoss: 1.0472\tLR: 0.000337\n",
      "Training Epoch: 24 [16768/50000]\tLoss: 1.2215\tLR: 0.000337\n",
      "Training Epoch: 24 [16896/50000]\tLoss: 1.1254\tLR: 0.000337\n",
      "Training Epoch: 24 [17024/50000]\tLoss: 1.2203\tLR: 0.000337\n",
      "Training Epoch: 24 [17152/50000]\tLoss: 1.2667\tLR: 0.000337\n",
      "Training Epoch: 24 [17280/50000]\tLoss: 1.2699\tLR: 0.000337\n",
      "Training Epoch: 24 [17408/50000]\tLoss: 1.1092\tLR: 0.000337\n",
      "Training Epoch: 24 [17536/50000]\tLoss: 0.9891\tLR: 0.000337\n",
      "Training Epoch: 24 [17664/50000]\tLoss: 1.0371\tLR: 0.000337\n",
      "Training Epoch: 24 [17792/50000]\tLoss: 0.9362\tLR: 0.000337\n",
      "Training Epoch: 24 [17920/50000]\tLoss: 1.2940\tLR: 0.000337\n",
      "Training Epoch: 24 [18048/50000]\tLoss: 1.1150\tLR: 0.000337\n",
      "Training Epoch: 24 [18176/50000]\tLoss: 0.9914\tLR: 0.000337\n",
      "Training Epoch: 24 [18304/50000]\tLoss: 0.8335\tLR: 0.000337\n",
      "Training Epoch: 24 [18432/50000]\tLoss: 1.1167\tLR: 0.000337\n",
      "Training Epoch: 24 [18560/50000]\tLoss: 1.0991\tLR: 0.000337\n",
      "Training Epoch: 24 [18688/50000]\tLoss: 1.0486\tLR: 0.000337\n",
      "Training Epoch: 24 [18816/50000]\tLoss: 1.0648\tLR: 0.000337\n",
      "Training Epoch: 24 [18944/50000]\tLoss: 1.0638\tLR: 0.000337\n",
      "Training Epoch: 24 [19072/50000]\tLoss: 1.1200\tLR: 0.000337\n",
      "Training Epoch: 24 [19200/50000]\tLoss: 1.2807\tLR: 0.000337\n",
      "Training Epoch: 24 [19328/50000]\tLoss: 1.1787\tLR: 0.000337\n",
      "Training Epoch: 24 [19456/50000]\tLoss: 1.1329\tLR: 0.000337\n",
      "Training Epoch: 24 [19584/50000]\tLoss: 1.2325\tLR: 0.000337\n",
      "Training Epoch: 24 [19712/50000]\tLoss: 1.0514\tLR: 0.000337\n",
      "Training Epoch: 24 [19840/50000]\tLoss: 1.1001\tLR: 0.000337\n",
      "Training Epoch: 24 [19968/50000]\tLoss: 0.8881\tLR: 0.000337\n",
      "Training Epoch: 24 [20096/50000]\tLoss: 1.0955\tLR: 0.000337\n",
      "Training Epoch: 24 [20224/50000]\tLoss: 0.9837\tLR: 0.000337\n",
      "Training Epoch: 24 [20352/50000]\tLoss: 1.1608\tLR: 0.000337\n",
      "Training Epoch: 24 [20480/50000]\tLoss: 1.1730\tLR: 0.000337\n",
      "Training Epoch: 24 [20608/50000]\tLoss: 1.0881\tLR: 0.000337\n",
      "Training Epoch: 24 [20736/50000]\tLoss: 1.0722\tLR: 0.000337\n",
      "Training Epoch: 24 [20864/50000]\tLoss: 0.9762\tLR: 0.000337\n",
      "Training Epoch: 24 [20992/50000]\tLoss: 1.0510\tLR: 0.000337\n",
      "Training Epoch: 24 [21120/50000]\tLoss: 0.9274\tLR: 0.000337\n",
      "Training Epoch: 24 [21248/50000]\tLoss: 0.9614\tLR: 0.000337\n",
      "Training Epoch: 24 [21376/50000]\tLoss: 0.9449\tLR: 0.000337\n",
      "Training Epoch: 24 [21504/50000]\tLoss: 0.9933\tLR: 0.000337\n",
      "Training Epoch: 24 [21632/50000]\tLoss: 1.0951\tLR: 0.000337\n",
      "Training Epoch: 24 [21760/50000]\tLoss: 1.1263\tLR: 0.000337\n",
      "Training Epoch: 24 [21888/50000]\tLoss: 1.0501\tLR: 0.000337\n",
      "Training Epoch: 24 [22016/50000]\tLoss: 1.3570\tLR: 0.000337\n",
      "Training Epoch: 24 [22144/50000]\tLoss: 1.0946\tLR: 0.000337\n",
      "Training Epoch: 24 [22272/50000]\tLoss: 1.0717\tLR: 0.000337\n",
      "Training Epoch: 24 [22400/50000]\tLoss: 1.0700\tLR: 0.000337\n",
      "Training Epoch: 24 [22528/50000]\tLoss: 0.9931\tLR: 0.000337\n",
      "Training Epoch: 24 [22656/50000]\tLoss: 0.9220\tLR: 0.000337\n",
      "Training Epoch: 24 [22784/50000]\tLoss: 0.9825\tLR: 0.000337\n",
      "Training Epoch: 24 [22912/50000]\tLoss: 1.0308\tLR: 0.000337\n",
      "Training Epoch: 24 [23040/50000]\tLoss: 1.0212\tLR: 0.000337\n",
      "Training Epoch: 24 [23168/50000]\tLoss: 1.2408\tLR: 0.000337\n",
      "Training Epoch: 24 [23296/50000]\tLoss: 1.0423\tLR: 0.000337\n",
      "Training Epoch: 24 [23424/50000]\tLoss: 1.2511\tLR: 0.000337\n",
      "Training Epoch: 24 [23552/50000]\tLoss: 0.8215\tLR: 0.000337\n",
      "Training Epoch: 24 [23680/50000]\tLoss: 1.2621\tLR: 0.000337\n",
      "Training Epoch: 24 [23808/50000]\tLoss: 1.0299\tLR: 0.000337\n",
      "Training Epoch: 24 [23936/50000]\tLoss: 1.1134\tLR: 0.000337\n",
      "Training Epoch: 24 [24064/50000]\tLoss: 1.0436\tLR: 0.000337\n",
      "Training Epoch: 24 [24192/50000]\tLoss: 1.0950\tLR: 0.000337\n",
      "Training Epoch: 24 [24320/50000]\tLoss: 1.1142\tLR: 0.000337\n",
      "Training Epoch: 24 [24448/50000]\tLoss: 1.1688\tLR: 0.000337\n",
      "Training Epoch: 24 [24576/50000]\tLoss: 1.1634\tLR: 0.000337\n",
      "Training Epoch: 24 [24704/50000]\tLoss: 1.1572\tLR: 0.000337\n",
      "Training Epoch: 24 [24832/50000]\tLoss: 1.0237\tLR: 0.000337\n",
      "Training Epoch: 24 [24960/50000]\tLoss: 0.8490\tLR: 0.000337\n",
      "Training Epoch: 24 [25088/50000]\tLoss: 0.9862\tLR: 0.000337\n",
      "Training Epoch: 24 [25216/50000]\tLoss: 1.0312\tLR: 0.000337\n",
      "Training Epoch: 24 [25344/50000]\tLoss: 1.0993\tLR: 0.000337\n",
      "Training Epoch: 24 [25472/50000]\tLoss: 1.2096\tLR: 0.000337\n",
      "Training Epoch: 24 [25600/50000]\tLoss: 1.1501\tLR: 0.000337\n",
      "Training Epoch: 24 [25728/50000]\tLoss: 0.9316\tLR: 0.000337\n",
      "Training Epoch: 24 [25856/50000]\tLoss: 0.8241\tLR: 0.000337\n",
      "Training Epoch: 24 [25984/50000]\tLoss: 1.0181\tLR: 0.000337\n",
      "Training Epoch: 24 [26112/50000]\tLoss: 1.1587\tLR: 0.000337\n",
      "Training Epoch: 24 [26240/50000]\tLoss: 1.1415\tLR: 0.000337\n",
      "Training Epoch: 24 [26368/50000]\tLoss: 1.2057\tLR: 0.000337\n",
      "Training Epoch: 24 [26496/50000]\tLoss: 0.8817\tLR: 0.000337\n",
      "Training Epoch: 24 [26624/50000]\tLoss: 1.1323\tLR: 0.000337\n",
      "Training Epoch: 24 [26752/50000]\tLoss: 0.9630\tLR: 0.000337\n",
      "Training Epoch: 24 [26880/50000]\tLoss: 1.1272\tLR: 0.000337\n",
      "Training Epoch: 24 [27008/50000]\tLoss: 1.2622\tLR: 0.000337\n",
      "Training Epoch: 24 [27136/50000]\tLoss: 1.3123\tLR: 0.000337\n",
      "Training Epoch: 24 [27264/50000]\tLoss: 1.1281\tLR: 0.000337\n",
      "Training Epoch: 24 [27392/50000]\tLoss: 1.0369\tLR: 0.000337\n",
      "Training Epoch: 24 [27520/50000]\tLoss: 1.1275\tLR: 0.000337\n",
      "Training Epoch: 24 [27648/50000]\tLoss: 1.0539\tLR: 0.000337\n",
      "Training Epoch: 24 [27776/50000]\tLoss: 1.1634\tLR: 0.000337\n",
      "Training Epoch: 24 [27904/50000]\tLoss: 0.9968\tLR: 0.000337\n",
      "Training Epoch: 24 [28032/50000]\tLoss: 1.0068\tLR: 0.000337\n",
      "Training Epoch: 24 [28160/50000]\tLoss: 0.8913\tLR: 0.000337\n",
      "Training Epoch: 24 [28288/50000]\tLoss: 0.9974\tLR: 0.000337\n",
      "Training Epoch: 24 [28416/50000]\tLoss: 1.0316\tLR: 0.000337\n",
      "Training Epoch: 24 [28544/50000]\tLoss: 1.0007\tLR: 0.000337\n",
      "Training Epoch: 24 [28672/50000]\tLoss: 1.0769\tLR: 0.000337\n",
      "Training Epoch: 24 [28800/50000]\tLoss: 0.9791\tLR: 0.000337\n",
      "Training Epoch: 24 [28928/50000]\tLoss: 1.0794\tLR: 0.000337\n",
      "Training Epoch: 24 [29056/50000]\tLoss: 1.1960\tLR: 0.000337\n",
      "Training Epoch: 24 [29184/50000]\tLoss: 1.1326\tLR: 0.000337\n",
      "Training Epoch: 24 [29312/50000]\tLoss: 1.0372\tLR: 0.000337\n",
      "Training Epoch: 24 [29440/50000]\tLoss: 1.2229\tLR: 0.000337\n",
      "Training Epoch: 24 [29568/50000]\tLoss: 1.0338\tLR: 0.000337\n",
      "Training Epoch: 24 [29696/50000]\tLoss: 1.0483\tLR: 0.000337\n",
      "Training Epoch: 24 [29824/50000]\tLoss: 0.9890\tLR: 0.000337\n",
      "Training Epoch: 24 [29952/50000]\tLoss: 0.9711\tLR: 0.000337\n",
      "Training Epoch: 24 [30080/50000]\tLoss: 1.0502\tLR: 0.000337\n",
      "Training Epoch: 24 [30208/50000]\tLoss: 1.0488\tLR: 0.000337\n",
      "Training Epoch: 24 [30336/50000]\tLoss: 0.9411\tLR: 0.000337\n",
      "Training Epoch: 24 [30464/50000]\tLoss: 1.0843\tLR: 0.000337\n",
      "Training Epoch: 24 [30592/50000]\tLoss: 1.0712\tLR: 0.000337\n",
      "Training Epoch: 24 [30720/50000]\tLoss: 0.9863\tLR: 0.000337\n",
      "Training Epoch: 24 [30848/50000]\tLoss: 1.2421\tLR: 0.000337\n",
      "Training Epoch: 24 [30976/50000]\tLoss: 1.0969\tLR: 0.000337\n",
      "Training Epoch: 24 [31104/50000]\tLoss: 1.1101\tLR: 0.000337\n",
      "Training Epoch: 24 [31232/50000]\tLoss: 1.1907\tLR: 0.000337\n",
      "Training Epoch: 24 [31360/50000]\tLoss: 1.0877\tLR: 0.000337\n",
      "Training Epoch: 24 [31488/50000]\tLoss: 0.9767\tLR: 0.000337\n",
      "Training Epoch: 24 [31616/50000]\tLoss: 1.0104\tLR: 0.000337\n",
      "Training Epoch: 24 [31744/50000]\tLoss: 1.1263\tLR: 0.000337\n",
      "Training Epoch: 24 [31872/50000]\tLoss: 0.9343\tLR: 0.000337\n",
      "Training Epoch: 24 [32000/50000]\tLoss: 0.9920\tLR: 0.000337\n",
      "Training Epoch: 24 [32128/50000]\tLoss: 0.9585\tLR: 0.000337\n",
      "Training Epoch: 24 [32256/50000]\tLoss: 0.9983\tLR: 0.000337\n",
      "Training Epoch: 24 [32384/50000]\tLoss: 0.8923\tLR: 0.000337\n",
      "Training Epoch: 24 [32512/50000]\tLoss: 1.0127\tLR: 0.000337\n",
      "Training Epoch: 24 [32640/50000]\tLoss: 1.1957\tLR: 0.000337\n",
      "Training Epoch: 24 [32768/50000]\tLoss: 1.1124\tLR: 0.000337\n",
      "Training Epoch: 24 [32896/50000]\tLoss: 1.0361\tLR: 0.000337\n",
      "Training Epoch: 24 [33024/50000]\tLoss: 1.0330\tLR: 0.000337\n",
      "Training Epoch: 24 [33152/50000]\tLoss: 0.9233\tLR: 0.000337\n",
      "Training Epoch: 24 [33280/50000]\tLoss: 1.0130\tLR: 0.000337\n",
      "Training Epoch: 24 [33408/50000]\tLoss: 1.0695\tLR: 0.000337\n",
      "Training Epoch: 24 [33536/50000]\tLoss: 1.2711\tLR: 0.000337\n",
      "Training Epoch: 24 [33664/50000]\tLoss: 1.0713\tLR: 0.000337\n",
      "Training Epoch: 24 [33792/50000]\tLoss: 1.1232\tLR: 0.000337\n",
      "Training Epoch: 24 [33920/50000]\tLoss: 1.2463\tLR: 0.000337\n",
      "Training Epoch: 24 [34048/50000]\tLoss: 1.2048\tLR: 0.000337\n",
      "Training Epoch: 24 [34176/50000]\tLoss: 1.0485\tLR: 0.000337\n",
      "Training Epoch: 24 [34304/50000]\tLoss: 1.0459\tLR: 0.000337\n",
      "Training Epoch: 24 [34432/50000]\tLoss: 1.2149\tLR: 0.000337\n",
      "Training Epoch: 24 [34560/50000]\tLoss: 0.9148\tLR: 0.000337\n",
      "Training Epoch: 24 [34688/50000]\tLoss: 1.0858\tLR: 0.000337\n",
      "Training Epoch: 24 [34816/50000]\tLoss: 0.9807\tLR: 0.000337\n",
      "Training Epoch: 24 [34944/50000]\tLoss: 0.8943\tLR: 0.000337\n",
      "Training Epoch: 24 [35072/50000]\tLoss: 1.0816\tLR: 0.000337\n",
      "Training Epoch: 24 [35200/50000]\tLoss: 1.0720\tLR: 0.000337\n",
      "Training Epoch: 24 [35328/50000]\tLoss: 1.1731\tLR: 0.000337\n",
      "Training Epoch: 24 [35456/50000]\tLoss: 0.9159\tLR: 0.000337\n",
      "Training Epoch: 24 [35584/50000]\tLoss: 1.1388\tLR: 0.000337\n",
      "Training Epoch: 24 [35712/50000]\tLoss: 0.8341\tLR: 0.000337\n",
      "Training Epoch: 24 [35840/50000]\tLoss: 1.0758\tLR: 0.000337\n",
      "Training Epoch: 24 [35968/50000]\tLoss: 1.1681\tLR: 0.000337\n",
      "Training Epoch: 24 [36096/50000]\tLoss: 1.0703\tLR: 0.000337\n",
      "Training Epoch: 24 [36224/50000]\tLoss: 1.0815\tLR: 0.000337\n",
      "Training Epoch: 24 [36352/50000]\tLoss: 1.0934\tLR: 0.000337\n",
      "Training Epoch: 24 [36480/50000]\tLoss: 1.3793\tLR: 0.000337\n",
      "Training Epoch: 24 [36608/50000]\tLoss: 1.0843\tLR: 0.000337\n",
      "Training Epoch: 24 [36736/50000]\tLoss: 1.1444\tLR: 0.000337\n",
      "Training Epoch: 24 [36864/50000]\tLoss: 1.1058\tLR: 0.000337\n",
      "Training Epoch: 24 [36992/50000]\tLoss: 1.1688\tLR: 0.000337\n",
      "Training Epoch: 24 [37120/50000]\tLoss: 1.2010\tLR: 0.000337\n",
      "Training Epoch: 24 [37248/50000]\tLoss: 1.0352\tLR: 0.000337\n",
      "Training Epoch: 24 [37376/50000]\tLoss: 1.1080\tLR: 0.000337\n",
      "Training Epoch: 24 [37504/50000]\tLoss: 1.1378\tLR: 0.000337\n",
      "Training Epoch: 24 [37632/50000]\tLoss: 1.1214\tLR: 0.000337\n",
      "Training Epoch: 24 [37760/50000]\tLoss: 1.3153\tLR: 0.000337\n",
      "Training Epoch: 24 [37888/50000]\tLoss: 1.1838\tLR: 0.000337\n",
      "Training Epoch: 24 [38016/50000]\tLoss: 1.0359\tLR: 0.000337\n",
      "Training Epoch: 24 [38144/50000]\tLoss: 1.2520\tLR: 0.000337\n",
      "Training Epoch: 24 [38272/50000]\tLoss: 0.9088\tLR: 0.000337\n",
      "Training Epoch: 24 [38400/50000]\tLoss: 0.9535\tLR: 0.000337\n",
      "Training Epoch: 24 [38528/50000]\tLoss: 1.2112\tLR: 0.000337\n",
      "Training Epoch: 24 [38656/50000]\tLoss: 1.2477\tLR: 0.000337\n",
      "Training Epoch: 24 [38784/50000]\tLoss: 1.2475\tLR: 0.000337\n",
      "Training Epoch: 24 [38912/50000]\tLoss: 1.0905\tLR: 0.000337\n",
      "Training Epoch: 24 [39040/50000]\tLoss: 1.2465\tLR: 0.000337\n",
      "Training Epoch: 24 [39168/50000]\tLoss: 1.2031\tLR: 0.000337\n",
      "Training Epoch: 24 [39296/50000]\tLoss: 1.1263\tLR: 0.000337\n",
      "Training Epoch: 24 [39424/50000]\tLoss: 1.0551\tLR: 0.000337\n",
      "Training Epoch: 24 [39552/50000]\tLoss: 1.2948\tLR: 0.000337\n",
      "Training Epoch: 24 [39680/50000]\tLoss: 0.9349\tLR: 0.000337\n",
      "Training Epoch: 24 [39808/50000]\tLoss: 1.1590\tLR: 0.000337\n",
      "Training Epoch: 24 [39936/50000]\tLoss: 1.1503\tLR: 0.000337\n",
      "Training Epoch: 24 [40064/50000]\tLoss: 1.2143\tLR: 0.000337\n",
      "Training Epoch: 24 [40192/50000]\tLoss: 1.1117\tLR: 0.000337\n",
      "Training Epoch: 24 [40320/50000]\tLoss: 1.0064\tLR: 0.000337\n",
      "Training Epoch: 24 [40448/50000]\tLoss: 1.2159\tLR: 0.000337\n",
      "Training Epoch: 24 [40576/50000]\tLoss: 1.3460\tLR: 0.000337\n",
      "Training Epoch: 24 [40704/50000]\tLoss: 0.9425\tLR: 0.000337\n",
      "Training Epoch: 24 [40832/50000]\tLoss: 1.1393\tLR: 0.000337\n",
      "Training Epoch: 24 [40960/50000]\tLoss: 1.3649\tLR: 0.000337\n",
      "Training Epoch: 24 [41088/50000]\tLoss: 0.9819\tLR: 0.000337\n",
      "Training Epoch: 24 [41216/50000]\tLoss: 1.2225\tLR: 0.000337\n",
      "Training Epoch: 24 [41344/50000]\tLoss: 0.9872\tLR: 0.000337\n",
      "Training Epoch: 24 [41472/50000]\tLoss: 1.1759\tLR: 0.000337\n",
      "Training Epoch: 24 [41600/50000]\tLoss: 1.1562\tLR: 0.000337\n",
      "Training Epoch: 24 [41728/50000]\tLoss: 1.1700\tLR: 0.000337\n",
      "Training Epoch: 24 [41856/50000]\tLoss: 1.3420\tLR: 0.000337\n",
      "Training Epoch: 24 [41984/50000]\tLoss: 1.1767\tLR: 0.000337\n",
      "Training Epoch: 24 [42112/50000]\tLoss: 0.8545\tLR: 0.000337\n",
      "Training Epoch: 24 [42240/50000]\tLoss: 1.0531\tLR: 0.000337\n",
      "Training Epoch: 24 [42368/50000]\tLoss: 0.9330\tLR: 0.000337\n",
      "Training Epoch: 24 [42496/50000]\tLoss: 1.0295\tLR: 0.000337\n",
      "Training Epoch: 24 [42624/50000]\tLoss: 1.3685\tLR: 0.000337\n",
      "Training Epoch: 24 [42752/50000]\tLoss: 1.1789\tLR: 0.000337\n",
      "Training Epoch: 24 [42880/50000]\tLoss: 1.1712\tLR: 0.000337\n",
      "Training Epoch: 24 [43008/50000]\tLoss: 1.1264\tLR: 0.000337\n",
      "Training Epoch: 24 [43136/50000]\tLoss: 1.1914\tLR: 0.000337\n",
      "Training Epoch: 24 [43264/50000]\tLoss: 1.0798\tLR: 0.000337\n",
      "Training Epoch: 24 [43392/50000]\tLoss: 1.3883\tLR: 0.000337\n",
      "Training Epoch: 24 [43520/50000]\tLoss: 1.0932\tLR: 0.000337\n",
      "Training Epoch: 24 [43648/50000]\tLoss: 1.0567\tLR: 0.000337\n",
      "Training Epoch: 24 [43776/50000]\tLoss: 1.1131\tLR: 0.000337\n",
      "Training Epoch: 24 [43904/50000]\tLoss: 0.7765\tLR: 0.000337\n",
      "Training Epoch: 24 [44032/50000]\tLoss: 0.9491\tLR: 0.000337\n",
      "Training Epoch: 24 [44160/50000]\tLoss: 0.9100\tLR: 0.000337\n",
      "Training Epoch: 24 [44288/50000]\tLoss: 1.4031\tLR: 0.000337\n",
      "Training Epoch: 24 [44416/50000]\tLoss: 0.8140\tLR: 0.000337\n",
      "Training Epoch: 24 [44544/50000]\tLoss: 1.1605\tLR: 0.000337\n",
      "Training Epoch: 24 [44672/50000]\tLoss: 1.2154\tLR: 0.000337\n",
      "Training Epoch: 24 [44800/50000]\tLoss: 1.0536\tLR: 0.000337\n",
      "Training Epoch: 24 [44928/50000]\tLoss: 0.9854\tLR: 0.000337\n",
      "Training Epoch: 24 [45056/50000]\tLoss: 1.0451\tLR: 0.000337\n",
      "Training Epoch: 24 [45184/50000]\tLoss: 0.9569\tLR: 0.000337\n",
      "Training Epoch: 24 [45312/50000]\tLoss: 1.1993\tLR: 0.000337\n",
      "Training Epoch: 24 [45440/50000]\tLoss: 1.1171\tLR: 0.000337\n",
      "Training Epoch: 24 [45568/50000]\tLoss: 1.0268\tLR: 0.000337\n",
      "Training Epoch: 24 [45696/50000]\tLoss: 0.9544\tLR: 0.000337\n",
      "Training Epoch: 24 [45824/50000]\tLoss: 1.2376\tLR: 0.000337\n",
      "Training Epoch: 24 [45952/50000]\tLoss: 1.0248\tLR: 0.000337\n",
      "Training Epoch: 24 [46080/50000]\tLoss: 1.1388\tLR: 0.000337\n",
      "Training Epoch: 24 [46208/50000]\tLoss: 0.8541\tLR: 0.000337\n",
      "Training Epoch: 24 [46336/50000]\tLoss: 1.1440\tLR: 0.000337\n",
      "Training Epoch: 24 [46464/50000]\tLoss: 0.9764\tLR: 0.000337\n",
      "Training Epoch: 24 [46592/50000]\tLoss: 0.8838\tLR: 0.000337\n",
      "Training Epoch: 24 [46720/50000]\tLoss: 0.9945\tLR: 0.000337\n",
      "Training Epoch: 24 [46848/50000]\tLoss: 0.9832\tLR: 0.000337\n",
      "Training Epoch: 24 [46976/50000]\tLoss: 1.0903\tLR: 0.000337\n",
      "Training Epoch: 24 [47104/50000]\tLoss: 1.1189\tLR: 0.000337\n",
      "Training Epoch: 24 [47232/50000]\tLoss: 0.8888\tLR: 0.000337\n",
      "Training Epoch: 24 [47360/50000]\tLoss: 1.1055\tLR: 0.000337\n",
      "Training Epoch: 24 [47488/50000]\tLoss: 0.8863\tLR: 0.000337\n",
      "Training Epoch: 24 [47616/50000]\tLoss: 1.1077\tLR: 0.000337\n",
      "Training Epoch: 24 [47744/50000]\tLoss: 1.0664\tLR: 0.000337\n",
      "Training Epoch: 24 [47872/50000]\tLoss: 1.2700\tLR: 0.000337\n",
      "Training Epoch: 24 [48000/50000]\tLoss: 0.9995\tLR: 0.000337\n",
      "Training Epoch: 24 [48128/50000]\tLoss: 1.2036\tLR: 0.000337\n",
      "Training Epoch: 24 [48256/50000]\tLoss: 1.0053\tLR: 0.000337\n",
      "Training Epoch: 24 [48384/50000]\tLoss: 1.0788\tLR: 0.000337\n",
      "Training Epoch: 24 [48512/50000]\tLoss: 1.1922\tLR: 0.000337\n",
      "Training Epoch: 24 [48640/50000]\tLoss: 1.0607\tLR: 0.000337\n",
      "Training Epoch: 24 [48768/50000]\tLoss: 0.8792\tLR: 0.000337\n",
      "Training Epoch: 24 [48896/50000]\tLoss: 1.1066\tLR: 0.000337\n",
      "Training Epoch: 24 [49024/50000]\tLoss: 1.2295\tLR: 0.000337\n",
      "Training Epoch: 24 [49152/50000]\tLoss: 1.0511\tLR: 0.000337\n",
      "Training Epoch: 24 [49280/50000]\tLoss: 1.2737\tLR: 0.000337\n",
      "Training Epoch: 24 [49408/50000]\tLoss: 1.0673\tLR: 0.000337\n",
      "Training Epoch: 24 [49536/50000]\tLoss: 1.0717\tLR: 0.000337\n",
      "Training Epoch: 24 [49664/50000]\tLoss: 1.4228\tLR: 0.000337\n",
      "Training Epoch: 24 [49792/50000]\tLoss: 0.8510\tLR: 0.000337\n",
      "Training Epoch: 24 [49920/50000]\tLoss: 1.1143\tLR: 0.000337\n",
      "Training Epoch: 24 [50000/50000]\tLoss: 1.3234\tLR: 0.000337\n",
      "Test set: Average loss: 0.0104, Accuracy: 0.6300\n",
      "\n",
      "\n",
      "best_acc:  tensor(0.6302, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    net.train()\n",
    "    for batch_index, (images, labels) in enumerate(CIFAR100_training_loader):\n",
    "        if epoch <= 1:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "            loss.item(),\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            epoch=epoch,\n",
    "            trained_samples=batch_index * 128 + len(images),\n",
    "            total_samples=len(CIFAR100_training_loader.dataset)\n",
    "        ))\n",
    "\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "\n",
    "def eval_training(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0.0 # cost function error\n",
    "    correct = 0.0\n",
    "\n",
    "    for (images, labels) in CIFAR100_test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "        test_loss / len(CIFAR100_test_loader.dataset),\n",
    "        correct.float() / len(CIFAR100_test_loader.dataset)\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "\n",
    "    return correct.float() / len(CIFAR100_test_loader.dataset)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-net', type=str, required=True, help='net type')\n",
    "#     parser.add_argument('-gpu', type=bool, default=True, help='use gpu or not')\n",
    "#     parser.add_argument('-w', type=int, default=2, help='number of workers for dataloader')\n",
    "#     parser.add_argument('-b', type=int, default=128, help='batch size for dataloader')\n",
    "#     parser.add_argument('-s', type=bool, default=True, help='whether shuffle the dataset')\n",
    "#     parser.add_argument('-warm', type=int, default=1, help='warm up training phase')\n",
    "#     parser.add_argument('-lr', type=float, default=0.1, help='initial learning rate')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    net= get_network()\n",
    "    \n",
    "        \n",
    "    #data preprocessing:\n",
    "    CIFAR100_training_loader = get_training_dataloader(\n",
    "        CIFAR100_TRAIN_MEAN,\n",
    "        CIFAR100_TRAIN_STD,\n",
    "        num_workers=2,\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    CIFAR100_test_loader = get_test_dataloader(\n",
    "        CIFAR100_TRAIN_MEAN,\n",
    "        CIFAR100_TRAIN_STD,\n",
    "        num_workers=2,\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=0.15) #learning rate decay\n",
    "    iter_per_epoch = len(CIFAR100_training_loader)\n",
    "    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * 1)\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_PATH, \"resnetcbam18\")\n",
    "\n",
    "    #create checkpoint folder to save model\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, EPOCH):\n",
    "        if epoch > 1:\n",
    "            train_scheduler.step(epoch)\n",
    "\n",
    "        train(epoch)\n",
    "        acc = eval_training(epoch)\n",
    "\n",
    "        #start to save best performance model after learning rate decay to 0.01 \n",
    "        if epoch > MILESTONES[1] and best_acc < acc:\n",
    "            torch.save(net.state_dict(), checkpoint_path.format(net=\"resnetcbam18\", epoch=epoch, type='best'))\n",
    "            best_acc = acc\n",
    "            continue\n",
    "\n",
    "        if not epoch % SAVE_EPOCH:\n",
    "            torch.save(net.state_dict(), checkpoint_path.format(net=\"resnetcbam18\", epoch=epoch, type='regular'))\n",
    "    print()\n",
    "    print(\"best_acc: \", best_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T19:11:41.753053Z",
     "iopub.status.busy": "2024-03-25T19:11:41.752704Z",
     "iopub.status.idle": "2024-03-25T19:11:41.805876Z",
     "shell.execute_reply": "2024-03-25T19:11:41.805166Z",
     "shell.execute_reply.started": "2024-03-25T19:11:41.753021Z"
    }
   },
   "outputs": [],
   "source": [
    "weights_file=\"./resnetcbam18.pth\"\n",
    "torch.save(net.state_dict(), weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T19:11:41.807202Z",
     "iopub.status.busy": "2024-03-25T19:11:41.806908Z",
     "iopub.status.idle": "2024-03-25T19:11:46.968674Z",
     "shell.execute_reply": "2024-03-25T19:11:46.967592Z",
     "shell.execute_reply.started": "2024-03-25T19:11:41.807178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "MobileNetCBAM(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): CBAM(\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (sa): SpatialAttention(\n",
      "          (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=100, bias=True)\n",
      ")\n",
      "iteration: 1\ttotal 79 iterations\n",
      "iteration: 2\ttotal 79 iterations\n",
      "iteration: 3\ttotal 79 iterations\n",
      "iteration: 4\ttotal 79 iterations\n",
      "iteration: 5\ttotal 79 iterations\n",
      "iteration: 6\ttotal 79 iterations\n",
      "iteration: 7\ttotal 79 iterations\n",
      "iteration: 8\ttotal 79 iterations\n",
      "iteration: 9\ttotal 79 iterations\n",
      "iteration: 10\ttotal 79 iterations\n",
      "iteration: 11\ttotal 79 iterations\n",
      "iteration: 12\ttotal 79 iterations\n",
      "iteration: 13\ttotal 79 iterations\n",
      "iteration: 14\ttotal 79 iterations\n",
      "iteration: 15\ttotal 79 iterations\n",
      "iteration: 16\ttotal 79 iterations\n",
      "iteration: 17\ttotal 79 iterations\n",
      "iteration: 18\ttotal 79 iterations\n",
      "iteration: 19\ttotal 79 iterations\n",
      "iteration: 20\ttotal 79 iterations\n",
      "iteration: 21\ttotal 79 iterations\n",
      "iteration: 22\ttotal 79 iterations\n",
      "iteration: 23\ttotal 79 iterations\n",
      "iteration: 24\ttotal 79 iterations\n",
      "iteration: 25\ttotal 79 iterations\n",
      "iteration: 26\ttotal 79 iterations\n",
      "iteration: 27\ttotal 79 iterations\n",
      "iteration: 28\ttotal 79 iterations\n",
      "iteration: 29\ttotal 79 iterations\n",
      "iteration: 30\ttotal 79 iterations\n",
      "iteration: 31\ttotal 79 iterations\n",
      "iteration: 32\ttotal 79 iterations\n",
      "iteration: 33\ttotal 79 iterations\n",
      "iteration: 34\ttotal 79 iterations\n",
      "iteration: 35\ttotal 79 iterations\n",
      "iteration: 36\ttotal 79 iterations\n",
      "iteration: 37\ttotal 79 iterations\n",
      "iteration: 38\ttotal 79 iterations\n",
      "iteration: 39\ttotal 79 iterations\n",
      "iteration: 40\ttotal 79 iterations\n",
      "iteration: 41\ttotal 79 iterations\n",
      "iteration: 42\ttotal 79 iterations\n",
      "iteration: 43\ttotal 79 iterations\n",
      "iteration: 44\ttotal 79 iterations\n",
      "iteration: 45\ttotal 79 iterations\n",
      "iteration: 46\ttotal 79 iterations\n",
      "iteration: 47\ttotal 79 iterations\n",
      "iteration: 48\ttotal 79 iterations\n",
      "iteration: 49\ttotal 79 iterations\n",
      "iteration: 50\ttotal 79 iterations\n",
      "iteration: 51\ttotal 79 iterations\n",
      "iteration: 52\ttotal 79 iterations\n",
      "iteration: 53\ttotal 79 iterations\n",
      "iteration: 54\ttotal 79 iterations\n",
      "iteration: 55\ttotal 79 iterations\n",
      "iteration: 56\ttotal 79 iterations\n",
      "iteration: 57\ttotal 79 iterations\n",
      "iteration: 58\ttotal 79 iterations\n",
      "iteration: 59\ttotal 79 iterations\n",
      "iteration: 60\ttotal 79 iterations\n",
      "iteration: 61\ttotal 79 iterations\n",
      "iteration: 62\ttotal 79 iterations\n",
      "iteration: 63\ttotal 79 iterations\n",
      "iteration: 64\ttotal 79 iterations\n",
      "iteration: 65\ttotal 79 iterations\n",
      "iteration: 66\ttotal 79 iterations\n",
      "iteration: 67\ttotal 79 iterations\n",
      "iteration: 68\ttotal 79 iterations\n",
      "iteration: 69\ttotal 79 iterations\n",
      "iteration: 70\ttotal 79 iterations\n",
      "iteration: 71\ttotal 79 iterations\n",
      "iteration: 72\ttotal 79 iterations\n",
      "iteration: 73\ttotal 79 iterations\n",
      "iteration: 74\ttotal 79 iterations\n",
      "iteration: 75\ttotal 79 iterations\n",
      "iteration: 76\ttotal 79 iterations\n",
      "iteration: 77\ttotal 79 iterations\n",
      "iteration: 78\ttotal 79 iterations\n",
      "iteration: 79\ttotal 79 iterations\n",
      "\n",
      "Top 1 err:  tensor(0.3700, device='cuda:0')\n",
      "Top 5 err:  tensor(0.1124, device='cuda:0')\n",
      "Parameter numbers: 3789454\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = get_network()\n",
    "    CIFAR100_test_loader = get_test_dataloader(\n",
    "        CIFAR100_TRAIN_MEAN,\n",
    "        CIFAR100_TRAIN_STD,\n",
    "        #CIFAR100_PATH,\n",
    "        num_workers=2,\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    net.load_state_dict(torch.load(weights_file), True)\n",
    "    print(net)\n",
    "    net.eval()\n",
    "\n",
    "    correct_1 = 0.0\n",
    "    correct_5 = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for n_iter, (image, label) in enumerate(CIFAR100_test_loader):\n",
    "        print(\"iteration: {}\\ttotal {} iterations\".format(n_iter + 1, len(CIFAR100_test_loader)))\n",
    "        image = Variable(image).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        output = net(image)\n",
    "        _, pred = output.topk(5, 1, largest=True, sorted=True)\n",
    "\n",
    "        label = label.view(label.size(0), -1).expand_as(pred)\n",
    "        correct = pred.eq(label).float()\n",
    "\n",
    "        #compute top 5\n",
    "        correct_5 += correct[:, :5].sum()\n",
    "\n",
    "        #compute top1 \n",
    "        correct_1 += correct[:, :1].sum()\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"Top 1 err: \", 1 - correct_1 / len(CIFAR100_test_loader.dataset))\n",
    "    print(\"Top 5 err: \", 1 - correct_5 / len(CIFAR100_test_loader.dataset))\n",
    "    print(\"Parameter numbers: {}\".format(sum(p.numel() for p in net.parameters())))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
