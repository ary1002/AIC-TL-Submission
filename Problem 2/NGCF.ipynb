{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "buyibBZYRr8L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(object):\n",
        "    def __init__(self, path, batch_size):\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        train_file = path + '/train.txt'\n",
        "        test_file = path + '/test.txt'\n",
        "\n",
        "        #get number of users and items\n",
        "        self.n_users, self.n_items = 0, 0\n",
        "        self.n_train, self.n_test = 0, 0\n",
        "        self.neg_pools = {}\n",
        "\n",
        "        self.exist_users = []\n",
        "\n",
        "        # search train_file for max user_id/item_id\n",
        "        with open(train_file) as f:\n",
        "            for l in f.readlines():\n",
        "                if len(l) > 0:\n",
        "                    l = l.strip('\\n').split(' ')\n",
        "                    items = [int(i) for i in l[1:]]\n",
        "                    # first element is the user_id, rest are items\n",
        "                    uid = int(l[0])\n",
        "                    self.exist_users.append(uid)\n",
        "                    # item/user with highest number is number of items/users\n",
        "                    self.n_items = max(self.n_items, max(items))\n",
        "                    self.n_users = max(self.n_users, uid)\n",
        "                    # number of interactions\n",
        "                    self.n_train += len(items)\n",
        "\n",
        "        # search test_file for max item_id\n",
        "        with open(test_file) as f:\n",
        "            for l in f.readlines():\n",
        "                if len(l) > 0:\n",
        "                    l = l.strip('\\n')\n",
        "                    try:\n",
        "                        items = [int(i) for i in l.split(' ')[1:]]\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if not items:\n",
        "                        print(\"empyt test exists\")\n",
        "                        pass\n",
        "                    else:\n",
        "                        self.n_items = max(self.n_items, max(items))\n",
        "                        self.n_test += len(items)\n",
        "        # adjust counters: user_id/item_id starts at 0\n",
        "        self.n_items += 1\n",
        "        self.n_users += 1\n",
        "\n",
        "        self.print_statistics()\n",
        "\n",
        "        # create interactions/ratings matrix 'R' # dok = dictionary of keys\n",
        "        print('Creating interaction matrices R_train and R_test...')\n",
        "        t1 = time()\n",
        "        self.R_train = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "        self.R_test = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "\n",
        "        self.train_items, self.test_set = {}, {}\n",
        "        with open(train_file) as f_train:\n",
        "            with open(test_file) as f_test:\n",
        "                for l in f_train.readlines():\n",
        "                    if len(l) == 0: break\n",
        "                    l = l.strip('\\n')\n",
        "                    items = [int(i) for i in l.split(' ')]\n",
        "                    uid, train_items = items[0], items[1:]\n",
        "                    # enter 1 if user interacted with item\n",
        "                    for i in train_items:\n",
        "                        self.R_train[uid, i] = 1.\n",
        "                    self.train_items[uid] = train_items\n",
        "\n",
        "                for l in f_test.readlines():\n",
        "                    if len(l) == 0: break\n",
        "                    l = l.strip('\\n')\n",
        "                    try:\n",
        "                        items = [int(i) for i in l.split(' ')]\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    uid, test_items = items[0], items[1:]\n",
        "                    for i in test_items:\n",
        "                        self.R_test[uid, i] = 1.0\n",
        "                    self.test_set[uid] = test_items\n",
        "        print('Complete. Interaction matrices R_train and R_test created in', time() - t1, 'sec')\n",
        "\n",
        "    # if exist, get adjacency matrix\n",
        "    def get_adj_mat(self):\n",
        "        try:\n",
        "            t1 = time()\n",
        "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
        "            print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\n",
        "\n",
        "        except Exception:\n",
        "            print('Creating adjacency-matrix...')\n",
        "            adj_mat = self.create_adj_mat()\n",
        "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
        "        return adj_mat\n",
        "\n",
        "    # create adjancency matrix\n",
        "    def create_adj_mat(self):\n",
        "        t1 = time()\n",
        "\n",
        "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
        "        adj_mat = adj_mat.tolil()\n",
        "        R = self.R_train.tolil() # to list of lists\n",
        "\n",
        "        adj_mat[:self.n_users, self.n_users:] = R\n",
        "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
        "        adj_mat = adj_mat.todok()\n",
        "        print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\n",
        "\n",
        "        t2 = time()\n",
        "\n",
        "        # normalize adjacency matrix\n",
        "        def normalized_adj_single(adj):\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "\n",
        "            d_inv = np.power(rowsum, -.5).flatten()\n",
        "            d_inv[np.isinf(d_inv)] = 0.\n",
        "            d_mat_inv = sp.diags(d_inv)\n",
        "\n",
        "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
        "            return norm_adj.tocoo()\n",
        "\n",
        "        print('Transforming adjacency-matrix to NGCF-adjacency matrix...')\n",
        "        ngcf_adj_mat = normalized_adj_single(adj_mat) + sp.eye(adj_mat.shape[0])\n",
        "\n",
        "        print('Complete. Transformed adjacency-matrix to NGCF-adjacency matrix in', time() - t2, 'sec.')\n",
        "        return ngcf_adj_mat.tocsr()\n",
        "\n",
        "    # create collections of N items that users never interacted with\n",
        "    def negative_pool(self):\n",
        "        t1 = time()\n",
        "        for u in self.train_items.keys():\n",
        "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
        "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
        "            self.neg_pools[u] = pools\n",
        "        print('refresh negative pools', time() - t1)\n",
        "\n",
        "    # sample data for mini-batches\n",
        "    def sample(self):\n",
        "        if self.batch_size <= self.n_users:\n",
        "            users = rd.sample(self.exist_users, self.batch_size)\n",
        "        else:\n",
        "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
        "\n",
        "        def sample_pos_items_for_u(u, num):\n",
        "            pos_items = self.train_items[u]\n",
        "            n_pos_items = len(pos_items)\n",
        "            pos_batch = []\n",
        "            while True:\n",
        "                if len(pos_batch) == num: break\n",
        "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
        "                pos_i_id = pos_items[pos_id]\n",
        "\n",
        "                if pos_i_id not in pos_batch:\n",
        "                    pos_batch.append(pos_i_id)\n",
        "            return pos_batch\n",
        "\n",
        "        def sample_neg_items_for_u(u, num):\n",
        "            neg_items = []\n",
        "            while True:\n",
        "                if len(neg_items) == num: break\n",
        "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
        "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
        "                    neg_items.append(neg_id)\n",
        "            return neg_items\n",
        "\n",
        "        def sample_neg_items_for_u_from_pools(u, num):\n",
        "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
        "            return rd.sample(neg_items, num)\n",
        "\n",
        "        pos_items, neg_items = [], []\n",
        "        for u in users:\n",
        "            pos_items += sample_pos_items_for_u(u, 1)\n",
        "            neg_items += sample_neg_items_for_u(u, 1)\n",
        "\n",
        "        return users, pos_items, neg_items\n",
        "\n",
        "    def get_num_users_items(self):\n",
        "        return self.n_users, self.n_items\n",
        "\n",
        "    def print_statistics(self):\n",
        "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
        "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
        "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))"
      ],
      "metadata": {
        "id": "dzzTgn8uRwGM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NGCF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout,\n",
        "        adj_mtx):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize Class attributes\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.emb_dim = emb_dim\n",
        "        self.adj_mtx = adj_mtx\n",
        "        self.laplacian = adj_mtx - sp.eye(adj_mtx.shape[0])\n",
        "        self.reg = reg\n",
        "        self.layers = layers\n",
        "        self.n_layers = len(self.layers)\n",
        "        self.node_dropout = node_dropout\n",
        "        self.mess_dropout = mess_dropout\n",
        "\n",
        "        #self.u_g_embeddings = nn.Parameter(torch.empty(n_users, emb_dim+np.sum(self.layers)))\n",
        "        #self.i_g_embeddings = nn.Parameter(torch.empty(n_items, emb_dim+np.sum(self.layers)))\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weight_dict = self._init_weights()\n",
        "        print(\"Weights initialized.\")\n",
        "\n",
        "        # Create Matrix 'A', PyTorch sparse tensor of SP adjacency_mtx\n",
        "        self.A = self._convert_sp_mat_to_sp_tensor(self.adj_mtx)\n",
        "        self.L = self._convert_sp_mat_to_sp_tensor(self.laplacian)\n",
        "\n",
        "    # initialize weights\n",
        "    def _init_weights(self):\n",
        "        print(\"Initializing weights...\")\n",
        "        weight_dict = nn.ParameterDict()\n",
        "\n",
        "        initializer = torch.nn.init.xavier_uniform_\n",
        "\n",
        "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
        "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
        "\n",
        "        weight_size_list = [self.emb_dim] + self.layers\n",
        "\n",
        "        for k in range(self.n_layers):\n",
        "            weight_dict['W_gc_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
        "            weight_dict['b_gc_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
        "\n",
        "            weight_dict['W_bi_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
        "            weight_dict['b_bi_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
        "\n",
        "        return weight_dict\n",
        "\n",
        "    # convert sparse matrix into sparse PyTorch tensor\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
        "        \"\"\"\n",
        "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "        X = Adjacency matrix, scipy sparse matrix\n",
        "        \"\"\"\n",
        "        coo = X.tocoo().astype(np.float32)\n",
        "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
        "        v = torch.FloatTensor(coo.data)\n",
        "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
        "        return res\n",
        "\n",
        "    # apply node_dropout\n",
        "    def _droupout_sparse(self, X):\n",
        "        \"\"\"\n",
        "        Drop individual locations in X\n",
        "\n",
        "        Arguments:\n",
        "        ---------\n",
        "        X = adjacency matrix (PyTorch sparse tensor)\n",
        "        dropout = fraction of nodes to drop\n",
        "        noise_shape = number of non non-zero entries of X\n",
        "        \"\"\"\n",
        "\n",
        "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
        "        i = X.coalesce().indices()\n",
        "        v = X.coalesce()._values()\n",
        "        i[:,node_dropout_mask] = 0\n",
        "        v[node_dropout_mask] = 0\n",
        "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
        "\n",
        "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        \"\"\"\n",
        "        Computes the forward pass\n",
        "\n",
        "        Arguments:\n",
        "        ---------\n",
        "        u = user\n",
        "        i = positive item (user interacted with item)\n",
        "        j = negative item (user did not interact with item)\n",
        "        \"\"\"\n",
        "        # apply drop-out mask\n",
        "        A_hat = self._droupout_sparse(self.A) if self.node_dropout > 0 else self.A\n",
        "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
        "\n",
        "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
        "\n",
        "        all_embeddings = [ego_embeddings]\n",
        "\n",
        "        # forward pass for 'n' propagation layers\n",
        "        for k in range(self.n_layers):\n",
        "\n",
        "            # weighted sum messages of neighbours\n",
        "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
        "            side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings)\n",
        "\n",
        "            # transformed sum weighted sum messages of neighbours\n",
        "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) + self.weight_dict['b_gc_%d' % k]\n",
        "\n",
        "            # bi messages of neighbours\n",
        "            bi_embeddings = torch.mul(ego_embeddings, side_L_embeddings)\n",
        "            # transformed bi messages of neighbours\n",
        "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) + self.weight_dict['b_bi_%d' % k]\n",
        "\n",
        "            # non-linear activation\n",
        "            ego_embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\n",
        "            # + message dropout\n",
        "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
        "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
        "\n",
        "            # normalize activation\n",
        "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
        "\n",
        "            all_embeddings.append(norm_embeddings)\n",
        "\n",
        "        all_embeddings = torch.cat(all_embeddings, 1)\n",
        "\n",
        "        # back to user/item dimension\n",
        "        u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\n",
        "\n",
        "        self.u_g_embeddings = nn.Parameter(u_g_embeddings)\n",
        "        self.i_g_embeddings = nn.Parameter(i_g_embeddings)\n",
        "\n",
        "        u_emb = u_g_embeddings[u] # user embeddings\n",
        "        p_emb = i_g_embeddings[i] # positive item embeddings\n",
        "        n_emb = i_g_embeddings[j] # negative item embeddings\n",
        "\n",
        "        y_ui = torch.mul(u_emb, p_emb).sum(dim=1)\n",
        "        y_uj = torch.mul(u_emb, n_emb).sum(dim=1)\n",
        "        log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\n",
        "\n",
        "        # compute bpr-loss\n",
        "        bpr_loss = -log_prob\n",
        "        if self.reg > 0.:\n",
        "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
        "            l2reg  = self.reg*l2norm\n",
        "            bpr_loss =  -log_prob + l2reg\n",
        "\n",
        "        return bpr_loss"
      ],
      "metadata": {
        "id": "KlGY0ikQR3yS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_stopping(log_value, best_value, stopping_step, flag_step, expected_order='asc'):\n",
        "    \"\"\"\n",
        "    Check if early_stopping is needed\n",
        "    Function copied from original code\n",
        "    \"\"\"\n",
        "    assert expected_order in ['asc', 'des']\n",
        "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'des' and log_value <= best_value):\n",
        "        stopping_step = 0\n",
        "        best_value = log_value\n",
        "    else:\n",
        "        stopping_step += 1\n",
        "\n",
        "    if stopping_step >= flag_step:\n",
        "        print(\"Early stopping at step: {} log:{}\".format(flag_step, log_value))\n",
        "        should_stop = True\n",
        "    else:\n",
        "        should_stop = False\n",
        "\n",
        "    return best_value, stopping_step, should_stop\n",
        "\n",
        "def train(model, data_generator, optimizer):\n",
        "    \"\"\"\n",
        "    Train the model PyTorch style\n",
        "\n",
        "    Arguments:\n",
        "    ---------\n",
        "    model: PyTorch model\n",
        "    data_generator: Data object\n",
        "    optimizer: PyTorch optimizer\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    n_batch = data_generator.n_train // data_generator.batch_size + 1\n",
        "    running_loss=0\n",
        "    for _ in range(n_batch):\n",
        "        u, i, j = data_generator.sample()\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(u,i,j)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss\n",
        "\n",
        "def split_matrix(X, n_splits=100):\n",
        "    \"\"\"\n",
        "    Split a matrix/Tensor into n_folds (for the user embeddings and the R matrices)\n",
        "\n",
        "    Arguments:\n",
        "    ---------\n",
        "    X: matrix to be split\n",
        "    n_folds: number of folds\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    splits: split matrices\n",
        "    \"\"\"\n",
        "    splits = []\n",
        "    chunk_size = X.shape[0] // n_splits\n",
        "    for i in range(n_splits):\n",
        "        start = i * chunk_size\n",
        "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
        "        splits.append(X[start:end])\n",
        "    return splits\n",
        "\n",
        "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
        "    \"\"\"\n",
        "    Compute NDCG@k\n",
        "\n",
        "    Arguments:\n",
        "    ---------\n",
        "    pred_items: binary tensor with 1s in those locations corresponding to the predicted item interactions\n",
        "    test_items: binary tensor with 1s in locations corresponding to the real test interactions\n",
        "    test_indices: tensor with the location of the top-k predicted items\n",
        "    k: k'th-order\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    NDCG@k\n",
        "    \"\"\"\n",
        "    r = (test_items * pred_items).gather(1, test_indices)\n",
        "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
        "    dcg = (r[:, :k]/f).sum(1)\n",
        "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
        "    ndcg = dcg/dcg_max\n",
        "    ndcg[torch.isnan(ndcg)] = 0\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "def eval_model(u_emb, i_emb, Rtr, Rte, k):\n",
        "    \"\"\"\n",
        "    Evaluate the model\n",
        "\n",
        "    Arguments:\n",
        "    ---------\n",
        "    u_emb: User embeddings\n",
        "    i_emb: Item embeddings\n",
        "    Rtr: Sparse matrix with the training interactions\n",
        "    Rte: Sparse matrix with the testing interactions\n",
        "    k : kth-order for metrics\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    result: Dictionary with lists correponding to the metrics at order k for k in Ks\n",
        "    \"\"\"\n",
        "    # split matrices\n",
        "    ue_splits = split_matrix(u_emb)\n",
        "    tr_splits = split_matrix(Rtr)\n",
        "    te_splits = split_matrix(Rte)\n",
        "\n",
        "    recall_k, ndcg_k= [], []\n",
        "    # compute results for split matrices\n",
        "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
        "\n",
        "        scores = torch.mm(ue_f, i_emb.t())\n",
        "\n",
        "        test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
        "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
        "        scores = scores * non_train_items\n",
        "\n",
        "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
        "        pred_items = torch.zeros_like(scores).float()\n",
        "        test_indices = test_indices.view(-1, k)\n",
        "        #pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())\n",
        "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices, dtype=torch.float32).cuda())\n",
        "\n",
        "\n",
        "        topk_preds = torch.zeros_like(scores).float()\n",
        "        topk_preds.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices, dtype=torch.float32).cuda())\n",
        "\n",
        "        TP = (test_items * topk_preds).sum(1)\n",
        "        rec = TP/test_items.sum(1)\n",
        "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
        "\n",
        "        recall_k.append(rec)\n",
        "        ndcg_k.append(ndcg)\n",
        "\n",
        "    return torch.cat(recall_k).mean(), torch.cat(ndcg_k).mean()"
      ],
      "metadata": {
        "id": "VLAYtovER6Ze"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "  #   parser.add_argument('--results_dir', type=str, default='results',\n",
        "  #                       help='Store model to path.')\n",
        "  #   parser.add_argument('--n_epochs', type=int, default=400,\n",
        "  #                       help='Number of epoch.')\n",
        "  #   parser.add_argument('--save_results', type=int, default=1,\n",
        "  #                       help='Save model and results')/content/test.txt\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # read parsed arguments\n",
        "    data_dir = \"/content/\"\n",
        "    dataset = \"ml-100k\"\n",
        "    batch_size = 512\n",
        "    layers = eval('[64,64]')\n",
        "    emb_dim = 64\n",
        "    lr = 0.0005\n",
        "    reg = 1e-5\n",
        "    mess_dropout = 0.0\n",
        "    node_dropout = 0.0\n",
        "    k = 20\n",
        "    n_epochs=300\n",
        "    eval_N=1\n",
        "    save_results=1\n",
        "\n",
        "\n",
        "    # generate the NGCF-adjacency matrix\n",
        "    data_generator = Data(path=data_dir + dataset, batch_size=batch_size)\n",
        "    adj_mtx = data_generator.get_adj_mat()\n",
        "\n",
        "    # create model name and save\n",
        "    modelname =  \"NGCF\" + \\\n",
        "        \"_bs_\" + str(batch_size) + \\\n",
        "        \"_nemb_\" + str(emb_dim) + \\\n",
        "        \"_layers_\" + str(layers) + \\\n",
        "        \"_nodedr_\" + str(node_dropout) + \\\n",
        "        \"_messdr_\" + str(mess_dropout) + \\\n",
        "        \"_reg_\" + str(reg) + \\\n",
        "        \"_lr_\"  + str(lr)\n",
        "\n",
        "    # create NGCF model\n",
        "    model = NGCF(data_generator.n_users,\n",
        "                 data_generator.n_items,\n",
        "                 emb_dim,\n",
        "                 layers,\n",
        "                 reg,\n",
        "                 node_dropout,\n",
        "                 mess_dropout,\n",
        "                 adj_mtx)\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # current best metric\n",
        "    cur_best_metric = 0\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Set values for early stopping\n",
        "    cur_best_loss, stopping_step, should_stop = 1e3, 0, False\n",
        "    today = datetime.now()\n",
        "\n",
        "    print(\"Start at \" + str(today))\n",
        "    print(\"Using \" + str(device) + \" for computations\")\n",
        "    print(\"Params on CUDA: \" + str(next(model.parameters()).is_cuda))\n",
        "\n",
        "    results = {\"Epoch\": [],\n",
        "               \"Loss\": [],\n",
        "               \"Recall\": [],\n",
        "               \"NDCG\": [],\n",
        "               \"Training Time\": []}\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        t1 = time()\n",
        "        loss = train(model, data_generator, optimizer)\n",
        "        training_time = time()-t1\n",
        "        print(\"Epoch: {}, Training time: {:.2f}s, Loss: {:.4f}\".\n",
        "            format(epoch, training_time, loss))\n",
        "\n",
        "        # print test evaluation metrics every N epochs (provided by eval_N)\n",
        "        if epoch % eval_N  == (eval_N - 1):\n",
        "            with torch.no_grad():\n",
        "                t2 = time()\n",
        "                recall, ndcg = eval_model(model.u_g_embeddings.detach(),\n",
        "                                          model.i_g_embeddings.detach(),\n",
        "                                          data_generator.R_train,\n",
        "                                          data_generator.R_test,\n",
        "                                          k)\n",
        "\n",
        "            print(\n",
        "                \"Evaluate current model:\\n\",\n",
        "                \"Epoch: {}, Validation time: {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
        "                \"Loss: {:.4f}:\".format(loss), \"\\n\",\n",
        "                \"Recall@{}: {:.4f}\".format(k, recall), \"\\n\",\n",
        "                \"NDCG@{}: {:.4f}\".format(k, ndcg)\n",
        "                )\n",
        "\n",
        "            cur_best_metric, stopping_step, should_stop = \\\n",
        "            early_stopping(recall, cur_best_metric, stopping_step, flag_step=5)\n",
        "\n",
        "            # save results in dict\n",
        "            results['Epoch'].append(epoch)\n",
        "            results['Loss'].append(loss)\n",
        "            results['Recall'].append(recall.item())\n",
        "            results['NDCG'].append(ndcg.item())\n",
        "            results['Training Time'].append(training_time)\n",
        "        else:\n",
        "            # save results in dict\n",
        "            results['Epoch'].append(epoch)\n",
        "            results['Loss'].append(loss)\n",
        "            results['Recall'].append(None)\n",
        "            results['NDCG'].append(None)\n",
        "            results['Training Time'].append(training_time)\n",
        "\n",
        "        if should_stop == True: break\n",
        "\n",
        "    # save\n",
        "    if save_results:\n",
        "        date = today.strftime(\"%d%m%Y_%H%M\")\n",
        "\n",
        "        # save model as .pt file\n",
        "        if os.path.isdir(\"./models\"):\n",
        "            torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
        "        else:\n",
        "            os.mkdir(\"./models\")\n",
        "            torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
        "\n",
        "        # save results as pandas dataframe\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.set_index('Epoch', inplace=True)\n",
        "        if os.path.isdir(\"./results\"):\n",
        "            results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
        "        else:\n",
        "            os.mkdir(\"./results\")\n",
        "            results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
        "        # plot loss\n",
        "        results_df['Loss'].plot(figsize=(12,8), title='Loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EHETn8PhR-MB",
        "outputId": "155df8e7-0636-44dc-b8cf-291ad8902af4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_users=943, n_items=1682\n",
            "n_interactions=100000\n",
            "n_train=80064, n_test=19936, sparsity=0.06305\n",
            "Creating interaction matrices R_train and R_test...\n",
            "Complete. Interaction matrices R_train and R_test created in 1.1273102760314941 sec\n",
            "Loaded adjacency-matrix (shape: (2625, 2625) ) in 0.013744592666625977 sec.\n",
            "Initializing weights...\n",
            "Weights initialized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fa1a3dcb8bd6>:62: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
            "  res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start at 2024-03-29 13:47:26.527232\n",
            "Using cuda for computations\n",
            "Params on CUDA: True\n",
            "Epoch: 0, Training time: 11.71s, Loss: 83.2929\n",
            "Evaluate current model:\n",
            " Epoch: 0, Validation time: 3.47s \n",
            " Loss: 83.2929: \n",
            " Recall@20: 0.0842 \n",
            " NDCG@20: 0.2764\n",
            "Epoch: 1, Training time: 5.63s, Loss: 59.3680\n",
            "Evaluate current model:\n",
            " Epoch: 1, Validation time: 1.15s \n",
            " Loss: 59.3680: \n",
            " Recall@20: 0.1339 \n",
            " NDCG@20: 0.3593\n",
            "Epoch: 2, Training time: 3.79s, Loss: 50.3174\n",
            "Evaluate current model:\n",
            " Epoch: 2, Validation time: 1.30s \n",
            " Loss: 50.3174: \n",
            " Recall@20: 0.1725 \n",
            " NDCG@20: 0.4284\n",
            "Epoch: 3, Training time: 4.44s, Loss: 46.6856\n",
            "Evaluate current model:\n",
            " Epoch: 3, Validation time: 1.29s \n",
            " Loss: 46.6856: \n",
            " Recall@20: 0.1956 \n",
            " NDCG@20: 0.4749\n",
            "Epoch: 4, Training time: 3.86s, Loss: 45.1973\n",
            "Evaluate current model:\n",
            " Epoch: 4, Validation time: 1.14s \n",
            " Loss: 45.1973: \n",
            " Recall@20: 0.2115 \n",
            " NDCG@20: 0.5059\n",
            "Epoch: 5, Training time: 4.83s, Loss: 43.4654\n",
            "Evaluate current model:\n",
            " Epoch: 5, Validation time: 1.65s \n",
            " Loss: 43.4654: \n",
            " Recall@20: 0.2371 \n",
            " NDCG@20: 0.5343\n",
            "Epoch: 6, Training time: 4.07s, Loss: 41.6914\n",
            "Evaluate current model:\n",
            " Epoch: 6, Validation time: 1.03s \n",
            " Loss: 41.6914: \n",
            " Recall@20: 0.2553 \n",
            " NDCG@20: 0.5606\n",
            "Epoch: 7, Training time: 4.48s, Loss: 40.1221\n",
            "Evaluate current model:\n",
            " Epoch: 7, Validation time: 1.06s \n",
            " Loss: 40.1221: \n",
            " Recall@20: 0.2663 \n",
            " NDCG@20: 0.5743\n",
            "Epoch: 8, Training time: 3.82s, Loss: 38.8602\n",
            "Evaluate current model:\n",
            " Epoch: 8, Validation time: 1.02s \n",
            " Loss: 38.8602: \n",
            " Recall@20: 0.2710 \n",
            " NDCG@20: 0.5879\n",
            "Epoch: 9, Training time: 4.12s, Loss: 37.5767\n",
            "Evaluate current model:\n",
            " Epoch: 9, Validation time: 1.23s \n",
            " Loss: 37.5767: \n",
            " Recall@20: 0.2764 \n",
            " NDCG@20: 0.5966\n",
            "Epoch: 10, Training time: 4.04s, Loss: 36.3812\n",
            "Evaluate current model:\n",
            " Epoch: 10, Validation time: 0.98s \n",
            " Loss: 36.3812: \n",
            " Recall@20: 0.2776 \n",
            " NDCG@20: 0.5976\n",
            "Epoch: 11, Training time: 3.75s, Loss: 35.3389\n",
            "Evaluate current model:\n",
            " Epoch: 11, Validation time: 1.39s \n",
            " Loss: 35.3389: \n",
            " Recall@20: 0.2845 \n",
            " NDCG@20: 0.6076\n",
            "Epoch: 12, Training time: 4.31s, Loss: 34.6480\n",
            "Evaluate current model:\n",
            " Epoch: 12, Validation time: 1.01s \n",
            " Loss: 34.6480: \n",
            " Recall@20: 0.2849 \n",
            " NDCG@20: 0.6156\n",
            "Epoch: 13, Training time: 3.80s, Loss: 33.7432\n",
            "Evaluate current model:\n",
            " Epoch: 13, Validation time: 0.99s \n",
            " Loss: 33.7432: \n",
            " Recall@20: 0.2887 \n",
            " NDCG@20: 0.6169\n",
            "Epoch: 14, Training time: 4.59s, Loss: 33.0979\n",
            "Evaluate current model:\n",
            " Epoch: 14, Validation time: 0.98s \n",
            " Loss: 33.0979: \n",
            " Recall@20: 0.2920 \n",
            " NDCG@20: 0.6221\n",
            "Epoch: 15, Training time: 3.77s, Loss: 32.7185\n",
            "Evaluate current model:\n",
            " Epoch: 15, Validation time: 0.98s \n",
            " Loss: 32.7185: \n",
            " Recall@20: 0.2929 \n",
            " NDCG@20: 0.6255\n",
            "Epoch: 16, Training time: 5.81s, Loss: 32.0195\n",
            "Evaluate current model:\n",
            " Epoch: 16, Validation time: 1.01s \n",
            " Loss: 32.0195: \n",
            " Recall@20: 0.2959 \n",
            " NDCG@20: 0.6240\n",
            "Epoch: 17, Training time: 3.94s, Loss: 31.1141\n",
            "Evaluate current model:\n",
            " Epoch: 17, Validation time: 0.99s \n",
            " Loss: 31.1141: \n",
            " Recall@20: 0.2976 \n",
            " NDCG@20: 0.6274\n",
            "Epoch: 18, Training time: 4.10s, Loss: 30.5640\n",
            "Evaluate current model:\n",
            " Epoch: 18, Validation time: 1.39s \n",
            " Loss: 30.5640: \n",
            " Recall@20: 0.2989 \n",
            " NDCG@20: 0.6293\n",
            "Epoch: 19, Training time: 3.90s, Loss: 30.3703\n",
            "Evaluate current model:\n",
            " Epoch: 19, Validation time: 0.99s \n",
            " Loss: 30.3703: \n",
            " Recall@20: 0.2996 \n",
            " NDCG@20: 0.6322\n",
            "Epoch: 20, Training time: 3.93s, Loss: 29.8166\n",
            "Evaluate current model:\n",
            " Epoch: 20, Validation time: 1.21s \n",
            " Loss: 29.8166: \n",
            " Recall@20: 0.3019 \n",
            " NDCG@20: 0.6367\n",
            "Epoch: 21, Training time: 4.28s, Loss: 29.3726\n",
            "Evaluate current model:\n",
            " Epoch: 21, Validation time: 1.00s \n",
            " Loss: 29.3726: \n",
            " Recall@20: 0.3032 \n",
            " NDCG@20: 0.6352\n",
            "Epoch: 22, Training time: 3.89s, Loss: 28.9784\n",
            "Evaluate current model:\n",
            " Epoch: 22, Validation time: 1.00s \n",
            " Loss: 28.9784: \n",
            " Recall@20: 0.3028 \n",
            " NDCG@20: 0.6366\n",
            "Epoch: 23, Training time: 4.52s, Loss: 28.4063\n",
            "Evaluate current model:\n",
            " Epoch: 23, Validation time: 0.98s \n",
            " Loss: 28.4063: \n",
            " Recall@20: 0.3048 \n",
            " NDCG@20: 0.6374\n",
            "Epoch: 24, Training time: 3.86s, Loss: 28.4898\n",
            "Evaluate current model:\n",
            " Epoch: 24, Validation time: 0.98s \n",
            " Loss: 28.4898: \n",
            " Recall@20: 0.3073 \n",
            " NDCG@20: 0.6392\n",
            "Epoch: 25, Training time: 4.32s, Loss: 27.9501\n",
            "Evaluate current model:\n",
            " Epoch: 25, Validation time: 1.19s \n",
            " Loss: 27.9501: \n",
            " Recall@20: 0.3074 \n",
            " NDCG@20: 0.6373\n",
            "Epoch: 26, Training time: 3.90s, Loss: 27.6374\n",
            "Evaluate current model:\n",
            " Epoch: 26, Validation time: 0.99s \n",
            " Loss: 27.6374: \n",
            " Recall@20: 0.3103 \n",
            " NDCG@20: 0.6402\n",
            "Epoch: 27, Training time: 3.99s, Loss: 27.0861\n",
            "Evaluate current model:\n",
            " Epoch: 27, Validation time: 1.57s \n",
            " Loss: 27.0861: \n",
            " Recall@20: 0.3107 \n",
            " NDCG@20: 0.6390\n",
            "Epoch: 28, Training time: 4.24s, Loss: 26.9961\n",
            "Evaluate current model:\n",
            " Epoch: 28, Validation time: 0.99s \n",
            " Loss: 26.9961: \n",
            " Recall@20: 0.3135 \n",
            " NDCG@20: 0.6423\n",
            "Epoch: 29, Training time: 3.74s, Loss: 26.3113\n",
            "Evaluate current model:\n",
            " Epoch: 29, Validation time: 0.98s \n",
            " Loss: 26.3113: \n",
            " Recall@20: 0.3130 \n",
            " NDCG@20: 0.6429\n",
            "Epoch: 30, Training time: 4.54s, Loss: 26.3467\n",
            "Evaluate current model:\n",
            " Epoch: 30, Validation time: 1.00s \n",
            " Loss: 26.3467: \n",
            " Recall@20: 0.3135 \n",
            " NDCG@20: 0.6458\n",
            "Epoch: 31, Training time: 3.81s, Loss: 26.3347\n",
            "Evaluate current model:\n",
            " Epoch: 31, Validation time: 1.00s \n",
            " Loss: 26.3347: \n",
            " Recall@20: 0.3141 \n",
            " NDCG@20: 0.6480\n",
            "Epoch: 32, Training time: 4.50s, Loss: 25.5641\n",
            "Evaluate current model:\n",
            " Epoch: 32, Validation time: 0.99s \n",
            " Loss: 25.5641: \n",
            " Recall@20: 0.3152 \n",
            " NDCG@20: 0.6491\n",
            "Epoch: 33, Training time: 4.00s, Loss: 25.6676\n",
            "Evaluate current model:\n",
            " Epoch: 33, Validation time: 0.98s \n",
            " Loss: 25.6676: \n",
            " Recall@20: 0.3158 \n",
            " NDCG@20: 0.6485\n",
            "Epoch: 34, Training time: 3.89s, Loss: 24.9361\n",
            "Evaluate current model:\n",
            " Epoch: 34, Validation time: 1.85s \n",
            " Loss: 24.9361: \n",
            " Recall@20: 0.3147 \n",
            " NDCG@20: 0.6481\n",
            "Epoch: 35, Training time: 3.95s, Loss: 25.1497\n",
            "Evaluate current model:\n",
            " Epoch: 35, Validation time: 0.99s \n",
            " Loss: 25.1497: \n",
            " Recall@20: 0.3168 \n",
            " NDCG@20: 0.6481\n",
            "Epoch: 36, Training time: 3.76s, Loss: 24.9630\n",
            "Evaluate current model:\n",
            " Epoch: 36, Validation time: 0.99s \n",
            " Loss: 24.9630: \n",
            " Recall@20: 0.3223 \n",
            " NDCG@20: 0.6496\n",
            "Epoch: 37, Training time: 4.49s, Loss: 24.2377\n",
            "Evaluate current model:\n",
            " Epoch: 37, Validation time: 1.00s \n",
            " Loss: 24.2377: \n",
            " Recall@20: 0.3208 \n",
            " NDCG@20: 0.6498\n",
            "Epoch: 38, Training time: 3.83s, Loss: 24.0661\n",
            "Evaluate current model:\n",
            " Epoch: 38, Validation time: 1.00s \n",
            " Loss: 24.0661: \n",
            " Recall@20: 0.3244 \n",
            " NDCG@20: 0.6486\n",
            "Epoch: 39, Training time: 4.59s, Loss: 23.8061\n",
            "Evaluate current model:\n",
            " Epoch: 39, Validation time: 1.71s \n",
            " Loss: 23.8061: \n",
            " Recall@20: 0.3261 \n",
            " NDCG@20: 0.6472\n",
            "Epoch: 40, Training time: 4.00s, Loss: 23.9076\n",
            "Evaluate current model:\n",
            " Epoch: 40, Validation time: 1.01s \n",
            " Loss: 23.9076: \n",
            " Recall@20: 0.3236 \n",
            " NDCG@20: 0.6485\n",
            "Epoch: 41, Training time: 4.52s, Loss: 23.6563\n",
            "Evaluate current model:\n",
            " Epoch: 41, Validation time: 1.04s \n",
            " Loss: 23.6563: \n",
            " Recall@20: 0.3264 \n",
            " NDCG@20: 0.6510\n",
            "Epoch: 42, Training time: 4.00s, Loss: 23.3590\n",
            "Evaluate current model:\n",
            " Epoch: 42, Validation time: 0.99s \n",
            " Loss: 23.3590: \n",
            " Recall@20: 0.3284 \n",
            " NDCG@20: 0.6556\n",
            "Epoch: 43, Training time: 3.86s, Loss: 22.8151\n",
            "Evaluate current model:\n",
            " Epoch: 43, Validation time: 1.64s \n",
            " Loss: 22.8151: \n",
            " Recall@20: 0.3264 \n",
            " NDCG@20: 0.6569\n",
            "Epoch: 44, Training time: 4.29s, Loss: 22.5779\n",
            "Evaluate current model:\n",
            " Epoch: 44, Validation time: 1.04s \n",
            " Loss: 22.5779: \n",
            " Recall@20: 0.3299 \n",
            " NDCG@20: 0.6580\n",
            "Epoch: 45, Training time: 3.94s, Loss: 22.6617\n",
            "Evaluate current model:\n",
            " Epoch: 45, Validation time: 1.01s \n",
            " Loss: 22.6617: \n",
            " Recall@20: 0.3311 \n",
            " NDCG@20: 0.6581\n",
            "Epoch: 46, Training time: 4.55s, Loss: 22.2257\n",
            "Evaluate current model:\n",
            " Epoch: 46, Validation time: 1.00s \n",
            " Loss: 22.2257: \n",
            " Recall@20: 0.3333 \n",
            " NDCG@20: 0.6604\n",
            "Epoch: 47, Training time: 3.80s, Loss: 21.9919\n",
            "Evaluate current model:\n",
            " Epoch: 47, Validation time: 0.99s \n",
            " Loss: 21.9919: \n",
            " Recall@20: 0.3339 \n",
            " NDCG@20: 0.6623\n",
            "Epoch: 48, Training time: 4.54s, Loss: 21.7971\n",
            "Evaluate current model:\n",
            " Epoch: 48, Validation time: 0.99s \n",
            " Loss: 21.7971: \n",
            " Recall@20: 0.3344 \n",
            " NDCG@20: 0.6610\n",
            "Epoch: 49, Training time: 3.76s, Loss: 21.4449\n",
            "Evaluate current model:\n",
            " Epoch: 49, Validation time: 1.00s \n",
            " Loss: 21.4449: \n",
            " Recall@20: 0.3348 \n",
            " NDCG@20: 0.6654\n",
            "Epoch: 50, Training time: 3.99s, Loss: 21.7425\n",
            "Evaluate current model:\n",
            " Epoch: 50, Validation time: 1.73s \n",
            " Loss: 21.7425: \n",
            " Recall@20: 0.3332 \n",
            " NDCG@20: 0.6648\n",
            "Epoch: 51, Training time: 4.00s, Loss: 21.6891\n",
            "Evaluate current model:\n",
            " Epoch: 51, Validation time: 1.01s \n",
            " Loss: 21.6891: \n",
            " Recall@20: 0.3344 \n",
            " NDCG@20: 0.6651\n",
            "Epoch: 52, Training time: 4.08s, Loss: 21.1790\n",
            "Evaluate current model:\n",
            " Epoch: 52, Validation time: 1.17s \n",
            " Loss: 21.1790: \n",
            " Recall@20: 0.3355 \n",
            " NDCG@20: 0.6637\n",
            "Epoch: 53, Training time: 4.44s, Loss: 21.0948\n",
            "Evaluate current model:\n",
            " Epoch: 53, Validation time: 1.01s \n",
            " Loss: 21.0948: \n",
            " Recall@20: 0.3363 \n",
            " NDCG@20: 0.6622\n",
            "Epoch: 54, Training time: 3.96s, Loss: 20.9087\n",
            "Evaluate current model:\n",
            " Epoch: 54, Validation time: 1.00s \n",
            " Loss: 20.9087: \n",
            " Recall@20: 0.3356 \n",
            " NDCG@20: 0.6633\n",
            "Epoch: 55, Training time: 4.46s, Loss: 20.5905\n",
            "Evaluate current model:\n",
            " Epoch: 55, Validation time: 0.99s \n",
            " Loss: 20.5905: \n",
            " Recall@20: 0.3385 \n",
            " NDCG@20: 0.6642\n",
            "Epoch: 56, Training time: 3.80s, Loss: 20.3456\n",
            "Evaluate current model:\n",
            " Epoch: 56, Validation time: 0.99s \n",
            " Loss: 20.3456: \n",
            " Recall@20: 0.3406 \n",
            " NDCG@20: 0.6657\n",
            "Epoch: 57, Training time: 4.54s, Loss: 20.2392\n",
            "Evaluate current model:\n",
            " Epoch: 57, Validation time: 1.06s \n",
            " Loss: 20.2392: \n",
            " Recall@20: 0.3411 \n",
            " NDCG@20: 0.6722\n",
            "Epoch: 58, Training time: 3.91s, Loss: 19.9555\n",
            "Evaluate current model:\n",
            " Epoch: 58, Validation time: 0.99s \n",
            " Loss: 19.9555: \n",
            " Recall@20: 0.3369 \n",
            " NDCG@20: 0.6725\n",
            "Epoch: 59, Training time: 3.76s, Loss: 19.9932\n",
            "Evaluate current model:\n",
            " Epoch: 59, Validation time: 1.41s \n",
            " Loss: 19.9932: \n",
            " Recall@20: 0.3387 \n",
            " NDCG@20: 0.6691\n",
            "Epoch: 60, Training time: 4.22s, Loss: 19.7235\n",
            "Evaluate current model:\n",
            " Epoch: 60, Validation time: 0.99s \n",
            " Loss: 19.7235: \n",
            " Recall@20: 0.3394 \n",
            " NDCG@20: 0.6716\n",
            "Epoch: 61, Training time: 3.72s, Loss: 19.6276\n",
            "Evaluate current model:\n",
            " Epoch: 61, Validation time: 0.98s \n",
            " Loss: 19.6276: \n",
            " Recall@20: 0.3398 \n",
            " NDCG@20: 0.6710\n",
            "Epoch: 62, Training time: 4.55s, Loss: 19.1863\n",
            "Evaluate current model:\n",
            " Epoch: 62, Validation time: 1.07s \n",
            " Loss: 19.1863: \n",
            " Recall@20: 0.3403 \n",
            " NDCG@20: 0.6729\n",
            "Early stopping at step: 5 log:0.34032726287841797\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}